[
    {
        "authors": [
            "Xinchen Xu",
            "Hong Wen",
            "Yongfeng Wang",
            "Huanhuan Song",
            "Tian Liu",
            "Shih-Yu Chang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "29 August 2024",
        "doi": "10.1109/JIOT.2024.3424672",
        "publisher": "IEEE",
        "abstract": "Satellites play a crucial role in Internet of Things (IoT) applications that require precise positioning. Satellite orbit prediction serves as the foundation for providing accurate terminal location services. However, traditional satellite orbit prediction faces challenges like measurement errors, estimation errors, and unmodeled orbit disturbances, leading to low prediction accuracy. To address this issue, this paper introduces a groundbreaking satellite digital twin system based on container technology. This system facilitates real-time mirroring, monitoring, optimization, and control of satellite orbit prediction with low power consumption. Leveraging the advantages of container technology allows for convenient and efficient model updating. Furthermore, a new satellite orbit error prediction model is explored within this system. This model utilizes the seasonal-trend decomposition using locally weighted regression (STL) method and the temporal convolutional network (TCN) algorithm. By decomposing satellite orbit data into multiple components, the proposed model achieves enhanced future orbit Prediction by combining predicted values from each component. Different from existing machine learning (ML) orbit prediction models, our proposed model explores the variation patterns of satellite orbit data from a trend and cycle perspective, rather than relying solely on collecting more data and training larger models to improve prediction accuracy, which makes the novel prediction scheme get good performance while keeping low prediction complexity. Extensive experiments validate the effectiveness of the proposed method using two publicly available satellite orbit datasets (ILRS catalogue and TLE catalogue). The experimental results show that compared with traditional orbit prediction models, the novel DT system has less model update time and occupies less memory. The mean absolute error (MAE) value of the new model is lower than the five ML models in existing researches, w...",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Orbits",
                "Predictive models",
                "Satellites",
                "Atmospheric modeling",
                "Data models",
                "Containers",
                "Accuracy"
            ],
            "Author Keywords": [
                "IoT",
                "Satellite",
                "Orbit Prediction",
                "Temporal Convolutional Network (TCN)",
                "Digital Twin"
            ]
        },
        "title": "Digital Twin Based Satellite Orbit Prediction for Internet of Things (IoT) Systems"
    },
    {
        "authors": [
            "Yuba R. Siwakoti",
            "Manish Bhurtel",
            "Danda B. Rawat",
            "Adam Oest",
            "RC Johnson"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "17 October 2024",
        "doi": "10.1109/TCE.2024.3482708",
        "publisher": "IEEE",
        "abstract": "The Internet of Things (IoT) devices are being abused by exploiting their vulnerabilities. Despite the significant efforts to improve IoT security, IoT devices are still at higher risk of exploitation than computer systems. First, this paper identifies vulnerable IoT devices by applying a sampling strategy incorporating Common Vulnerabilities and Exposures (CVE) entries, Shodan’s exposure, and public research documents. Then, we investigated IoT abuses in financial crimes for 17 months (October 2021 to February 2023) by mapping IoT devices exposed by Shodan with proxies found in the darknet, underground forums, and Telegram channels. After investigation, we conclude with reasonable confidence that exposed IoT devices are taken over and abused as proxies in criminal activities such as credential stuffing attacks and financial crimes like illegal money transfers, cryptocurrency trading and stealing, and credit card fraud. Our study reveals that cameras (IP, network, security) are mostly abused IoT devices as proxies, followed by NAS storage.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Security",
                "Computer architecture",
                "Dark Web",
                "IP networks",
                "Cryptocurrency",
                "Credit cards",
                "Vectors",
                "Phishing",
                "Object recognition"
            ],
            "Author Keywords": [
                "IoT security",
                "vulnerable IoT devices",
                "IoT abuses",
                "IoT proxies",
                "malicious infrastructure",
                "financial crimes"
            ]
        },
        "title": "Your IP Camera Can Be Abused for Payments: A Study of IoT Exploitation for Financial Services Leveraging Shodan and Criminal Infrastructures"
    },
    {
        "authors": [
            "Urikhimbam Boby Clinton",
            "Nazrul Hoque"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "08 November 2024",
        "doi": "10.1109/ACCESS.2024.3494052",
        "publisher": "IEEE",
        "abstract": "As the growth of IoT networks increases exponentially, the number of cyber attacks is also increasing on IoT networks day-by-day. This results in the vital requirement of cyber security mechanisms to secure IoT networks from cyberattacks. To build such a security mechanism, researchers and cybersecurity practitioners need relevant IoT datasets. However, until now, only a few publicly available IoT network intrusion datasets exist in the literature. Moreover, these datasets lack coverage of IoT network traffic containing IoT application layer protocols like AMQP, XMPP, STOMP, etc. and various IoT-based attacks. So, in this work, we generate a new realistic and comprehensive IoT network intrusion dataset called MU-IoT for IoT cybersecurity. The network traffic contained in the MU-IoT dataset is collected from the Manipur University (MU) IoT Smart Lab, which is our own IoT network testbed. The testbed includes more than 30 devices, which include both physical and emulated IoT devices, and general-purpose devices such as laptops, desktops, servers, etc., that are usually present in a Smart Lab. The dataset contains normal network traffic generated from various applications and attack network traffic comprised of 16 attack types, including IoT-based attacks, which are categorized into six categories. From the testbed, we collected 22.8 GB of raw network data and extracted 15.8 GB of preprocessed network data by using our own feature extraction approach. The preprocessed network data contains 121 flow-based features and 3 class labels of more than 34.8 million records. Additionally, this dataset covers extensive IoT-specific application protocols and a variety of normal network behaviours, which is a notable advantage over existing datasets. The MU-IoT dataset can be utilized to develop and validate machine learning-based Intrusion Detection and Mitigation Systems (IDMS). Moreover, the MU-IoT dataset helps to validate the centralized and federated learning-based IDMS. In ...",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Telecommunication traffic",
                "Feature extraction",
                "Cyberattack",
                "Network intrusion",
                "Security",
                "Protocols",
                "Performance evaluation",
                "Costs",
                "Industrial Internet of Things"
            ],
            "Author Keywords": [
                "Internet of Things",
                "IoT dataset",
                "Cybersecurity",
                "Network Testbed",
                "Machine Learning"
            ]
        },
        "title": "MU-IoT: A new IoT intrusion dataset for network and application layer attacks analysis"
    },
    {
        "authors": [
            "Atsuko Yokotani",
            "Horoshi Mineno",
            "Masaki Mitsuuchi",
            "Koichi Ishibashi",
            "Tetsuya Yokotani"
        ],
        "published_in": "Published in: IEICE Communications Express ( Early Access )",
        "date_of_publication": "10 October 2024",
        "doi": "10.23919/comex.2024COL0012",
        "publisher": "IEICE",
        "abstract": "IoT platforms have been actively discussed for deploying various IoT services using a horizontal approach. The IoT Data Exchange Platform (IoT-DEP) is one of these platforms, standardized as ISO/IEC 30161 series in ISO/IEC JTC 1/SC 41. These standardized documents specify the requirements, architecture, and functional blocks of the communication components. However, detailed communication sequences among the communication components are considered an implementation issue. This paper proposes a content-centric network with Network initiative And Traffic control (C-NAT) applied to the IoT-DEP as a promising candidate for communication sequences. Moreover, it proposes the reinforcement of C-NAT for low-latency and reliable industrial services when applied to IoT-DEP. Specifically, although the original C-NAT is intended to provide cyclic communication, this study proposes support for non-cyclic communication in reinforced C-NAT.",
        "issn": {
            "Electronic ISSN": "2187-0136"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Servers",
                "Protocols",
                "ISO Standards",
                "Low latency communication",
                "IEC Standards",
                "Traffic control",
                "Reliability",
                "Delays",
                "Proposals"
            ],
            "Author Keywords": [
                "Information Centric Network",
                "Content Centric Network",
                "Data Aware Network",
                "IoT service",
                "IoT platform",
                "C-NAT"
            ]
        },
        "title": "Overview of C-NAT and its reinforcement for IoT data exchange platform"
    },
    {
        "authors": [
            "Amar N.Alsheavi",
            "Ammar Hawbani",
            "Xingfu Wang",
            "Wajdy Othman",
            "Liang Zhao",
            "Zhi Liu",
            "Saeed Hamood Alsamhi",
            "Mohammed A. A. Al-qaness"
        ],
        "published_in": "Published in: IEEE Transactions on Sustainable Computing ( Early Access )",
        "date_of_publication": "06 November 2024",
        "doi": "10.1109/TSUSC.2024.3492152",
        "publisher": "IEEE",
        "abstract": "This paper reviews three main aspects of authentication protocols of Internet of Things (IoT): classifications and limitations, current trends, and opportunities. First, we explore the significance of IoT authentication protocols in ensuring secure communication and the protection of transmitted and received data, focusing on the classifications and associated limitations. Second, we discuss the latest developments and trends, such as using blockchain technology and machine learning to enhance authentication protocols. Third, we highlight the future opportunities, including the development of human-centric authentication designs and improved platform interoperability. At the end of this paper, we provided some insights gained for the new researcher, offering analyses of the trends and challenges in this field, giving recommendations for improving IoT authentication protocols, and emphasizing the need for further research and cooperation to develop advanced security solutions.",
        "issn": {
            "Electronic ISSN": "2377-3782"
        },
        "keywords": {
            "IEEE Keywords": [
                "Authentication",
                "Security",
                "Internet of Things",
                "Protocols",
                "Blockchains",
                "Market research",
                "Object recognition",
                "Focusing",
                "Surveys",
                "Machine learning"
            ],
            "Author Keywords": [
                "Authentication protocols",
                "biometric authentication",
                "blockchain technology in IoT",
                "internet of things (IoT)",
                "IoT authentication",
                "IoT device security",
                "machine learning in authentication",
                "smart home security"
            ]
        },
        "title": "IoT Authentication Protocols: Classification, Trend and Opportunities"
    },
    {
        "authors": [
            "Phu Lai",
            "Wei Xiang",
            "William Damario Lukito",
            "Khoa Tran Phan",
            "Peng Cheng",
            "Chang Liu",
            "Guoqiang Mao"
        ],
        "published_in": "Published in: IEEE Transactions on Vehicular Technology ( Early Access )",
        "date_of_publication": "09 September 2024",
        "doi": "10.1109/TVT.2024.3456114",
        "publisher": "IEEE",
        "abstract": "Cell-free massive multiple-input multiple-output (CFmMIMO) coordinates a great number of distributed access points (APs) with central processing units (CPUs), effectively reducing interference and ensuring uniform service quality for user equipment (UEs). However, its cooperative nature can result in intense fronthaul signaling between CPUs in large-scale networks. To reduce the inter-CPU fronthaul signaling for systems with limited fronthaul capacity, we propose a low-complexity online UE-AP association approach for scalable CFmMIMO that combines network- and user-centric clustering methodologies, relies on local channel information only, and can handle dynamic UE arrivals. Numerical results demonstrate that compared to the state-of-the-art method on fronthaul signaling minimization, our approach can save up to 94% of the fronthaul signaling load and 83% of the CPU processing power at the cost of only up to 8.6% spectral efficiency loss, or no loss in some cases.",
        "issn": {
            "Print ISSN": "0018-9545",
            "Electronic ISSN": "1939-9359"
        },
        "keywords": {
            "IEEE Keywords": [
                "Central Processing Unit",
                "Channel estimation",
                "Scalability",
                "Interference",
                "Correlation",
                "Vectors",
                "Training"
            ],
            "Author Keywords": [
                "Cell-free massive MIMO",
                "user association",
                "access point selection",
                "fronthaul",
                "online algorithm",
                "scalability"
            ]
        },
        "title": "Hybrid Network- and User-Centric Scalable Cell-Free Massive MIMO for Fronthaul Signaling Minimization"
    },
    {
        "authors": [
            "Baichuan Huang",
            "Azra Abtahi",
            "Amir Aminifar"
        ],
        "published_in": "Published in: IEEE Transactions on Circuits and Systems for Artificial Intelligence ( Early Access )",
        "date_of_publication": "07 November 2024",
        "doi": "10.1109/TCASAI.2024.3493036",
        "publisher": "IEEE",
        "abstract": "Internet of Things (IoT) are one of the key enablers of personalized health. However, IoT devices often have stringent constraints in terms of resources, e.g., energy budget, and, therefore, limited possibilities to exploit the state-of-the-art Deep Neural Networks (DNNs). Energy-aware Neural Architecture Search (NAS) is proposed to tackle this challenge, by exploring lightweight DNN architectures on a single IoT device, but not leveraging the inherently distributed nature of IoT systems. As a result, the joint optimization of DNN architectures and DNN computation partitioning/offloading has not been addressed to date. In this paper, we propose an energy-aware NAS framework for distributed IoT, aiming to search for distributed DNNs to maximize prediction performance subjected to Flash Memory (Flash), Random Access Memory (RAM), and energy constraints. Our framework searches for lightweight DNN architecture with optimized prediction performance and its corresponding optimal computation partitioning to offload the partial DNN from edge to fog in a joint optimization. We evaluate our framework in the context of two common health applications, namely, seizure detection and arrhythmia classification, and demonstrate the effectiveness of our proposed joint optimization framework compared to NAS benchmarks.",
        "issn": {
            "Electronic ISSN": "2996-6647"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Computer architecture",
                "Random access memory",
                "Optimization",
                "Energy consumption",
                "Accuracy",
                "Image edge detection",
                "Arrhythmia",
                "Mobile handsets",
                "Artificial intelligence"
            ],
            "Author Keywords": [
                "Energy-aware neural architecture search (NAS)",
                "computation offloading",
                "mobile edge computing",
                "distributed computing",
                "battery-powered Internet of Things (IoT)",
                "low-power IoT",
                "low-power wearables",
                "energy optimization"
            ]
        },
        "title": "Energy-Aware Integrated Neural Architecture Search and Partitioning for Distributed Internet of Things (IoT)"
    },
    {
        "authors": [
            "Charalampos Armeniakos",
            "Petros S. Bithas",
            "Konstantinos Maliatsos",
            "Athanasios G. Kanatas"
        ],
        "published_in": "Published in: IEEE Communications Letters ( Early Access )",
        "date_of_publication": "09 October 2024",
        "doi": "10.1109/LCOMM.2024.3476936",
        "publisher": "IEEE",
        "abstract": "This letter studies the joint energy and signal-to-interference-plus-noise (SINR)-based coverage probability in Unmanned Aerial Vehicle (UAV)-assisted radio frequency (RF)-powered Internet of Things (IoT) networks. The UAVs are spatially distributed in an aerial corridor that is modeled as a one-dimensional (1D) binomial point process (BPP). By accurately capturing the line-of-sight (LoS) probability of a UAV through large-scale fading: i) an exact form expression for the energy coverage probability is derived and ii) a tight approximation for the overall coverage performance is obtained. Among several key findings, numerical results reveal the optimal number of deployed UAV-BSs that maximizes the joint coverage probability, as well as the optimal length of the UAV corridors when designing such UAV-assisted IoT networks.",
        "issn": {
            "Print ISSN": "1089-7798",
            "Electronic ISSN": "1558-2558"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Autonomous aerial vehicles",
                "Receivers",
                "Signal to noise ratio",
                "Method of moments",
                "Interference",
                "Shadow mapping",
                "Stochastic processes",
                "Geometry",
                "Gamma distribution"
            ],
            "Author Keywords": [
                "Energy harvesting",
                "Internet of Things (IoT)",
                "stochastic geometry",
                "unmanned aerial vehicle (UAV)"
            ]
        },
        "title": "Joint Energy and SINR Coverage Probability in UAV Corridor-assisted RF-powered IoT Networks"
    },
    {
        "authors": [
            "Jiaxu Cong",
            "Jingyu Wang",
            "Bin Tong",
            "Delong Shang"
        ],
        "published_in": "Published in: IEEE Transactions on Circuits and Systems II: Express Briefs ( Early Access )",
        "date_of_publication": "07 October 2024",
        "doi": "10.1109/TCSII.2024.3474700",
        "publisher": "IEEE",
        "abstract": "In the rapidly evolving landscape of the Internet of Things (IoT), efficient and low-power communication solutions with minimal I/O count are essential for the effective connectivity of a multitude of devices. This paper presents Spike Refresh Receiver-Transmitter (SRRT), a novel ultra-low power unidirectional single-wire inter-chip communication protocol designed specifically for IoT applications. The protocol distinguishes continuous identical data through spike refresh and ensures data reliability via stability detection. We conduct theoretical analysis of the SRRT and performed post-layout simulations using layouts generated by ICC. The results show that the SRRT achieves a power consumption of 0.0605mW, a performance of 200Mbps, an energy efficiency of 0.3025pJ/bit, and an area of 0.001508mm2. Additionally, the protocol is validated in a real-world environment using a PCB comprising two FPGAs.",
        "issn": {
            "Print ISSN": "1549-7747",
            "Electronic ISSN": "1558-3791"
        },
        "keywords": {
            "IEEE Keywords": [
                "Circuits",
                "Circuit stability",
                "Stability criteria",
                "Power system stability",
                "Protocols",
                "Receivers",
                "Internet of Things",
                "Delays",
                "Clocks",
                "Wires"
            ],
            "Author Keywords": [
                "IoT",
                "inter-chip communication",
                "asynchronous",
                "ultra low power",
                "event driven"
            ]
        },
        "title": "SRRT: An Ultra-Low Power Unidirectional Single-Wire Inter-Chip Communication for IoT"
    },
    {
        "authors": [
            "Fengqi Li",
            "Jiaheng Wang",
            "Weilin Xie",
            "Ning Tong",
            "Deguang Wang"
        ],
        "published_in": "Published in: IEEE Transactions on Emerging Topics in Computing ( Early Access )",
        "date_of_publication": "16 October 2024",
        "doi": "10.1109/TETC.2024.3472059",
        "publisher": "IEEE",
        "abstract": "The proliferation of IoT devices, advancements in edge computing, and innovations in AI technology have created an ideal environment for the birth and growth of Edge AI. With the trend towards the Internet of Everything (IoE), the EdgeAI- Human-IoT architectural framework highlights the necessity for efficient data exchange interconnectivity. Ensuring secure data sharing and efficient data storage are pivotal challenges in achieving seamless data interconnection. Owing to its simplicity, ease of deployment, and consensus-reaching capabilities, the RAFT consensus algorithm, which is commonly used in distributed storage, faces limitations as the IoT scale expands. The computational, communication, and storage capabilities of nodes are constraints, and the security of data remains a concern. To address these complex challenges, we introduce the X-RAFT consensus algorithm, which is tailored for blockchain technology. This algorithm enhances system performance and robustness, mitigates the impact of system load, enhances system sustainability, and increases Byzantine fault tolerance. Through analysis and simulations, our proposed solution has been evidenced to provide reliable security and efficient performance.",
        "issn": {
            "Electronic ISSN": "2168-6750"
        },
        "keywords": {
            "IEEE Keywords": [
                "Blockchains",
                "Internet of Things",
                "Voting",
                "Servers",
                "Fault tolerant systems",
                "Fault tolerance",
                "Consensus algorithm",
                "Computer crashes",
                "Throughput",
                "Sharding"
            ],
            "Author Keywords": [
                "EdgeAI-Human-IoT",
                "Internet of Things (IoT)",
                "Blockchain",
                "RAFT"
            ]
        },
        "title": "X-RAFT: Improve RAFT Consensus To Make Blockchain Better Secure EdgeAI-Human-IoT Data"
    },
    {
        "authors": [
            "Aditya Nugur",
            "M. Pipattanasomporn",
            "M. Kuzlu",
            "S. Rahman"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "07 December 2018",
        "doi": "10.1109/JIOT.2018.2885652",
        "publisher": "IEEE",
        "abstract": "Due to the depletion of energy resources and increased energy demand, there is an increased focus on the energy consumption and management in buildings. Many building energy management (BEM) software platforms are commercially available to monitor and control energy consumption. These platforms are hosted on the physical hardware within the building, due to which the hardware specifications limit their performance. To address this limitation, cloud technology emerged which facilitates software to be deployed at a remote location that has scalable hardware resources. Conventional BEM software can leverage such a cloud platform to offer scalable and maintenance-free installation. Once hosted on a remote cloud platform, BEM software lacks direct connectivity to building sensors/controllers, hence requires a device to support remote accessibility. Most devices are bound to a local area network and therefore need an additional functional layer on top of its communication stack to perform Network Address Translation (NAT)-Traversal. This functionality is implemented on a scalable software which connects to the devices in the network and acts as a gateway for cloud-based BEM software to access devices in the local area network. Any message sent to this gateway is translated to a respective device protocol. This paper describes the design and implementation of such an Internet of Things (IoT) gateway for a cloud-based BEM system that requires support for BACnet, Modbus, and HTTP RESTful interface devices.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Cloud computing",
                "Software",
                "Logic gates",
                "Protocols",
                "Buildings",
                "Internet of Things",
                "Performance evaluation"
            ],
            "Author Keywords": [
                "Building Energy Management",
                "Fog Computing",
                "IoT gateway",
                "Cloud computing",
                "Internet of things."
            ]
        },
        "title": "Design and Development of an IoT Gateway for Smart Building Applications"
    },
    {
        "authors": [
            "Waltenegus Dargie"
        ],
        "published_in": "Published in: IEEE Sensors Letters ( Early Access )",
        "date_of_publication": "25 October 2024",
        "doi": "10.1109/LSENS.2024.3486582",
        "publisher": "IEEE",
        "abstract": "Low-power IoT sensing nodes can be embedded into various physical environments to monitor vital parameters. Some of these environments impose rough and extreme operation conditions, severely limiting the performance of these nodes. Modeling these environments is vital to make the nodes adaptive. In this paper, we propose a model to estimate the complex motion of nodes deployed on the surface of different water bodies. The model relies on received power statistics only. Experiment results confirm that the model is reliable, achieving an estimation accuracy of 93%.",
        "issn": {
            "Electronic ISSN": "2475-1472"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Sea surface",
                "Mathematical models",
                "Accuracy",
                "Adaptation models",
                "Wireless sensor networks",
                "Peer-to-peer computing",
                "Optical surface waves",
                "Wireless communication",
                "Three-dimensional displays"
            ],
            "Author Keywords": [
                "3D water motion",
                "IMU",
                "IoT",
                "MS estimation",
                "received power",
                "RSSI",
                "wireless sensor networks"
            ]
        },
        "title": "Estimation of Motion Statistics from Statistics of Received Power in Low-Power IoT Sensing Nodes"
    },
    {
        "authors": [
            "Zheng Shen",
            "Yuxiao Zhao",
            "Hao Min"
        ],
        "published_in": "Published in: IEEE Microwave and Wireless Technology Letters ( Early Access )",
        "date_of_publication": "07 October 2024",
        "doi": "10.1109/LMWT.2024.3468610",
        "publisher": "IEEE",
        "abstract": "This letter presents and designs a 2.4-GHz SAW-less uncertain-intermediate frequency (IF) receiver with blocker tolerant for long-distance ultralow-power (ULP) Internet of Things (IoT). By employing RF downconversion followed by noncoherent digital demodulation, the receiver maintains high sensitivity and low power. A free-running oscillator, without a power-hungry phase-locked loop, is used to generate LO with low power. Instead of a costly and bulky external SAW filter, a two-phase passive mixer combined with a low-power transimpedance amplifier (TIA) is adopted by impedance mapping to solve the low robustness to interferers of the uncertain-IF receiver; as a consequence, an equivalent on-chip RF bandpass high- Q\nfilter is obtained to suppress the interference at the RF input and a good balance is obtained between noise figure, power, and blocker tolerance. Fabricated in a 55-nm CMOS process, at 2.4-GHz LO frequency, the receiver has a 21.4-dB rejection ratio for 10-MHz out-of-band (OB) interference, consuming only 253 \\mu\nW under 0.8-V supply with 12.9-dB NF and -\n6.2-dBm OB-IIP3 at the front end.",
        "issn": {
            "Electronic ISSN": "2771-9588",
            "Print ISSN": "2771-957X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Receivers",
                "Radio frequency",
                "Mixers",
                "Impedance",
                "Gain",
                "Band-pass filters",
                "Sensitivity",
                "Microwave filters",
                "Frequency measurement",
                "Power demand"
            ],
            "Author Keywords": [
                "Blocker tolerant",
                "high linearity",
                "long-distance communication",
                "SAW-less",
                "ultralow-power (ULP) Internet of Things (IoT)",
                "uncertain-intermediate frequency (IF)"
            ]
        },
        "title": "A 253- $\\mu $ W 2.4-GHz SAW-Less Mixer-First Uncertain-IF Receiver With Blocker Tolerant for Long-Distance Ultralow-Power IoT"
    },
    {
        "authors": [
            "Naiyao Liang",
            "Zuyuan Yang",
            "Wei Han",
            "Zhenni Li",
            "Shengli Xie"
        ],
        "published_in": "Published in: IEEE Transactions on Emerging Topics in Computational Intelligence ( Early Access )",
        "date_of_publication": "16 September 2024",
        "doi": "10.1109/TETCI.2024.3423459",
        "publisher": "IEEE",
        "abstract": "With the development of camera and sensor technologies, multi-view data are ubiquitous and require more technologies to process them. Multi-view clustering with graph fusion has recently attracted considerable attention as multiple graphs defined by views can provide more comprehensive information for clustering. Different from previous methods that rarely consider the locality of the fused graph, in this paper, we propose an\nℓ\n0\n-norm constrained graph fusion model with the ability to preserve the consistent local structure of the fused graph, as well as the view weights which are obtained adaptively. Also, to solve the proposed model, we design an efficient algorithm with a closed-form solution for each variable, together with the analysis of the convergence. Experimental results indicate that the learned consistent local structure can refine and guide the graph fusion to achieve a better graph, and our method outperforms the state-of-the-art graph fusion methods.",
        "issn": {
            "Electronic ISSN": "2471-285X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Optimization",
                "Computational intelligence",
                "Laplace equations",
                "Information technology",
                "Electronic mail",
                "Eigenvalues and eigenfunctions",
                "Closed-form solutions"
            ],
            "Author Keywords": [
                "Multi-view clustering",
                "graph fusion",
                "consistent",
                "local structure"
            ]
        },
        "title": "Multi-View Clustering With Consistent Local Structure-Guided Graph Fusion"
    },
    {
        "authors": [
            "Shamsher Ullah",
            "Jianqiang Li Jie Chen",
            "Ikram Ali",
            "Salabat Khan",
            "Muhammad Tanveer Hussain",
            "Farhan Ullah",
            "Victor C. M. Leung"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "11 October 2024",
        "doi": "10.1109/JIOT.2024.3472029",
        "publisher": "IEEE",
        "abstract": "Homomorphic encryption (HE) is one of the more sophisticated methods of homomorphic cryptography (HC). HC efficiently contacts the interacting parties in open IoT and light-weighted network environments. This approach is capable of analyzing encrypted data without decryption. The operations use private and public keys. Then, during the assessment or evaluation, users may access the original data. Before conducting tests or evaluations, the customer must first encrypt the data and then decrypt it. Since consumers use several main cycles for the whole operation, which creates noise and computation overheads, the growth rate of computation overheads has increased. The growing ratio of noise to computation rate can interrupt the whole system, resulting in machine instability, protection, and privacy concerns. To resolve the security and privacy issues, the proposed schemes used different hardness assumptions, such as over-integer, learning with error, ideal lattices, bootstrapping, etc. In this paper, we presents a comprehensive review of HE and its many varieties. The numerous possible applications of HE are covered at a high level in order to highlight the extent to which HE is used in the IoT and other lighted-weighted intelligent industry environments in a variety of various domains.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Cryptography",
                "Encryption",
                "Security",
                "Data privacy",
                "Privacy",
                "Receivers",
                "Protection",
                "Cloud computing",
                "Internet of Things",
                "Noise"
            ],
            "Author Keywords": [
                "IoT",
                "cloud-based IoT systems",
                "homomorphic encryption",
                "light-weighted network environments"
            ]
        },
        "title": "Homomorphic Encryption Applications for IoT and Light-Weighted Environments: A Review"
    },
    {
        "authors": [
            "Hania Mohamed",
            "Nickolaos Koroniotis",
            "Nour Moustafa",
            "Francesco Schiliro",
            "Albert Y. Zomaya"
        ],
        "published_in": "Published in: IEEE Open Journal of the Communications Society ( Early Access )",
        "date_of_publication": "06 November 2024",
        "doi": "10.1109/OJCOMS.2024.3492919",
        "publisher": "IEEE",
        "abstract": "The proliferation of the Internet of Things (IoT) systems has fueled a surge in cybercrime, particularly through advanced persistent threats, such as botnets and ransomware, posing challenges for centralized Digital Forensics (DF) solutions in tracking decentralized attacks and ensuring data privacy. Despite these challenges, existing research has primarily focused on traditional DF methods, overlooking the unique demands of IoT environments. Federated Learning (FL) provides a promising solution for addressing these challenges by offering a privacy-preserving solution for detecting and investigating cyberattacks in IoT networks without compromising data privacy. However, the potential of FL in the context of IoT forensics remains largely unexplored. This paper bridges this gap by reviewing recent studies in IoT forensics and proposing a novel IoT Learning Forensics (IoT-LF) framework to detect and trace cyberattacks in IoT environments. In this framework, a multi-dimensional view of the environment, including telemetry, network, and application, is considered for data gathering. In addition, FL cycles are employed to automate the examination and analysis of these data during the investigation process. The feasibility and functionality of this framework are validated by a Proof of Concept, achieving a detection accuracy of approximately 81.69%, when trained on the TON-IoT dataset. Moreover, the research challenges, lessons learned, and future research solutions for applying FL for DF in an IoT environment are discussed.",
        "issn": {
            "Electronic ISSN": "2644-125X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Surveys",
                "Computer crime",
                "Data privacy",
                "Federated learning",
                "Digital forensics",
                "Water resources",
                "Medical services",
                "Data models",
                "Australia"
            ],
            "Author Keywords": [
                "APT",
                "deep learning",
                "digital forensics",
                "federated learning",
                "Internet of things"
            ]
        },
        "title": "Harnessing Federated Learning for Digital Forensics in IoT: A Survey and Introduction to the IoT-LF Framework"
    },
    {
        "authors": [
            "Lun Tang",
            "Enqiao Kou",
            "Weijia Zhang",
            "Qianlin Wu",
            "Qianbin Chen"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "28 August 2024",
        "doi": "10.1109/JIOT.2024.3448429",
        "publisher": "IEEE",
        "abstract": "Anomaly detection using multivariate time series (MTS) is critical for detecting abnormal traffic and device failures in 5G Internet of Things (IoT) devices. The current anomaly detection framework lacks the ability to model multi-dimensional long time series and to address issues such as resource overhead, privacy protection, and data security in distributed learning modes within the Internet of Things. Therefore, the paper proposes an anomaly detection framework integrating knowledge distillation and swarm learning for 5G IoT (IoT-FKGDL-SL). Firstly, to model the correlations between different variables, a new method for capturing correlations between variables through clustering is proposed. Secondly, to perform long-term modeling of multivariate time series, a long-time series anomaly detection model called IoT-FKGD is proposed, based on multi-scale dilated convolution and locality-sensitive hashing (LSH) attention. Finally, a framework based on IoT-FKGD is proposed to detect traffic anomalies of IoT devices under a swarm learning architecture that incorporates knowledge distillation. The effectiveness of the IoT-FKGDL-SL framework is demonstrated by comparing it with advanced anomaly detection methods on real datasets. Experimental results show that on a long time scale, the precision, recall, and F1-score of anomaly detection using this framework all surpass those of baseline methods.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Anomaly detection",
                "Time series analysis",
                "Correlation",
                "5G mobile communication",
                "Servers",
                "Convolution"
            ],
            "Author Keywords": [
                "5G Internet of Things",
                "anomaly detection",
                "attention",
                "graph convolution",
                "swarm learning",
                "knowledge distillation"
            ]
        },
        "title": "IoT-FKGDL-SL: Anomaly Detection Framework Integrating Knowledge Distillation and a Swarm Learning for 5G IoT"
    },
    {
        "authors": [
            "Xinhua Cui",
            "Youliang Tian",
            "Xinyu Zhang",
            "HongWei Lin",
            "Mengqian Li"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "28 October 2024",
        "doi": "10.1109/JIOT.2024.3479219",
        "publisher": "IEEE",
        "abstract": "In edge computing environments, the rapid growth of Internet of Things (IoT) devices presents significant challenges for data processing. These devices are often resource-constrained, eading to a trade-off between achieving efficiency and ensuring security. On one hand, traditional certificateless encryption methods are computationally expensive; on the other hand, offloading the computational load to third-party entities can enhance efficiency but still introduces security risks. To address these issues, this paper proposes a novel lightweight certificateless edge-assisted encryption scheme (CL-EAED). Ours scheme offloads computationally intensive tasks to edge servers, ensuring that edge-assisted processing does not expose sensitive information and only needs to be performed once. This approach effectively prevents data leakage and enhances both the efficiency and security of task offloading. Moreover, the CL-EAED scheme achieves IND-CCA security in standard model (SM) and has been validated using the ProVerif tool. Experimental evaluations demonstrate that CL-EAED eliminates the dependency on computationally intensive pairing operations, significantly reducing computational and communication costs. It outperforms existing solutions in terms of energy consumption, latency, and scalability, fully meeting the requirements of practical applications.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Encryption",
                "Internet of Things",
                "Data processing",
                "Performance evaluation",
                "Cryptography",
                "Computational modeling",
                "Edge computing",
                "Servers",
                "Scalability",
                "Real-time systems"
            ],
            "Author Keywords": [
                "Certificateless cryptography",
                "Internet of Thing (IoT)",
                "edge-assisted",
                "lightweight",
                "resource-constrained",
                "IoT Devices",
                "standard model"
            ]
        },
        "title": "A Lightweight Certificateless Edge-Assisted Encryption for IoT Devices: Enhancing Security and Performance"
    },
    {
        "authors": [
            "Yansong Yin",
            "Kun Xie",
            "Shiming He",
            "Yanbiao Li",
            "Jigang Wen",
            "Zulong Diao",
            "Dafang Zhang",
            "Gaogang Xie"
        ],
        "published_in": "Published in: IEEE Transactions on Services Computing ( Early Access )",
        "date_of_publication": "24 September 2024",
        "doi": "10.1109/TSC.2024.3466854",
        "publisher": "IEEE",
        "abstract": "The rapid expansion of the Internet of Things (IoT) has led to growing concerns about the security of IoT devices. A crucial aspect of ensuring their security is IoT device identification, which involves pinpointing the specific type of device. Existing solutions, however, either necessitate complex feature engineering or struggle to handle the ever-increasing number of new devices in open IoT environments. To tackle these challenges, this paper introduces GraphIoT, a lightweight IoT device detection method based on graph classifiers. GraphIoT leverages lightweight flow information, such as packet length, direction, and timestamp, to create an IoT Device Traffic Graph Representation (IoT-DTGR). This representation offers a comprehensive view of IoT device flows while preserving features in bidirectional IoT Device-Gateway interactions. By transforming the IoT device detection problem into a graph classification problem, GraphIoT employs a powerful Graph Neural Network that takes into account both node and edge features, as well as subgraph structures in IoT-DTGRs, to classify graphs and consequently identify device types. Additionally, the paper proposes an incremental learning framework called CL-GraphIoT that continuously learns features of new IoT device flows without forgetting previously learned device features. This is achieved through two strategies: parameter sharing and sample replaying. The paper gathers a real-world dataset from 18 IoT devices and conducts experiments on two datasets: the gathered real-world dataset and an open-source dataset covering 21 IoT device types. The experimental results demonstrate that both GraphIoT and CL-GraphIoT outperform state-of-the-art methods, achieving high accuracy in device detection with fast processing speed.",
        "issn": {
            "Electronic ISSN": "1939-1374"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Logic gates",
                "Feature extraction",
                "Protocols",
                "Learning systems",
                "Object recognition",
                "IP networks"
            ],
            "Author Keywords": [
                "IoT device detection",
                "Graph neural networks",
                "Incremental learning"
            ]
        },
        "title": "GraphIoT: Lightweight IoT Device Detection based on Graph Classifiers and Incremental Learning"
    },
    {
        "authors": [
            "Jiushuang Wang",
            "Ying Liu",
            "Weiting Zhang",
            "Chenhao Ying",
            "Jiawen Kang",
            "Yikun Li"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "22 August 2024",
        "doi": "10.1109/JIOT.2024.3448204",
        "publisher": "IEEE",
        "abstract": "Federated learning (FL) is an innovative distributed privacy-preserving machine learning paradigm, which enables participants to collaboratively train artificial intelligence (AI) models without disclosing private data. Nevertheless, malicious participants have the potential to introduce vicious models via poisoning attacks, which jeopardizes the convergence and accuracy of the global model in FL. In this paper, we propose a secure FL distributed architecture based on deep deterministic policy gradient (DDPG), which advances the accuracy of the global model and enhances system robustness. Specifically, we model the accuracy optimization problem with the goal of minimizing the overall loss function of participating devices during each FL iteration. Furthermore, we design the device nodes selection mechanism, named FedSAP, which leverages social attribute perception. Particularly, we first construct the device node selection problem as a markov decision drocess (MDP), and then apply social attribute perception and attribute information to the state space ensuring the reliability of the device. Moreover, the long short term memory (LSTM) algorithm is introduced into the actor-critic network structure to learn part of the hidden state through memory inference. The extensive experimental results show that FedSAP can effectively select reliable nodes and significantly improve the accuracy of the global model.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Data models",
                "Training",
                "Internet of Things",
                "Accuracy",
                "Reliability",
                "Security",
                "Servers"
            ],
            "Author Keywords": [
                "Software defined network Internet of Things (SDN-IoT)",
                "federated learning (FL)",
                "poisoning attack",
                "deep deterministic policy gradient (DDPG"
            ]
        },
        "title": "FedSAP: Secure Federated Learning in SDN-IoT via DRL-Enabled Social Attribute Perception"
    },
    {
        "authors": [
            "Sadegh Torabi",
            "Dorde Klisura",
            "Joseph Khoury",
            "Elias Bou-Harb",
            "Chadi Assi",
            "Mourad Debbabi"
        ],
        "published_in": "Published in: IEEE Transactions on Dependable and Secure Computing ( Early Access )",
        "date_of_publication": "04 September 2024",
        "doi": "10.1109/TDSC.2024.3454573",
        "publisher": "IEEE",
        "abstract": "This study presents a large-scale empirical analysis of real-life Internet-of-Things (IoT) malware by conducting a comprehensive analysis of 160,000 malicious executables detected by specialized IoT honeypots over five years. Our findings contribute to improving the knowledge of IoT malware characteristics and inter-relationships, which in return, contribute towards strengthening cybersecurity measures for IoT threat detection/mitigation. To achieve these goals, we leverage various malware analysis techniques to extract useful information from the executable files. Our analysis demonstrate that in contrast to non-IoT malware, we were able to extract unsolicited IP addresses and command strings from the majority of the analyzed IoT malware binaries using off-the-shelf de-obfuscation techniques/tools. Additionally, by correlating the extracted information and performing consequent similarity analysis using NLP-based features, we were able to reveal closely related samples with shared implementation across the adversarial infrastructure. Thus, contributing to labeling previously unseen/unknown IoT malware samples while uncovering emerging, possibly new variants. Finally, given such findings, we discuss the applications of a real-time IoT honeypot, which enables capturing real-time commands from malware-infected IoT devices while enabling timely and effective IoT-malware detection, analysis, labeling, and mitigation.",
        "issn": {
            "Print ISSN": "1545-5971",
            "Electronic ISSN": "1941-0018"
        },
        "keywords": {
            "IEEE Keywords": [
                "Malware",
                "Internet of Things",
                "IP networks",
                "Labeling",
                "Data mining",
                "Codes",
                "Feature extraction"
            ],
            "Author Keywords": [
                "IoT malware",
                "malware analysis",
                "similarity analysis",
                "adversarial IP address"
            ]
        },
        "title": "Internet-Wide Analysis, Characterization, and Family Attribution of IoT Malware: A Comprehensive Longitudinal Study"
    },
    {
        "authors": [
            "Shachi Sharma",
            "Prakash Datt Bhatt"
        ],
        "published_in": "Published in: IEEE Transactions on Network and Service Management ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/TNSM.2024.3489598",
        "publisher": "IEEE",
        "abstract": "The presence of diverse traffic types is well-established in IoT networks. Various probability distributions have been found to describe packets inter-arrival time contrasting with the familiar exponential distribution in traditional networks. These findings suggest the need to develop appropriate traffic models for performance analysis of IoT network systems. An essential component in IoT network is gateway as it provides connectivity to the core Internet. The IoT gateway also performs functions such as protocol translation and traffic aggregation. Therefore, efficient design of the IoT gateway is necessary for better network management. The paper presents a new analytical model, N-Gamma/M/1, for analyzing the performance of IoT gateway. The equivalence of N-Gamma/M/1 and Gamma/M/1 models is proved mathematically. Additionally, an in-depth performance evaluation of the IoT gateway under various arrival patterns is conducted through simulation. The numerical analysis of the proposed N-Gamma/M/1 model emphasizes the need for more buffers at the gateway when traffic from input devices has varying values of gamma distribution parameters. It is also noted that the IoT gateway experiences a longer mean queue length resulting in higher mean waiting time and packet loss when inter-arrival time distribution of packets follows generalized Pareto, Weibull and lognormal distributions with different parameter values. This makes the task of IoT network management challenging. Adaptive and intelligent resource allocation policies along with dynamic congestion control algorithms may provide a solution to minimize packet loss and ensure quality of service.",
        "issn": {
            "Electronic ISSN": "1932-4537"
        },
        "keywords": {
            "IEEE Keywords": [
                "Logic gates",
                "Internet of Things",
                "Analytical models",
                "Protocols",
                "Performance evaluation",
                "Quality of service",
                "Performance analysis",
                "Wireless fidelity",
                "Cloud computing",
                "Zigbee"
            ],
            "Author Keywords": [
                "IoT gateway",
                "N-Gamma/M/1 model",
                "network management",
                "performance analysis",
                "quality of service",
                "simulation"
            ]
        },
        "title": "Performance Modeling of IoT-Cloud Gateway under Diverse Traffic Characteristics"
    },
    {
        "authors": [
            "Xue Zhang",
            "Mingjiang Wang",
            "Xiao Zeng",
            "Xuyi Zhuang"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "01 October 2024",
        "doi": "10.1109/ACCESS.2024.3471613",
        "publisher": "IEEE",
        "abstract": "In the pursuit of developing an efficient and harmonious human-computer interaction interface, Emotion Recognition in Conversations (ERC) is particularly important. It requires the system to delicately capture and understand the nuances of human emotional fluctuations during the communication process. Currently, although emotional signals are prevalent in various modalities of conversation such as audio, video, and text, multimodal Emotion Recognition in Conversations (ERC) still remains a challenging problem to tackle due to its inherent complexity. Previous research has tended to rely on a single modality, particularly text information, while neglecting the rich emotional cues present in audio and video modalities. Based on the current research status and challenges such as inadequate extraction of contextual emotional dynamic features and data scarcity, a multimodal emotion recognition method called Attention-based Fusion Contextual Attention Network (Af-CAN) has been proposed to break through these limitations. Af-CAN is meticulously designed with a multimodal feature fusion mechanism that can extract emotion-relevant features from different sources of information and uses advanced attention mechanisms to integrate these features, ensuring the comprehensiveness and accuracy of emotion recognition. Furthermore, in response to the characteristics of emotional dynamics and context dependency in conversations, this framework introduces a special context modeling unit capable of tracking the evolution of emotional states in the conversation and the mutual influence of emotions between speakers. Experimental evaluations carried out on multiple standard datasets have shown that Af-CAN outperforms existing ERC systems on various evaluation metrics, particularly showing significant advantages in handling complex emotional changes in conversations, laying a solid foundation for advancing the application of emotional intelligence in human-computer interaction.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Emotion recognition",
                "Transfer learning",
                "Attention mechanisms",
                "Oral communication",
                "Data models",
                "Data mining",
                "Context modeling",
                "Training",
                "Long short term memory",
                "Multimodal sensors"
            ],
            "Author Keywords": [
                "Emotion recognition",
                "transfer learning",
                "multimodal"
            ]
        },
        "title": "Af-CAN: Multimodal emotion recognition method based on situational attention mechanism"
    },
    {
        "authors": [
            "Jing Zhang",
            "Zichen Pan",
            "Jie Cui",
            "Hong Zhong",
            "Jiaxin Li",
            "Debiao He"
        ],
        "published_in": "Published in: IEEE Transactions on Network Science and Engineering ( Early Access )",
        "date_of_publication": "11 September 2024",
        "doi": "10.1109/TNSE.2024.3459087",
        "publisher": "IEEE",
        "abstract": "Vehicular intrusion detection systems (IDS) are crucial to ensure the security of vehicular ad hoc networks (VANETs). However, most current IDS for vehicles have been developed using closed datasets, resulting in a limited detection range. Furthermore, in the real world, updates to IDS often fall behind the emergence of novel and unknown attacks, rendering these systems ineffective in defending against such attacks. To overcome this limitation and protect against network attacks in open scenarios, we propose a novel vehicular intrusion detection method that uses meta-recognition. This method utilizes a new neural network to extract joint features and calibrate the predicted values of a pre-trained model via extreme value theory (EVT). In addition, to adapt to the VANETs environment, we introduce temperature scaling and tail separation sampling methods to enhance the modeling effect and increase the prediction accuracy. Comprehensive experiments indicated that the proposed method can detect known attacks at a fine-grained level, identify unknown attacks, and outperform the current state-of-the-art schemes.",
        "issn": {
            "Electronic ISSN": "2327-4697"
        },
        "keywords": {
            "IEEE Keywords": [
                "Intrusion detection",
                "Training",
                "Accuracy",
                "Feature extraction",
                "Aerospace electronics",
                "Tail",
                "Security"
            ],
            "Author Keywords": [
                "intrusion detection",
                "meta-recognition",
                "extreme value theory",
                "VANETs"
            ]
        },
        "title": "Toward Open-Set Intrusion Detection in VANETs: An Efficient Meta-Recognition Approach"
    },
    {
        "authors": [
            "Tianyu Wei",
            "Bo Dong",
            "Yuxuan Hu",
            "Pofeng Lin",
            "Wobin Huang"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "10 October 2024",
        "doi": "10.1109/JSEN.2024.3467374",
        "publisher": "IEEE",
        "abstract": "A flexible Radio-frequency(RF) sensor based on Pb 1.2 (Zr 0.52 Ti 0.48 ) O 3 /polymer thin films is proposed. The RF sensor is a Film Bulk Acoustic Resonator(FBAR) type sensor with sand-witched Pb 1.2 (Zr 0.52 Ti 0.48 ) O 3 film. Experimental results show that at 255 MHz, the sensor exhibits high sensitivity to both bending and pressure while is insensitive to temperature, its pressure and bending sensitivities are -15.9 kHz∙Pa -1 and -280 kHz∙μm -1 , respectively, while it is almost temperature insensitive, with only 0.472 % frequency drift between 24°C and 50°C. This type of sensor can operate in passive and wireless way. Moreover, it can be Frequency Division Multiplexed(FDM) since each sensor can operated in different frequency. Additionally, it possesses the other inherent advantages of high sensitivity, flexible and wearable adaptability, and excellent stability and reliability. The sensor holds promising potential for practical applications in intelligent skin, robotic arms and vital signal detection.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Lead",
                "Temperature sensors",
                "Zirconium",
                "Lead zirconate titanate",
                "Sensitivity",
                "Tactile sensors",
                "Intelligent sensors",
                "Sensor phenomena and characterization",
                "Substrates"
            ],
            "Author Keywords": [
                "conductive polymer",
                "Film Bulk Acoustic Resonator",
                "Frequency Division Multiplexed",
                "flexible",
                "nano-film",
                "Radio-frequency sensor"
            ]
        },
        "title": "A flexible Film Bulk Acoustic Resonator Radio-frequency sensor based on Pb1.2(Zr0.52Ti0.48)O3/polymer thin films"
    },
    {
        "authors": [
            "Sujitha Lakshminarayana",
            "Amit Praseed",
            "P. Santhi Thilagam"
        ],
        "published_in": "Published in: IEEE Communications Surveys & Tutorials ( Early Access )",
        "date_of_publication": "04 March 2024",
        "doi": "10.1109/COMST.2024.3372630",
        "publisher": "IEEE",
        "abstract": "The Internet of Things (IoT) is one of the most promising new millennial technologies, having numerous applications in our surrounding environment. The fundamental goal of an IoT system is to ensure effective communication between users and their devices, which is accomplished through the application layer of IoT. For this reason, the security of protocols employed at the IoT application layer are extremely significant. Message Queuing Telemetry Transport (MQTT) is being widely adopted as the application layer protocol for resource-constrained IoT devices. The reason for the widespread usage of the MQTT protocol in IoT devices is its highly appealing features, such as packet-agnostic communication, high scalability, low power consumption, low implementation cost, fast and reliable message delivery. These capabilities of the MQTT protocol make it a potential and viable target for adversaries. Therefore, we initially emphasize on the emerging MQTT vulnerabilities and provide a classification of identified MQTT vulnerabilities for the IoT paradigm. Then, this paper reviews attacks against the MQTT protocol and the corresponding defense mechanisms for MQTT-based IoT deployments. Furthermore, MQTT attacks are categorized and investigated with reference to crucial characteristics that aid in comprehending how these attacks are carried out. The defense mechanisms are discussed in detail, with a particular focus on techniques for identifying vulnerabilities, detecting and preventing attacks against the MQTT protocol. This work also discloses lessons learned by identifying and providing insightful findings, open challenges, and future research directions. Such a discussion is anticipated to propel more research efforts in this burgeoning area and pave a secure path toward expanding and fully realizing the MQTT protocol in IoT technology.",
        "issn": {
            "Electronic ISSN": "1553-877X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Protocols",
                "Security",
                "Surveys",
                "Cameras",
                "Pediatrics",
                "Standards"
            ],
            "Author Keywords": [
                "IoT",
                "application layer",
                "MQTT",
                "vulnerabilities",
                "attacks",
                "taxonomy",
                "testing",
                "detection",
                "defense"
            ]
        },
        "title": "Securing the IoT Application Layer from an MQTT Protocol Perspective: Challenges and Research Prospects"
    },
    {
        "authors": [
            "Zhenzhen Gong",
            "Omar Hashash",
            "Yingze Wang",
            "Qimei Cui",
            "Wei Ni",
            "Walid Saad",
            "Kei Sakaguchi"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "27 May 2024",
        "doi": "10.1109/JIOT.2024.3406220",
        "publisher": "IEEE",
        "abstract": "In this paper, a novel joint energy and age of information (AoI) optimization framework for IoT devices in a non-stationary environment is presented. In particular, IoT devices that are distributed in the real-world are required to efficiently utilize their computing resources so as to balance the freshness of their data and their energy consumption. To optimize the performance of IoT devices in such a dynamic setting, a novel lifelong reinforcement learning (RL) solution that enables IoT devices to continuously adapt their policies to each newly encountered environment is proposed. Given that IoT devices have limited energy and computing resources, an unmanned aerial vehicle (UAV) is leveraged to visit the IoT devices and update the policy of each device sequentially. As such, the UAV is exploited as a mobile learning agent that can learn a shared knowledge base with a feature base in its training phase, and feature sets of a zero-shot learning method in its testing phase, to generalize between the environments. To optimize the trajectory and flying velocity of the UAV, an actor-critic network is leveraged so as to minimize the UAV energy consumption. Simulation results show that the proposed lifelong RL solution can outperform the state-of-art benchmarks by enhancing the balanced cost of IoT devices by 8.3% when incorporating warm-start policies for unseen environments. In addition, our solution achieves up to 49.38% reduction in terms of energy consumption by the UAV in comparison to the random flying strategy.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Autonomous aerial vehicles",
                "Performance evaluation",
                "Knowledge based systems",
                "Feature extraction",
                "Energy consumption",
                "Costs"
            ],
            "Author Keywords": [
                "Internet of Things (IoT)",
                "Unmanned Aerial Vehicle (UAV)",
                "Age of Information (AoI)",
                "Lifelong Learning"
            ]
        },
        "title": "UAV-Aided Lifelong Learning for AoI and Energy Optimization in Non-Stationary IoT Networks"
    },
    {
        "authors": [
            "Yufei An",
            "F. Richard Yu",
            "Ying He",
            "Jianqiang Li",
            "Jianyong Chen",
            "Victor C.M. Leung"
        ],
        "published_in": "Published in: IEEE Transactions on Network and Service Management ( Early Access )",
        "date_of_publication": "05 September 2024",
        "doi": "10.1109/TNSM.2024.3454777",
        "publisher": "IEEE",
        "abstract": "The advancement of Internet of Things (IoT) technology has significantly transformed the dynamic between humans and devices, as well as device-to-device interactions. This paradigm shift has led to profound changes in human lifestyles and production processes. Through the interconnectedness of numerous sensors and controllers via networks, the IoT facilitates the seamless integration of humans with diverse devices, leading to substantial economic advantages. Nevertheless, the burgeoning IoT industry and the rapid proliferation of various IoT devices have also introduced a multitude of security vulnerabilities. Cyber attackers frequently exploit cyber attacks to compromise IoT devices, jeopardizing user privacy and property security, thereby posing a grave menace to the overall security of the IoT ecosystem. In this paper, we propose a novel IoT web attack detection system based on a joint embedded prediction architecture (JEPA), which effectively alleviates the security issues faced by IoT. It can obtain high-level semantic features in IoT traffic data through non-generative self-supervised learning. These features can more effectively distinguish normal data from attack data and help improve the overall detection performance of the system. Moreover, we propose a feature interaction module based on a dual-branch network, which effectively fuses low-level features and high-level features, and comprehensively aggregates global features and local features. Simulation results on multiple datasets show that our proposed system has better detection performance and robustness.",
        "issn": {
            "Electronic ISSN": "1932-4537"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Feature extraction",
                "Security",
                "Uniform resource locators",
                "Data models",
                "Service-oriented architecture",
                "Robustness"
            ],
            "Author Keywords": [
                "Internet of things (IoT)",
                "web attack",
                "joint embedded prediction architecture (JEPA)",
                "deep learning"
            ]
        },
        "title": "A Deep Learning System for Detecting IoT Web Attacks With a Joint Embedded Prediction Architecture (JEPA)"
    },
    {
        "authors": [
            "Muhammad Maaz",
            "Ghufran Ahmed",
            "Ahmad Sami Al-Shamayleh",
            "Adnan Akhunzada",
            "Shahbaz Siddiqui",
            "Abdulla Hussein Al-Ghushami"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "16 October 2024",
        "doi": "10.1109/ACCESS.2024.3482005",
        "publisher": "IEEE",
        "abstract": "The Internet of Things (IoT) has dramatically changed human context with the environment, ensuring productivity, comfort, and quality of life through a variety of services and applications. Nevertheless, the rapid growth of IoT devices has introduced significant security concerns like device vulnerabilities, unauthorized access, and potential data breaches.This article deals with an immediate call to empower IoT resilience against a wide range of sophisticated and prevalent cybersecurity threats. We developed two novel hybrid deep learning mechanisms, CNN-GRU (Convolutional Gated Recurrent Neural Networks) and CNN-LSTM (Convolutional Long Short-Term Memory Neural Networks), and extensively evaluated their performance on the state-of-the-art Kitsune and TON-IoT publicly available datasets. These benchmark datasets contain a variety of multivariate IoT attacks. The aim is to demonstrate the robustness of the proposed algorithms in effectively identifying telnet, password, distributed denial of service (DDoS), injection, and backdoor vulnerabilities in IoT ecosystems. We achieved approximately 99.6% accuracy in correctly distinguishing between malevolent and non-malicious activities on the Kitsune dataset. Additionally, the TON-IoT dataset demonstrated a remarkable accuracy rate of 99.00%, with minimal drops and low false alert rates. The time efficiency of both proposed algorithms renders them well-suited for deployment in IoT ecosystems. We evaluated and cross validated the proposed techniques with current benchmarks. Consequently, the proposed hybrid deep learning anomaly detection approaches not only enhance IoT security but also provide a robust control system for addressing emerging multivariate cyber threats.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Deep learning",
                "Intrusion detection",
                "Accuracy",
                "Long short term memory",
                "Iron",
                "Computer security",
                "Feature extraction",
                "Ecosystems",
                "Convolutional neural networks"
            ],
            "Author Keywords": [
                "IoT",
                "Machine learning (ML)",
                "Deep Learning (DL)",
                "Cybersecurity",
                "DDOS",
                "Injection Attacks",
                "Backdoor",
                "Botnet"
            ]
        },
        "title": "Empowering IoT Resilience: Hybrid Deep Learning Techniques for Enhanced Security"
    },
    {
        "authors": [
            "Rui Chen",
            "Zeqing Chen",
            "Shouzhi Xu",
            "Kai Ma",
            "Xiaojun Liu",
            "Liping Fan"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "13 September 2024",
        "doi": "10.1109/JIOT.2024.3460052",
        "publisher": "IEEE",
        "abstract": "The cell-free (CF) network architecture has promising applications in Internet of Things (IoT) networks due to its dynamic and adaptive nature in resource allocation. However, due to the openness of wireless channels and the information interaction of sensitive data, the security issues of CF IoT networks have become more severe, especially in the case of powerful opponents. In this paper, a high-level secure transmission technique is utilized to protect the privacy of users, which is called covert communication. Specifically, in the CF IoT network conceived, one IoT device secretly transmits sensitive information to an access point (AP) under the cover of a selected jammer to avoid being detected by other curious IoT devices, which are regarded as potential eavesdroppers (Eves) and determined by the central processing unit (CPU) based on greedy degree. Then, we analyze the joint impact of jamming and greedy degree on the detection error probability (DEP) for Eve, and determine Eve’s minimum DEP as well as the corresponding optimal threshold. To enhance energy efficiency while ensuring communication security, three jamming scheduling schemes have been proposed, namely, the jamming scheduling for both the receiver (Bob) and Eve (JSBE), the jamming scheduling for Eve (JSE) and the jamming scheduling for Bob (JSB). Numerical results indicate that greedy degree dominates the average minimum DEP for Eve when IoT devices tend to be lightweight. Furthermore, JSBE scheme demonstrates superiority in terms of covert rate, covert outage probability (COP), and covert energy efficiency (CEE), which provides an effective solution for high security transmission of energy-constrained nodes in CF IoT networks.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Jamming",
                "Security",
                "Power control",
                "Uplink",
                "Interference",
                "Wireless sensor networks"
            ],
            "Author Keywords": [
                "CF IoT network",
                "covert communication",
                "jamming scheduling scheme",
                "greedy degree",
                "CEE"
            ]
        },
        "title": "Greedy Degree and Jamming Aided Covert Uplink Transmission in Cell-Free IoT Networks"
    },
    {
        "authors": [
            "N Sharmila Kumari",
            "H S Vimala",
            "C N Pruthvi",
            "J Shreyas"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "16 September 2024",
        "doi": "10.1109/ACCESS.2024.3462170",
        "publisher": "IEEE",
        "abstract": "Context: The growing number of linked devices, the limitations of some IoT devices, worries about data privacy, risks to physical safety, and the ever-changing threats all highlight why it’s crucial to have strong security measures in the IoT application layer. We need these measures to protect our devices and data, keep our personal information safe, and prevent potential harm or attacks. It’s important to make sure our IoT applications have effective security to counter these risks. Objective: The study’s goal is to examine the research on IoT application layer security with attacks, applications and protocols and demands further focus in current and future research areas. Method: To find the relevant literature, a systematic investigation was done, in 221 articles, 123 articles are selected as a most relevant articles and classified as security in IoT applications and protocols. Result: This paper mainly concentrated on security in IoT application layer, the articles are grouped as attacks, applications and protocols. Based on IoT applications, smart home (15.3%), smart health care (12.2%), smart home (13.3%), smart agriculture (17.3%), smart vehicles (16.3%), smart grid(15.3%) and Industrial IoT (10.2%) represents the majority of articles. This survey further analyses the performance metrics, parameters and performance evaluation of the existing literature. Conclusion: The results confirm that we need these measures to protect our devices and data, keep our personal information safe, and prevent potential harm or attacks. It’s important to make sure our IoT applications have effective security to counter these risks.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Security",
                "Surveys",
                "Protocols",
                "Performance evaluation",
                "Data privacy",
                "Threat modeling",
                "Systematic literature review",
                "Smart healthcare",
                "Smart agriculture",
                "Risk management"
            ],
            "Author Keywords": [
                "Attacks",
                "Application layer",
                "IoT",
                "Security",
                "Protocols"
            ]
        },
        "title": "Holistic Survey on Security in IoT Application Layer: Attacks, Protocols, and Applications"
    },
    {
        "authors": [
            "Yuanhua Fu",
            "Zhiming He"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "16 September 2024",
        "doi": "10.1109/JIOT.2024.3461813",
        "publisher": "IEEE",
        "abstract": "Massive wireless connections for Internet of Things (IoT) terminals require large amounts of spectrum resource, cognitive radio-enabled IoT (CR-IoT) is emerged as an effective solution to alleviate the scarcity of spectrum resources. However, CR-IoT faces several problems such as spectrum sensing data falsification (SSDF) attacks when using of cooperative spectrum sensing, and energy efficiency due to equipped with low-powered IoT terminals. To tackle these challenges, secure power allocation is considered a promising technique for improving energy efficiency. In this paper, a joint spectrum sensing and secure power allocation scheme is proposed under SSDF attacks. Firstly, to ensure fair resource allocation and resistance against SSDF attacks, a weighted data transmission time allocation scheme is designed based on the trust degree, where the trust degree is obtained by using an online learning algorithm and is updated in the sensed power allocation process. Then, a joint optimization problem of spectrum sensing time, number of cooperative nodes, and transmit power is formulated to maximize the CR-IoT sum energy efficiency, subject to the constraints of CR-IoT node’s average transmission power, minimum data rate, and the interference to the primary user. To handle the non-convexity of the resulting problem, an alternating iterative optimization algorithm is proposed to iteratively solve three subproblems: spectrum sensing time optimization, number of cooperative nodes optimization, and transmission power optimization. Simulation results demonstrate that the proposed scheme is effective in improving energy efficiency under various SSDF attack scenarios.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Resource management",
                "Internet of Things",
                "Optimization",
                "Energy consumption",
                "Throughput",
                "Reliability"
            ],
            "Author Keywords": [
                "Cognitive radio Internet of Things (CR-IoT)",
                "cooperative spectrum sensing",
                "SSDF attack",
                "energy efficiency",
                "power allocation"
            ]
        },
        "title": "Energy-Efficient Joint Spectrum Sensing and Power Allocation in Cognitive IoT under SSDF Attack"
    },
    {
        "authors": [
            "Ying Wang",
            "Qianqian Zhang",
            "Tongyan Wei",
            "Lin Cong",
            "Peng Yu",
            "Shaoyong Guo",
            "Xuesong Qiu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "03 September 2024",
        "doi": "10.1109/JIOT.2024.3454064",
        "publisher": "IEEE",
        "abstract": "With the rapid development of the Internet of Things (IoT), more and more IoT traffic is generated in the data network. Accurate perception of IoT traffic changes will facilitate traffic engineering decisions, thus ensuring the performance of IoT applications. However, current traffic prediction methods ignore the limitations of actual application environment. In this paper, we propose an IoT traffic prediction method based on horizontal federated learning to predict traffic trends under the cooperation of the cloud and the edge side. In order to improve the accuracy of IoT traffic prediction, a traffic prediction model SMN3-CIFGA is proposed to predict IoT traffic based on traffic feature extraction in a limited hardware environment. In addition, in order to improve the communication efficiency in the distributed training process of the traffic prediction model, we propose a gradient compression algorithm based on dynamic threshold GCADT. The experimental results demonstrate that compared with current methods, the average training time of the GCADT algorithm is reduced by about 6.21%, the transmission gradient size of the GCADT is reduced by about 66.71%, the average training time of the classification model SMN3 is reduced by about 40%, and the testing set prediction accuracy of SMN3-CIFGA can reach 97.61%.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Federated learning",
                "Predictive models",
                "Training",
                "Data models",
                "Accuracy",
                "Cloud computing"
            ],
            "Author Keywords": [
                "IoT traffic prediction",
                "horizontal federated learning",
                "gradient compression"
            ]
        },
        "title": "Lightweight Federated Learning Driven Traffic Prediction for Heterogeneous IoT Networks"
    },
    {
        "authors": [
            "Shailendra Pratap Singh",
            "Naween Kumar",
            "Gyanendra Kumar",
            "Balamurugan Balusamy",
            "Ali Kashif Bashir",
            "Yasser D. Al-Otaibi"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "07 June 2024",
        "doi": "10.1109/TCE.2024.3411037",
        "publisher": "IEEE",
        "abstract": "The advent of 6G-enabled networks marks a transformative era in the Internet of Things (IoT), promising unparalleled connectivity and innovation. These networks are set to revolutionize the IoT landscape by offering remarkable capabilities, including ultra-high data speeds, ultra-low latency, and extensive network coverage and connectivity. However, optimizing such networks’ is a complex challenge, mainly when dealing with numerous conflicting objectives. So far, existing works have employed heuristic or meta-heuristic algorithms to address this issue. This research introduces a novel approach’, Hybrid Multi-Objective Optimization’, which combines Multi-Objective forms of Red fox (RFOX) optimization with Differential Evolution (DE) to address this issue. This hybrid framework is designed to solve the complexity of Multi-Objective optimization within the context of 6G-enabled IoT networks. It leverages the flexibility and search capabilities of RFOX, along with the population-based search techniques of DE. The primary objective of this research paper is to identify the Pareto-optimal front, which encapsulates the complex trade-offs among various conflict objectives in Multi-Objective optimization. Extensive simulation outcomes demonstrate the significant efficacy of the proposed Algorithm for its adaptability, diversity, and multi-objective optimization capabilities compared to existing ones in terms of data throughput, delay, energy efficiency, and packet loss ratio in 6G-enabled IoT applications.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "6G mobile communication",
                "Optimization",
                "Throughput",
                "Quality of service",
                "Energy efficiency",
                "Packet loss"
            ],
            "Author Keywords": [
                "Adaptation",
                "Red fox Optimization",
                "Differential Evolution",
                "Multi-Objective Evolutionary Algorithms",
                "Internet of Things"
            ]
        },
        "title": "A Hybrid Multi-Objective Optimisation for 6G-Enabled Internet of Things (IoT)"
    },
    {
        "authors": [
            "Nour Moustafa",
            "Izhar Ahmed Khan",
            "Mohammed Hassanin",
            "David Ormrod",
            "Dechang Pi",
            "Imran Razzak",
            "Jill Slay"
        ],
        "published_in": "Published in: IEEE Transactions on Industrial Informatics ( Early Access )",
        "date_of_publication": "20 October 2022",
        "doi": "10.1109/TII.2022.3214652",
        "publisher": "IEEE",
        "abstract": "The integration of satellite systems with smart computing and networking technologies, such as the Internet of Things (IoT), has intensely augmented sophisticated cyberattacks against satellite environments. Resisting cyber threats to complex and large-scale satellite configurations has been enormously challenging, owing to the deficiency of high-quality samples of attack data collected from distributed satellite networks. This study proposes a novel federated learning-based deep learning framework for intrusion detection, named DFSat, to identify cyberattacks from IoT-integrated satellite networks. We develop a distributed deep learning-enabled attack detection method using a recurrent neural network. We then build a federated learning architecture which, utilizes several IoT-integrated satellite networks to preserve the privacy and security of DFSat's parameters throughout the learning process. Extensive experiments have been conducted using communication rounds on an IoT-based network dataset to validate the efficiency of DFSat. The results revealed that the proposed framework significantly distinguishes complex cyberattacks, outperforming recent state-of-the-art intrusion detection techniques, validating its usefulness as a viable deployment framework in IoT-integrated satellite networks.",
        "issn": {
            "Print ISSN": "1551-3203",
            "Electronic ISSN": "1941-0050"
        },
        "keywords": {
            "IEEE Keywords": [
                "Satellites",
                "Security",
                "Internet of Things",
                "Intrusion detection",
                "Earth",
                "Computer architecture",
                "Low earth orbit satellites"
            ],
            "Author Keywords": [
                "Cyber security",
                "Smart Enterprise Systems",
                "Federated learning",
                "Intrusion Detection",
                "Internet of Things (IoT)",
                "Satellite Systems"
            ]
        },
        "title": "DFSat: Deep Federated Learning for Identifying Cyber Threats in IoT-based Satellite Networks"
    },
    {
        "authors": [
            "Amit Sagu",
            "Nasib Singh Gill",
            "Preeti Gulia",
            "Ishaani Priyadarshini",
            "Jyotir Moy Chatterjee"
        ],
        "published_in": "Published in: IEEE Transactions on Big Data ( Early Access )",
        "date_of_publication": "01 March 2024",
        "doi": "10.1109/TBDATA.2024.3372368",
        "publisher": "IEEE",
        "abstract": "The Internet of Things (IoT) is being prominently used in smart cities and a wide range of applications in society. The benefits of IoT are evident, but cyber terrorism and security concerns inhibit many organizations and users from deploying it. Cyber-physical systems that are IoT-enabled might be difficult to secure since security solutions designed for general information/operational technology systems may not work as well in an environment. Thus, deep learning (DL) can assist as a powerful tool for building IoT-enabled cyber-physical systems with automatic anomaly detection. In this paper, two distinct DL models have been employed i.e., Deep Belief Network (DBN) and Convolutional Neural Network (CNN), considered hybrid classifiers, to create a framework for detecting attacks in IoT-enabled cyber-physical systems. However, DL models need to be trained in such a way that will increase their classification accuracy. Therefore, this paper also aims to present a new hybrid optimization algorithm called “Seagull Adapted Elephant Herding Optimization” (SAEHO) to tune the weights of the hybrid classifier. The “Hybrid Classifier + SAEHO” framework takes the feature extracted dataset as an input and classifies the network as either attack or benign. Using sensitivity, precision, accuracy, and specificity, two datasets were compared. In every performance metric, the proposed framework outperforms conventional methods.",
        "issn": {
            "Electronic ISSN": "2332-7790"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Feature extraction",
                "Security",
                "Cyber-physical systems",
                "Support vector machines",
                "Metaheuristics",
                "Convolutional neural networks"
            ],
            "Author Keywords": [
                "Deep Learning",
                "IoT Enabled Cyber-Physical Systems",
                "Optimization Technique",
                "cyberattacks",
                "cyber-security",
                "cyber-physical infrastructure"
            ]
        },
        "title": "Hybrid Optimization Algorithm for Detection of Security Attacks in IoT-Enabled Cyber-Physical Systems"
    },
    {
        "authors": [
            "Thien Thi Thanh Le",
            "Naveed UL Hassan",
            "Xiaoming Chen",
            "Mohamed-Slim Alouini",
            "Zhu Han",
            "Chau Yuen"
        ],
        "published_in": "Published in: IEEE Communications Surveys & Tutorials ( Early Access )",
        "date_of_publication": "04 April 2024",
        "doi": "10.1109/COMST.2024.3385347",
        "publisher": "IEEE",
        "abstract": "Low-Earth orbit (LEO) satellites can play an important role in providing seamless coverage for the Internet of Things (IoT). In satellite-based IoT (SIoT) networks, IoT devices can communicate directly with a satellite or through a gateway, which is called direct-access SIoT and indirect-access SIoT, respectively. As the number of IoT devices requiring satellite access is increasing, the role of medium access control (MAC) protocols becomes critical in reducing the latency and improving the quality of service (QoS). In this tutorial, we provide a comprehensive review of random access (RA) protocols, more specifically, grant-free RA (GFRA) protocols, which are more efficient in handling the communication requirements of SIoT networks. We discuss the challenges that arise in designing RA protocols under time-frequency resource and preamble limitations, high mobility of satellites, sporadic traffic from IoT networks, and diverse QoS requirements of IoT applications. We also highlight future research directions, including cross-layer optimization, joint activity detection and channel estimation (JAD-CE), reinforcement-learning-based solution, protocol design for dense LEO satellite networks, and reconfigurable intelligent surface (RIS) aided SIoT.",
        "issn": {
            "Electronic ISSN": "1553-877X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Satellites",
                "Protocols",
                "Temperature sensors",
                "Satellite broadcasting",
                "Quality of service",
                "Low earth orbit satellites"
            ],
            "Author Keywords": [
                "Satellite communication",
                "IoT networks",
                "grant-free random access",
                "resource allocation",
                "reinforcement learning",
                "MAC protocol"
            ]
        },
        "title": "A Survey on Random Access Protocols in Direct-Access LEO Satellite-Based IoT Communication"
    },
    {
        "authors": [
            "Atul Banotra",
            "Deepak Mishra",
            "Sudhakar Modem"
        ],
        "published_in": "Published in: IEEE Transactions on Sustainable Computing ( Early Access )",
        "date_of_publication": "14 August 2024",
        "doi": "10.1109/TSUSC.2024.3443450",
        "publisher": "IEEE",
        "abstract": "The Internet of Things (IoT) applications require uninterrupted network operation which is often hindered by battery energy constraints. Literature suggests that solar energy harvesting is a promising approach to powering IoT devices in a sustainable manner. However, the available literature overlooks key factors of determining effective solar panel size and cost while considering the IoT consumption for sustainable operation. This article tackles these pivotal aspects by investigating viability of commercially available solar panels as a sustainable energy source for IoT applications. A novel stochastic computation model is introduced to characterize the unpredictability of solar irradiance across three different time regions of the day. By employing distribution fitting models, the proposed computation model accurately determines the required solar panel size in cm\n2\nand panel cost in Indian Rupees for the sustainable operation of the IoT application. Further, the proposed model incorporates the assessment of outage and sustainability probabilities for user-specified solar panel size and cost. These insights are significant in settings where energy efficiency and sustainability are crucial. Numerical results are presented to validate the derived distribution models and performance metrics for sustainable IoT applications. The effectiveness and accuracy of the proposed model are validated by comparing results with baseline model.",
        "issn": {
            "Electronic ISSN": "2377-3782"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Costs",
                "Solar irradiance",
                "Solar panels",
                "Stochastic processes",
                "Sensors",
                "Computational modeling"
            ],
            "Author Keywords": [
                "Sustainable IoT",
                "solar energy harvesting",
                "solar irradiance",
                "solar panel size and cost",
                "stochastic computation model"
            ]
        },
        "title": "Stochastic Computation Model for Solar Panel Size and Cost of Sustainable IoT Networks"
    },
    {
        "authors": [
            "Vukan Ninkovic",
            "Dejan Vukobratovic",
            "Dragisa Miskovic",
            "Marco Zennaro"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "28 October 2024",
        "doi": "10.1109/JIOT.2024.3487246",
        "publisher": "IEEE",
        "abstract": "The significance of distributed learning and inference algorithms in Internet of Things (IoT) network is growing since they flexibly distribute computation load between IoT devices and the infrastructure, enhance data privacy, and minimize latency. However, a notable challenge stems from the influence of communication channel conditions on their performance. In this work, we introduce COMSPLIT: a novel communication-aware design for split learning (SL) and inference paradigm tailored to processing time series data in IoT networks. COMSPLIT provides a versatile framework for deploying adaptable SL in IoT networks affected by diverse channel conditions. In conjunction with the integration of an early-exit strategy, and addressing IoT scenarios containing devices with heterogeneous computational capabilities, COMSPLIT represents a comprehensive design solution for communication-aware SL in IoT networks. Numerical results show superior performance of COMSPLIT compared to vanilla SL approaches (that assume ideal communication channel), demonstrating its ability to offer both design simplicity and adaptability to different channel conditions.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Servers",
                "Training",
                "Performance evaluation",
                "Distance learning",
                "Computer aided instruction",
                "Artificial intelligence",
                "Computational modeling",
                "Wireless communication",
                "Time series analysis"
            ],
            "Author Keywords": [
                "Split learning (SL)",
                "Internet of Things (IoT)",
                "Edge Computing",
                "Distributed learning"
            ]
        },
        "title": "COMSPLIT: A Communication–Aware Split Learning Design for Heterogeneous IoT Platforms"
    },
    {
        "authors": [
            "Linna Fan",
            "Bo Wu",
            "Xuan Shen",
            "Jun He",
            "Guanglei Song",
            "Gang Yang",
            "Chaocan Xiang",
            "Duohe Ma",
            "Yongfeng Huang"
        ],
        "published_in": "Published in: IEEE Transactions on Mobile Computing ( Early Access )",
        "date_of_publication": "28 October 2024",
        "doi": "10.1109/TMC.2024.3486717",
        "publisher": "IEEE",
        "abstract": "IoT device identification is vital for network asset and security management. However, existing methods use statistical features that can not identify IoT devices accurately in complex network environments. GraphIoT proposes using non-statistical features and building a heterogeneous graph neural network to identify IoT devices accurately. However, heterogeneous graph neural networks lack interpretability, which reduces trust in the model. Besides, it is difficult to deploy on resource-constrained devices, limiting the broad application of IoT device identification. To make IoT device identification interpretable, easy to deploy, and with high accuracy, we get the interpretation results of GraphIoT through interpretability and further build the rule set based on the interpretation results. Considering there is no suitable interpreter for GraphIoT with many nodes and edges, we propose HGExplainer , which reduces the time complexity by splitting the interpretation target into important relation solving and edge solving and uses a novel solution method, ExpandTree. Then, we also designed a rule extractor, which can build rule sets based on the interpretation results. Experimental results on Yourthings and UNSW datasets show that HGExplainer can build high fidelity, concise sample-level explanations in less than 3 seconds, and the established rule set can precisely identify IoT devices.",
        "issn": {
            "Print ISSN": "1536-1233",
            "Electronic ISSN": "1558-0660"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Object recognition",
                "Feature extraction",
                "Mobile computing",
                "Graph neural networks",
                "Accuracy",
                "Training",
                "Fans",
                "Security management",
                "Limiting"
            ],
            "Author Keywords": [
                "Fingerprinting",
                "heterogeneous graph",
                "interpretability",
                "IoT",
                "traffic analysis"
            ]
        },
        "title": "HGExplainer: Heterogeneous Graph Explainer for IoT Device Identification"
    },
    {
        "authors": [
            "Prashant Kumar",
            "Naveen Chauhan",
            "Nisha Chaurasia",
            "Krishna Kant Agarwal",
            "Ankit Vidyarthi",
            "Deepak Gupta"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "15 July 2024",
        "doi": "10.1109/TCE.2024.3424239",
        "publisher": "IEEE",
        "abstract": "Consumer-centric IoT (Internet of Things) opportunistic networks refer to communication networks that prioritize the needs and preferences of end-users or consumers. In the context of IoT, opportunistic networks are characterized by the ability of devices to establish communication links opportunistically based on availability and proximity. These kinds of networks deploy by seed and grow based on the invitation. Because of their different characteristics, routing in Consumer-centric IoT opportunistic networks has become very challenging. Thus, in these conditions, it becomes imperative to use the available resources efficiently. Considering all these issues, this article proposes a novel message forwarding scheme, BeRout, for Consumer-centric IoT opportunistic networks. The proposed scheme is based on the benevolence behavior of nodes. To achieve high delivery ratio in IoT opportunistic network scenarios, BeRout, to node’s past behavior and activities. To use buffer space effectively and efficiently, a buffer management scheme has been proposed. To show the effectiveness and efficiency of the proposed BeRout scheme simulation is performed on the ONE simulator and results are compared with the existing state-of-the-art routing protocols.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Routing",
                "Routing protocols",
                "Epidemics",
                "Electronic mail",
                "Relays",
                "History"
            ],
            "Author Keywords": [
                "Consumer-centric IoT",
                "Opportunistic networks",
                "delay-tolerant network",
                "benevolence behavior",
                "buffer management"
            ]
        },
        "title": "Benevolence Behavior Based Message Forwarding Scheme for Consumer-Centric IoT Opportunistic Networks"
    },
    {
        "authors": [
            "Zhao Zhang",
            "Chunxiang Xu",
            "Man Ho Allen Au",
            "Changsong Jiang"
        ],
        "published_in": "Published in: IEEE Transactions on Mobile Computing ( Early Access )",
        "date_of_publication": "28 October 2024",
        "doi": "10.1109/TMC.2024.3486719",
        "publisher": "IEEE",
        "abstract": "IoT-based sharing economy is a win-win business model, where a transferor owns idle IoT devices and transfers the right to use a device to a user for a fee. Considering usage of multiple devices and privacy preservation, anonymous single-sign-on (ASSO) is a feasible solution for authentication. ASSO allows a user to access multiple devices with one token issued by the transferor and prevents the transferor from identifying the user. We also observe that in the scenario of IoT-based sharing economy, the token should (i) support attributes since a device should be available only to users with specific attributes (e.g., age) and (ii) avoid incurring significant communication/computation overhead as IoT devices are resource-constrained. In this paper, we proposed PILOT, a privacy-preserving single-sign-on with fine-grained access control for IoT devices. When a user attempts to access a device, he/she requests a token from the transferor. The token is actually a blind signature that cannot be tracked, and contains the user's attributes which facilitate fine-grained access control on the device. Besides, the token consists of only four group elements and verification of the token involves only several exponentiation operations. This renders PILOT superior in terms of communication/computation overhead and suitable for IoT devices.",
        "issn": {
            "Print ISSN": "1536-1233",
            "Electronic ISSN": "1558-0660"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Authentication",
                "Sharing economy",
                "Privacy",
                "Passwords",
                "Cryptography",
                "Access control",
                "Security",
                "Mobile computing",
                "Smart devices"
            ],
            "Author Keywords": [
                "Access control",
                "internet of things (IoT)",
                "privacy preservation",
                "sharing economy",
                "single-sign-on"
            ]
        },
        "title": "Privacy-Preserving Single-Sign-On With Fine-Grained Access Control for IoT Devices"
    },
    {
        "authors": [
            "Muhammad Zohaib",
            "Abeer Abdulaziz Alsanad",
            "Muhammad Azeem Akbar"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "19 September 2024",
        "doi": "10.1109/ACCESS.2024.3464102",
        "publisher": "IEEE",
        "abstract": "The advent of the Internet of Things (IoT) marks a significant milestone in digital innovation, transforming numerous aspects of daily life. As IoT continues to proliferate, ensuring robust security becomes increasingly critical, drawing attention from both academic and industry circles. This study seeks to systematically identify and categorize the primary success factors essential for securing IoT systems. By synthesizing insights from comprehensive literature reviews and detailed questionnaires, we have identified 21 pivotal success factors frequently referenced in both scholarly research and practical implementations. These success factors are organized into four key categories: Security protocols and Standards, Threat Detection and Prevention Mechanisms, Device Security management and Governance, Risk and Compliance (GRC). To evaluate the relative importance of these factors, we employ the fuzzy Analytic Hierarchy Process (fuzzy-AHP), a method recognized for its effectiveness in addressing complex decision-making challenge within IT contexts. This innovative application of fuzzy-AHP in the IoT security domain facilitates the nuanced prioritization of these success factors. Our research offers a structured hierarchy of IoT security success factors, providing critical guidance for practitioners and academics in developing more resilient and effective security strategies for the evolving IoT ecosystem.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Security",
                "Privacy",
                "Smart homes",
                "Smart cities",
                "Analytic hierarchy process",
                "Taxonomy",
                "Fuzzy systems"
            ],
            "Author Keywords": [
                "IoT Security",
                "Success factors",
                "Prioritization-based-Taxonomy",
                "Fuzzy Analytic Hierarchy Process (fuzzy-AHP)"
            ]
        },
        "title": "Success Factors of IoT Security: A Structured Analysis Using Fuzzy-AHP"
    },
    {
        "authors": [
            "Jehad Ali",
            "Houbing Herbert Song",
            "Vandana Sharma",
            "Mahmoud Ahmad Al-Khasawneh"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "03 October 2024",
        "doi": "10.1109/TCE.2024.3472707",
        "publisher": "IEEE",
        "abstract": "Security and privacy are significant concerns in software-defined networking (SDN)-applied Internet of Things (IoT) environments, due to the proliferation of connected devices and the potential for cyberattacks. Hence, robust security mechanisms need to be developed, including authentication, encryption, and distributed denial of service (DDoS) attack detection, tailored to the constraints of low-power IoT devices. Selecting a suitable tiny machine learning (TinyML) algorithm for low-power IoT devices for DDoS attack detection involves considering various factors such as computational complexity, robustness in dealing with heterogeneous data, accuracy, and the specific constraints of the target IoT device. In this paper, we present a two-fold approach for the optimal TinyML algorithm selection leveraging the hybrid analytical network process (HANP). First, we make a comparative analysis (qualitative) of the machine learning algorithm in the context of suitability for TinyML in the domain of SD-IoT devices and generate the weights of suitability for TinyML applications in SD-IoT. Then we evaluate the performance of the machine learning algorithms and validate the results of the model to demonstrate the effectiveness of the proposed method. Finally, we see the effect of dimensionality reduction with respect to features and how it affects the precision, recall, accuracy, and F1 score. The results demonstrate the effectiveness of the scheme.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Machine learning algorithms",
                "Tiny machine learning",
                "Denial-of-service attack",
                "Feature extraction",
                "Accuracy",
                "Performance evaluation",
                "Inference algorithms",
                "Computer crime",
                "Training"
            ],
            "Author Keywords": [
                "Low power IoT",
                "SDN",
                "DDoS attacks",
                "Machine learning",
                "Decision making"
            ]
        },
        "title": "DDoS Intrusions Detection in Low Power SD-IoT Devices Leveraging Effective Machine Learning"
    },
    {
        "authors": [
            "Nwamaka Okafor",
            "Ruchita Ingle",
            "Ugochukwu Matthew",
            "Matthew Saunders",
            "Declan Delaney"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "04 September 2024",
        "doi": "10.1109/JIOT.2024.3454241",
        "publisher": "IEEE",
        "abstract": "Advances in Internet of Things (IoT) technologies have resulted in a significant surge in the utilization of sensor devices across diverse domains for environmental sensing and monitoring. The applications of IoT sensor devices in environmental monitoring span a wide range, including the surveillance of biodiverse areas such as peatlands, forests, and oceans, as well as air quality monitoring, commercial agriculture, and the safeguarding of endangered species. This paper provides a long term evaluation of IoT sensors data quality in environmental monitoring networks, particularly focusing on peatlands. IoT sensors have the capacity to provide high resolution spatiotemporal dataset in environmental monitoring networks. Sensor data quality plays a significant role in increasing the adoption of IoT devices for environmental data gathering. However, logistics challenges(i.e., in harsh and unfavourable weather conditions) along with low-cost components limitations adds on to the data collection errors. This paper identifies specific challenges and issues related to IoT sensor data quality in different peatland ecotopes. These challenges include sensor placement and calibration, data validation and fusion, environmental interference, and the management of data gaps and uncertainties. This research work evaluates methods for improving data quality in peatland monitoring network by encompassing advanced sensor calibration techniques, data validation algorithms, machine learning approaches, data processing and data fusion strategies.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Monitoring",
                "Internet of Things",
                "Atmospheric modeling",
                "Atmospheric measurements",
                "Environmental monitoring",
                "Data integrity",
                "Ecosystems"
            ],
            "Author Keywords": [
                "Calibration",
                "Data quality",
                "Data processing",
                "Data validation",
                "IoT sensors",
                "Machine learning",
                "Peatlands"
            ]
        },
        "title": "Assessing and Improving IoT Sensor Data Quality in Environmental Monitoring Networks: A Focus on Peatlands"
    },
    {
        "authors": [
            "Najm Us Sama",
            "Saeed Ullah",
            "S. M. Ahsan Kazmi",
            "Manuel Mazzara"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "05 November 2024",
        "doi": "10.1109/ACCESS.2024.3491831",
        "publisher": "IEEE",
        "abstract": "As the Internet of Things (IoT) landscape rapidly evolves, robust network security measures are imperative. In particular, Intrusion Detection Systems play a very important role in the preservation of an IoT environment from malicious activities. This paper provides a comprehensive performance comparison of various machine learning classifiers, including K-Nearest Neighbors, Gradient Boosting, XGBoost, Support Vector Machines, Random Forests, Decision Trees, and Extremely Randomized Trees, for intrusion detection in IoT networks. Comparative analysis shows that although all models did very well, the ensemble methods—GB, XGBoost, RF, and ERT—constantly performed better than others in F1-Score, recall, accuracy, and precision. Among them, ERT is turned out to be the most effective model for real-time attack detection on IoT devices, with an accuracy of 99.7% besides excellent precision and recall. XGBoost and RF also turn out to have high reliability and accuracy with F1-Scores of 0.95. These findings further underscore that ensemble methods outperform in intrusion detection for IoT networks and, thus, offer important insights to improve security within networks and protect critical IoT-based infrastructures from a variety of threats.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Accuracy",
                "Intrusion detection",
                "Machine learning",
                "Feature extraction",
                "Security",
                "Nearest neighbor methods",
                "Support vector machines",
                "Organizations",
                "Classification algorithms"
            ],
            "Author Keywords": [
                "Accuracy",
                "Internet of Things (IoT)",
                "Intrusion Detection Systems (IDS)",
                "Machine Learning classifiers"
            ]
        },
        "title": "Cutting-Edge Intrusion Detection in IoT Networks: A Focus on Ensemble Models"
    },
    {
        "authors": [
            "Kyeongwon Lee",
            "Sangmin Jeon",
            "Kangju Lee",
            "Woojoo Lee",
            "Massoud Pedram"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "23 September 2024",
        "doi": "10.1109/JIOT.2024.3466228",
        "publisher": "IEEE",
        "abstract": "The adoption of Ultra-Wideband (UWB) radar technology in IoT and healthcare applications for respiration detection is rapidly expanding, opening up a wide array of potential use cases. Despite its burgeoning utility, the integration of UWB radar-based respiration detection in IoT endnode devices faces significant challenges due to the memory-intensive nature of these tasks, which strain the capabilities of IoT processors. This paper introduces a streamlined UWB radar-based respiration detection application designed for operation on IoT processors, emphasizing that when executed on conventional IoT processors, the limited processing power still results in significant data loss, underscoring the need for enhanced processing solutions. To address these challenges, we propose the adoption of Processing-in-Memory (PIM) technology and unveil the novel Radar-PIM architecture. This architecture is meticulously engineered to boost the efficiency of respiration detection while ensuring seamless integration with existing embedded processor frameworks. The paper extensively describes the Radar-PIM architecture and its operational mechanisms. We further demonstrate its superior performance by implementing and empirically testing a Radar-PIM processor prototype. Next, we present an optimization strategy tailored for designing energy-efficient Radar-PIM processors, specifically adapted for diverse UWB radar-based respiration detection applications. For instance, a Radar-PIM processor prototype, optimized for a particular application, achieved approximately 42% energy savings compared to its unoptimized counterpart and delivered performance nearly three times greater than that of a multicore processor with equivalent power consumption. This demonstrates the transformative potential of our proposed solution in enhancing the capabilities of radar-based respiration detection systems for IoT endnodes.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Program processors",
                "Internet of Things",
                "Ultra wideband radar",
                "Computer architecture",
                "Signal processing algorithms",
                "Random access memory",
                "Prototypes"
            ],
            "Author Keywords": [
                "IR-UWB Radar",
                "IoT Processor",
                "Respiration Detection",
                "RISC-V",
                "Low-Power Designs"
            ]
        },
        "title": "Radar-PIM: Developing IoT Processors Utilizing Processing-in-Memory Architecture for Ultra-Wideband Radar-Based Respiration Detection"
    },
    {
        "authors": [
            "Adila Mebrek",
            "Abdulsalam Yassine"
        ],
        "published_in": "Published in: IEEE Transactions on Emerging Topics in Computational Intelligence ( Early Access )",
        "date_of_publication": "31 August 2021",
        "doi": "10.1109/TETCI.2021.3102214",
        "publisher": "IEEE",
        "abstract": "Fog computing is an emerging paradigm that allows IoT devices and applications to process their data at the network’s edge. However, fog servers have limited computational resources compared to the cloud system; therefore, they cannot accommodate the ever-increasing computational demand from IoT devices. In such a challenging environment, IoT users’ decision to offload their tasks to the fog node or the central cloud server is affected by the environment’s current dynamics as other IoT users are also competing to maximize their resource utilization. In this paper, we propose a computational model that considers energy consumption and transmission latency as decision parameters for task offloading of IoT applications. Second, we model the competition as a game where IoT devices’ decision for the optimal distribution of tasks is captured in a joint optimization problem for energy consumption and latency. Third, we propose a decentralized task distribution algorithm where the players learn to update their strategy based on other players’ actions. We also prove that the solution of the proposed algorithm converges to Nash equilibrium (NE). Finally, we carry out extensive evaluations and compare our computational model and results with existing studies.",
        "issn": {
            "Electronic ISSN": "2471-285X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling",
                "Internet of Things",
                "Task analysis",
                "Servers",
                "Resource management",
                "Games",
                "Optimization"
            ],
            "Author Keywords": [
                "Computational intelligence",
                "energy consumption",
                "fog and cloud computing",
                "game theory",
                "Internet of Things",
                "latency",
                "Nash equilibrium",
                "resource allocation"
            ]
        },
        "title": "Intelligent Resource Allocation and Task Offloading Model for IoT Applications in Fog Networks: A Game-Theoretic Approach"
    },
    {
        "authors": [
            "Prabal Verma",
            "Sandeep K. Sood",
            "Harkiran Kaur",
            "Mohit Kumar",
            "Huaming Wu",
            "Sukhpal Singh Gill"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "10 June 2024",
        "doi": "10.1109/TCE.2024.3411657",
        "publisher": "IEEE",
        "abstract": "Edge computing plays a crucial role in the processing of Consumer Internet of Things (IoT)-enabled latency-sensitive applications. In smart homes, dynamic action strategies based on multiple IoT objects with edge processing can be the best solution for handling adverse events. To overcome these challenges, the use of Stochastic Game Net (SGN) forming IoT devices as players with predefined action sets is one of the feasible solutions. Relative to this context, the edge-assisted IoT-enabled data-driven SGN model is proposed to handle various events in the smart home environment. Stochastic Petri Nets (SPNs) and game theory are integrated into our proposed model to build data-driven dynamic SGNs for the smart home environment. Dynamic SGNs for a comprehensive smart home system are generated in real-time through transitions based on sensor data, enhancing interoperability and scalability in smart home environments. We use the Net logo tool and state-of-the-art smart home sensor datasets to generate dynamic SGNs for various events. Experimental results demonstrate the effectiveness of the proposed model within a data-driven smart home environment. It shows that the present work significantly outperforms other state-of-the-art techniques in terms of decision-making at the edge layer. Moreover, using the proposed system the energy efficacy increased to around 39mJ/K nodes, and the average temporal delay for different events was reduced significantly.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Smart homes",
                "Internet of Things",
                "Games",
                "Intelligent sensors",
                "Sensors",
                "Stochastic processes",
                "Game theory"
            ],
            "Author Keywords": [
                "Edge Computing",
                "Stochastic Game Net",
                "Game Theory",
                "Consumer IoT",
                "Data-Driven Modeling"
            ]
        },
        "title": "Data Driven Stochastic Game Network-Based Smart Home Monitoring System Using IoT-Enabled Edge Computing Environments"
    },
    {
        "authors": [
            "Hossein Taghizadeh",
            "Bardia Safaei",
            "Amir Mahdi Hosseini Monazzah",
            "Elyas Oustad",
            "Sahar Rezagholi Lalani",
            "Alireza Ejlali"
        ],
        "published_in": "Published in: IEEE Transactions on Network and Service Management ( Early Access )",
        "date_of_publication": "26 August 2024",
        "doi": "10.1109/TNSM.2024.3450011",
        "publisher": "IEEE",
        "abstract": "RPL is introduced to conduct path selection in Lowpower and Lossy Networks (LLN), including IoT. A routing policy in RPL is governed by its objective function, which corresponds to the requirements of the IoT application, e.g., energy-efficiency, and reliability in terms of Packet Delivery Ratio (PDR). In many applications, it is not possible to connect the nodes to the power outlet. Also, since nodes may be geographically inaccessible, replacing the depleted batteries is infeasible. Hence, harvesters are an admirable replacement for traditional batteries to prevent energy hole problem, and consequently to enhance the lifetime and reliability of IoT networks. Nevertheless, the unstable level of energy absorption in harvesters necessitates developing a routing policy, which could consider harvesting aspects. Furthermore, since the rates of absorption, and consumption are incredibly dynamic in different parts of the network, learningbased techniques could be employed in the routing process to provide energy-efficiency. Accordingly, this paper introduces LANTERN; a learning-based routing policy for improving PDR in energy-harvesting IoT networks. In addition to the rate of energy absorption, and consumption, LANTERN utilizes the remaining energy in its routing policy. In this regard, LANTERN introduces a novel routing metric called Energy Exponential Moving Average (EEMA) to perform its path selection. Based on diversified simulations conducted in Cooja, with prolonging the lifetime of the network by 5.7x, and mitigating the probability of energy hole problem, LANTERN improves the PDR by up to 97%, compared to the state-of-the-art. Also, the consumed energy per successfully delivered packet is reduced by 76%.",
        "issn": {
            "Electronic ISSN": "1932-4537"
        },
        "keywords": {
            "IEEE Keywords": [
                "Routing",
                "Internet of Things",
                "Measurement",
                "Reliability",
                "Energy efficiency",
                "Batteries",
                "Standards"
            ],
            "Author Keywords": [
                "IoT",
                "Routing",
                "RPL",
                "Energy Harvesting",
                "Solar",
                "Reliability",
                "PDR",
                "Energy Consumption",
                "Network Lifetime"
            ]
        },
        "title": "LANTERN:Learning-Based Routing Policy for Reliable Energy-Harvesting IoT Networks"
    },
    {
        "authors": [
            "Tianjiao Du",
            "Xiaolin Gui",
            "Xiaoyu Teng",
            "Kaiyuan Zhang",
            "Dewang Ren"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "03 July 2024",
        "doi": "10.1109/JIOT.2024.3421616",
        "publisher": "IEEE",
        "abstract": "The use of Unmanned Aerial Vehicles (UAVs) is a promising solution for collecting data from wireless Internet of Things (IoT) devices and offloading to mobile edge computing (MEC) server embedded access points (APs) equipped with powerful servers. This paper presents a solution to optimize the energy efficiency of UAV relaying in the IoT system, which assists in programming the multiple UAV flight trajectories and bandwidth allocation schemes to scientifically and energy-efficiently transmit the data from IoT devices to MEC servers. Furthermore, in order to ensure the continuous and effective data relaying of the UAVs, we deploy a number of decentralized wireless charging stations (CSs) in the system to replenish the UAVs’ energy and enable them to provide long-term services. Specifically, we propose a deep reinforcement learning-based efficient IoT data relaying method, where we mainly apply deep deterministic policy gradient (DDPG) to solve this dynamic programming problem with large action spaces. Experimental results demonstrate that the DDPG-based method for UAV efficient data Collection and Offloading (DDPG-UCO) algorithm outperforms other five baseline methods in terms of the UAV energy efficiency, amount of data relayed and data interaction energy consumption rate while maintaining a high level of geographical fairness of the relaying service.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Autonomous aerial vehicles",
                "Servers",
                "Trajectory",
                "Energy efficiency",
                "Task analysis",
                "Relays"
            ],
            "Author Keywords": [
                "Internet of Things (IoT)",
                "UAV flight trajectories",
                "bandwidth allocation",
                "charging stations (CSs)",
                "deep reinforcement learning (DRL)"
            ]
        },
        "title": "Dynamic Trajectory Design and Bandwidth Adjustment for Energy-Efficient UAV-Assisted Relaying With Deep Reinforcement Learning in MEC IoT System"
    },
    {
        "authors": [
            "Xiao Li",
            "Liquan Chen",
            "Ju Jia",
            "Zhongyuan Qin",
            "Zhangjie Fu"
        ],
        "published_in": "Published in: IEEE Transactions on Industrial Informatics ( Early Access )",
        "date_of_publication": "08 October 2024",
        "doi": "10.1109/TII.2024.3449995",
        "publisher": "IEEE",
        "abstract": "Computer vision (CV) applications empower various Internet of Things (IoT) scenarios. However, their advancements in image generation and manipulation tools make it increasingly easy to produce highly deceptive forged images, escalating the risk of image forgery. Cryptography-based methods can secure images but cannot support direct CV applications with compromised visual legibility. Existing generative adversarial network (GAN)-based steganography methods can effectively facilitate CV applications and image forgery prevention with high indistinguishability between stego and cover images. However, they are inefficient in resource-constrained IoT scenarios. Therefore, we propose a lightweight image forgery prevention scheme for IoT using GAN-based steganography. Our scheme embeds identity data within images. If forged, it fails to recover, triggering alerts. Our scheme can significantly improve efficiency with a lightweight generator designed by incorporating blueprint separable convolutions, sum connections and discrete wavelet transform while ensuring high effectiveness. Real-world IoT experimental results demonstrate this.",
        "issn": {
            "Print ISSN": "1551-3203",
            "Electronic ISSN": "1941-0050"
        },
        "keywords": {
            "IEEE Keywords": [
                "Forgery",
                "Discrete wavelet transforms",
                "Steganography",
                "Generators",
                "Prevention and mitigation",
                "Image segmentation",
                "Image coding",
                "Generative adversarial networks",
                "Crops",
                "Convolution"
            ],
            "Author Keywords": [
                "Generative adversarial network (GAN)",
                "image forgery prevention",
                "Internet of Things (IoT)",
                "multimedia security",
                "steganography"
            ]
        },
        "title": "A Lightweight Image Forgery Prevention Scheme for IoT Using GAN-Based Steganography"
    },
    {
        "authors": [
            "Quynh Tu Ngo",
            "Beeshanga Jayawickrama",
            "Ying He",
            "Eryk Dutkiewicz"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "11 November 2024",
        "doi": "10.1109/JIOT.2024.3495567",
        "publisher": "IEEE",
        "abstract": "The advancement of 6G technology significantly enhances the Internet of Things (IoT) applications, especially in remote areas where traditional cellular infrastructure is not feasible. Satellite communication, a crucial component of 6G, extends IoT connectivity to these underserved regions. In this context, the growing interest in low earth orbit (LEO) satellite communication stems from its recent advancements in offering high data rate services and minimizing service latency. Next-generation LEO satellite systems, with regenerative capabilities, allow for adaptability in bandwidth management and on-board data processing. However, the scarcity of satellite spectrum presents a barrier to the expansion of LEO satellite networks and the development of integrated terrestrial-space infrastructures. To address this challenge, we propose constructing a Radio Environment Map (REM) aboard LEO satellites to opportunistically tap into the unused spectrum of geostationary (GEO) satellites within a cognitive GEO-LEO satellite IoT network. This solution facilitates REM construction through collaboration among neighboring LEO satellites while also considering the frequency reuse scheme of GEO satellites. Our REM construction approach leverages cyclostationary-based sensing at LEO satellites, serving the dual purpose of REM construction and Doppler shift estimation to track multiple GEO frequency signals. Following REM construction, LEO satellites utilize deep learning techniques to predict GEO spectrum occupancy without further sensing, thereby optimizing secondary spectrum utilization of the IoT network. We propose a deep learning neural network architecture based on a sequence-to-sequence model tailored for spectrum prediction at LEO satellites. Simulations demonstrate superior performance in detection probability of the proposed deep learning network compared to convolutional long short-term memory networks, achieving this with lower computational complexity.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Low earth orbit satellites",
                "Satellite broadcasting",
                "Sensors",
                "Satellites",
                "Internet of Things",
                "Doppler shift",
                "Deep learning",
                "Structural beams",
                "Satellite communications",
                "Protocols"
            ],
            "Author Keywords": [
                "Cognitive GEO-LEO satellite IoT networks",
                "LEO spectrum sensing",
                "spectrum access",
                "satellite-based REM",
                "deep learning"
            ]
        },
        "title": "A Novel Satellite-Based REM Construction in Cognitive GEO-LEO Satellite IoT Networks"
    },
    {
        "authors": [
            "Bin Li",
            "Yulin Hu",
            "Zhicheng Dong",
            "Erdal Panayirci",
            "Huilin Jiang",
            "Qiang Wu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "09 September 2024",
        "doi": "10.1109/JIOT.2024.3456603",
        "publisher": "IEEE",
        "abstract": "To meet the needs of high energy efficiency (EE) and various heterogeneous services for 6G, in this paper, we probe into the EE of reconfigurable intelligent surfaces (RISs) sub-surface (SSF) architecture-aided cell-free Internet of Things (CF-IoT) networks. Specifically, we jointly optimize the base station (BS)-RIS-IoT device (ID) joint associations, the RIS’s phase shift matrix (PSM), and the BS’s transmit power to enhance CF-IoT’s EE. The elevated complexity (NP-hard) and non-convexity of the formulated problem pose significant challenges, making the solution highly difficult and intricate. To handle this challenging problem, we first develop an alternating optimization framework based on block coordinate descent, which can decouple the original problem into several subproblems. We then carefully design the corresponding low-complexity algorithm for each subproblem to solve it. Moreover, the proposed joint optimization framework serves as a versatile solution applicable to a wide range of scenarios aiming to maximize EE with the assistance of RISs. Simulations confirm that deploying RISs in CF-IoT scenarios is beneficial for improving the EE of the system, and the SSF architecture can further enhance the EE of the system.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Optimization",
                "Internet of Things",
                "6G mobile communication",
                "Complexity theory",
                "Array signal processing",
                "Uplink",
                "Receiving antennas"
            ],
            "Author Keywords": [
                "6G",
                "RIS",
                "SSF Architecture",
                "CF-MMIMO",
                "IOT"
            ]
        },
        "title": "Joint Resource Allocation in Multi-RIS and Massive MIMO Aided Cell-Free IoT Networks"
    },
    {
        "authors": [
            "Yanghe Pan",
            "Zhou Su",
            "Yuntao Wang",
            "Ruidong Li",
            "Yuan Wu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "26 August 2024",
        "doi": "10.1109/JIOT.2024.3439599",
        "publisher": "IEEE",
        "abstract": "Federated learning (FL) has gained widespread adoption in Internet of Things (IoT) applications, promoting the evolution of IoT towards artificial intelligence of Things (AIoT). However, IoT devices are still vulnerable to various privacy inference attacks in FL. While current solutions aim to protect the privacy of devices during model training, the published model is still at risk from external privacy attacks during model deployment. To address the privacy concerns throughout the entire FL lifecycle, this paper proposes a privacy-enhanced and efficient federated knowledge transfer framework for IoT, named PEFKT, which integrates the knowledge transfer method and local differential privacy (LDP) mechanism. In PEFKT, we devise a data diversity-driven grouping strategy to tackle the non-independent and identically distributed (non-IID) issue in IoT. Additionally, we design a quality-aware soft-label aggregation algorithm to facilitate effective knowledge transfer, thereby improving the performance of the student model. Finally, we provide rigorous privacy analysis and validate the feasibility and effectiveness of PEFKT through extensive experiments on real datasets.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Data models",
                "Training",
                "Privacy",
                "Knowledge transfer",
                "Data privacy",
                "Predictive models"
            ],
            "Author Keywords": [
                "Federated learning",
                "knowledge transfer",
                "differential privacy",
                "IoT"
            ]
        },
        "title": "Privacy-Enhanced and Efficient Federated Knowledge Transfer Framework in IoT"
    },
    {
        "authors": [
            "Muhammad Usama Tanveer",
            "Kashif Munir",
            "Madiha Amjad",
            "Syed Ali Jafar Zaidi",
            "Amine Bermak",
            "Atiq Ur Rehman"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "11 November 2024",
        "doi": "10.1109/ACCESS.2024.3495708",
        "publisher": "IEEE",
        "abstract": "The rapid increase in the number of IoT devices has made ensuring robust real-time attack detection more critical than ever. The volume of data being accessed in real-time by these devices presents unique security challenges that traditional detection techniques struggle to address with the required precision and efficiency. To overcome these limitations, we have developed Ensemble-Guard IoT; an innovative ensemble model combining Gaussian Naive Bayes (GNB), Logistic Regression (LR) and Random Forest (RF) through soft voting classifiers. Ensemble learning by combining multiple machine learning models offers a significant advantage in reducing computational costs compared to deep learning models, making it a practical solution for real-time applications. We performed a thorough evaluation of our proposed scheme in terms of accuracy 99.63%, precision1.00%, recall 99%, f1-score 1.00% and computation time 524.40s. We also compared the performance of our scheme with the classical schemes. Our comprehensive evaluation demonstrate that Ensemble-Guard achieves highest average accuracy of 99.63% thus validating the effectiveness of our scheme in identifying IoT attacks in real time.This hybrid voting system combines the predictions from different classifiers, ensuring a more balanced and accurate final decision.Ensemble-Guard IoT is a significant step forward in safeguarding IoT infrastructures, offering a scalable and cost-effective solution to the evolving threat landscape.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Real-time systems",
                "Accuracy",
                "Security",
                "Computational modeling",
                "Telecommunication traffic",
                "Object recognition",
                "Botnet",
                "Bayes methods",
                "Adaptation models"
            ],
            "Author Keywords": [
                "Ensemble-Guard IoT",
                "Ensemble learning",
                "Cybersecurity",
                "Machine learning and Real time attack detection"
            ]
        },
        "title": "Ensemble-Guard IoT: A Lightweight Ensemble Model for Real-Time Attack Detection on Imbalanced Dataset"
    },
    {
        "authors": [
            "Wafaa Anani",
            "Abdelkader Ouda"
        ],
        "published_in": "Published in: IEEE Canadian Journal of Electrical and Computer Engineering ( Early Access )",
        "date_of_publication": "04 October 2024",
        "doi": "10.1109/ICJECE.2024.3409156",
        "publisher": "IEEE",
        "abstract": "The escalating demand for secure communication in the Internet of Things (IoT), particularly in energy-sensitive devices such as smart meters, highlights a critical challenge: achieving robust security without excessive energy consumption. While various solutions have been proposed to minimize energy use, many fail to address the unique constraints of the IoT devices effectively. This article introduces an innovative approach by proposing a secure, lightweight wireless meter-bus (wM-Bus) protocol, specifically designed for the stringent resource constraints of the IoT environments. By incorporating the noise protocol framework (NPF), our protocol significantly reduces computational and power requirements without compromising security integrity. Through a methodical implementation that spanned five distinct phases, including a comparative analysis with the conventional transport layer security (TLS), our findings are compelling. The NPF, particularly with its NX and XX patterns, dramatically surpasses TLS in performance, extending operational lifetimes to approximately 9 and 7.88 years, respectively, in contrast to the 3.81 years offered by TLS. These results not only demonstrate the superior efficiency of the NPF in the IoT settings but also highlight its potential in striking an optimal balance between security and operational longevity.",
        "issn": {
            "Electronic ISSN": "2694-1783"
        },
        "keywords": {
            "IEEE Keywords": [
                "Security",
                "Protocols",
                "Internet of Things",
                "Noise",
                "Cryptography",
                "Meters",
                "Smart meters",
                "Wireless sensor networks",
                "Wireless communication",
                "Public key"
            ],
            "Author Keywords": [
                "Advanced metering infrastructure (AMI)",
                "cryptography",
                "Internet of Things (IoT)",
                "lightweight",
                "noise protocol framework (NPF)",
                "protocol",
                "security",
                "smart metering",
                "wireless meter-bus (wM-Bus)"
            ]
        },
        "title": "A Secure Lightweight Wireless M-Bus Protocol for IoT: Leveraging the Noise Protocol Framework Un protocole Bus-C sans fil léger et sécurisé pour les applications de l’IdO: Exploiter le cadre du protocole Noise"
    },
    {
        "authors": [
            "Shailendra Pratap Singh",
            "Naween Kumar",
            "Norah Saleh Alghamdi",
            "Gaurav Dhiman",
            "Wattana Viriyasitavat",
            "Assadaporn Sapsomboon"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "21 August 2024",
        "doi": "10.1109/TCE.2024.3446988",
        "publisher": "IEEE",
        "abstract": "The data transfer volume is massive in next-generation Wireless Sensor Networks (6G-enabled WSNs) in smart city with consumer electronics-based high communication density, especially for multimedia data. Deploying multiple IoT nodes on such networks makes the process complex and challenging. In such cases, quality of Service (QoS) is critical as it ensures critical network performance and leverages improved end-user experience. There have been some existing heuristic/meta-heuristic works to address the QoS in next-generation WSNs; however, they are sensitive to their parametric values due to a lack of expert knowledge. Some are less robust and less adaptable in dynamic networks due to poorer balanced exploration of the solution space, exploitation of known semi-optimal/optimal solutions, and inefficient resource utilization in constrained environments such as edge devices. The suggested consumer electronics-based research presents an innovative solution’, RL-MODE’, which incorporates Reinforcement Learning-Enhanced Multiobjective Optimisation Algorithms to address QoS management difficulties in edge-enabled WSN-IoT systems. The proposed methodology optimises competing objectives simultaneously, such as minimising energy use and latency while maximizing throughput and coverage, all while keeping the resource-constrained nature of edge devices in mind. The proposed RL-MODE Algorithm comprises Multiobjective Differential Evolution (MODE) Algorithm and a new Reinforcement Learning (RL) adaption technique to develop Pareto-optimal solutions by analysing the complicated linkages between input parameters, edge resources, and QoS parameters. Simulations and experiments with Next-Gen WSN-IoT applications show the effectiveness of the proposed method. This not only improves QoS in WSN-IoT applications, but it also increases resource utilisation and scalability in edge computing settings.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Quality of service",
                "Wireless sensor networks",
                "Optimization",
                "Measurement",
                "Throughput",
                "Heuristic algorithms",
                "Resource management"
            ],
            "Author Keywords": [
                "Consumer electronics",
                "Next-Gen WSN-IoT",
                "Quality of Service",
                "Multi-objective Differential Evolution",
                "Smart City",
                "Reinforcement Learning",
                "Maximized throughput",
                "Reduced energy usage",
                "Reduced latency"
            ]
        },
        "title": "Next-Gen WSN Enabled IoT for Consumer Electronics in Smart City: Elevating Quality of Service Through Reinforcement Learning-Enhanced Multi-Objective Strategies"
    },
    {
        "authors": [
            "Jie Su",
            "Peng Sun",
            "Yuting Jiang",
            "Zhenyu Wen",
            "Fangda Guo",
            "Yiming Wu",
            "Zhen Hong",
            "Haoran Duan",
            "Yawen Huang",
            "Rajiv Ranjan",
            "Yefeng Zheng"
        ],
        "published_in": "Published in: IEEE Transactions on Neural Networks and Learning Systems ( Early Access )",
        "date_of_publication": "23 August 2024",
        "doi": "10.1109/TNNLS.2024.3441597",
        "publisher": "IEEE",
        "abstract": "The rapid growth of the Internet of Things (IoT) has led to the widespread adoption of the IoT networks in numerous digital applications. To counter physical threats in these systems, automatic modulation classification (AMC) has emerged as an effective approach for identifying the modulation format of signals in noisy environments. However, identifying those threats can be particularly challenging due to the scarcity of labeled data, which is a common issue in various IoT applications, such as anomaly detection for unmanned aerial vehicles (UAVs) and intrusion detection in the IoT networks. Few-shot learning (FSL) offers a promising solution by enabling models to grasp the concepts of new classes using only a limited number of labeled samples. However, prevalent FSL techniques are primarily tailored for tasks in the computer vision domain and are not suitable for the wireless signal domain. Instead of designing a new FSL model, this work suggests a novel approach that enhances wireless signals to be more efficiently processed by the existing state-of-the-art (SOTA) FSL models. We present the semantic-consistent signal pretransformation (ScSP), a parameterized transformation architecture that ensures signals with identical semantics exhibit similar representations. ScSP is designed to integrate seamlessly with various SOTA FSL models for signal modulation recognition and supports commonly used deep learning backbones. Our evaluation indicates that ScSP boosts the performance of numerous SOTA FSL models, while preserving flexibility.",
        "issn": {
            "Print ISSN": "2162-237X",
            "Electronic ISSN": "2162-2388"
        },
        "keywords": {
            "IEEE Keywords": [
                "Modulation",
                "Internet of Things",
                "Noise",
                "Task analysis",
                "Feature extraction",
                "Adaptation models",
                "Training"
            ],
            "Author Keywords": [
                "Deep learning",
                "few-shot learning (FSL)",
                "Internet of Things (IoT)",
                "signal processing"
            ]
        },
        "title": "A Semantic-Consistent Few-Shot Modulation Recognition Framework for IoT Applications"
    },
    {
        "authors": [
            "Xianglong Zhang",
            "Feng Li",
            "Huanle Zhang",
            "Haoxin Zhang",
            "Zhijian Huang",
            "Lisheng Fan",
            "Xiuzhen Cheng",
            "Pengfei Hu"
        ],
        "published_in": "Published in: IEEE Transactions on Mobile Computing ( Early Access )",
        "date_of_publication": "24 October 2024",
        "doi": "10.1109/TMC.2024.3486218",
        "publisher": "IEEE",
        "abstract": "Neural network models have become integral to Internet of Things (IoT) systems, with applications spanning from industrial automation to critical infrastructure management. Despite their prevalence, the deployment of these models within IoT systems introduces distinctive security vulnerabilities. In particular, adversaries may execute model poisoning attacks, which aim to alter the decision-making processes of embedded models, leading to erroneous outcomes. Existing model poisoning attacks necessitate access to extensive auxiliary datasets, such as the training dataset itself or one with same distribution. These requirements often render such attacks impractical in IoT contexts, given the constrained storage and computational resources of IoT devices. This paper proposes the first model poisoning attack against interpreters without auxiliary datasets to manipulate the model's behavior. We evaluate the attack on three real-world datasets, and results indicate that this attack can successfully coerce the targeted interpreters to produce outcomes aligned with an adversary's intentions, while maintaining nearly indistinguishable performance from the original model, thereby ensuring its stealthiness. Furthermore, beyond directly affected interpreters, our experiments reveal that four additional interpreters coupled to the poisoned model are indirectly influenced, underscoring the attack's transferability.",
        "issn": {
            "Print ISSN": "1536-1233",
            "Electronic ISSN": "1558-0660"
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling",
                "Data models",
                "Internet of Things",
                "Training",
                "Predictive models",
                "Neural networks",
                "Monitoring",
                "Mobile computing",
                "Dogs",
                "Biomedical monitoring"
            ],
            "Author Keywords": [
                "model poisoning attack",
                "IoT devices",
                "neural networks",
                "interpreter"
            ]
        },
        "title": "Model Poisoning Attack against Neural Network Interpreters in IoT Devices"
    },
    {
        "authors": [
            "Mohammad Bany Taha",
            "Fawaz A. Khasawneh",
            "Ahmad Nahar Quttoum",
            "Muteb Alshammari",
            "Zakaria Alomari"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "05 November 2024",
        "doi": "10.1109/ACCESS.2024.3491951",
        "publisher": "IEEE",
        "abstract": "As the adoption of Internet of Things (IoT) systems, particularly those integrated with cloud technology, continues to expand, ensuring data security and privacy while maintaining optimal performance becomes increasingly challenging. Complex encryption algorithms, when run on IoT devices with limited resources, can significantly hinder processing speed and resource efficiency. This paper introduces an innovative Attribute-Based Encryption (ABE) framework that offloads computationally intensive cryptographic operations to a proxy server. This approach alleviates the computational strain on resource-constrained IoT devices, allowing them to efficiently handle encryption and decryption tasks despite their limited processing power, memory, and battery life. Additionally, we present a robust security model that ensures the privacy and integrity of data in IoT environments, in line with the requirements of ABE. We conduct an extensive performance analysis, evaluating key metrics such as execution time, ciphertext size, and memory usage, demonstrating that our proposed scheme surpasses existing state-of-the-art methods in efficiency. The primary contributions of this work include the development of a lightweight ABE offloading framework, the creation of a strong security model, and a thorough performance assessment that highlights the scheme’s efficiency and practicality for real-world IoT applications.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Encryption",
                "Cryptography",
                "Security",
                "Servers",
                "Data privacy",
                "Protocols",
                "Logic gates",
                "Information technology",
                "Data processing",
                "Data security"
            ],
            "Author Keywords": [
                "Access Policy Attributes",
                "Attribute-Based Encryption (ABE)",
                "Cloud",
                "Computational Overhead",
                "Data Security",
                "Internet of Things (IoT)"
            ]
        },
        "title": "Outsourcing Attribute-Based Encryption to Enhance IoT Security and Performance"
    },
    {
        "authors": [
            "Raju Dhakal",
            "Waleed Raza",
            "Vijayanth Tummala",
            "Laxima Niure Kandel"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "11 November 2024",
        "doi": "10.1109/ACCESS.2024.3495702",
        "publisher": "IEEE",
        "abstract": "Internet of Things (IoT) networks face significant cybersecurity risks primarily due to their extensive connectivity, node heterogeneity, lack of robust security measures, and the considerable amount of sensitive data they accumulate and transmit. Cyber intrusions in IoT networks can result in severe effects, including privacy violations, data breaches, and even physical harm. For traditional centralized learning (CL)-based intrusion detection (ID) and identification methods to work, the local IoT data has to be sent to a third-party central server for training. This uses a lot of bandwidth and poses privacy risks. To deal with this challenge, federated learning (FL) emerges as a promising solution for ID as it enables on-device learning without transmitting private IoT data to a central server. To the best of our knowledge, existing FL-based IDs are limited to binary classification. This paper addresses this limitation by implementing an FL-based ID with multi-class classification of intrusions on the N-BaIoT dataset. Enabling multi-class classification of intrusions allows for implementing more targeted and effective attack-specific countermeasures. We implement multi-class intrusion classification on both CL and FL-based methods (FedAvg and FedAvg+) and focus on key metrics such as accuracy and F1-score. Our results demonstrates that the FedAvg+ approach yields performance comparable to CL while offering added advantage of enhanced privacy. Additionally, the FL-based method outperforms traditional CL, particularly in identifying intrusions from Mirai and Bashlite botnet attacks.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Servers",
                "Data models",
                "Federated learning",
                "Intrusion detection",
                "Accuracy",
                "Training",
                "Sensors",
                "Performance evaluation",
                "Malware"
            ],
            "Author Keywords": [
                "Internet of Things (IoT)",
                "Artificial Neural Network (ANN)",
                "Federated Learning (FL)",
                "Mirai Botnet"
            ]
        },
        "title": "Enhancing Intrusion Detection in IoT Networks Through Federated Learning"
    },
    {
        "authors": [
            "Nikos Andriopoulos",
            "Nikos Kanakaris",
            "Alexios Birbas",
            "Alex Papalexopoulos",
            "Michael Birbas"
        ],
        "published_in": "Published in: IEEE Transactions on Industrial Cyber-Physical Systems ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/TICPS.2024.3490497",
        "publisher": "IEEE",
        "abstract": "The decentralization of the energy grid, coupled with advanced communication technologies (LoRaWan, LTE -4G/5G- etc.), improves its operation and enhances its cyber-physical characteristics, thereby increasing the need for cyber resilience. This paper reviews the benefits of an IoT-based Local Energy Market (LEM), located at the grid's edge, suitable to capitalize on the reduced energy costs and increased flexibility offered by the heavy penetration of Distributed Energy Resources (DERs). LEM utilizes IoT-enabled devices such as DERs, Electric Vehicles (EVs), and smart meters to optimize energy services within user preferences and grid constraints. However, the increase in IoT devices introduces vulnerabilities to cyber-attacks, potentially compromising the LEM's operation and financial stability. We analyze the effects of cyber-risks through three attack scenarios involving False Data Injection on smart meters, Behind The Meter appliances, and photovoltaics, demonstrating possible outcomes like voltage violations and increased operational costs. To improve cyber-resilience, we propose a nodal Distribution Locational Marginal Pricing (DLMP) market architecture that addresses voltage and congestion issues and uses abnormal nodal prices as early attack indicators for swift intrusion detection. The IoT-based nodal market approach secures a robust local energy market structure, highlighting the essential role of cyber-security in modern power grids, and offers uninterrupted operations and enhanced resilience in decentralized energy systems.",
        "issn": {
            "Electronic ISSN": "2832-7004"
        },
        "keywords": {
            "IEEE Keywords": [
                "Security",
                "Resilience",
                "Pricing",
                "Real-time systems",
                "Internet of Things",
                "Cyberattack",
                "Transactive energy",
                "Reviews",
                "Cyber-physical systems",
                "Computer architecture"
            ],
            "Author Keywords": [
                "Cyber-security",
                "DERs",
                "Energy market",
                "Industry 4.0",
                "IoT devices",
                "Local Energy Market",
                "Nodal DSO Energy Market"
            ]
        },
        "title": "Cyber-Resilient Operation of IoT-Enabled Power Grid: A Nodal Local Energy Market Approach"
    },
    {
        "authors": [
            "G. GIAMBENE",
            "E. O. ADDO",
            "Q. CHEN",
            "S. KOTA"
        ],
        "published_in": "Published in: IEEE Transactions on Aerospace and Electronic Systems ( Early Access )",
        "date_of_publication": "01 October 2024",
        "doi": "10.1109/TAES.2024.3471991",
        "publisher": "IEEE",
        "abstract": "This paper deals with a Non-Terrestrial Network (NTN) where an Unmanned Aerial Vehicle (UAV), combined with backhaul satellite connectivity, can provide Internet access to remote areas for a wide range of 5G/6G Internet of Things (IoT) applications. We consider environmental monitoring and smart agriculture applications for unconnected areas where battery-powered sensors on the ground use opportunistic connectivity with a fixed-wing UAV to deliver measurement data to the Internet. Adopting the intermediate NTN layer with the UAV allows low-power transmissions from ground IoT devices. A detailed model has been provided to study the communication from ground low-power IoT devices and the UAV hosting a LoRa Gateway (GW). A modified LoRaWAN access protocol based on beacons transmitted by the GW has been proposed to coordinate the UAV passes with sensor transmissions. An analytical approach has been described to determine the mean delay for detecting an area where sensors report critical measurements. This model, validated by simulations, has allowed optimizing key LoRa access protocol parameters jointly with the UAV speed under sensor energy consumption constraints. The proposed access protocol has been shown to achieve good performance compared to other schemes, thus proving the feasibility of our IoT-NTN system.",
        "issn": {
            "Print ISSN": "0018-9251",
            "Electronic ISSN": "1557-9603"
        },
        "keywords": {
            "IEEE Keywords": [
                "Autonomous aerial vehicles",
                "Sensors",
                "Internet of Things",
                "LoRaWAN",
                "Satellites",
                "Intelligent sensors",
                "Low earth orbit satellites",
                "Batteries",
                "Soil",
                "Sensor systems"
            ],
            "Author Keywords": [
                "IoT",
                "LoRaWAN",
                "NTN",
                "Satellite Networks"
            ]
        },
        "title": "Design and Analysis of Low-Power IoT in Remote Areas with NTN Opportunistic Connectivity"
    },
    {
        "authors": [
            "Amjad Saleem",
            "Sahar Shah",
            "Hasnain Iftikhar",
            "Justyna Zywiołek",
            "Olayan Albalawi"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "28 October 2024",
        "doi": "10.1109/ACCESS.2024.3486927",
        "publisher": "IEEE",
        "abstract": "The Internet of Things (IoT) has revolutionized both professional and personal spheres by enabling the widespread adoption of real-time applications and seamless data transmission over long distances. However, this rapid advancement presents significant challenges, particularly regarding security. IoT devices, often constrained by limited processing capabilities, struggle to implement robust security measures, underscoring the need to address these concerns within the IoT ecosystem. This paper conducts a comprehensive survey of the latest algorithms, techniques, and concepts in IoT, including novel algorithms that have been overlooked by previous studies. The selected literature is categorized based on performance, data security, data quality, and data transmission protocols, thereby identifying opportunities for future research. Additionally, this paper offers a bibliometric overview, providing comprehensive insights that aid researchers, engineers, and scientists in selecting suitable algorithms for specific applications and considering avenues for future improvements.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Data integrity",
                "Protocols",
                "Cleaning",
                "Radiofrequency identification",
                "Wireless sensor networks",
                "Surveys",
                "Data communication",
                "Communication system security",
                "Standards"
            ],
            "Author Keywords": [
                "Data quality techniques",
                "IoT protocols",
                "Data quality dimensions",
                "Systematic review"
            ]
        },
        "title": "A Comprehensive Systematic Survey of IoT Protocols: Implications for Data Quality and Performance"
    },
    {
        "authors": [
            "Likitha Lasantha",
            "Shahed I. Khan",
            "Biplob Ray",
            "Nemai C. Karmakar",
            "Hossein Masoumi",
            "Matthew Josh"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "28 August 2024",
        "doi": "10.1109/JIOT.2024.3450840",
        "publisher": "IEEE",
        "abstract": "Accurate dielectric constant measurements are crucial in Internet of Things (IoT) sensing applications to characterise materials and their properties. This paper introduces an innovative capacitor-based chipless radio frequency identification (RFID) sensory array tailored specifically for precise measurements of dielectric constants in IoT contexts. The array incorporates Pi-shaped resonators and cylindrical capacitors, addressing challenges such as low accuracy, limited εr measuring range, and the reliance on bulky vector network analysers (VNA) as readers. Theoretical modelling, design considerations, sensor calibration, and validation processes are detailed, highlighting the array’s precision and adaptability. Real-world IoT applications are demonstrated, showcasing the array’s potential with a low-cost portable multiple-input multiple-output (MIMO) reader, Walabot. This cost-effective solution overcomes conventional limitations, offering a versatile approach to dielectric constant measurements and opening up new possibilities for diverse IoT sensing applications. The integration of this sensory array with IoT technologies demonstrates the feasibility of smarter material characterisation.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Resonators",
                "Sensors",
                "Internet of Things",
                "Dielectric constant",
                "Inductance",
                "Semiconductor device measurement",
                "Dielectric measurement"
            ],
            "Author Keywords": [
                "Chipless radio frequency identification (RFID)",
                "dielectric constant",
                "Internet of Things (IoT)",
                "relative permittivity",
                "sensory tag",
                "Walabot"
            ]
        },
        "title": "Chipless RFID Sensory Array for IoT Dielectric Sensing and Material Characterisation"
    },
    {
        "authors": [
            "Jia-Chin Lin"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "16 October 2024",
        "doi": "10.1109/JIOT.2024.3481064",
        "publisher": "IEEE",
        "abstract": "For rapid deployment, Internet-of-Things (IoT) technology operating in nonterrestrial networks (NTNs) usually follows 3GPP specifications. The 3GPP aims to establish unified specifications for global narrowband IoT (NB-IoT) operations. This paper investigates a unified random access process for an NB-IoT protocol seamlessly operating between NTNs and terrestrial networks without global navigation satellite system (GNSS) connectivity. For high accuracy and backward compatibility, the common Doppler shift is employed for pre-and post-compensations at a narrowband physical random access channel (NPRACH) receiver; meanwhile, a common propagation delay is employed for pre-time advance (pre-TA) and random access opportunity (RAO) postponement at the NPRACH receiver on the satellite. Derivations of joint maximum likelihood estimation result in a bank of matched filters (MFs), whose outputs are employed to test for the presence of user equipment (UE). The squared magnitudes of the symbol-level matched filter (SLMF) outputs are coherently accumulated to improve the signal-to-interference-plus-noise ratio performance of the proposed NPRACH detector, instead of the conventional extension of the length of the MF impulse response. The residual frequency error and excess timing delay are detected at half of the subcarrier spacing and half of the symbol duration, respectively, via a grid search. The false alarm and miss probabilities are derived in the presence of a high Doppler frequency, long differential delay, random initial phase errors, and different channel gains through which the preambles of multiple UEs propagate. Computer simulations confirm the theoretical analyses and show that the proposed technique performs effective random access tasks with low miss probabilities.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Satellites",
                "Propagation delay",
                "Doppler shift",
                "Delays",
                "3GPP",
                "Uplink",
                "Symbols",
                "Receivers",
                "Narrowband",
                "Global navigation satellite system"
            ],
            "Author Keywords": [
                "NB-IoT",
                "random access",
                "NPRACH",
                "matched filter",
                "false alarm",
                "detection",
                "nonterrestrial network",
                "Doppler shift"
            ]
        },
        "title": "A Unified Random Access Procedure for an NB-IoT Protocol in Nonterrestrial and Terrestrial Networks"
    },
    {
        "authors": [
            "Runchen Xu",
            "Zheng Chang",
            "Zhu Han",
            "Sahil Garg",
            "Georges Kaddoum",
            "Joel J. P. C. Rodrigues"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "14 August 2024",
        "doi": "10.1109/JIOT.2024.3443701",
        "publisher": "IEEE",
        "abstract": "Intergrating multi-access edge computing (MEC) with the Internet of Things (IoT) is able to provide IoT sufficient computational resources in addition to its capabilities of sensing and communication. In this paper, given the limited computational and energy resources, IoT devices (IDs) are allowed to offload computational tasks to MEC severs for execution. However, as the number of IDs increases dramatically, jointly optimizing the usage of sensing, communication and computational resources becomes challenging due to the exponential growth in interactions among the IDs. In this paper, we address the energy-efficient joint optimization problem for sensing and computation in the MEC-assisted IoT system, aiming to ensure the freshness of the status update and minimize the energy consumption of IDs. To reduce the computation complexity, we introduce the concept of the general mean field N-player Markov game (GMFG), and reformulate it as a mean-field game (MFG) with teams, leveraging the network structure of states. Considering the advantages of reinforcement learning (RL) for solving dynamic problems, we propose a MFG based actor-critic algorithm (MFGAC) to minimize the long-term average system cost. Through extensive simulations, we demonstrate that the proposed method is effective and can outperform other schemes under different scenarios.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Internet of Things",
                "Optimization",
                "Energy consumption",
                "Games",
                "Task analysis",
                "Costs"
            ],
            "Author Keywords": [
                "MEC",
                "IoT",
                "energy-efficient",
                "reinforcement learning (RL)",
                "mean-field game (MFG)"
            ]
        },
        "title": "Energy-Efficient Joint Optimization of Sensing and Computation in MEC-Assisted IoT Using Mean-Field Game"
    },
    {
        "authors": [
            "Qian Wang",
            "Xuehang Wang",
            "Han Liu",
            "Yan Wang",
            "Jiadong Ren",
            "Bing Zhang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "11 September 2024",
        "doi": "10.1109/JIOT.2024.3457894",
        "publisher": "IEEE",
        "abstract": "In the field of Internet of Things (IoT), the intrusion detection data is scarce because of the network security and privacy. This paper proposes a domain adaptive IoT intrusion detection algorithm based on GWR-GCN feature extraction and conditional domain adversary, which aims to improve intrusion detection in the IoT domain by learning from other intrusion detection domains with rich data. Firstly, a GWR-GCN based domain-invariant feature extraction method is proposed, where the Growing When Required network (GWR) calculates the correlation between the original data, and the related data is connected into a graph by Hebb learning principle. The Graph Convolutional Neural network (GCN) is used to mine the feature information of the graph-structured data and extract the optimal domain-invariant features. Secondly, a Copula-based data distribution alignment method is proposed to decompose the overall feature distribution difference between the source and target domains into the marginal distribution difference of a single feature and the joint distribution difference between features. Meanwhile, the correlation between features on the data distribution is considered to further reduce the data distribution difference and improve the cross-domain ability. Finally, a conditional domain adversarial intrusion detection model is proposed to improve the detection performance by adding class information as a condition in the discriminator, considering the correlation between features and classes, and reducing the effect of domain shift on distributional alignment. In order to verify the proposed algorithm, experiments are conducted on traditional network and IoT domain datasets, and the superiority is verified on multiple evaluation indicators.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Intrusion detection",
                "Data mining",
                "Internet of Things",
                "Correlation",
                "Adaptation models",
                "Classification algorithms"
            ],
            "Author Keywords": [
                "IoT intrusion detection",
                "domain adaptation",
                "conditional domain adversary",
                "graph convolutional neural network"
            ]
        },
        "title": "A Domain Adaptive IoT Intrusion Detection Algorithm Based on GWR-GCN Feature Extraction and Conditional Domain Adversary"
    },
    {
        "authors": [
            "Amar Rasheed",
            "Mohamed Baza",
            "Gautam Srivastava",
            "Narashimha Karpoor",
            "Cihan Varol"
        ],
        "published_in": "Published in: IEEE Transactions on Network and Service Management ( Early Access )",
        "date_of_publication": "22 August 2024",
        "doi": "10.1109/TNSM.2024.3448312",
        "publisher": "IEEE",
        "abstract": "The rapid proliferation of Unmanned Aircraft Systems (UAS) introduces new threats to national security. UAS technologies have dramatically revolutionized legitimate business operations while providing powerful weaponizing systems to malicious actors and criminals. Due to their inherited wireless capabilities, they are an easy target for cyber threats. In response to this challenge, the implementation of many Intrusion Detection Systems (IDS), which support anomaly detection on UAS, have been proposed in the past. However, such systems often require offline training with heavy processing, making them unsuitable for UAS deployment. This is pertinent for drone systems that support dynamic changes in mission operational tasks. This paper presents a novel system architecture that utilizes sensing systems capabilities available on existing IoT infrastructure for supporting rapid infield adaptive models training and parameters estimation services for UAS. We have devised a cluster-oriented distributed training algorithm based on LSTM with mini-batch gradient descent, with hundreds of IoT platforms per cluster collaboratively performing model parameters estimation tasks. The proposed architecture is based on deploying a multilayer system that facilitates secure dissemination of power consumption behavioral patterns for the flight sensing system between the UAS layer and the IoT layer. The model was implemented and deployed on a real IoT-enabled platform based on NXP-Kinetis K64 ̶ 120 MHz. Furthermore, model training and validation were performed by applying various datasets contaminated with different percentages of malicious data. Our anomaly detection model achieved high prediction accuracy with an ROC-AUC score of 0.9332. The model maintains minimal power consumption overheads and low training time during the processing of a data batch.",
        "issn": {
            "Electronic ISSN": "1932-4537"
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Computational modeling",
                "Adaptation models",
                "Task analysis",
                "Internet of Things",
                "Systems architecture",
                "Long short term memory"
            ],
            "Author Keywords": [
                "UAS",
                "IDS",
                "anomaly detection",
                "IoT",
                "drone"
            ]
        },
        "title": "IoTDL2AIDS: Towards IoT-Based System Architecture Supporting Distributed LSTM Learning for Adaptive IDS on UAS"
    },
    {
        "authors": [
            "Sihui Xue",
            "Huakun Huang",
            "Jia Liu",
            "Qinglin Yang",
            "Lingjun Zhao",
            "Huijun Wu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/JIOT.2024.3485874",
        "publisher": "IEEE",
        "abstract": "The accurate imputation of missing load data in building energy consumption is essential for optimizing energy management and scheduling in internet of things (IoT)-based smart energy management systems. However, in real-world applications, building load data often suffers from the issue of missing critical samples due to IoT device failures and maintenance. To address this problem, we propose an effective scheme by designing a load data augmentation model named the DAM based on deep neural networks. In the DAM, the partial missing data are generated in each round, followed by stacking with the semi-dataset to perform a new generation round. After several rounds, the missing critical load data are recovered with high precision. A building load dataset collected from a real IoT-based energy-efficiency management system is used for evaluation in this work. Experimental results demonstrate that the proposed scheme can effectively replenish the missing critical data and exhibit excellent stability. Additionally, we compare the prediction performance of the DAM approach with other comparison methods. The results show that our proposed approach outperforms the comparison methods, achieving the highest R2 score of 0.963. Hence, the DAM approach presents an effective solution for addressing the problem of missing critical data in IoT-based smart energy management systems, which is vital for optimizing energy dispatch.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Buildings",
                "Dams",
                "Internet of Things",
                "Logic gates",
                "Load modeling",
                "Accuracy",
                "Imputation",
                "Data models",
                "Convolutional neural networks",
                "Time series analysis"
            ],
            "Author Keywords": [
                "Deep Learning",
                "Augmentation",
                "IoT",
                "Energy Consumption",
                "Smart Energy Management"
            ]
        },
        "title": "An Effective Scheme to Solve Critical Data Missing Problems for IoT-Based Smart Energy Management"
    },
    {
        "authors": [
            "Baosheng Li",
            "Weifeng Gao",
            "Jin Xie",
            "Hong Li",
            "Maoguo Gong"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "10 September 2024",
        "doi": "10.1109/JIOT.2024.3457230",
        "publisher": "IEEE",
        "abstract": "We consider a federated semi-supervised learning (FSSL) scenario in heterogeneous Internet of Things (IoT) healthcare systems where only a tiny fraction of IoT healthcare devices possess labels. The challenge is that most IoT devices need label information and exhibit heterogeneous data distributions. This paper proposes a unified FSSL framework (FSSL-HD) tailored for IoT healthcare systems with heterogeneous distributions. The FSSL-HD framework introduces a novel objective function for unlabeled healthcare devices to enhance generalization performance and reduce inter-model discrepancies. A straightforward loss-based aggregation mechanism is designed to reinforce distillation from labeled healthcare devices to supervise unlabeled samples. Multiple iterations for labeled devices and dynamically adjusted confidence thresholds are proposed to improve model performance. Our theoretical analysis of the generalization error in FSSL suggests directions for enhancing performance by improving the self-training capabilities of unlabeled devices and reinforcing the distillation and transfer of supervision signals from labeled devices. Our empirical experiments on federated benchmark datasets and medical image datasets show that FSSL-HD surpasses the performance of state-of-the-art methods. The code is available at https://github.com/baoshengli96/FSSL-HD.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Training",
                "Medical services",
                "Data models",
                "Semisupervised learning",
                "Performance evaluation",
                "Training data"
            ],
            "Author Keywords": [
                "Federated semi-supervised learning",
                "IoT healthcare systems",
                "heterogeneous distribution",
                "imbalanced class distribution",
                "pseudo-labeling"
            ]
        },
        "title": "A Unified Framework for Federated Semi-Supervised Learning in Heterogeneous IoT Healthcare Systems"
    },
    {
        "authors": [
            "Yunkai Wei",
            "Zikang Wan",
            "Yinan Xiao",
            "Supeng Leng",
            "Kezhi Wang",
            "Kun Yang"
        ],
        "published_in": "Published in: IEEE Transactions on Network Science and Engineering ( Early Access )",
        "date_of_publication": "08 October 2024",
        "doi": "10.1109/TNSE.2024.3476168",
        "publisher": "IEEE",
        "abstract": "Unmanned Aerial Vehicles (UAV) can provide mobile edge computing (MEC) service for resource-limited devices in Internet of Things (IoT). In such scenario, partial offloading can be used to balance the computing task between the UAV and the IoT devices for higher efficiency. However, traditional partial offloading is not suitable for training deep neural network (DNN), since DNN models cannot be portioned with a continuous ratio. In this paper, we introduce a split offloading scheme, which can flexibly split the DNN training task into two parts based on the DNN layers, and allocate them to the IoT device and UAV respectively. We present a scheme to synchronize the training and communicating period of DNN layers in the UAV and IoT device, and thus reduce the model training time. Based on this scheme, an optimization model is proposed to minimize the UAV energy consumption, which jointly optimizes the UAV trajectory, the DNN split position and the service time scheduling. We divide the problem into two subproblems and solve it with an iterative solution. Simulation results show the proposed scheme can reduce the model training time and the UAV energy consumption by up to 25% and 14.4% compared with benchmark schemes, respectively.",
        "issn": {
            "Electronic ISSN": "2327-4697"
        },
        "keywords": {
            "IEEE Keywords": [
                "Autonomous aerial vehicles",
                "Training",
                "Internet of Things",
                "Artificial neural networks",
                "Servers",
                "Collaboration",
                "Synchronization",
                "Pipelines",
                "Energy consumption",
                "Optimization models"
            ],
            "Author Keywords": [
                "DNN",
                "split offloading",
                "UAV",
                "IoT",
                "trajectory optimization"
            ]
        },
        "title": "Joint Split Offloading and Trajectory Scheduling for UAV-enabled Mobile Edge Computing in IoT Network"
    },
    {
        "authors": [
            "Kun Cao",
            "Mingsong Chen",
            "Stamatis Karnouskos",
            "Shiyan Hu"
        ],
        "published_in": "Published in: IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems ( Early Access )",
        "date_of_publication": "02 August 2024",
        "doi": "10.1109/TCAD.2024.3437344",
        "publisher": "IEEE",
        "abstract": "Over the past few years, the integration of mobile edge computing and serverless computing, known as serverless mobile edge computing (SMEC), has garnered considerable attention. Despite abundant existing works on SMEC exploration, there remains an unaddressed gap in guaranteeing dependable application outputs due to ignoring the threat of both soft and bit errors on SMEC infrastructures. Furthermore, existing works fall short of accommodating the personalized requirements and approximate computation of Internet-of-things (IoT) applications, thereby resulting in holistic quality-of-service (QoS) degradation of SMEC systems typically provisioned by limited edge resources. In this paper, we investigate the reliability-aware personalized deployment of approximate computation IoT applications for QoS maximization in SMEC environments. To this end, we propose a hybrid methodology composed of offline and online optimization phases. At the offline phase, a decomposition-based function placement method is devised to accomplish function-to-server mapping by integrating convex optimization, cross-entropy method, and incremental control techniques. At the online phase, a lightweight reinforcement learning scheme based on proximal policy optimization (PPO) is developed to handle the inherent dynamicity of IoT applications. We also build a simulation platform upon the real-world base station distribution in Shanghai Telecom and the practical cluster trace in the Alibaba open program. Evaluations demonstrate that our hybrid approach boosts the holistic QoS by 63.9% compared with the state-of-the-art peer algorithms.",
        "issn": {
            "Print ISSN": "0278-0070",
            "Electronic ISSN": "1937-4151"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Quality of service",
                "Reliability",
                "Servers",
                "Dynamic programming",
                "Heuristic algorithms",
                "Dynamic scheduling"
            ],
            "Author Keywords": [
                "Serverless mobile edge computing",
                "approximate computation",
                "personalized IoT deployment",
                "reliability"
            ]
        },
        "title": "Reliability-Aware Personalized Deployment of Approximate Computation IoT Applications in Serverless Mobile Edge Computing"
    },
    {
        "authors": [
            "Mohammadali Farahpoor",
            "Oscar Esparza",
            "Miguel Soriano"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "18 December 2023",
        "doi": "10.1109/ACCESS.2023.3343920",
        "publisher": "IEEE",
        "abstract": "In the context of fleet management, various challenges, including equipment breakdowns, rising maintenance costs, inefficient resource utilization, and outdated telematics systems, necessitate a transformative approach. Traditional telematics systems encounter limitations such as closed compatibility with specific brands, unique network protocols, and insufficient data analysis and decision-making support. Therefore, these challenges can be addressed by using IoT-driven solutions. This paper introduces an IoT-driven system called the \"intelligent dispatching and health monitoring system\" (IDHMS), which is designed to enhance fleet operations in industries such as mining, construction, and agriculture while addressing issues related to legacy telematics systems. The architecture of IDHMS is a comprehensive IoT-driven framework that integrates embedded hardware, cloud-based software, and flexible network infrastructure. This system facilitates real-time communication, gathering and analysis of data, and decision-making for fleet management in such industries. The IDHMS addresses the limitations of conventional telematics systems by offering standardized protocols, open APIs, robust security measures, comprehensive data analytics capabilities, and a multipurpose ecosystem. Its data-driven insights empower informed decision-making, drive continuous improvement, and enable strategic resource allocation for cost reduction, increased productivity, and competitiveness in dynamic industries. The practical implementation of IDHMS in significant Middle Eastern mines underscores its compatibility with multiple brands, scalability, and potential adoption in various industries.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Telematics",
                "Monitoring",
                "Data mining",
                "Real-time systems",
                "Protocols",
                "Machinery",
                "Dispatching",
                "Cloud computing",
                "Embedded systems",
                "Internet of Things",
                "Inventory management",
                "Vehicles"
            ],
            "Author Keywords": [
                "Cloud computing",
                "embedded systems",
                "fleet management",
                "IoT",
                "telematics"
            ]
        },
        "title": "Comprehensive IoT-driven Fleet Management System for Industrial Vehicles"
    },
    {
        "authors": [
            "Archana Ojha",
            "Sahil Manikchand Chaudhari",
            "Prasenjit Chanak"
        ],
        "published_in": "Published in: IEEE Transactions on Sustainable Computing ( Early Access )",
        "date_of_publication": "17 September 2024",
        "doi": "10.1109/TSUSC.2024.3462512",
        "publisher": "IEEE",
        "abstract": "Nowadays, the Internet of Things (IoT) plays a significant role in the development of various real-life applications such as smart cities, healthcare, precision agriculture, and industrial automation. Wireless Sensor Networks (WSNs) are a major ingredient of these IoT-based applications. In WSNs, sensor nodes that are close to the Base Station (BS) relay more data packets compared to other nodes, which creates high energy consumption at nodes close to the BS. As a result, an energy imbalance is created among the sensor nodes. Therefore, sensor nodes close to BS die early as compared to the faraway sensor nodes. These early dead nodes drastically increase data collection delay within the network. Furthermore, the early death of the sensor nodes partitions the network into different isolated sub-networks/segments. The formation of isolated segments causes premature death of the network. This paper proposes a Deep Policy Dynamic Programming (DPDP) based intelligent data routing scheme for IoT-enabled WSNs. The proposed scheme identifies an optimal number of Cluster Heads (CHs) and forms clusters to reduce the energy consumption of the deployed sensor nodes and prevent the early death of sensor nodes. Furthermore, the proposed scheme identifies an optimal number of Rendezvous Points (RPs) and designs an optimal path for Mobile Sink (MS) based data collection. Optimal RP selection and path design algorithms prevent the premature death of the network and significantly improve the overall performance of the network. Extensive simulations and test-bed experiments are conducted to test the performance of the proposed scheme. The simulation and test-bed results show that the proposed scheme outperforms as compared to the existing state-of-the-art approaches in terms of network lifetime, network stability, data loss due to buffer overflow, residual energy, and delay.",
        "issn": {
            "Electronic ISSN": "2377-3782"
        },
        "keywords": {
            "IEEE Keywords": [
                "Wireless sensor networks",
                "Data collection",
                "Delays",
                "Clustering algorithms",
                "Routing",
                "Internet of Things",
                "Buffer overflows"
            ],
            "Author Keywords": [
                "Data routing",
                "deep policy dynamic programming (DPDP)",
                "heterogeneous wireless sensor networks (HWSNs)",
                "internet of things (IoT)",
                "mobile sink (MS)",
                "rendezvous points (RPs)"
            ]
        },
        "title": "A Deep Policy Dynamic Programming Based Intelligent Data Routing Scheme for IoT-Enabled Wireless Sensor Networks"
    },
    {
        "authors": [
            "Pratik Chakraborty",
            "Amrita Mukherjee",
            "Shankar Prakriya"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "02 September 2024",
        "doi": "10.1109/JIOT.2024.3453029",
        "publisher": "IEEE",
        "abstract": "This paper presents an optimization strategy for an Industrial Internet of Things (IoT) configuration, with particular emphasis on the enhancement of spectrum efficiency and reliable connectivity. Our analysis delves into the intricacies of a communication model designed to facilitate the concurrent exchange of information between sensor-cum-actuator nodes and a controlling station through the shared spectrum of a licensed cellular network. Pairing between the controller and any sensor-cum-actuator node is facilitated by a simple opportunistic selection mechanism that does not involve additional channel overheads. To improve spectrum efficiency, all IoT nodes are equipped with full-duplex (FD) functionality, while reliability is improved by making the network adaptive. The transmit powers of IoT nodes are chosen by apportioning the interference temperature limit (ITL), in a manner that cumulative interference caused to the cellular network does not exceed the ITL. Under this constraint, the random transmit powers induce significant SNR variations, which we exploit through adaptive network strategies involving careful choice of transmit powers and switching between FD and half-duplex (HD) modes, for optimal performance. We establish that adaptation is more effective when assisted by instantaneous channel knowledge, which leads to the evolution of the statistically optimum scheme (SOS), local channel aware scheme (LCAS), and the global channel aware scheme (GCAS) in this paper. Analysis is presented for network throughput for each of the evolved schemes, and critical parameters responsible for adaptation are presented in closed-form. The accuracy of our analytical findings is verified through computer simulations.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Interference",
                "Switches",
                "Resource management",
                "Throughput",
                "Reliability",
                "Actuators",
                "Industrial Internet of Things"
            ],
            "Author Keywords": [
                "Communications and Networking for IoT",
                "Resource-Constrained Networks",
                "Sensor and Actuator Networks",
                "Spectrum Sharing and Cognitive Radio"
            ]
        },
        "title": "Novel Power Control With Duplex Mode Selection Schemes in Two-Way IoT-Type Communication"
    },
    {
        "authors": [
            "Zhou Fang",
            "Shuping Dang",
            "Yang Yang",
            "Zhihui Ge",
            "Xiangcheng Li",
            "Zhenrong Zhang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "06 November 2024",
        "doi": "10.1109/JIOT.2024.3492321",
        "publisher": "IEEE",
        "abstract": "The imminent deployment of sixth-generation (6G) wireless communication systems promises new opportunities and challenges for model training using data from edge devices in the Internet of Things (IoT). However, current research has yet to fully address the efficiency and scalability challenges arising from the extensive connectivity of edge devices across various scenarios. The presence of malicious devices further intensifies system uncertainty during large-scale data interactions and model training, making it difficult for a single model to effectively manage the complexities introduced by heterogeneous devices and dynamic network conditions. To overcome these challenges, we propose FedSC, an innovative edge computing framework that leverages side-chain technology for efficient edge node management and employs federated learning to enable robust cross-device and cross-scenario model interactions. To accelerate the multi-model aggregation process, we introduce an asynchronous cross-domain iterative algorithm (ACDI) based on smart contracts. Additionally, to mitigate the impact of malicious and inactive nodes, we propose a robust consensus algorithm and a committee mechanism for leader node election based on contribution value. Experimental results demonstrate that the proposed FedSC achieves a 3.2% and 44.23% accuracy improvement on i.i.d. and non-i.i.d. dataset, respectively, along with a remarkable latency reduction of 256.51%, compared to FedAvg. Our work is conducive to the training of multiple models in different IoT scenarios, utilizing substantial amounts of IoT device data and facilitating collaboration between models. Furthermore, it enables the provision of fundamental services to diverse applications in 6G.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Federated learning",
                "Computational modeling",
                "6G mobile communication",
                "Data models",
                "Edge computing",
                "Blockchains",
                "Performance evaluation",
                "Training",
                "Collaboration"
            ],
            "Author Keywords": [
                "Federated learning",
                "edge computing",
                "blockchain",
                "side-chain",
                "Internet of Things (IoT)",
                "6G communications"
            ]
        },
        "title": "FedSC: A Sidechain-Enhanced Edge Computing Framework for 6G IoT Multiple Scenarios"
    },
    {
        "authors": [
            "Pengcheng Guo",
            "Miao Yu",
            "Wanli Ni",
            "Kang An",
            "Miaomiao Gu"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "09 August 2024",
        "doi": "10.1109/TCE.2024.3441028",
        "publisher": "IEEE",
        "abstract": "In the Internet of Things (IoT), the proliferation of smart, interconnected consumer electronics (CE) has heightened the demand for reliable signal transmission. However, noise interference continues to be a critical challenge that can impact the overall performance of IoT-enabled devices and the stability of communication. This study introduces sub-sampling denoising compensation network (SDCN) engineered to enhance signal reliability within the IoT ecosystem. SDCN employs a self-supervised learning approach, eliminating the need for traditional paired training datasets. It incorporates a sub-sampler to create training signal pairs, a denoising network for the noise reduction, and a signal residual compensation module to preserve the original signal’s characteristics. This comprehensive solution ensures that signals transmitted between devices remain complete and accurate, even in the presence of substantial noise. The framework’s effectiveness is validated through a series of experiments, demonstrating its superiority in terms of signal transmission reliability.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Noise reduction",
                "Noise measurement",
                "Noise",
                "Training",
                "Reliability",
                "Consumer electronics",
                "Internet of Things"
            ],
            "Author Keywords": [
                "Internet of Things (IoT)",
                "consumer electronics",
                "self-supervised learning",
                "signal transmission reliability"
            ]
        },
        "title": "IoT-ReliableComm: A Self-Supervised Approach to Signal Transmission Reliability in Interconnected Consumer Electronics"
    },
    {
        "authors": [
            "Muhammad Shafiq",
            "Zhihong Tian",
            "Yuan Liu",
            "Ahamed Aljuhani",
            "Yang Li"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "24 October 2023",
        "doi": "10.1109/TCE.2023.3327136",
        "publisher": "IEEE",
        "abstract": "In the emerging era of the tactile Internet of Things (IoT) for consumer electronics, achieving seamless connectivity and efficient resource allocation poses critical challenges. To address these challenges and enhance resource allocation in tactile IoT for consumer electronics, in this paper, we first introduce an approach, \"ESC&RAO\" (Enabling Seamless Connectivity Resource Allocation), optimizing resource allocation strategies to improve overall system performance and user experience. Then based on the proposed ESC&RAO framework, we propose a novel algorithm that dynamically selects the most valuable resources within the tactile IoT network based on specified criteria. The algorithm leverages advanced optimization techniques and real-time data analysis to intelligently manage and select resource allocation, considering specified criteria, network conditions, and user requirements. However, from the analysis of the results, it is evident that our proposed method is effective and received effective performance results.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Resource management",
                "Internet of Things",
                "Consumer electronics",
                "System performance",
                "User experience",
                "Optimization",
                "Heuristic algorithms"
            ],
            "Author Keywords": [
                "Tactile IoT",
                "Selection",
                "Resources Allocations",
                "Consumer Electronics"
            ]
        },
        "title": "ESC&RAO: Enabling Seamless Connectivity Resource Allocation in Tactile IoT for Consumer Electronics"
    },
    {
        "authors": [
            "Enrico Testi",
            "Enrico Paolini"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "30 September 2024",
        "doi": "10.1109/JIOT.2024.3470113",
        "publisher": "IEEE",
        "abstract": "We investigate the packet collision probability among uncoordinated devices, in the uplink of DtS-IoT. Both the satellite spot shape and its motion along its orbital path are considered in the analysis. We analyze the probability of no uplink collision under two DtS-IoT settings: (i) unconfirmed ALOHA over a single channel, e.g., LoRaWAN class A with LoRa-CSS modulation; (ii) unconfirmed ALOHA with frequency hopping compliant with LoRaWAN class A with LR-FHSS. A closed-form solution is derived for the former, while an upper bound is found for the latter. The analytical results are validated by comparison with the outcomes of extensive simulations, showing that the obtained closed-form expressions accurately predict the probability of no collision. Moreover, the upper bound for the frequency hopping case is proven to be tighter when there are less than 35 hopping channels. Finally, we provide a concise performance comparison between LoRa-CSS and LR-FHSS in DtS-IoT, showing that LR-FHSS significantly increases the number of devices that can simultaneously transmit during a satellite pass, thereby enhancing uplink capacity.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Satellites",
                "Satellite broadcasting",
                "Uplink",
                "Shape",
                "Low earth orbit satellites",
                "Upper bound",
                "Frequency modulation",
                "Performance evaluation",
                "LoRaWAN"
            ],
            "Author Keywords": [
                "Direct-to-satellite IoT",
                "LEO satellite",
                "machine-to-machine communications",
                "frequency hopping spread spectrum",
                "Internet of Remote Things"
            ]
        },
        "title": "Packet Collision Probability of Direct-to-Satellite IoT Systems"
    },
    {
        "authors": [
            "GawnYong Park",
            "A Saranya",
            "M Karuppasamy",
            "JungYoon Kim"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "08 July 2024",
        "doi": "10.1109/TCE.2024.3424253",
        "publisher": "IEEE",
        "abstract": "Technological advancements in the Internet of Things (IoT) have transformed traditional Consumer Electronics (CE) into next-generation devices with enhanced connectivity and intelligence. This networked connectivity among sensors, actuators, appliances, and other consumer devices improves data availability and enables automatic control within the CE network. However, the diversity, decentralization, and proliferation of CE devices have exponentially increased data traffic. Additionally, traditional static network infrastructure approaches require manual configuration and exclusive management of these devices. Our research introduces a new way to improve the Quality of Service (QoS) in Consumer Electronics. The proposed work uses advanced technologies like Hopfield Networks for managing resources in real-time, Split Learning for secure distributed learning, and Enhanced Split Hyena Optimized Routing for faster data transfer. Hopfield Networks help allocate resources on the fly, resulting in better speed, efficiency, and less power usage. While Split Learning (SL) has been suggested for training models with limited resources, it is not widely used in decentralized and resource-limited IoT devices. Our proposed approach tackles the challenges in CE using Hopfield Networks, Split Learning, and Enhanced Split Hyena Optimized Routing. Our method achieves an impressive 95% success rate in delivering data, showing that it’s effective for reliable and efficient IoT communication.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Quality of service",
                "Routing",
                "Consumer electronics",
                "Task analysis",
                "Hopfield neural networks",
                "Computer crime"
            ],
            "Author Keywords": [
                "Consumer Electronics",
                "Smart Application",
                "Enhanced Split Hyena Optimized Routing",
                "Hopfield Learning",
                "Internet of Things (IoT)",
                "Quality of Service (QoS)",
                "Split Learning"
            ]
        },
        "title": "Enhancing Quality of Service for IoT Application in Smart Cities: A Hybrid Split Learning & Optimized Routing Approach"
    },
    {
        "authors": [
            "Haitham H. Esmat",
            "Xiaohao Xia",
            "Yinxuan Wu",
            "Beatriz Lorenzo",
            "Linke Guo"
        ],
        "published_in": "Published in: IEEE/ACM Transactions on Networking ( Early Access )",
        "date_of_publication": "09 September 2024",
        "doi": "10.1109/TNET.2024.3436712",
        "publisher": "IEEE",
        "abstract": "Heterogeneous Internet of Things (IoT) networks, which operate using various protocols and spectrum bands like WiFi, Bluetooth, Zigbee, and LoRa, bring many opportunities to collaborate and achieve timely data collection. However, several challenges must be addressed due to heterogeneous data patterns, coverage, spectrum bands, and mobility. This paper introduces a cross-technology IoT network architecture design that facilitates collaboration between service providers (SPs) to share their spectrum bands and offload computing tasks from heterogeneous IoT devices using multi-protocol mobile gateways (M-MGs). The objective is to minimize the age of information (AoI) and energy consumption by jointly optimizing collaboration between M-MGs and SPs for bandwidth allocation, relaying, and cross-technology data scheduling. A pricing mechanism is presented to incentivize different levels of collaboration and matching between M-MGs and SPs. Given the uncertainty due to mobility and task requests, we design a cross-technology federated matching algorithm (CT-Fed-Match) based on a multi-agent actor-critic approach in which M-MGs and SPs learn their strategies in a distributed manner. Furthermore, we incorporate federated learning to enhance the convergence of the learning process. The numerical results demonstrate that our CT-Fed-Match-RC algorithm with cross-technology and relaying collaboration reduces the AoI by 30 times and collects 8 times more packets than existing approaches.",
        "issn": {
            "Print ISSN": "1063-6692",
            "Electronic ISSN": "1558-2566"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Collaboration",
                "Protocols",
                "Pricing",
                "Resource management",
                "Wireless communication",
                "Network architecture"
            ],
            "Author Keywords": [
                "Age of information (AoI)",
                "cross-technology offloading",
                "federated learning",
                "heterogeneous IoT",
                "multi-agent deep reinforcement learning",
                "mobile edge computing"
            ]
        },
        "title": "Cross-Technology Federated Matching for Age of Information Minimization in Heterogeneous IoT"
    },
    {
        "authors": [
            "Jinghai Duan",
            "Jun Tao",
            "Dingwen Chi",
            "Xiaoqian Li",
            "Yifan Xu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "19 September 2024",
        "doi": "10.1109/JIOT.2024.3464100",
        "publisher": "IEEE",
        "abstract": "Zero-power communication, enabled by energy harvesting, backscattering, and low-power computing, is capable of fulfilling the requirements of emerging Internet of Things (IoT) communication scenarios that demand low cost, compact size, and minimal power consumption. Thus, it holds great potential as a transformative technology for the future of IoT. Trusted access and secure transmission remain essential in zero-power communication scenarios. Nevertheless, conventional complex security mechanisms become impractical due to limited power consumption and resources. This work presents a lightweight security protocol for authentication. Initially, a sliding window algorithm, utilizing the Hamming distance, is designed to generate the message digest. This algorithm leverages the remaining electric quantity of the transmitter as a secret parameter for authentication. Subsequently, a key distribution function based on the hash chain is employed to ensure the security of the session key. The protocol’s security attributes regarding transmitted data and its ability to withstand common attacks are demonstrated through formal security analysis and the utilization of the ProVerif analysis tool. Extensive simulations validate the efficacy of the proposed security algorithms, which are well-suited for lightweight IoT devices with severely constrained resources and outperform benchmark algorithms.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Authentication",
                "Protocols",
                "Security",
                "Receivers",
                "Internet of Things",
                "Physical unclonable function",
                "Backscatter"
            ],
            "Author Keywords": [
                "Authentication protocol",
                "backscatter",
                "Internet of Things (IoT)",
                "lightweight",
                "zero-power communication"
            ]
        },
        "title": "Towards Energy Variations for IoT Lightweight Authentication in Backscatter Communication"
    },
    {
        "authors": [
            "Hiroshi Yamamoto",
            "Keigo Shimatani",
            "Keigo Uchiyama"
        ],
        "published_in": "Published in: IEICE Transactions on Communications ( Early Access )",
        "date_of_publication": "22 August 2024",
        "doi": "10.23919/transcom.2024CEI0009",
        "publisher": "IEICE",
        "abstract": "In order to support a person's various activities (e.g., primary industry, elderly care), an IoT (Internet of Things) system supporting sensing technologies that observe the status of various people, things, and places in the real world is attracting attention. The existing studies have utilized the camera device for the sensing technology to observe the condition of the real world, but the use of the camera device has problems related to the limitation of the observation period and the invasion of privacy. Therefore, new IoT systems utilizing three-dimensional LiDAR (Light Detection And Ranging) have been proposed because they can obtain three-dimensional spatial information about the real world by solving problems. However, several problems exist with the use of 3D LiDAR in the deployment on the real fields. The 3D LiDAR requires much electric power for observing the three-dimensional spatial information. In addition, the annotation process of a large volume of point-cloud data for constructing a machine-learning model requires significant time and effort. Therefore, in this study, we propose IoT systems utilizing 3D LiDAR for observing the status of targets and equipping them with new technologies to improve the practicality of the use of 3D LiDAR. First, a linkage function is designed to achieve power saving for an entire system. The linkage function operates only a sensing technology with low power consumption during normal operation and activates the 3D LiDAR with high power consumption only when it is estimated that an observation target is approaching. Second, a self-learning function is built to analyze the data collected by not only the 3D LiDAR but also the camera for automatically generating a large amount of training data with the correct label which is estimated by analyzing the camera images. Through the experimental evaluations using a prototype system, it is confirmed that the sensing technologies can correctly be interconnected to reduce the total ...",
        "issn": {
            "Electronic ISSN": "1745-1345",
            "Print ISSN": "0916-8516"
        },
        "keywords": {
            "IEEE Keywords": [
                "Laser radar",
                "Three-dimensional displays",
                "Sensors",
                "Animals",
                "Machine learning",
                "Cameras",
                "Servers"
            ],
            "Author Keywords": [
                "IoT",
                "3D LiDAR",
                "point-cloud analysis",
                "machine learning"
            ]
        },
        "title": "Design of IoT systems using three-dimensional spatial information and technologies for improving usability in actual fields"
    },
    {
        "authors": [
            "Xin Wang",
            "Jianhui Lv",
            "Byung-Gyu Kim",
            "Carsten Maple",
            "B.D. Parameshachari",
            "Adam Slowik",
            "Keqin Li"
        ],
        "published_in": "Published in: IEEE Transactions on Cloud Computing ( Early Access )",
        "date_of_publication": "12 September 2024",
        "doi": "10.1109/TCC.2024.3459789",
        "publisher": "IEEE",
        "abstract": "The proliferation of multimedia-enabled IoT devices and edge computing enables a new class of data-intensive applications. However, analyzing the massive volumes of multimedia data presents significant privacy challenges. We propose a novel framework called generative adversarial privacy (GAP) that leverages generative adversarial networks (GANs) to synthesize privacy-preserving surrogate data for multimedia analytics across the IoT-Edge continuum. GAP carefully perturbs the GAN's training process to provide rigorous differential privacy guarantees without compromising utility. Moreover, we present optimization strategies, including dynamic privacy budget allocation, adaptive gradient clipping, and weight clustering to improve convergence and data quality under a constrained privacy budget. Theoretical analysis proves that GAP provides rigorous privacy protections while enabling high-fidelity analytics. Extensive experiments on real-world multimedia datasets demonstrate that GAP outperforms existing methods, producing high-quality synthetic data for privacy-preserving multimedia processing in diverse IoT-Edge applications.",
        "issn": {
            "Electronic ISSN": "2168-7161"
        },
        "keywords": {
            "IEEE Keywords": [
                "Data privacy",
                "Differential privacy",
                "Streaming media",
                "Generative adversarial networks",
                "Internet of Things",
                "Privacy",
                "Training"
            ],
            "Author Keywords": [
                "Multimedia data",
                "generative adversarial privacy",
                "generative adversarial networks",
                "IoT-Edge continuum"
            ]
        },
        "title": "Generative Adversarial Privacy for Multimedia Analytics Across the IoT-Edge Continuum"
    },
    {
        "authors": [
            "Walid Osamy",
            "Ahmed M. Khedr",
            "Ahmed Salim"
        ],
        "published_in": "Published in: IEEE Transactions on Sustainable Computing ( Early Access )",
        "date_of_publication": "24 June 2024",
        "doi": "10.1109/TSUSC.2024.3418136",
        "publisher": "IEEE",
        "abstract": "Load balancing in IoT-based Wireless Sensor Networks (WSNs) is essential for improving energy efficiency, reliability, and network lifetime, promoting the development of smart and sustainable cities through informed decision-making and resource optimization. This paper introduces a Workload Aware Clustering Technique (WLACT) to enhance energy efficiency and extend the network lifespan of IoT-based WSNs. WLACT focuses on overcoming challenges such as uneven workload distribution and complex scheme designs in existing clustering methods, highlighting the importance of load balancing, optimized data aggregation, and effective energy resource management in IoT-based heterogeneous WSNs. WLACT adapts Chicken Swarm Optimization (CSO) for efficient workload-aware clustering of WSNs, while also introducing the concept of average imbalanced workload parameter for clustered WSNs and utilizing it as an evaluation metric. By considering node heterogeneity and formulating an objective function to minimize workload imbalances among nodes during clustering, WLACT aims to achieve efficient energy resource utilization, improved reliability, and long-term operational support within smart city environments. A new cluster joining procedure for non-CHs based on multiple factors is also designed. Results reveal the superior performance of WLACT in terms of energy efficiency, workload balance, reliability, and network lifetime, making it a promising technique for sustainable smart city development.",
        "issn": {
            "Electronic ISSN": "2377-3782"
        },
        "keywords": {
            "IEEE Keywords": [
                "Wireless sensor networks",
                "Clustering algorithms",
                "Optimization",
                "Smart cities",
                "Load management",
                "Energy efficiency",
                "Energy resources"
            ],
            "Author Keywords": [
                "Data collection",
                "internet of things (IoT)",
                "load balancing",
                "sustainable smart city",
                "sustainable urbanization",
                "urban problems",
                "wireless sensor network"
            ]
        },
        "title": "Design Workload Aware Data Collection Technique for IoT-enabled WSNs in Sustainable Smart Cities"
    },
    {
        "authors": [
            "Zekui Liu",
            "Peifu Chen",
            "Zongshan Wang",
            "Ali Ala",
            "Mohamed Shakeel Pethuraj"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "16 October 2024",
        "doi": "10.1109/JSEN.2024.3477960",
        "publisher": "IEEE",
        "abstract": "Scenario: The Internet of Things(IoT) has revolutionized the agricultural sector, enabling precision farming methods that optimize utilization of resources and improve crop yields. One significant aspect of this synergy is the use of smart sensors to monitor and control various environmental parameters, such as temperature, humidity, and pH levels. Maintaining rapid sensor response and accuracy is crucial for achieving precise agricultural management. Problem Specification: In IoT-based systems, maintaining precise pH sensing capabilities is crucial but often challenged by environmental factors such as temperature variations, leading to tolerance gradation errors beyond the [-0.01, +0.01] acceptable ranges, impeding timely remote adjustments. The process of quickly balancing the crop condition from a remote place becomes burdensome during these scenarios. Solution: For reducing the tolerance gradation error, the research introduces Optimized pH Sensing Synergy Alignment (OpHSS) with a trans-classifier learning network for monitoring and correcting the actual sensing ranges. Through iterative adjustments and independent functions across hidden layers, the proposed OpHSS aims for faster convergence to acceptable ranges. Thus, the model achieves faster convergence towards standardized tolerance levels, effectively minimizing sensor response times and boosts the overall precision and efficiency in precision farming practices Convergence: The network uses trans iterations to gradually correct ahead and behind gradation errors, ensuring they converge towards a standard tolerance value within the acceptable range. Key Improvements: OpHSS reduces sensor response time, quick convergence, and improved pH monitoring accuracy. It also improves crop yields and resource management by enhancing pH sensing synergy in precision farming using IoT backbone networks.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Intelligent sensors",
                "Temperature sensors",
                "Soil",
                "Farming",
                "Smart agriculture",
                "Accuracy",
                "Monitoring",
                "Sensor systems",
                "Robot sensing systems"
            ],
            "Author Keywords": [
                "IoT Backbone Networks",
                "pH Sensing",
                "Smart Sensors",
                "Precision Farming",
                "Tolerance Gradation Error"
            ]
        },
        "title": "OpHSS: Optimized pH Sensing Synergy Alignment in IoT Backbone Networks for Precision Farming"
    },
    {
        "authors": [
            "Sandeep Mahato",
            "Mohammad S. Obaidat",
            "Subrata Dutta",
            "Debasis Giri",
            "M. Shamim Hossain"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "11 September 2024",
        "doi": "10.1109/JIOT.2024.3452421",
        "publisher": "IEEE",
        "abstract": "The increasing prevalence of malicious activities in IoT-enabled healthcare and Internet of Medical Things (IoMT) systems necessitates robust intrusion detection mechanisms. This article introduces a novel approach combining meta-heuristic optimization and machine learning techniques to analyze network traffic for enhanced detection accuracy. Our proposed method utilizes eleven chaotic maps and the K-Nearest Neighbor (KNN) algorithm to identify malicious activity in IoMT and IoT network systems. Recognizing the significance of feature selection in network traffic intrusion detection, we employ the Chaotic Grey Wolf Optimizer (CGWO) to select the most relevant and impactful features for learning strategically. Our approach demonstrates superior performance through comprehensive experiments compared to well-known meta-heuristic algorithms and prior art methods, as evidenced by various evaluation metrics. This research contributes to advancing intrusion detection systems in healthcare IoMT and IoT, offering a reliable and efficient solution to safeguard against evolving cyber threats.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Accuracy",
                "Nearest neighbor methods",
                "Support vector machines",
                "Medical services",
                "Medical diagnostic imaging",
                "Intrusion detection"
            ],
            "Author Keywords": [
                "Enter keywords Medical IoT",
                "Intrusion detection system",
                "artificial intelligence",
                "malicious activity",
                "Grey Wolf Optimizer",
                "Meta-heuristic algorithm",
                "chaotic map"
            ]
        },
        "title": "Enhancing Malicious Activity Detection in IoT-Enabled Network and IoMT Systems Through Metaheuristic Optimization and Machine Learning"
    },
    {
        "authors": [
            "Yan Li",
            "Jie Yang",
            "Shang-Ling Shih",
            "Wan-Ting Shih",
            "Chao-Kai Wen",
            "Shi Jin"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "01 July 2024",
        "doi": "10.1109/JIOT.2024.3421577",
        "publisher": "IEEE",
        "abstract": "Internet of Things (IoT) device localization is fundamental to smart home functionalities, including indoor navigation and tracking of individuals. Traditional localization relies on relative methods utilizing the positions of anchors within a home environment, yet struggles with precision due to inherent inaccuracies in these anchor positions. In response, we introduce a cutting-edge smartphone-based localization system for IoT devices, leveraging the precise positioning capabilities of smartphones equipped with motion sensors. Our system employs artificial intelligence (AI) to merge channel state information from proximal trajectory points of a single smartphone, significantly enhancing line of sight (LoS) angle of arrival (AoA) estimation accuracy, particularly under severe multipath conditions. Additionally, we have developed an AI-based anomaly detection algorithm to further increase the reliability of LoS-AoA estimation. This algorithm improves measurement reliability by analyzing the correlation between the accuracy of reversed feature reconstruction and the LoS-AoA estimation. Utilizing a straightforward least squares algorithm in conjunction with accurate LoS-AoA estimation and smartphone positional data, our system efficiently identifies IoT device locations. Validated through extensive simulations and experimental tests with a receiving antenna array comprising just two patch antenna elements in the horizontal direction, our methodology has been shown to attain decimeter-level localization accuracy in nearly 90% of cases, demonstrating robust performance even in challenging real-world scenarios. Additionally, our proposed anomaly detection algorithm trained on Wi-Fi data can be directly applied to ultra-wideband, also outperforming the most advanced techniques.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Location awareness",
                "Internet of Things",
                "Wireless fidelity",
                "Smart phones",
                "Estimation",
                "Accuracy",
                "Trajectory"
            ],
            "Author Keywords": [
                "IoT devices localization",
                "channel state information",
                "artificial intelligence",
                "anomaly detection"
            ]
        },
        "title": "Efficient IoT Devices Localization Through Wi-Fi CSI Feature Fusion and Anomaly Detection"
    },
    {
        "authors": [
            "Pavel Masek",
            "Dmitri Moltchanov",
            "Martin Stusek",
            "Radek Mozny",
            "Yevgeni Koucheryavy",
            "Jiri Hosek"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "11 November 2024",
        "doi": "10.1109/JIOT.2024.3495698",
        "publisher": "IEEE",
        "abstract": "The increasing demand for power distribution systems in terms of control with nearly immediate response requires deploying a new type of user equipment (UE) that demands permanent connectivity. In NB-IoT systems, the traffic generated by such UEs may constitute a large part of the overall load. In this paper, we first propose a detailed two-dimensional Markov chain model to capture the system’s behavior with the mixture of conventional stochastic and regular traffic types. To provide a computationally efficient solution, we then apply the state aggregation technique to reduce it to a one-dimensional model and develop approximations and associated numerical algorithms for assessing the mean delay when transmitting the considered traffic. Our results show that a single NB-IoT cell remains stable for up to 72×104 conventional UEs and 9×103 UEs demanding permanent connectivity. The presence of the latter UEs type has a linear effect on their delay, but affects conventional UEs more drastically. A delay bound of 10 s specified in ITU-R M.2410 is met for the conventional UEs, even under a high number of permanently connected UEs (103). However, the delay on the side of the latter UEs is violated even for 100 permanently connected UEs requiring redesigning the NB-IoT channel access mechanism or expanding resources.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Delays",
                "Electricity",
                "Meters",
                "Data communication",
                "5G mobile communication",
                "Stochastic processes",
                "Smart meters",
                "Signal to noise ratio",
                "Interference"
            ],
            "Author Keywords": [
                "5G Mobile Networks",
                "mMTC",
                "Narrowband IoT",
                "Delay Performance",
                "Mixture of Traffic Types"
            ]
        },
        "title": "Quantifying NB-IoT Performance in 5G Use-Cases With Mixture of Regular and Stochastic Traffic"
    },
    {
        "authors": [
            "Vaibhav Agarwal",
            "Shashikala Tapaswi",
            "Prasenjit Chanak"
        ],
        "published_in": "Published in: IEEE Transactions on Green Communications and Networking ( Early Access )",
        "date_of_publication": "29 April 2024",
        "doi": "10.1109/TGCN.2024.3394908",
        "publisher": "IEEE",
        "abstract": "Wireless Sensor Networks (WSNs) act as an integral part of any Internet of Things (IoT) based system. In IoT-based applications such as disaster management, industry automation, and healthcare, the end user demands real-time data for decision-making. In these applications, query-driven WSNs play a vital role in real-time decision-making. Existing state-of-the-art query-driven approaches suffer from a huge query processing delay, end-to-end delay, and poor network lifetime. Therefore, this paper presents an energy-efficient query processing mechanism for IoT-enabled WSNs where mobile sinks-based query processing is performed to reduce end-to-end delay and improve overall network performance. The proposed scheme uses a minimal set cover algorithm to identify the optimal number of rendezvous points. Furthermore, it selects the optimal number of mobile sinks using an improved shark smell optimization algorithm. Extensive simulations and mathematical analysis have shown that the proposed scheme outperformed as compared to the existing state-of-the-art algorithms such as LEDC, QDWSN, QWRP, and QDVGDD. The proposed scheme depicts 41.26%, 39.84%, 40.77%, 39.74%, and 40.15% improvement in terms of average energy consumption, query processing delay, end-to-end delay, network lifetime, and data delivery ratio, respectively.",
        "issn": {
            "Electronic ISSN": "2473-2400"
        },
        "keywords": {
            "IEEE Keywords": [
                "Wireless sensor networks",
                "Query processing",
                "Delays",
                "Energy consumption",
                "Internet of Things",
                "Wheels",
                "Sharks"
            ],
            "Author Keywords": [
                "Internet of Things (IoT)",
                "Mobile Sink (MS)",
                "Query Processing",
                "Shark Smell Optimization (SSO)",
                "Wireless Sensor Networks (WSNs)"
            ]
        },
        "title": "Energy Efficient Query Processing Mechanism for IoT-Enabled WSNs"
    },
    {
        "authors": [
            "Ismael Martinez",
            "Abdelhakim Senhaji Hafid",
            "Michel Gendreau"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/JIOT.2024.3491075",
        "publisher": "IEEE",
        "abstract": "The fog-computing paradigm provides low-latency processing and storage between Internet of Things (IoT) applications and Cloud data centres. Normal IoT activity may produce infrequent yet substantial spikes in user traffic, which would require a large static fog infrastructure to service. Instead, we consider the viability of having a smaller static fog infrastructure, and supplementing additional traffic spikes with fog-enabled unmanned aerial vehicles (fog-UAVs). This article formulates the optimal design & dimensioning of fog-UAVs and fog-UAV charging/ deployment stations as a probabilistic location set covering problem (PLSCP). We model the fog-UAV PLSCP as a mixed-integer linear program (MILP), from which we derive several relaxed models including one based on the Benders decomposition technique. Finally, we simulate our models over a set of citywide IoT hotspots with various traffic percentile thresholds, and evaluate our results.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Autonomous aerial vehicles",
                "Internet of Things",
                "Costs",
                "Batteries",
                "Reliability",
                "Servers",
                "Estimation",
                "Charging stations",
                "Probabilistic logic",
                "Maintenance"
            ],
            "Author Keywords": [
                "Fog computing",
                "fog design & dimensioning",
                "Internet of Things (IoT)",
                "mixed-integer linear programming (MILP)",
                "Benders decomposition",
                "probabilistic set covering",
                "unmanned aerial vehicles"
            ]
        },
        "title": "Design & Dimensioning of a UAV Set Covering in High-Traffic IoT-Fog Environments"
    },
    {
        "authors": [
            "P. Y Dibal",
            "E. A Hashim",
            "E. N Onwuka",
            "S Zubair"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/ACCESS.2024.3491632",
        "publisher": "IEEE",
        "abstract": "The global drive towards a net-zero world and commitments from advanced countries towards a significant cut of greenhouse gases by 2030 are objectives that have compelled a new technological paradigm that must align with new global initiatives towards meeting agreed objectives towards the net-zero world. The Internet-of-Things (IoT) is a technology that has shown strong viability in assisting nations and corporations in meeting global commitments to the net-zero goal. To be effective, the operation of IoT devices must meet very tight constraints; consequently, the design of the electronics, and applications that run on IoT devices must be optimal especially in power utilization considerations. It is in this regard that this paper presents the design of a power prediction model for a Cortex M processor. The design was achieved through a unique methodology in which an instruction dissector played a central role in determining the exact instructions that were executed, rather than using a generalized set of instructions from an instruction grouping. Multivariate Adaptive Regression Splines (MARS) was used as the machine learning technique for deriving the prediction models. A key feature of using MARS is that the predictor variables having the greatest effect on the models were identified as the MVN and SBC instructions. A performance comparison of the design in this paper was made with similar designs where it was shown that the approach of the design in this work yielded more robust models with a reported error rate of 0.000629193 for logic power and 0.023096787 for data power.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Predictive models",
                "Data models",
                "Benchmark testing",
                "Estimation",
                "Mathematical models",
                "Internet of Things",
                "Software",
                "Net zero",
                "Ions",
                "Program processors"
            ],
            "Author Keywords": [
                "Data power",
                "IoT",
                "Logic power",
                "Power prediction model"
            ]
        },
        "title": "Development of a Model for Prediction of IoT Processor Power Utilization using Instruction Dissection Based Technique"
    },
    {
        "authors": [
            "Meng Shen",
            "Jin Meng",
            "Ke Xu",
            "Shui Yu",
            "Liehuang Zhu"
        ],
        "published_in": "Published in: IEEE Transactions on Big Data ( Early Access )",
        "date_of_publication": "20 May 2024",
        "doi": "10.1109/TBDATA.2024.3403388",
        "publisher": "IEEE",
        "abstract": "Depending on large-scale devices, the Internet of Things (IoT) provides massive data support for resource sharing and intelligent decision, but privacy risks also increase. As a popular distributed learning framework, Federated Learning (FL) is widely used because it does not need to share raw data while only parameters to collaboratively train models. However, Federated Learning is not spared by some emerging attacks, e.g., membership inference attack. Therefore, for IoT devices with limited resources, it is challenging to design a defense scheme against the membership inference attack ensuring high model utility, strong membership privacy and acceptable time efficiency. In this paper, we propose MemDefense, a lightweight defense mechanism to prevent membership inference attack from local models and global models in IoT-based FL, while maintaining high model utility. MemDefense adds crafted pruning perturbations to local models at each round of FL by deploying two key components, i.e., parameter filter and noise generator. Specifically, the parameter filter selects the apposite model parameters which have little impact on the model test accuracy and contribute more to member inference attacks. Then, the noise generator is used to find the pruning noise that can reduce the attack accuracy while keeping high model accuracy, protecting each participant's membership privacy. We comprehensively evaluate MemDefense with different deep learning models and multiple benchmark datasets. The experimental results show that lowcost MemDefense drastically reduces the attack accuracy within limited drop of classification accuracy, meeting the requirements for model utility, membership privacy and time efficiency.",
        "issn": {
            "Electronic ISSN": "2332-7790"
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling",
                "Privacy",
                "Predictive models",
                "Data models",
                "Internet of Things",
                "Glass box",
                "Closed box"
            ],
            "Author Keywords": [
                "IoT",
                "Federated Learning",
                "membership inference attack",
                "defense",
                "pruning perturbations"
            ]
        },
        "title": "MemDefense: Defending against Membership Inference Attacks in IoT-based Federated Learning via Pruning Perturbations"
    },
    {
        "authors": [
            "Lijuan Xu",
            "Ziyu Han",
            "Dawei Zhao",
            "Xin Li",
            "Fuqiang Yu",
            "Chuan Chen"
        ],
        "published_in": "Published in: IEEE Transactions on Sustainable Computing ( Early Access )",
        "date_of_publication": "26 April 2024",
        "doi": "10.1109/TSUSC.2024.3386667",
        "publisher": "IEEE",
        "abstract": "Anomaly detection plays a vital role as a crucial security measure for edge devices in Artificial Intelligence and Internet of Things (AIoT). With the rapid development of IoT ( Internet of Things), changes in system configurations and the introduction of new devices can lead to significant alterations in device relationships and data flows within the IoT, thereby triggering concept drift. Previously trained anomaly detection models fail to adapt to the changed distribution of streaming data, resulting in a high number of false positive events. This paper aims to address the issue of concept drift in IoT anomaly detection by proposing a comprehensive Concept Drift Detection, Interpretation, and Adaptation framework (CDDIA). We focus on accurately capturing the concept drift of normal data in unsupervised scenarios. To interpret drift samples, we integrate a search optimization algorithm and the SHAP method, providing a comprehensive interpretation of drift samples at both the sample and feature levels. Simultaneously, by utilizing the sample-level interpretation results for filtering new and old samples, we retrain the anomaly detection model to mitigate the impact of concept drift and reduce the false positive rate. This integrated strategy demonstrates significant advantages in maintaining model stability and reliability. The experimental results indicate that our method outperforms five baseline methods in adaptability across three datasets and provides interpretability for samples experiencing concept drift.",
        "issn": {
            "Electronic ISSN": "2377-3782"
        },
        "keywords": {
            "IEEE Keywords": [
                "Adaptation models",
                "Internet of Things",
                "Data models",
                "Anomaly detection",
                "Error analysis",
                "Training",
                "Random forests"
            ],
            "Author Keywords": [
                "Anomaly detection",
                "concept drift",
                "interpretability",
                "IoT"
            ]
        },
        "title": "Addressing Concept Drift in IoT Anomaly Detection: Drift Detection, Interpretation, and Adaptation"
    },
    {
        "authors": [
            "Mostafa Emara",
            "Nour Kouzayha",
            "Hesham ElSawy",
            "Tareq Y. Al-Naffouri"
        ],
        "published_in": "Published in: IEEE Transactions on Communications ( Early Access )",
        "date_of_publication": "02 September 2024",
        "doi": "10.1109/TCOMM.2024.3453399",
        "publisher": "IEEE",
        "abstract": "Feedback transmissions are used to acknowledge correct packet reception, trigger erroneous packet re-transmissions, and adapt transmission parameters (e.g., rate and power). Despite the feedback paramount role in establishing reliable communication links, the majority of the literature overlooks its impact by assuming genie-aided systems with flawless and instantaneous feedback. However, this idealistic assumption is no longer valid for large-scale Internet of Things (IoT) networks, characterized by energy-constrained devices, susceptible to interference, and serving delay-sensitive applications. Furthermore, feedback-free operation is necessitated for IoT receivers with stringent energy constraints. In this context, this paper explicitly accounts for the impact of feedback in energy-constrained delay-sensitive large-scale IoT networks. We consider a time-slotted system with closed-loop and open-loop rate adaptation schemes, where packets are fragmented to operate at a reliable transmission rate satisfying packet delivery deadlines. In the closed-loop scheme, the delivery of each fragment is acknowledged through an error-prone feedback channel. The open-loop scheme has no feedback mechanism, and hence, a predetermined fragment repetition strategy is employed to improve transmission reliability. Using stochastic geometry and queueing theory, we develop a novel spatiotemporal framework for both schemes to quantify the impact of feedback on network performance in terms of transmission reliability, latency, and energy consumption.",
        "issn": {
            "Print ISSN": "0090-6778",
            "Electronic ISSN": "1558-0857"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Spatiotemporal phenomena",
                "Energy consumption",
                "Receivers",
                "Analytical models",
                "Adaptation models",
                "Queueing analysis"
            ],
            "Author Keywords": [
                "IoT networks",
                "Rate adaptation",
                "Spatiotemporal analysis",
                "Markov chains",
                "Open-loop and closed-loop feedback"
            ]
        },
        "title": "Rate Adaptation in Delay-Sensitive and Energy-Constrained Large-Scale IoT Networks"
    },
    {
        "authors": [
            "Haoxuan Wang",
            "Kun Xie",
            "Xin Wang",
            "Jigang Wen",
            "Ruotian Xie",
            "Zulong Diao",
            "Wei Liang",
            "Gaogang Xie",
            "Jiannong Cao"
        ],
        "published_in": "Published in: IEEE Transactions on Services Computing ( Early Access )",
        "date_of_publication": "18 September 2024",
        "doi": "10.1109/TSC.2024.3463496",
        "publisher": "IEEE",
        "abstract": "Accurately identifying IoT device types is crucial for IoT security and resource management. However, existing traffic-based device identification algorithms incur high measurement, storage, and computation costs, as they continuously need to capture, store, and parse device traffic. To overcome these challenges, we propose an innovative framework that employs a discontinuous traffic measurement strategy, reducing the number of packets captured, stored, and parsed. To ensure accurate identification, we introduce several novel techniques. First, we propose a graph neural network-based tensor completion model to estimate missing traffic features in unmeasured time slots. Our model can utilize historical information to flexibly and efficiently estimate missing features. Second, we propose a convolutional neural network-based classifier for device identification. The classifier utilizes traffic features and node embeddings learned from the tensor completion model to achieve precise device identification. Through extensive experiments on real IoT traffic traces, we demonstrate that our framework achieves high accuracy while significantly reducing costs. For instance, by capturing only 30% of the packets, our framework can identify devices with a high accuracy of 0.9558. Moreover, compared to current tensor completion methods, our method can estimate missing values with higher accuracy and achieve a 1.53-fold speedup over the next-fastest baseline.",
        "issn": {
            "Electronic ISSN": "1939-1374"
        },
        "keywords": {
            "IEEE Keywords": [
                "Object recognition",
                "Internet of Things",
                "Tensors",
                "Feature extraction",
                "Accuracy",
                "Logic gates",
                "Data models"
            ],
            "Author Keywords": [
                "Device-type identification",
                "graph neural networks",
                "internet of things (IoT)",
                "tensor completion"
            ]
        },
        "title": "GDI: A Novel IoT Device Identification Framework Via Graph Neural Network-Based Tensor Completion"
    },
    {
        "authors": [
            "Keyu Chen",
            "Shitong Fang",
            "Tao Yan",
            "Xinyuan Chuai",
            "Xin Li",
            "Zhihui Lai",
            "Junrui Liang",
            "Wei-Hsin Liao"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "06 August 2024",
        "doi": "10.1109/JIOT.2024.3439563",
        "publisher": "IEEE",
        "abstract": "Vibration energy harvesters have been widely investigated to supply sustainable power for devices in the Internet of Things (IoT). Despite the advances in vibration energy harvesters, there still remain challenges in the design and optimization of energy harvesters to meet the increasing power demand of long-range IoT applications. In this paper, an auxetic piezoelectric energy harvester with tapered thickness (TAEH) is proposed to improve the efficiency of energy harvesting by achieving the uniform stress and high average stress. Compared with traditional auxetic piezoelectric energy harvester with uniform thickness (UAEH), the stress distribution with tapered thickness is more uniform, which can contribute to a higher power output with lower maximum stress. Furthermore, the multi-objective optimization is used to further improve the average stress values without increasing the maximum stress, thus increasing the energy output. Finite element analysis is performed to validate the performance of the energy harvesters. In the experimental validation, it is found that with the tapered thickness introduced to the auxetic energy harvester, the maximum power output and power density of TAEH can be increased by 212.84% and 279.08%, respectively, compared with UAEH. After the optimization, these two indices of the optimized TAEH (OTAEH) are further increased by 24.32% and 27.59%, respectively. Specifically, a high power density of 0.148 mW/g is achieved in the OTAEH, indicating its high vibration energy harvesting performance with lightweight. Finally, it is demonstrated that the OTAEH can generate more than 40.94 mJ within 27.6 s to successfully power an IoT device for temperature sensing and long-range data transmission.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Auxetic materials",
                "Circuits",
                "Structural beams",
                "Stress",
                "Internet of Things",
                "Optimization",
                "Sensors"
            ],
            "Author Keywords": [
                "Auxetic structure",
                "tapered thickness",
                "optimization",
                "piezoelectric",
                "IoT application"
            ]
        },
        "title": "Design and Optimization of an Auxetic Piezoelectric Energy Harvester With Tapered Thickness for IoT Applications"
    },
    {
        "authors": [
            "Rui Tang",
            "Ruizhi Zhang",
            "Yongjun Xu",
            "Chau Yuen"
        ],
        "published_in": "Published in: IEEE Transactions on Cognitive Communications and Networking ( Early Access )",
        "date_of_publication": "30 May 2024",
        "doi": "10.1109/TCCN.2024.3407096",
        "publisher": "IEEE",
        "abstract": "In this paper, we investigate a resource allocation problem for a multi-unmanned aerial vehicle (UAV)-assisted full-duplex wireless-powered Internet-of-things (IoT) network, where the slot partition, power allocation, user association, and three dimensional (3D) UAV placement are jointly considered to maximize the sum bit rate of all IoT devices under the imperfect self-interference cancellation and generalized probabilistic air-ground channel model. To deal with the formulated mixed-integer non-convex problem, we propose a novel resource allocation strategy with three nested parts by integrating the model-based optimization theory with the data-based learning theory. Particularly, the data-based deep deterministic policy gradient algorithm is only explicitly used to train the 3D UAV placement policy, while the model-based Lagrange dual theory and matching theory are implicitly used to explore the hidden tractability of the rest two parts and design efficient algorithms, where the optimization results are passed onto the data-based part through reward values. Simulation results show that the proposed strategy greatly cuts down the execution time of the exhausting search-based genetic algorithm by 4 orders of magnitude at the cost of less than 5.1 percent performance loss.",
        "issn": {
            "Electronic ISSN": "2332-7731"
        },
        "keywords": {
            "IEEE Keywords": [
                "Autonomous aerial vehicles",
                "Resource management",
                "Internet of Things",
                "Trajectory",
                "Optimization",
                "Three-dimensional displays",
                "Bit rate"
            ],
            "Author Keywords": [
                "IoT",
                "UAV swarm",
                "wireless power transfer",
                "full duplex",
                "resource allocation",
                "convex optimization theory",
                "matching theory",
                "deep reinforcement learning"
            ]
        },
        "title": "Deep Reinforcement Learning-Based Resource Allocation for Multi-UAV-Assisted Full-Duplex Wireless-Powered IoT Networks"
    },
    {
        "authors": [
            "Zijian Tian",
            "Haishun Liu",
            "Jiaqi Wu",
            "Wei Chen",
            "Ruihan Zheng",
            "Zehua Wang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "29 July 2024",
        "doi": "10.1109/JIOT.2024.3435130",
        "publisher": "IEEE",
        "abstract": "With the rapid development of IoT and UAV technology, for the whole IoT system of vehicle detection, the middle and high level of information transmission and server processing has made a breakthrough, so at the bottom of the real-time detection of the vehicle by the UAV is the key to the whole system. However, UAV vehicle detection faces the challenges of too many small targets in the image leading to low detection accuracy, limited hardware platform resources requiring control of model size, and real-time detection requiring high inference speed. Aiming at the above problems, we propose a lightweight model PrFu-YOLO based on YOLOv8 improvement, which achieves a good balance between accuracy, inference speed and model size. And it realises real-time vehicle detection embedded in a UAV platform. To solve the problem of low vehicle detection accuracy, we design a new structure PrFuFPN based on adding a small target detection layer to achieve more advanced feature fusion. To address the limited resources of the platform and the problem of real-time vehicle detection, we add GhostConv to the structure and constantly try to adjust the parameters of the network. Finally, extensive experiments were conducted on the VisDrone2019 and CARPK datasets to fully evaluate the model. Compared to YOLOv8s on the VisDrone2019 test set, mAP50 was improved by 10.05%, mA95 by 14.54%, the number of parameters was reduced by 8.13%, and the model size was reduced by 12.5%, while the FPS of 67.13 fully met the needs of real-time detection.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Vehicle detection",
                "Feature extraction",
                "Autonomous aerial vehicles",
                "Accuracy",
                "Real-time systems",
                "Internet of Things",
                "Computational modeling"
            ],
            "Author Keywords": [
                "IoT",
                "UAV",
                "Real-time vehicle detection",
                "PrFuFPN",
                "WIOUv3",
                "YOLOv8"
            ]
        },
        "title": "PrFu-YOLO: A Lightweight Network Model for UAV-Assisted Real-Time Vehicle Detection Towards an IoT Underlayer"
    },
    {
        "authors": [
            "Ling Lyu",
            "Zexin Qiao",
            "Yanpeng Dai",
            "Nan Cheng",
            "Cailian Chen",
            "Xinping Guan",
            "Xuemin Shen"
        ],
        "published_in": "Published in: IEEE Transactions on Vehicular Technology ( Early Access )",
        "date_of_publication": "01 October 2024",
        "doi": "10.1109/TVT.2024.3472030",
        "publisher": "IEEE",
        "abstract": "The wide application of Internet of Things (IoT) technology in industrial automation promotes the emergence of industrial IoT systems, in which state estimation plays a crucial role in conjecturing system states with sensory data delivered over wireless channels. In this paper, we propose an automated guided vehicle (AGV)-assisted adaptive cooperative transmission scheme to minimize the mean square error of state estimation at a low energy cost. Specifically, a novel performance index, estimation gain, is introduced to evaluate the benefit of scheduling one sensor for estimation error reduction. Then, sensor scheduling and data transmission are jointly optimized to minimize the time-accumulated estimation error, which is challenging to directly solve due to the unclear impact of imperfect transmission on estimation performance. To this end, an estimation gain-based algorithm is designed to determine the scheduled sensors. Besides, an iterative algorithm is designed to solve the adaptive cooperative transmission problem. Simulation results show that the proposed scheme outperforms benchmark schemes in reducing estimation error and energy consumption.",
        "issn": {
            "Print ISSN": "0018-9545",
            "Electronic ISSN": "1939-9359"
        },
        "keywords": {
            "IEEE Keywords": [
                "State estimation",
                "Industrial Internet of Things",
                "Estimation error",
                "Job shop scheduling",
                "Adaptive systems",
                "Wireless sensor networks",
                "Prediction algorithms",
                "Data communication",
                "Channel estimation",
                "Accuracy"
            ],
            "Author Keywords": [
                "Industrial IoT systems",
                "cooperative transmission",
                "sensor scheduling",
                "state estimation",
                "AGV"
            ]
        },
        "title": "AGV-Assisted Adaptive Cooperative Transmission for State Estimation in Industrial IoT Systems"
    },
    {
        "authors": [
            "Xuan Wu",
            "Xuanye Lin",
            "Zhen Zhang",
            "Chien-Ming Chen",
            "Thippa R. Gadekallu",
            "Saru Kumari",
            "Sachin Kumar"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "08 July 2024",
        "doi": "10.1109/TCE.2024.3417890",
        "publisher": "IEEE",
        "abstract": "Presently, the prevalence of large language models has driven the rapid popularization of question-and-answer applications. However, the training and deployment of large language models involve high-resource computing, posing a challenge for many small or edge devices in the Internet of Things (IoT). Therefore, in the context of IoT edge consumer electronic devices, deploying question-and-answer models based on TinyML becomes more meaningful. In this paper, we propose a tiny deep learning-based Question-Answer scheme to realize end-to-end dialog service by Machine Reading Comprehension. In order to obtain semantic representation, we design and apply a pre-training model to generate the semantic embedding representation of questions and answers and propose a pretraining fine-tuning method based on the twin model. In addition, we also introduce a compression method based on embedded representation and design a forward compression network and a cyclic compression network based on an encoder. The experimental results show that our method is more accurate than state-of-the-art schemes.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Biological system modeling",
                "Internet of Things",
                "Task analysis",
                "Adaptation models",
                "Accuracy",
                "Question answering (information retrieval)"
            ],
            "Author Keywords": [
                "TinyML",
                "IoT Edge Consumer Devices",
                "Intelligent Question-Answer Service"
            ]
        },
        "title": "TinyML-Enabled Intelligent Question-Answer Services in IoT Edge Consumer Devices"
    },
    {
        "authors": [
            "Xuzhe Zheng",
            "Xiaokang Zhou",
            "Wei Liang",
            "Kevin I-Kai Wang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "20 August 2024",
        "doi": "10.1109/JIOT.2024.3446551",
        "publisher": "IEEE",
        "abstract": "Due to the high dependence of social development on electricity, the failure of energy system equipment often leads to inestimable losses. The use of Internet of Things (IoT) technology to collect real-time data from energy devices and the use of Artificial Intelligence (AI) technology to prognostic faults in the devices is essential to achieve power system security. Although these realtime data contain rich descriptions of relevant equipment faults and solutions, the complex data structure and severe imbalance of condition monitoring (CM) data leads to poor performance of AI methods. In this study, we propose a Multi-task Correlation Constrained Topology Learning model for smart prognostic and health management in IoT. In particular, the proposed model mainly includes the feature extraction module, the multi-task topology network (MTTN) module, and the class balance loss (CBL) algorithm module. First, Bi-LSTM is employed to extract the word collocation features and numerical features of the data to focus on the important semantic features in complex structured data. Second, an MTTN is constructed to efficiently utilize the topological dependency between multiple tasks. Finally, a CBL loss function is applied to enhance the focus on minority classes. Experiment and evaluation results demonstrate that our model has superior learning efficiency and prediction performance, and that our model performs better on extremely imbalanced data. Our model correctly predicts the faulty equipment, the fault cause and the fault severity level, providing a rapid and precise reference for advance repair or replacement of energy system equipment.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Prognostics and health management",
                "Task analysis",
                "Multitasking",
                "Data models",
                "Internet of Things",
                "Maintenance"
            ],
            "Author Keywords": [
                "Prognostic and Health Management",
                "Imbalanced Learning",
                "Multi-task Learning",
                "Energy IoT",
                "Deep Learning"
            ]
        },
        "title": "Multi-Task Correlation Constrained Topological Learning Toward Smart Prognostic and Health Management in IoT"
    },
    {
        "authors": [
            "Amit Kr. Pandey",
            "Ravi Kumar Gangwar",
            "Raghvendra Kumar Chaudhary"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "20 September 2024",
        "doi": "10.1109/JIOT.2024.3464590",
        "publisher": "IEEE",
        "abstract": "This article proposes a low-profile, compact dualband shielded quarter-mode substrate integrated waveguide (SD-QMSIW) self-diplexing multiple-input multiple-output (MIMO) antenna for Internet of Things (IoT) heterogeneous networks (HetNet). The antenna is designed for Worldwide Interoperability for Microwave Access (WiMAX) and wireless LAN (WLAN) applications across the Internet of Things (IoT) environment. The MIMO configuration comprises two identical radiating self-diplexing antenna (SDA) elements arranged orthogonally. Each of these proposed SDA elements utilizes two modified L-shaped slots positioned on the upper conducting plane of the SD-QMSIW cavity, facilitating the construction of a miniaturized single-element SDA. This single-element SDA is driven by two separate 50 Ω microstrip feed lines, enabling radiation at two distinct resonance frequencies, 3.5 GHz and 5.8 GHz, with better isolation greater than 37 dB between the ports. The proposed SDA design method is exceedingly flexible, allowing us to independently construct the antenna at two distinct frequency bands. Furthermore, a lumped circuit model of the proposed SDA element is anticipated to validate the designed single element SDA structure in conjunction with Electromagnetic (EM) simulations of the antenna prototype. Moreover, the MIMO configuration of the proposed SDA element enables compact antenna circuit size of 0.29λ g 2, isolation greater than 19 dB between the various ports of each SDA element, along with gains of 4.54 and 5.47 dBi at ports resonating at two identical frequencies. The outcomes of MIMO antenna features involving envelope correlation coefficient (ECC), effective diversity gain (EDG), and multiplexing efficiency (MUX) have been evaluated and found to be satisfactory for the proposed SD-QMSIW-based SDMA.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Antennas",
                "Internet of Things",
                "Wireless LAN",
                "WiMAX",
                "Multiaccess communication",
                "Substrates",
                "Resonant frequency"
            ],
            "Author Keywords": [
                "Multiple-input multiple-output antenna (MIMO)",
                "Self-diplexing antenna (SDA)",
                "substrate integrated waveguide (SIW)",
                "Internet of Things (IoT)",
                "envelope correlation coefficient (ECC)"
            ]
        },
        "title": "A Compact SD-QMSIW-Based Self-Diplexing MIMO Antenna Using Two Modified L-Shaped Slots as Radiators for IoT Applications"
    },
    {
        "authors": [
            "Jinsha Liu",
            "Boning Li",
            "Takaharu Yamazaki",
            "Jianting Cao"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "03 October 2024",
        "doi": "10.1109/ACCESS.2024.3472909",
        "publisher": "IEEE",
        "abstract": "This paper explores the integration of Electroencephalography (EEG) with Brain-Computer Interface (BCI) technologies, highlighting the application in facilitating communication and control for individuals with severe motor disabilities. We focus on the capabilities of EEG to decode brain signals using modalities such as SSVEP, which is enhanced through Canonical Correlation Analysis (CCA) algorithm to improve classification accuracy. Additionally, we discuss the merging of these technologies with the Internet of Things (IoT) to extend BCI applications into daily activities through multi-device control system.Traditional BCI systems are typically designed for fixed, single-task scenarios, and often do not account for the complexities of multi-objective, cross-device tasks encountered in real-world environments. By integrating BCI technology with IoT, we address the limitations of single-task scenarios, enabling the use of independent BCI systems to perform tasks such as making phone dialing, moving wheelchairs, controlling robots, and transporting objects across different devices. This advancement provides effective support for the deployment of BCI technology in real-world applications. Our study demonstrates the implementation of a robust SSVEP-based BCI system that achieves a 97.0% classification accuracy across 500 trials, proving its efficacy in real-world scenarios. This research addresses the challenges of adapting BCI system to complex, varied environments and provides a viable solution for improving the quality of life for those with severe motor disabilities.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Electroencephalography",
                "Visualization",
                "Correlation",
                "Control systems",
                "Motors",
                "Frequency control",
                "Machine learning",
                "Accuracy",
                "Training",
                "Resonant frequency",
                "Brain-computer interfaces"
            ],
            "Author Keywords": [
                "BCI",
                "IoT",
                "EEG",
                "Machine Learning",
                "SSVEP"
            ]
        },
        "title": "Developing an SSVEP-Based BCI System for Multi-Objective Control via IoT Integration"
    },
    {
        "authors": [
            "William D. Lukito",
            "Wei Xiang",
            "Phu Lai",
            "Peng Cheng",
            "Chang Liu",
            "Kan Yu",
            "Xiaoyan Zhu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "01 October 2024",
        "doi": "10.1109/JIOT.2024.3472019",
        "publisher": "IEEE",
        "abstract": "In this study, we investigate the use of simultaneous transmitting and reflecting reconfigurable intelligent surfaces (STAR-RISs) mounted on energy-efficient unmanned aerial vehicles (UAVs) to support satellite Internet of Things (IoT) communications served by low earth orbit (LEO) satellites. First, we propose a STAR-RIS-equipped UAV framework termed Integrated STAR-RIS and UAV (ISRU). Then, we aim to optimize energy efficiency by jointly adjusting the UAV’s flight path, STAR-RIS phase-shifts, and power allocation among IoT devices, all while maintaining equitable user fairness level. However, solving this problem presents considerable challenges due to the non-convexity and NP-hardness properties of the objective function and constraints. To address, our work introduces a Dinkelbach-based alternating optimization (AO) procedure termed Integrated Trajectory, Phase-Shift, and Power Allocation (ITPP). Our simulation results show that the integration of ISRU and ITPP can achieve 67% higher sum-rates than non-ISRU schemes and save up to 40% more energy than unoptimized trajectory schemes.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Autonomous aerial vehicles",
                "Internet of Things",
                "Satellites",
                "Trajectory",
                "Resource management",
                "Relays",
                "Low earth orbit satellites",
                "Satellite broadcasting",
                "Optimization",
                "Energy efficiency"
            ],
            "Author Keywords": [
                "satellite IoT",
                "STAR-RIS",
                "UAV",
                "energy-efficient",
                "trajectory optimization",
                "phase-shift design",
                "power allocation"
            ]
        },
        "title": "Integrated STAR-RIS and UAV for Satellite IoT Communications: An Energy-Efficient Approach"
    },
    {
        "authors": [
            "Waqas Khalid",
            "Alexandros-Apostolos A. Boulogeorgos",
            "Trinh Van Chien",
            "Junse Lee",
            "Howon Lee",
            "Heejung Yu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "16 September 2024",
        "doi": "10.1109/JIOT.2024.3461814",
        "publisher": "IEEE",
        "abstract": "Wireless-powered communications (WPCs) are increasingly crucial for extending the lifespan of low-power Internet of Things (IoT) devices. Furthermore, reconfigurable intelligent surfaces (RISs) can create favorable electromagnetic environments by providing alternative signal paths to counteract blockages. The strategic integration of WPC and RIS technologies can significantly enhance energy transfer and data transmission efficiency. However, passive RISs suffer from double-fading attenuation over RIS-aided cascaded links. In this article, we propose the application of an active RIS within WPC-enabled IoT networks. The enhanced flexibility of the active RIS in terms of energy transfer and information transmission is investigated using adjustable parameters. We derive novel closed-form expressions for the ergodic rate and outage probability by incorporating key parameters, including signal amplification, active noise, power consumption, and phase quantization errors. Additionally, we explore the optimization of WPC scenarios, focusing on the time-switching factor and power consumption of the active RIS. The results validate our analysis, demonstrating that an active RIS significantly enhances WPC performance compared to a passive RIS.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Reconfigurable intelligent surfaces",
                "Power demand",
                "Wireless communication",
                "Fading channels",
                "Vectors",
                "Information processing"
            ],
            "Author Keywords": [
                "Wireless powered communications (WPCs)",
                "Internet of Things (IoT)",
                "active reconfigurable intelligent surface (RIS)",
                "power consumption"
            ]
        },
        "title": "Optimal Operation of Active RIS-Aided Wireless Powered Communications in IoT Networks"
    },
    {
        "authors": [
            "Mahima Karim",
            "Md. Arafatur Rahman",
            "Sze Wei Tan",
            "Mohammed Atiquzzaman",
            "Prashant Pillai",
            "Ali H. Alenezi"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "08 July 2024",
        "doi": "10.1109/ACCESS.2024.3424418",
        "publisher": "IEEE",
        "abstract": "IoT is a groundbreaking technology that enables wireless interoperability between devices without human intervention. It has become an integral part of human life and has made significant advancements in the automotive industry. The combination of vehicular networks and IoT has opened up new possibilities but has also led to challenges such as degraded QoS (Quality of Service), network congestion, packet loss, transmission delay, intrusions, etc. These problems are also apparent in vehicular health monitoring systems. To overcome such challenges, it is necessary to develop an intricate framework that can address them all. TCP/IP is a five-layer-based standardized communication protocol that facilitates communication between devices over the internet. Unique network protocols and functions are authorized to each of the five layers. In this paper, a comprehensive survey is done on all the recent studies on the five communication layer protocols, their functionalities, and practical challenges associated with each of them for an intra- vehicular health monitoring system (IVHMS). Analyzing them, a novel taxonomy is also proposed with visual representation. In the end, this paper proposes a cross-layer-based framework to address communication- related complexities. This cross-layer model aims to integrate all the dynamic capabilities of the five protocol layers (Application, Transport, Network, Data Link & Physical) and resolve issues within each layer by implementing functionalities of other layer protocols. Future research scopes and possibilities of the proposed system are also discussed.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Protocols",
                "Automobiles",
                "Monitoring",
                "TCPIP",
                "GSM",
                "Zigbee",
                "Wireless fidelity",
                "Internet of Things",
                "Intelligent transportation systems",
                "Vehicular ad hoc networks",
                "Internet of Vehicles"
            ],
            "Author Keywords": [
                "Intra-vehicular health monitoring system",
                "Intelligent Transportation",
                "Internet of Things (IoT)",
                "Internet of Vehicles (IoV)",
                "TCP/IP protocols",
                "Cross-Layer protocols"
            ]
        },
        "title": "Intra-vehicular Communication Protocol for IoT Enabled Vehicle Health Monitoring System: challenges, issues and Solutions"
    },
    {
        "authors": [
            "A Sai Venkateshwar Rao",
            "Tarachand Amgoth",
            "Ansuman Bhattacharya"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "10 October 2024",
        "doi": "10.1109/TCE.2024.3477628",
        "publisher": "IEEE",
        "abstract": "Recent advancements in information technologies use the Internet of Things (IoT) for various communication tasks. However, many existing works neglect image data compression, especially in smooth regions, leading to higher costs and energy consumption. Additionally, the balance between compressed image and quality was often not achieved. This research proposes an effective data compression and recovery method using the compressive sensing-based image compression FCSP-based Chronological Archery Algorithm (FCSP-based CAA) model for IoT-LTE systems. The process involves three phases: authentication, communication, and data compression.Data security is ensured through authentication, followed by communication using the Low Energy Adaptive Clustering Hierarchy (LEACH) routing protocol. The compressed image data is then recovered by addressing collaborative recovery issues using the proposed Chronological Archery Algorithm (CAA) model. This model is based on an objective function called \"Feature similarity, Collaborative sparsity Structural Similarity and Peak signal to noise (FCSP)\" measure. Experimental results demonstrate that the proposed methodology achieves enhanced compression performance compared to existing techniques, with a Peak Signal-To-Noise Ratio (PSNR) of 81.876dB, Mean Square Error (MSE) of 0.005, and Structural Similarity Index Measure (SSIM) of 70.876.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Image coding",
                "Internet of Things",
                "Sensors",
                "Long Term Evolution",
                "Data communication",
                "Collaboration",
                "Data models",
                "Compressed sensing",
                "Performance evaluation",
                "Image quality"
            ],
            "Author Keywords": [
                "Low Energy Adaptive Clustering Hierarchy",
                "Chronological Archery Algorithm",
                "IoT",
                "LTE",
                "Feature Similarity Indexing Method"
            ]
        },
        "title": "Chronological Archery Algorithm for Image Compression Using Compressive Sensing in IoT–LTE System"
    },
    {
        "authors": [
            "Shimin Sun",
            "Xiangyun Liu",
            "Meiyu Wang",
            "Ze Wang",
            "Li Han"
        ],
        "published_in": "Published in: IEEE Transactions on Network and Service Management ( Early Access )",
        "date_of_publication": "25 October 2024",
        "doi": "10.1109/TNSM.2024.3486379",
        "publisher": "IEEE",
        "abstract": "In the evolving landscape of Software Defined Internet of Thing (SD-IoT), the proliferation of IoT devices and applications has led to a drastic expansion of network traffic. Because of the overwhelming influx of control messages, the SDN controller may not have sufficient capacity to adequately address them. The main challenge is to properly deploy multiple controllers to enhance resource utilization and boost network performance, taking into account different factors for varying network scenarios. This paper presents a Predictive and Elitism genetic algorithm with Non-cooperative Game (PENG) strategy, tailored to address the control plane load imbalance. PENG incorporates a Gated Recurrent Units (GRU) based traffic prediction model, an improved elitism genetic algorithm, and the non-cooperative game theory to synergistically optimize the load balancing strategy. The study formulates a multi-objective optimization model that takes into account the degree of load balancing, control plane latency, and migration expenses as utility functions. This paper is structured around pivotal modules such as traffic prediction, controller overload identification, switch migration, and controller-switch mapping matrix reconfiguration. Moreover, an improved elitism genetic reallocation algorithm (IEGR) is designed, featuring a novel similarity factor to expedite convergence and improve the accuracy of identifying the optimal solution. Further, the detailed algorithm of PENG is present, outlining proactive and predictive switch migration to preemptively address potential load imbalance. The proposed methodology is simulated and the experimental results demonstrate that the proposal outperforms the comparisons in optimizing the load balancing degree, reducing average latency and migration cost.",
        "issn": {
            "Electronic ISSN": "1932-4537"
        },
        "keywords": {
            "IEEE Keywords": [
                "Load management",
                "Switches",
                "Internet of Things",
                "Genetic algorithms",
                "Prediction algorithms",
                "Heuristic algorithms",
                "Load modeling",
                "Artificial intelligence",
                "Quality of service",
                "Games"
            ],
            "Author Keywords": [
                "Software Defined Internet of Thing (SD-IoTs)",
                "load balancing",
                "switch migration",
                "elitism genetic algorithm",
                "non-cooperative game theory"
            ]
        },
        "title": "Predictive Control Plane Balancing in SD-IoT Networks Based on Elitism Genetic Algorithm and Non-Cooperative Game Theory"
    },
    {
        "authors": [
            "Shaopeng Guan",
            "Youliang Cao",
            "Yuan Zhang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "28 October 2024",
        "doi": "10.1109/JIOT.2024.3487154",
        "publisher": "IEEE",
        "abstract": "Data privacy preservation and secure sharing are key technical challenges faced by smart wearable healthcare IoT systems. Blockchain technology enables privacy preservation for medical data through encryption. However, conventional data encryption hampers data analysis and sharing, and decrypted data still carries the risk of leakage. Homomorphic encryption is a technique that allows computation directly on encrypted data without decryption, thus reducing the risk of data leakage during sharing. In this paper, we propose a blockchain-based privacy preservation and sharing scheme for healthcare IoT data. First, we use an improved homomorphic encryption technique to encrypt and process electronic health records (EHRs), optimizing the modular exponentiation process with a fast exponentiation algorithm, enabling users to efficiently perform data computation and analysis while keeping the data encrypted. Second, we employ symmetric searchable encryption (SSE) to encrypt homomorphic keys and user identity information, and use a Bloom filter as the mapping structure between data keywords and unique identifiers. This approach enhances search efficiency while preserving data privacy, allowing for secure search and analysis on ciphertext. Finally, smart contracts are designed to implement access control during the data-sharing process, increasing the security and transparency of data sharing. Experimental results show that the proposed homomorphic encryption scheme reduces the encryption and decryption time by an average of 34% under different key sizes, while the optimized SSE technique keeps ciphertext retrieval time at a constant level. The proposed scheme provides an effective solution for secure and efficient data analysis and retrieval, ensuring privacy preservation for the secure use and sharing of medical data.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Data privacy",
                "Cryptography",
                "Blockchains",
                "Homomorphic encryption",
                "Medical services",
                "Protection",
                "Security",
                "Medical diagnostic imaging",
                "Information filters",
                "Internet of Things"
            ],
            "Author Keywords": [
                "Healthcare IoT",
                "Privacy Preservation",
                "Data Sharing",
                "Blockchain",
                "Homomorphic Encryption"
            ]
        },
        "title": "Blockchain-Enhanced Data Privacy Protection and Secure Sharing Scheme for Healthcare IoT"
    },
    {
        "authors": [
            "Bohong Wang",
            "Yingying Wang",
            "Qinglai Guo",
            "Yanxi Lin",
            "Yang Yu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "21 October 2024",
        "doi": "10.1109/JIOT.2024.3483929",
        "publisher": "IEEE",
        "abstract": "With the increase in data volume, more types of data are being used and shared, especially in the power Internet of Things (IoT). However, the processes of data sharing may lead to unexpected information leakage because of the ubiquitous relevance among the different data, thus it is necessary for data owners to conduct preventive audits for data applications before data sharing to avoid the risk of key information leakage. Considering that the same data may play completely different roles in different application scenarios, data owners should know the expected data applications of the data buyers in advance and provide modified data that are less relevant to the private information of the data owners and more relevant to the nonprivate information that the data buyers need. In this paper, data sharing in the power IoT is regarded as the background, and the mutual information of the data and their implicit information is selected as the data feature parameter to indicate the relevance between the data and their implicit information or the ability to infer the implicit information from the data. Therefore, preventive audits should be conducted based on changes in the data feature parameters before and after data sharing. The probability exchange adjustment method is pro-posed as the theoretical basis of preventive audits under simplified consumption, and the corresponding optimization models are constructed and extended to more practical scenarios with multivari-ate characteristics. Finally, case studies are used to validate the effectiveness of the proposed preventive audits.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Electricity",
                "Internet of Things",
                "Mutual information",
                "Data privacy",
                "Big Data applications",
                "Uncertainty",
                "Smart meters",
                "Resource management",
                "Renewable energy sources",
                "Real-time systems"
            ],
            "Author Keywords": [
                "Preventive audits",
                "Internet of Things (IoT)",
                "privacyutility trade-off",
                "data sharing"
            ]
        },
        "title": "Preventive Audits for Data Applications Before Data Sharing in the Power IoT"
    },
    {
        "authors": [
            "Kin-Lu Wong",
            "Cheng-Hsuan Shaw",
            "Wan-Ting Li",
            "Wei-Yu Li"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "11 November 2024",
        "doi": "10.1109/ACCESS.2024.3495041",
        "publisher": "IEEE",
        "abstract": "A small-size on-metal four-antenna (OM4A) module having an ultra-wide band (UWB) covering 3.3-8.4 GHz (fractional bandwidth 87.2%) for Internet-of-Things (IoT) mobile device MIMO antennas is presented. The UWB OM4A module is the first ground-backed four-antenna module that can cover the fifth-generation (5G) mid-band (3.3-5.0 GHz), future potential six-generation (6G) upper mid-band (6.425-8.4 GHz), and WiFi-6E 5/6 GHz band (5.15-7.125 GHz) for MIMO applications. The OM4A module uses a small circular patch antenna of diameter 47.5 mm (patch size 0.21λ 2 with respect to the wavelength at 3.3 GHz) and thickness 3 mm (about 0.033λ) to provide four uncorrelated UWB antennas covering 3.3-8.4 GHz. To obtain four UWB antennas, the circular patch is separated into four quadrants; each quadrant is bounded by two orthogonal shorting-pin rows which function like metal walls. In addition, two orthogonal linear slots are added along the centerlines of the four quadrants. In this case, each antenna can effectively generate four resonant modes including a quarter-circle TM 1/2,1/2 -like mode, a semi-circle TM 21 mode, a semi-circle TM 31 mode, and a semi-circle TM 41 mode to form the target UWB. By applying two OM4A modules, eight uncorrelated antennas can be provided for 5G/6G/WiFi-6E IoT mobile device MIMO applications. A field test study of using two OM4A modules as eight receive antennas in the MIMO system with four spatial streams (8 × 4 MIMO) is conducted in the campus public outdoor scenario. Testing results of 4.8-4.9 GHz, 6.1-6.2 GHz, and 7.025-7.125 GHz respectively in the 5G mid-band, WiFi-6E 5/6 GHz band, and 6G upper mid-band are analyzed. Details of the UWB OM4A module and MIMO field test are presented.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Ultra wideband antennas",
                "Mobile antennas",
                "5G mobile communication",
                "6G mobile communication",
                "Mobile handsets",
                "Metals",
                "Receiving antennas",
                "MIMO communication",
                "Streams",
                "Spectral efficiency"
            ],
            "Author Keywords": [
                "Mobile antennas",
                "MIMO antennas",
                "on-metal four-antenna modules",
                "UWB MIMO antennas",
                "IoT mobile device MIMO antennas",
                "5G mid-band MIMO antennas",
                "6G upper mid-band MIMO antennas",
                "WiFi-6E MIMO antennas",
                "8 × 4 MIMO systems",
                "4 × 4 MIMO systems"
            ]
        },
        "title": "Ultra-Wideband On-Metal Four-Antenna Module for 5G/6G/WiFi-6E IoT Mobile Device MIMO Antennas and Its Field Test Study"
    },
    {
        "authors": [
            "Fahad Masood",
            "Muhammad Abbas Khan",
            "Mohammed S. Alshehri",
            "Wad Ghaban",
            "Faisal Saeed",
            "Hussain Mobarak Albarakati",
            "Ahmed Alkhayyat"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "19 June 2024",
        "doi": "10.1109/TCE.2024.3416035",
        "publisher": "IEEE",
        "abstract": "Wireless Sensor Networks (WSNs) integration with the Internet of Things (IoT) expands its potential by providing ideal communication and data sharing across devices, allowing more considerable monitoring and management in Consumer Electronics (CE). WSNs have an essential limitation in terms of energy resources since sensor nodes frequently run on limited power from batteries. This limitation necessitates the consideration of energy-efficient techniques to extend the network’s lifetime. In this article, an integrated approach has been presented to improve the energy efficiency of Wireless Sensor IoT Networks (WSINs) by leveraging modern machine learning algorithms with stochastic optimization. Recursive Feature Elimination (RFE) is utilized for the feature selection thus optimizing the input features for various machine learning models. These models are rigorously evaluated for their aptness to predict and mitigate energy consumption concerns inside WSINs. Subsequently, the stochastic optimization technique utilizes the uniform and normal distributions to model energy consumption situations. The results show that RFE-driven feature selection has significant effects on model performance and that Random Forest is effective at reaching higher accuracy. This research provides valuable perspectives for the design and implementation of WSINs in CE, supporting sustainable smart devices, by addressing energy consumption concerns using an optimized approach.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Wireless sensor networks",
                "Energy efficiency",
                "Internet of Things",
                "Machine learning",
                "Consumer electronics",
                "Feature extraction",
                "Routing"
            ],
            "Author Keywords": [
                "WSINs",
                "IoT",
                "RFE",
                "Energy Efficiency",
                "Consumer Electronics"
            ]
        },
        "title": "AI-Based Wireless Sensor IoT Networks for Energy-Efficient Consumer Electronics Using Stochastic Optimization"
    },
    {
        "authors": [
            "Jieun Lee",
            "JaeHoon Yoo",
            "Jiho Lee",
            "Yura Choi",
            "Seong Ki Yoo",
            "JaeSeung Song"
        ],
        "published_in": "Published in: IEEE Network ( Early Access )",
        "date_of_publication": "28 March 2024",
        "doi": "10.1109/MNET.2024.3382969",
        "publisher": "IEEE",
        "abstract": "Since 2008, the Korean government has instituted network separation technology, which physically isolates external internet networks from internal networks, aiming to thwart cyber-attacks. Consequently, the domestic financial sector was largely unaffected during global crises (2017 WannaCry ransomware outbreak and the 2021 Log4j vulnerability incident). However, there exist certain vulnerabilities owing to the presumption of their relative safety against cyber intrusions and the integration of cloud and Internet of Things (IoT) technologies in the current smart revolution. The existing network separation measures only mitigate one facet of potential cyber threats, rendering a comprehensive defense elusive. The rise of “air-gap” attacks, which exploit the isolated space between closed and external networks to illicitly transfer data and the existing research primarily substantiating the potential for data breaches from closed networks to their external counterparts are problems yet to be addressed. Thus, our study proposed a tangible optical air-gap attack methodology, harnessing readily available optical mediums within closed networks. Intricate measurement metrics that consider vital factors of the transmission environment were proposed. Moreover, acknowledging the proliferating integration of IoT devices, such as smart bulbs, to facilitate automation within closed networks, this study demonstrated the viability of optical air-gap attacks using these devices.",
        "issn": {
            "Print ISSN": "0890-8044",
            "Electronic ISSN": "1558-156X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Air gaps",
                "Optical sensors",
                "Optical devices",
                "Electromagnetic scattering",
                "Acoustics",
                "Internet of Things",
                "Light emitting diodes"
            ],
            "Author Keywords": [
                "Network Security",
                "Air-Gap Attack",
                "Closed Network",
                "Network Separation",
                "Optical Air-Gap Attack",
                "IoT Air-Gap Attack"
            ]
        },
        "title": "Optical Air-Gap Attacks: Analysis and IoT Threat Implications"
    },
    {
        "authors": [
            "Yi Wang",
            "Junlei Zhi",
            "Shaochuan Yang",
            "Zheng Chu",
            "Baofeng Ji",
            "Hui Guo",
            "Meng Hua",
            "Chunguo Li"
        ],
        "published_in": "Published in: IEEE Communications Letters ( Early Access )",
        "date_of_publication": "08 November 2024",
        "doi": "10.1109/LCOMM.2024.3494030",
        "publisher": "IEEE",
        "abstract": "This paper explores an intelligent reflecting surface (IRS) enhanced wireless powered Internet of Things (WP-IoT) network, wherein massive IoT nodes are wirelessly charged by radio frequency signals and then transmit information by means of an IRS to promote the system performance. To evaluate the network performance, we aim at maximizing the total throughput while adhering to constraints pertaining to fairness-aware individual signal-to-noise ratio (SNR), the time allocations (TAs) as well as the unit-modulus IRS phase shifts. However, the intricate coupling of these variables renders the optimization problem nonconvex, thus posing a challenge for direct solution. To deal with this dilemma, we first resort to employing the Lagrange dual method and Karush–Kuhn–Tucker (KKT) conditions to transform the sum of logarithmic objective function into sum of fractional counterpart, and further derive the analytical solutions of TAs for downlink wireless energy transfer (WET) and uplink wireless information transfer (WIT). Then, the Riemannian manifold optimization (RMO) is utilized to iteratively derive the IRS phase shifts in term of semi-closed-form expression. Lastly, numerical simulations are conducted to examine the efficacy of the proposed algorithm in enhancing performance in comparison to the existing benchmark schemes.",
        "issn": {
            "Print ISSN": "1089-7798",
            "Electronic ISSN": "1558-2558"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Wireless communication",
                "Signal to noise ratio",
                "Throughput",
                "Resource management",
                "Three-dimensional displays",
                "Optimization",
                "Mathematical models",
                "Uplink",
                "System performance"
            ],
            "Author Keywords": [
                "Intelligent Reflecting Surface (IRS)",
                "Wireless Powered Internet of Things (WP-IoT) Network",
                "Lagrange dual method",
                "Karush–Kuhn–Tucker (KKT) conditions"
            ]
        },
        "title": "Enhancing Fairness-Aware Massive Wireless Powered IoT Connectivity by IRS"
    },
    {
        "authors": [
            "Prakash Tekchandani",
            "Abhishek Bisht",
            "Ashok Kumar Das",
            "Neeraj Kumar",
            "Marimuthu Karuppiah",
            "Pandi Vijayakumar",
            "Youngho Park"
        ],
        "published_in": "Published in: IEEE Transactions on Big Data ( Early Access )",
        "date_of_publication": "29 April 2024",
        "doi": "10.1109/TBDATA.2024.3394700",
        "publisher": "IEEE",
        "abstract": "With the rise of Big data generated by Internet of Things (IoT) smart devices, there is an increasing need to leverage its potential while protecting privacy and maintaining confidentiality. Privacy and confidentiality in big data aims to enable data analysis and machine learning on large-scale datasets without compromising the dataset sensitive information. Usually current big data analytics models either efficiently achieves privacy or confidentiality. In this article, we aim to design a novel blockchain-enabled secured collaborative machine learning approach that provides privacy and confidentially on large scale datasets generated by IoT devices. Blockchain is used as secured platform to store and access data as well as to provide immutability and traceability. We also propose an efficient approach to obtain robust machine learning model through use of cryptographic techniques and differential privacy in which the data among involved parties is shared in a secured way while maintaining privacy and confidentiality of the data. The experimental evaluation along with security and performance analysis show that the proposed approach provides accuracy and scalability without compromising the privacy and security.",
        "issn": {
            "Electronic ISSN": "2332-7790"
        },
        "keywords": {
            "IEEE Keywords": [
                "Data models",
                "Blockchains",
                "Big Data",
                "Security",
                "Privacy",
                "Differential privacy",
                "Machine learning"
            ],
            "Author Keywords": [
                "Internet of Things (IoT)",
                "Differential privacy",
                "Collaborative model learning",
                "Blockchain",
                "Big data analytics",
                "Security"
            ]
        },
        "title": "Blockchain-Enabled Secure Collaborative Model Learning using Differential Privacy for IoT-Based Big Data Analytics"
    },
    {
        "authors": [
            "Silvano Cortesi",
            "Michele Crabolu",
            "Prodromos-Vasileios Mekikis",
            "Giovanni Bellusci",
            "Christian Vogt",
            "Michele Magno"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "14 October 2024",
        "doi": "10.1109/JIOT.2024.3479458",
        "publisher": "IEEE",
        "abstract": "Asset tracking solutions have proven their significance in industrial contexts, as evidenced by their successful commercialization (e.g., Hilti On!Track). However, a seamless solution for matching assets with their users, such as operators of construction power tools, is still missing. By enabling assetuser matching, organizations gain valuable insights that can be used to optimize user health and safety, asset utilization, and maintenance. This paper introduces a novel approach to address this gap by leveraging existing Bluetooth Low Energy (BLE)-enabled low-power Internet of Things (IoT) devices. The proposed framework comprises the following components: i) a wearable device, ii) an IoT device attached to or embedded in the assets, iii) an algorithm to estimate the distance between assets and operators by exploiting simple received signal strength indicator (RSSI) measurements via an Extended Kalman Filter (EKF), and iv) a cloud-based algorithm that collects all estimated distances to derive the correct asset-operator matching. The effectiveness of the proposed system has been validated through indoor and outdoor experiments in a construction setting for identifying the operator of a power tool. A physical prototype was developed to evaluate the algorithms in a realistic setup. The results demonstrated a median accuracy of 0.49m in estimating the distance between assets and users, and up to 98.6% in correctly matching users with their assets.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Accuracy",
                "Estimation",
                "Position measurement",
                "Monitoring",
                "Antenna measurements",
                "Radiofrequency identification",
                "Costs",
                "Bayes methods",
                "Asset management"
            ],
            "Author Keywords": [
                "iot",
                "edge computing",
                "sensor network",
                "signal processing",
                "cloud computation",
                "embedded systems",
                "low-power",
                "bluetooth low energy",
                "tracking"
            ]
        },
        "title": "A Proximity-Based Approach for Dynamically Matching Industrial Assets and Their Operators Using Low-Power IoT Devices"
    },
    {
        "authors": [
            "Ali M. Al Shahrani"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "07 June 2024",
        "doi": "10.1109/TCE.2024.3411478",
        "publisher": "IEEE",
        "abstract": "The enhanced high-order chirp spread spectrum (Chirp Spread Spectrum, CSS) modulation technique known as Lora (Long Range) technology has emerged as a popular modulation method for the terrestrial Internet of Things (IoT) on 6G connectivity. Artificial Intelligence (AI) has been a major driver of the consumer electronics industry’s amazing and revolutionary progress over the last ten years. However, the receiver commonly faces significant latency, low signal-to-noise ratio, and Doppler frequency shift due to the lengthy communication range and quick movement. In this work, a satellite Internet of Things system with LoRa technology is used to investigate the non-coherent soft demodulation approach and related receiver synchronization. This study investigates the non-coherent soft demodulation technique and accompanying receiver synchronization by using LoRa technology to a satellite IoT system. The simulation results demonstrate that the turbo-LoRa system can achieve near-ideal error performance at very low SNR and little pilot overhead. At BER=10-5, it outperforms the Hamming-LoRa system by 3 dB. The findings of the simulation indicate that the delay and numerous turbo-LoRa system with the suggested synchronization method and non-coherent soft demodulation algorithm can achieve good bit error performance with fewer pilot symbols when the Puller frequency shift is present concurrently.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "6G mobile communication",
                "Consumer electronics",
                "Demodulation",
                "Synchronization",
                "Chirp",
                "Receivers"
            ],
            "Author Keywords": [
                "6G communication",
                "Satellite Communication",
                "IoT",
                "Synchronization Scheme",
                "Long Range technology"
            ]
        },
        "title": "IoT Enabled Intelligent Receiver Synchronization System for Long-Range 6G Consumer Technology"
    },
    {
        "authors": [
            "Yijie Lin",
            "Chia-Chen Lin",
            "Ching-Chun Chang",
            "Chin-Chen Chang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "30 September 2024",
        "doi": "10.1109/JIOT.2024.3467152",
        "publisher": "IEEE",
        "abstract": "Since the COVID-19 outbreak, there’s been a growing need for contactless healthcare to meet medical diagnosis demands. Electronic health systems using the Internet of Things (IoT) are rapidly advancing, transmitting significant amounts of private medical data online. In telemedicine, where patients are diagnosed remotely, sensitive information such as patient records may be embedded into medical images for security purposes. Due to the large file sizes of medical images produced by equipment, compression is essential for fast transmission. To safeguard medical images in telemedicine and address bandwidth limitations, we utilize data hiding techniques and absolute moment block truncation coding (AMBTC) compression to introduce an IoT-driven electronic health protection mechanism. Our mechanism employs diverse methods to compress and embed data across various image blocks. Additionally, it offers a flexible adaptation to meet different application requirements concerning embedding capacity, visual quality, and file size by adjusting thresholds and variants. Compared to alternative methods, our approach delivers superior payload capacity and efficiency while preserving visual fidelity.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Image coding",
                "Medical diagnostic imaging",
                "Internet of Things",
                "Encoding",
                "Telemedicine",
                "Visualization",
                "Image reconstruction",
                "Vector quantization",
                "Biomedical monitoring",
                "Protection"
            ],
            "Author Keywords": [
                "AMBTC",
                "Data hiding",
                "Electronic health system",
                "IoT",
                "Medical images",
                "Security protection"
            ]
        },
        "title": "An IoT-Based Electronic Health Protection Mechanism With AMBTC Compressed Images"
    },
    {
        "authors": [
            "Abdulaziz Aldaej"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "20 January 2019",
        "doi": "10.1109/ACCESS.2019.2893445",
        "publisher": "IEEE",
        "abstract": "Retracted.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [],
            "Author Keywords": []
        },
        "title": "Notice of Retraction: Enhancing Cyber Security in Modern Internet of things (IoT) Using Intrusion Prevention Algorithm for IoT (IPAI)"
    },
    {
        "authors": [
            "Zhicheng Li",
            "Ming Li",
            "Yankun Wang",
            "Yang Wang"
        ],
        "published_in": "Published in: IEEE Transactions on Automation Science and Engineering ( Early Access )",
        "date_of_publication": "18 March 2024",
        "doi": "10.1109/TASE.2024.3375339",
        "publisher": "IEEE",
        "abstract": "Transportation systems often use agent-to-agent communication technologies to enhance control performance by transmitting information across wireless networks to keep a reasonable inter-distance between agents. The agent-to-agent data transmission is influenced by the fading channel and the limited communication bandwidth. Denial of Service attack model is used to describe the attack’s influence on control system. Thus, the communication model is built as Bernoulli distribution, Markovian distribution and so on. But in practical situations, it is more complicated than we imagine. To further push the theoretical results to practical situations, we investigate the jamming attack with non-stationary stochastic process in the leader-follower systems and introduce hidden Markovian distribution model (HMM) to describe this special jamming attack. Furthermore, the controller design method is presented to achieve stochastic stability of the leader-follower system. The structure of the control algorithm is a special MPC method, which is divided into a state feedback part and a modification part. The state feedback part is a typical LMIs controller to guarantee the system’s stability, while the modification part is used to improve the influence of the model mismatch and uncertainty. The highlights of the results are the following two points. Firstly, the more complex and precise transportation system model is founded by the hidden Markovian distribution. Secondly, the controller design method is also presented for the multi-agent systems with this HMM transportation system model. The existing results always find the worst case scenario, and design the controller in this scenario. Usually, the controller is very conservative and even unstable. The results in this paper can change the controller in different scenarios to adapt to the corresponding jamming stochastic distribution model, which is the novelty of this paper. We use two examples to illustrate the proposed results. T...",
        "issn": {
            "Print ISSN": "1545-5955",
            "Electronic ISSN": "1558-3783"
        },
        "keywords": {
            "IEEE Keywords": [
                "Hidden Markov models",
                "Jamming",
                "Stochastic processes",
                "Control systems",
                "Design methodology",
                "Adaptation models",
                "Transportation"
            ],
            "Author Keywords": [
                "Leader-follower system",
                "denial of service",
                "hidden Markovian distribution",
                "multi-agent system",
                "fading channel model"
            ]
        },
        "title": "Controller Design for Leader-Follower Systems With Hidden Markovian Jamming Attack"
    },
    {
        "authors": [
            "Chengying Mao",
            "Zheng Zhu",
            "Tsong Yueh Chen",
            "Dave Towey",
            "Linlin Wen",
            "Jifu Chen"
        ],
        "published_in": "Published in: IEEE Transactions on Reliability ( Early Access )",
        "date_of_publication": "26 August 2024",
        "doi": "10.1109/TR.2024.3441319",
        "publisher": "IEEE",
        "abstract": "Software developers can only obtain a very small amount of information from the individual failure-causing inputs, which makes debugging difficult. Therefore, it is necessary to explore additional failure-causing inputs (failure regions) using the known failure-causing inputs. In order to accurately and efficiently identify the failure region, we propose a novel two-stage search algorithm, TS-FRI. In the initial exploration stage, a round-robin search identifies several boundary failure-causing points, and the failure region's centroid is estimated. During the main search stage, the boundary failure-causing points are identified through iterative division of the input domain with an equally sized partitioning strategy. This results in the boundary points being as dispersed as possible around the failure-region boundary, with the polytope formed by the points approximating the failure region (e.g., a polygon in two dimensions). The proposed algorithm is validated through simulation and empirical analysis: The experimental results show that the TS-FRI accuracy is at least comparable to the best accuracy of the compared three algorithms, and can be ten times better. In addition, TS-FRI only takes a quarter of the computation time and half the failure-validation cost of the other algorithms.",
        "issn": {
            "Print ISSN": "0018-9529",
            "Electronic ISSN": "1558-1721"
        },
        "keywords": {
            "IEEE Keywords": [
                "Software",
                "Software algorithms",
                "Codes",
                "Costs",
                "Accuracy",
                "Debugging",
                "Software testing"
            ],
            "Author Keywords": [
                "Accuracy and efficiency",
                "failure region identification",
                "failure-based testing",
                "software failure",
                "software testing and debugging"
            ]
        },
        "title": "A Two-Stage Algorithm for Identifying Software Failure Regions"
    },
    {
        "authors": [
            "Yongju Tong",
            "Junlong Chen",
            "Minrui Xu",
            "Jiawen Kang",
            "Zehui Xiong",
            "Dusit Niyato",
            "Chau Yuen",
            "Zhu Han"
        ],
        "published_in": "Published in: IEEE Transactions on Cognitive Communications and Networking ( Early Access )",
        "date_of_publication": "24 July 2024",
        "doi": "10.1109/TCCN.2024.3431931",
        "publisher": "IEEE",
        "abstract": "Vehicular Metaverses are developed to enhance the modern automotive industry with an immersive and safe experience among connected vehicles and roadside infrastructures, e.g., RoadSide Units (RSUs). For seamless synchronization with virtual spaces, Vehicle Twins (VTs) are constructed as digital representations of physical entities. However, resource-intensive VTs updating and high mobility of vehicles require intensive computation, communication, and storage resources, especially for their migration among RSUs with limited coverages. To address these issues, we propose an attribute-aware auction-based mechanism to optimize resource allocation during VTs migration by considering both price and non-monetary attributes, e.g., location and reputation. In this mechanism, we propose a two-stage matching for vehicular users and Metaverse service providers in multi-attribute resource markets. First, the resource attributes matching algorithm obtains the resource attributes perfect matching, namely, buyers and sellers can participate in a double Dutch auction (DDA). Then, we train a DDA auctioneer using a generative pre-trained transformer (GPT)-based deep reinforcement learning (DRL) algorithm to adjust the auction clocks efficiently during the auction process. We compare the performance of social welfare and auction information exchange costs with state-of-the-art baselines under different settings. Simulation results show that our proposed GPT-based DRL auction schemes have better performance than others.",
        "issn": {
            "Electronic ISSN": "2332-7731"
        },
        "keywords": {
            "IEEE Keywords": [
                "Task analysis",
                "Resource management",
                "Costs",
                "Clocks",
                "Synchronization",
                "Pricing",
                "Real-time systems"
            ],
            "Author Keywords": [
                "Vehicular Metaverses",
                "VTs migration",
                "Multi-attribute Auction",
                "Machine Learning",
                "Resource Allocation"
            ]
        },
        "title": "Multi-attribute Auction-Based Resource Allocation for Twins Migration in Vehicular Metaverses: A GPT-Based DRL Approach"
    },
    {
        "authors": [
            "Ahsan Rafiq",
            "Min Wei",
            "Ping Wang",
            "Deepak Kumar Jain"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "17 June 2024",
        "doi": "10.1109/TCE.2024.3414324",
        "publisher": "IEEE",
        "abstract": "Standardizing the Industrial Internet of Things (IIoT) with 6TiSCH enables the IIoT nodes to utilize low power with high reliability. It ensures the allocation of resources for IIoT nodes with minimal power consumption. The existing work focused on various issues in the 6TiSCH-based IIoT in terms of network formation, and scheduling. However, security, energy efficiency, and latency remain challenging issues. The aforementioned issues are resolved by adopting different technologies in the 6TiSCH IIoT environment such as edge computing and federated learning named AIFed-6TiSCH IIoT. In the 6TiSCH layer, all IIoT nodes are authenticated to the Trusted Authority (TA) to reduce unwanted malicious traffic using Hybrid Authentication Algorithm (HAS) named Mchacha-poly 1305 algorithm based on several credentials. The authenticated nodes are allowed to form the network, for that we utilized a multicast CORONA-based DODAG structure using the Dijkstra algorithm. Eventually, new node joining is initiated by an already joined node which transmits Enhanced Beacons (EBs) in adaptive time intervals for synchronizing the new node. The synchronized nodes aim to select the optimal parent for transmitting data to the root node using an Enhanced Sea Gull (ESG) optimization algorithm based on mobility and several information. After that, optimal channel selection and scheduling are done using a Multi Criteria Decision Making (MCDM) algorithm named TOPSIS and a hybrid scheduling algorithm (i.e., Autonomous, and centralized scheduling) based on federated learning. By Performing channel selection, the congestion rate is reduced and federated learning ensures privacy and communication reliability. In the edge layer, data from the root are forwarded via a smart gateway to the edge servers for local processing. The security and latency issues in the edge layer are mitigated by performing Deep Reinforcement Learning (DRL) based migration named Enhanced Multi-Agent Deep Deterministic Policy Gra...",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Industrial Internet of Things",
                "Energy consumption",
                "Synchronization",
                "Routing",
                "Task analysis",
                "Reliability",
                "Federated learning"
            ],
            "Author Keywords": [
                "Industrial Internet of Things (IIoT)",
                "6TiSCH",
                "Artificial Intelligence (AI)",
                "Edge Computing",
                "Federated Learning (FL)",
                "Hybrid Scheduling",
                "Migration"
            ]
        },
        "title": "Delay Aware 6TiSCH IIoT Networks for Energy Efficient Data Transmission by Adopting Federated Learning and Edge Computing"
    },
    {
        "authors": [
            "Kungan Zeng",
            "Qianming Wang",
            "Jianhao Tang",
            "Zhenni Li",
            "Kan Xie",
            "Shengli Xie"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "07 November 2024",
        "doi": "10.1109/JIOT.2024.3485099",
        "publisher": "IEEE",
        "abstract": "The reception of Non-Line-of-Sight (NLOS) signals in urban areas, such as urban canyons and overpasses, can cause severe errors in Global Navigation Satellite System (GNSS) positioning. Machine learning-based NLOS mitigation methods have become increasingly popular. However, existing methods cannot obtain satisfactory NLOS recognition accuracy across multiple locations or scenarios. Furthermore, in scenarios with severe occlusion, directly removing recognized NLOS signals may reduce the number of available satellites for positioning algorithms, resulting in lower positioning precision. To address these issues, this study proposes a deep learning-based NLOS interference mitigation method to improve the precision of GNSS single-point positioning (SPP). First, to improve NLOS signal recognition across multiple locations, we propose the DSN model for NLOS recognition, which utilizes self-attention networks to construct both spatial and temporal channels for modeling spatial environmental characteristics and signal temporal features, respectively. Second, to mitigate the interference from NLOS signals, we design a novel weighting scheme using NLOS recognition results to revise the elevation angle-based scheme. Next, we propose the SPP-DSN algorithm by combining the DSN model and the designed weighting scheme to improve positioning precision in urban areas. Finally, we collected real-world data to conduct experiments to investigate the performance of our proposed method. The experimental results show that our proposed DSN model can effectively improve NLOS recognition accuracy across multiple locations. Compared to the regular SPP algorithm, our proposed SPP-DSN method can enhance positioning precision by over 29% in urban canyons and more than 10% under overpasses.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Satellites",
                "Global navigation satellite system",
                "Interference",
                "Accuracy",
                "Urban areas",
                "Mathematical models",
                "Prevention and mitigation",
                "Feature extraction",
                "Prediction algorithms",
                "Internet of Things"
            ],
            "Author Keywords": [
                "GNSS",
                "NLOS interference",
                "self-attention mechanism",
                "SPP"
            ]
        },
        "title": "Mitigating NLOS Interference in GNSS Single-Point Positioning Based on Dual Self-Attention Networks"
    },
    {
        "authors": [
            "Chunming Xiao",
            "Yonghui Huang",
            "Haonan Huang",
            "Qibin Zhao",
            "Guoxu Zhou"
        ],
        "published_in": "Published in: IEEE Transactions on Emerging Topics in Computational Intelligence ( Early Access )",
        "date_of_publication": "24 June 2024",
        "doi": "10.1109/TETCI.2024.3412999",
        "publisher": "IEEE",
        "abstract": "Tensorizedmulti-view subspace clustering has attracted intensive attention to achieve promising clustering performance by effectively modeling both consistency and high-order correlation structure from multiple views. However, existing tensorized multi-view subspace clustering methods often neglect the prevalent diversity among different views, which are specific attributes unique to each view. In this paper, we focus on simultaneously exploiting the multi-view consistency and diversity information in a tensorized multi-view subspace framework. In the diversity part, an additional position-aware exclusivity term is introduced to explore the unique features of each view to enhance feature complementarity between different views. Meanwhile, we explore inter-view similarity by minimizing the tensor Schatten p -norm, which well captures both consistency and high-order correlation information of multi-view data. Then, the objective function can be efficiently optimized by the alternating direction method of multipliers. Extensive experiments on eight benchmark datasets demonstrate that the proposed method outperforms the state-of-the-art multi-view clustering methods.",
        "issn": {
            "Electronic ISSN": "2471-285X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Tensors",
                "Correlation",
                "Data models",
                "Automation",
                "Matrix decomposition",
                "Linear programming",
                "Clustering methods"
            ],
            "Author Keywords": [
                "Consistency",
                "diversity",
                "multi-view subspace clustering",
                "schatten $p$ -norm",
                "tensor"
            ]
        },
        "title": "Consistency and Diversity Induced Tensorized Multi-View Subspace Clustering"
    },
    {
        "authors": [
            "Junyoung Maeng",
            "Inho Park",
            "Jinwoo Jeon",
            "Hyunjin Kim",
            "Hoi Lee",
            "Chulwoo Kim"
        ],
        "published_in": "Published in: IEEE Transactions on Circuits and Systems II: Express Briefs ( Early Access )",
        "date_of_publication": "02 August 2024",
        "doi": "10.1109/TCSII.2024.3437449",
        "publisher": "IEEE",
        "abstract": "This brief proposes a reconfigurable DC.DC converter for an energy-harvesting system with a tri-mode recharging method for miniature Internet of Things (IoT) batteries operating at various temperatures. The proposed system and IoT battery operate at temperatures as low as –5 ° C with an internal resistance (RINT) of up to 1.9 kY. The proposed recharging method utilizes a low-dropout regulator and DC.DC converter with a negative inductor current to rapidly recharge a storage capacitor (CSTOR) and isolate the IoT battery from the CSTOR. With a 180 nm CMOS process, the charge of the CSTOR can be restored rapidly with the aid of the IoT battery, which reduces the ratio of the activation time (tACT) to the recharging time (tRCHG) to 0.27 at –5 ° C.",
        "issn": {
            "Print ISSN": "1549-7747",
            "Electronic ISSN": "1558-3791"
        },
        "keywords": {
            "IEEE Keywords": [
                "Batteries",
                "Logic gates",
                "Switches",
                "Inductors",
                "Internet of Things",
                "Capacitors",
                "Buck converters"
            ],
            "Author Keywords": [
                "Energy harvesting",
                "single inductor",
                "solid-state battery",
                "storage capacitor recharging",
                "thin-film battery"
            ]
        },
        "title": "A Tri-Mode Reconfigurable DC-DC Converter With Photovoltaic Energy Harvesting for Miniature IoT Batteries"
    },
    {
        "authors": [
            "Megha Sharma",
            "Abhinav Tomar",
            "Abhishek Hazra"
        ],
        "published_in": "Published in: IEEE Consumer Electronics Magazine ( Early Access )",
        "date_of_publication": "27 September 2024",
        "doi": "10.1109/MCE.2024.3470340",
        "publisher": "IEEE",
        "abstract": "Industry 5.0 emphasises the profound synergy between intelligent systems and human involvement in various applications, combining precise automation in production with important cognitive abilities. Integrating robust, low-latency networking with AI and IoT technology expedites industrial advancement, showcasing the smooth merging of the physical and digital domains in Industry 5.0. We explain different design concepts that govern the integration of AI and IoT in Industry 5.0. We highlight the significance of task offloading using Deep Reinforcement Learning (DRL) approaches. In addition, we explore the fundamental structure, established guidelines, and communication methods that support IoT systems enhanced by AI. Our goal is to seamlessly integrate the capabilities of AI with the underlying infrastructure of IoT. Finally, we emphasise important research obstacles and remaining concerns, which require collaborative efforts to achieve the full potential of Industry 5.0.",
        "issn": {
            "Print ISSN": "2162-2248",
            "Electronic ISSN": "2162-2256"
        },
        "keywords": {
            "IEEE Keywords": [
                "Fifth Industrial Revolution",
                "Artificial intelligence",
                "Industries",
                "Decision making",
                "Robots",
                "Robot sensing systems",
                "Protocols",
                "Industrial Internet of Things",
                "Service robots",
                "Real-time systems"
            ],
            "Author Keywords": []
        },
        "title": "From Connectivity to Intelligence: The Game-Changing Role of AI and IoT in Industry 5.0"
    },
    {
        "authors": [
            "Chittaranjan Swain",
            "Manmath Narayan Sahoo",
            "Anurag Satpathy",
            "Sambit Bakshi",
            "Soumya K. Ghosh"
        ],
        "published_in": "Published in: IEEE Transactions on Services Computing ( Early Access )",
        "date_of_publication": "02 August 2024",
        "doi": "10.1109/TSC.2024.3436648",
        "publisher": "IEEE",
        "abstract": "Resource-constrained Internet of Things (IoT) devices depend on remote Cloud/Fog Nodes (FNs) to execute deadline-sensitive services. Offloading computations of real-time services to a remote cloud server results in intolerable latency due to intermittent channels, higher transmission delays, and scarce spectrum resources. Therefore, offloading to nearby FNs is preferable; however, it introduces several significant issues: ( i ) allocation of limited FN resources, ( ii ) deadline constraint of heterogeneous services, and ( iii ) requirement of computationally inexpensive and scalable strategies. This paper proposes a M-DAFTO model to tackle the abovementioned issues and generate a fair offloading plan in polynomial time. The offloading problem is modeled as a many-to-one matching game with maximum and minimum quotas at each FN. Because the deferred acceptance (DA) algorithm fails to operate with minimum quotas, we adopt a variant of the DA algorithm, a multistage deferred acceptance (MSDA) algorithm, to solve the offloading problem. The overall goal of M-DAFTO is to reduce the aggregate offloading delay with increased assignment of tasks to FNs. Extensive simulation and analysis confirm a 30.26% and a 93.53% reduction in offloading delay and outages (unassigned tasks) compared to the baselines.",
        "issn": {
            "Electronic ISSN": "1939-1374"
        },
        "keywords": {
            "IEEE Keywords": [
                "Delays",
                "Task analysis",
                "Internet of Things",
                "Real-time systems",
                "Games",
                "Quality of service",
                "Schedules"
            ],
            "Author Keywords": [
                "Task Offloading",
                "IoT",
                "Fog Systems",
                "Matching Theory",
                "Deferred Acceptance Algorithm",
                "Analytic Hierarchy Process",
                "Technique for Order Performance by Similarity to Ideal Solution"
            ]
        },
        "title": "M-DAFTO: Multi-Stage Deferred Acceptance based Fair Task Offloading in IoT-Fog Systems"
    },
    {
        "authors": [
            "Ridhima Verma",
            "Suman Kumar"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/JSEN.2024.3487638",
        "publisher": "IEEE",
        "abstract": "Birds play a pivotal role in maintaining global biodiversity by serving as vital agents in the key ecosystem functions, such as seed dispersal, insect regulation, and pollination. However, escalating anthropogenic pressures such as deforestation, poaching, and climate change have increasingly imperiled the avian populations worldwide. Consequently, effective monitoring strategies are essential for conservation efforts. However, the traditional monitoring methods often fall short due to limitations in power efficiency and data storage. This paper presents the design and development of AviEar, a novel wireless sensor node tailored for monitoring of avian species. The developed node is an Internet of Things (IoT) device which hosts a MEMS microphone, an ultra-low power ARM Cortex MCU and a storage unit. The proposed system seamlessly integrates acoustic data recording, on-board signal processing, storage, and cloud-based uploads to facilitate remote monitoring. A standout feature is its rapid target species detection algorithm, approximately executing within a mere 1.443 seconds. Without real-time onboard processing, the system would generate redundant data and experience increased battery drain. Its real-time selective logging and transmission framework yields an impressive operational span of up to 2 months at an 8 kHz sampling rate. The field experiments demonstrate AviEar’s ability to provide avian acoustic data with 99.6% precision, 95% recall, 97.2% F1-score, a mean 0.77 confidence score, and remarkable power efficiency, showcasing its suitability for sustainable monitoring solutions. Moreover, the outcomes of these deployments furnish conservation decision-makers and researchers with invaluable datasets, empowering them to conduct comprehensive and large-scale monitoring initiatives.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Climate change",
                "Acoustics",
                "Birds",
                "Microelectromechanical systems",
                "Microphones",
                "Wireless sensor networks",
                "Biodiversity",
                "Ecosystems",
                "Internet of Things",
                "Low power electronics"
            ],
            "Author Keywords": [
                "Acoustic monitoring",
                "bird vocalizations",
                "IoT",
                "low power",
                "MEMS microphone",
                "wireless sensor"
            ]
        },
        "title": "AviEar: An IoT-based Low Power Solution for Acoustic Monitoring of Avian Species"
    },
    {
        "authors": [
            "Wangyang Yu",
            "Jing Zhang",
            "Lu Liu",
            "Yuan Liu",
            "Xiaojun Zhai",
            "Ruhul Kabir Howlader"
        ],
        "published_in": "Published in: IEEE Transactions on Sustainable Computing ( Early Access )",
        "date_of_publication": "13 August 2024",
        "doi": "10.1109/TSUSC.2024.3441722",
        "publisher": "IEEE",
        "abstract": "A causal relationship forms when one event triggers another's change or occurrence. Causality helps to understand connections among events, explain phenomena, and facilitate better decision-making. In IoT systems, massive consumption of energy may lead to specific types of airpollution. There are causal relationships among air pollutants. Analyzing their interactions allows for targeted adjustments in energy use, like shifting to cleaner energy and cutting high-emission sources. This reduces air pollution and boosts energy sustainability, aiding sustainable development. This paper introduces a distributed data-driven machine learning method for high-level causal analysis (DMHC), which extracts general and high-level Complex Event Processing (CEP) rules from unlabeled data. CEP rules can capture the interactions among events and represent the causal relationships among them. DMHC deploys a two-layer LSTM attention mechanism model and decision tree algorithm to filter and label data, extracting general CEP rules. Afterward, it proceeds to generate event logs based on general rules with heuristic mining (HM), extracting high-level CEP rules that pertain to causal relationships. These high-level rules complement the extracted general rules and reflect the causal relationships among the general rules. The proposed high-level methodology is validated using a real air quality dataset.",
        "issn": {
            "Electronic ISSN": "2377-3782"
        },
        "keywords": {
            "IEEE Keywords": [
                "Data mining",
                "Long short term memory",
                "Analytical models",
                "Air pollution",
                "Energy consumption",
                "Data models",
                "Atmospheric modeling"
            ],
            "Author Keywords": [
                "Energy management",
                "IoT systems",
                "Machine learning",
                "Causal analysis",
                "Petri nets",
                "CEP"
            ]
        },
        "title": "A Distributed Data-Driven and Machine Learning Method for High-Level Causal Analysis in Sustainable IoT Systems"
    },
    {
        "authors": [
            "Sourav Saha",
            "Ashok Kumar Das",
            "Mohammad Wazid",
            "YoungHo Park",
            "Sahil Garg",
            "Mubarak Alrashoud"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "22 April 2024",
        "doi": "10.1109/TCE.2024.3391667",
        "publisher": "IEEE",
        "abstract": "6G (sixth-generation wireless), the successor to 5G cellular technology, operates at higher frequencies than its predecessor and supports significantly greater capacity and markedly reduced latency. Healthcare is treated as a complex system with various stakeholders, like doctors, patients, hospitals, pharmaceutical companies as well as healthcare decision-makers. The innovations in the Internet of Things (IoT) and incorporating emerging technology in the healthcare systems provide the quality of services to the people and save millions of lives. However, patient privacy and secure interchange of medical data from various healthcare providers need to be adequately addressed. Furthermore, incorporating blockchain in the healthcare system helps to make the system more transparent and secure due to inherent properties of the blockchain. In addition, Big Data analytics helps in analyzing large datasets from hundreds of patients, and then in identifying various clusters and correlation among datasets, and also in developing predictive models. In this paper, we aim to propose a new smart contract-based access control for 6G-enabled blockchain assisted in the healthcare system (in short, we call it as SACS). SACS provides a patient to communicate with its healthcare management authority securely and helps to interchange his/her medical information across healthcare providers. A detailed security analysis, experimental results and comparative study assure that the proposed SACS is secure by preventing possible active and passive attacks, and requires less computational and communication costs as compared to those for other relevant competing schemes.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Blockchains",
                "Hospitals",
                "6G mobile communication",
                "Data privacy",
                "Privacy",
                "Medical diagnostic imaging",
                "Consumer electronics"
            ],
            "Author Keywords": [
                "Internet of Things (IoT)",
                "6G",
                "helathcare applications",
                "access control and key agreement",
                "Blockchain",
                "Big Data analytics",
                "security"
            ]
        },
        "title": "Smart Contract-Based Access Control Scheme for Blockchain Assisted 6G-Enabled IoT-Based Big Data Driven Healthcare Cyber Physical Systems"
    },
    {
        "authors": [
            "Qiran Wang",
            "Zhengdi Shen"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "19 September 2024",
        "doi": "10.1109/JIOT.2024.3464099",
        "publisher": "IEEE",
        "abstract": "Internet of Things (IoT) localization is crucial in smart applications. DV-Hop based algorithms offer cost-effective and scalable solutions relying on connectivity information. However, most algorithms have limitations in irregular areas, including a tendency to over-estimate the distances between nodes. To address this issue, we propose distance upper bound estimations by triangle inequality (UBETI) to adjust the DV-Hop objectives and mitigate the distance overestimation in irregular areas. In addition, we propose a crossover strategy for the genetic algorithms (GAs) in the optimization, named Stats-crossover (STCR). ST-CR generates the offspring of the population leveraging the statistical data of the parent population and improves the optimization performance. The two proposed strategies can be incorporated into existing DV-Hop algorithms and boost their performance. Evaluation results show that compared with other highly regarded methods, the proposed algorithm has a meaningful improvement, particularly in irregular areas. Ablation studies demonstrate the effectiveness of each step in the proposed algorithm.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Estimation",
                "Location awareness",
                "Optimization",
                "Linear programming",
                "Internet of Things",
                "Accuracy",
                "Upper bound"
            ],
            "Author Keywords": [
                "IoT localization",
                "DV-Hop",
                "distance estimation model",
                "genetic algorithm",
                "crossover",
                "multi-objective optimization"
            ]
        },
        "title": "UBETI: A DV-Hop Localization Algorithm for IoT With Distance Upper Bound Estimation by Triangle Inequality"
    },
    {
        "authors": [
            "Shuyi Wang",
            "Haotong Cao",
            "Longxiang Yang",
            "Sahil Garg",
            "Georges Kaddoum",
            "Mubarak Alrashoud"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "11 July 2024",
        "doi": "10.1109/TCE.2024.3426483",
        "publisher": "IEEE",
        "abstract": "The rapid development of technologies such as the Internet of Things, SDN/NFV, and 6G is driving up the demand for dynamic deployment of service function chains (SFC). These technologies are making network architectures more complex and service deployments more dynamic and adaptable.More than ever, there are situations that call for multi-objective SFC dynamic deployment, which necessitates resource game optimization across multiple objectives. For the first time, multi-objective optimization in dynamic SFC deployment scenarios is realized using a multi-agent deep reinforcement learning system based on graph convolutional network(GCN) in this study.Here we mainly focus on the game optimization problem of two objectives: minimum delay time and minimum resource utilization.Three sample complex networks are used to evaluate the proposed methodology: Random, BA scale-free, and Small-world. The results of the simulation indicate that the proposed method can be well applied in IoT scenarios. In general,this method is superior to other mainstream methods in terms of reward and convergence performance.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Heuristic algorithms",
                "Optimization",
                "Feature extraction",
                "Internet of Things",
                "Games",
                "Dynamic scheduling",
                "Deep reinforcement learning"
            ],
            "Author Keywords": [
                "IoT",
                "multi-agent",
                "graph convolutional network",
                "deep reinforcement learning",
                "service function chain"
            ]
        },
        "title": "GCN-Based Multi-Agent Deep Reinforcement Learning for Dynamic Service Function Chain Deployment in IoT"
    },
    {
        "authors": [
            "Zichen Wang",
            "Zhijun Meng",
            "Tuo Tian",
            "Weiqi Gai",
            "Guodong Zhao",
            "Jingjing Wang",
            "Chunxiao Jiang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "25 September 2024",
        "doi": "10.1109/JIOT.2024.3467396",
        "publisher": "IEEE",
        "abstract": "Due to the outstanding maneuverability, unmanned aerial vehicles (UAVs) garner increasing applications in the Internet of Things (IoT), such as data collection, environmental monitoring, emergency communication, search and rescue, and autonomous exploration is the foundation of these missions which can obtain a pre-built map automatically. However, current methods suffer from low efficiency. To address this, we propose a hierarchical exploration framework for UAVs with limited field-of-view (FOV) sensor, encompassing frontier and viewpoint generation, global coverage path planning, and active perception trajectory generation. Firstly, we employ the random seeds frontier generation and anisotropic Gaussian sampling for environment information update, which can efficiently utilize sensor’s sensing range. Then, we design an appropriate heuristic function to represent the connection cost between different viewpoints and solve the global coverage path as a travelling salesman problem (TSP) to balance the long-term and short-term information gain. Moreover, active perception trajectory planning is proposed to enhance flight safety, smoothness, and exploration efficiency. Simulation and real-world scenario results indicate that the proposed method achieves higher efficiency in frontier generation and viewpoint sampling, and the difficulty of solving global coverage path does not significantly increase with the environment scale. Our proposed method improves exploration efficiency by 17-27% compared to the state-of-the-art (SOTA) method.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Autonomous aerial vehicles",
                "Internet of Things",
                "Sensors",
                "Planning",
                "Trajectory",
                "Servers",
                "Data collection"
            ],
            "Author Keywords": [
                "Unmanned aerial vehicles(UAV)",
                "path planning",
                "autonomous exploration",
                "UAV applications",
                "IoT applications"
            ]
        },
        "title": "Efficient Autonomous UAV Exploration Framework with Limited FOV Sensors for IoT Applications"
    },
    {
        "authors": [
            "Karthikeyan Gopalakrishnan",
            "Arun Kumar Balakrishnan",
            "Kousalya Govardhanan",
            "Kandala NV P S Rajesh"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "28 October 2024",
        "doi": "10.1109/JIOT.2024.3486991",
        "publisher": "IEEE",
        "abstract": "Non-epileptic seizures are a clinical symptom of abnormally high synchronous cortical activity known as psychogenic non-epileptic seizures (PNES) as they exhibit no outward signs of neurological damage. The need for differentiating PNES from full-body General Seizures (GTCS) decreases therapy time and ensures proper hospice. Internet-of-Medical-Things (IoMT) provide a closed-loop mechanism to accurately measure seizures. The erratic nature of seizures has drawn the attention where false detection could have catastrophic impact. The paper discusses the VPSI 2.0 (Vibration Profile Seizure Identifier) where vibration profile analysis of an ictal patient is measured in realtime to classify seizures in an IoMT framework. The novel seizure detection model has been proposed for differentiating between multiple seizure types. The Simultaneous Equation (S.E) protocol is developed for noninvasive stigma-free monitoring of seizures for continual monitoring. S.E based IoT seizure classifier is helpful to mitigate challenges present in detecting real-time occurrences of seizure by 95.683% in a controlled environment.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Monitoring",
                "Epilepsy",
                "Mathematical models",
                "Electroencephalography",
                "Biomedical monitoring",
                "Sensors",
                "Vibrations",
                "Brain modeling",
                "Internet of Things",
                "Protocols"
            ],
            "Author Keywords": [
                "IoT",
                "IoMT",
                "Seizures",
                "GTCS",
                "PNES"
            ]
        },
        "title": "VPSI 2.0: IoT-Based Hybrid Protocol with Simultaneous Equations for Real-Time Seizure Classification and False-Negative Mitigation"
    },
    {
        "authors": [
            "Weibo Hao",
            "Fang Ye",
            "Yuan Tian"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "24 September 2024",
        "doi": "10.1109/JIOT.2024.3467068",
        "publisher": "IEEE",
        "abstract": "The UAV, serving as an aerial platform, is capable of assisting IoT devices in providing global emergency coverage. To incentivize the Macro Base Station (MBS) operator to lease idle spectrum resources to the Unmanned Aerial Vehicle (UAV) operator, an appropriate cross-layer spectrum trading mechanism needs to be established. To address the time-variation and asymmetry of information in spectrum trading, this paper adopts multi-term contract theory to analyze this issue. We formulate the problem of designing multi-term optimal trading for these three scenarios: one-dimensional static transactions, multi-dimensional static transactions, and dynamic transactions. We employed deviations instead of derivatives to decompose the original problem into several subproblems. Based on the numerical results, we conclude that multi-term transactions cannot be simply decomposed into repeated single-term transactions. The optimal contract design should fully consider the composition of private information in multi-term transactions, as well as obtaining discounts from future revenue commitments.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Contracts",
                "Internet of Things",
                "Autonomous aerial vehicles",
                "Resource management",
                "Satellites",
                "Base stations",
                "Dynamic scheduling"
            ],
            "Author Keywords": [
                "UAV-Enabled IoT",
                "Space-Air-Ground Integrated Networks",
                "Spectrum Resource Trading",
                "Contract Theory",
                "Multi-Term Contract"
            ]
        },
        "title": "Design of Multi-Term Spectrum Resource Trading Contracts in UAV-Enabled IoT"
    },
    {
        "authors": [
            "Xuan Zhou",
            "Tanping Zhou",
            "Yuan Tian",
            "Weidong Zhong",
            "Xiaoyuan Yang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "14 August 2024",
        "doi": "10.1109/JIOT.2024.3443282",
        "publisher": "IEEE",
        "abstract": "As the Internet of Things (IoT) is booming, the transmission speed of data in the network is getting more and more attention. Network coding is an effective technique to improve network throughput. In network coding, the encoded packets must be integrity-checked to prevent pollution attacks. Some linearly homomorphic signature schemes based on bilinear pairs have been used to check the integrity of packets, and so far the scheme LZL20 is the most efficient signature scheme among them. Here, we first analyze the security model and signature structure of LZL20, and find that there is a security vulnerability in the scheme. Experiments show that for a 12~18 KB file, our signature forgery algorithm can forge a message/signature pair with 100% probability within 3~5 ms. Then, we construct a linearly homomorphic signature scheme with higher signature efficiency and shorter signature length. In random oracle model, we proved the scheme is existentially-unforgeable under adaptive chosen message attacks. We theoretically analyze our signature length to be 320 bits shorter than LZL20. Finally, we implement our scheme, and for a 12~18 KB file, experiments show that the signature time of our scheme is 60.93%~62.59% of that of LZL20.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Vectors",
                "Security",
                "Network coding",
                "Internet of Things",
                "Pollution",
                "Forgery",
                "Symbols"
            ],
            "Author Keywords": [
                "IoT",
                "network coding",
                "signature collision attack",
                "linearly homomorphic signature"
            ]
        },
        "title": "Linearly Homomorphic Signature Scheme With High Signature Efficiency and Its Application in IoT"
    },
    {
        "authors": [
            "Hao Liu",
            "Yinghai Zhou",
            "Binxing Fang",
            "Yanbin Sun",
            "Ning Hu",
            "Zhihong Tian"
        ],
        "published_in": "Published in: IEEE Transactions on Mobile Computing ( Early Access )",
        "date_of_publication": "10 September 2024",
        "doi": "10.1109/TMC.2024.3455564",
        "publisher": "IEEE",
        "abstract": "With the rapid development of mobile and wireless technologies, the industrial manufacturing sector has entered the era of automation. The proliferation of mobile devices, sensor networks, and remote monitoring systems enables factory equipment to be more flexibly connected and controlled. However, the trend towards industrial networks also brings new challenges. Industrial control systems (ICSs) and programmable logic controllers (PLCs) are more susceptible to hacker attacks and interference. Honeypoints have been developed to protect ICSs from addressing these threats, including potential internal attacks. Honeypoints are active deception systems that mitigate the limitations of conventional defense mechanisms, which successfully entice and neutralize internal enemies. This paper presents the PLC Honeypoint Communication Generator (PHCG), enhancing honeypoint protective capabilities in industrial IoT systems. Using an automated construction process, PHCG provides a convenient and efficient deployment method, ensuring quick and effective functioning. The functionality of a PLC relies on a data generation model trained on PLC response data. This model allows PHCG to imitate genuine PLC responses accurately when given authorized commands. The experimental results illustrate the adaptability of information produced by PHCG in different communication processes, with satisfactory timescales for both model training and data generation.",
        "issn": {
            "Print ISSN": "1536-1233",
            "Electronic ISSN": "1558-0660"
        },
        "keywords": {
            "IEEE Keywords": [
                "Main-secondary",
                "Threat modeling",
                "Security",
                "Wireless communication",
                "Computational modeling",
                "Communication system security",
                "Transformers"
            ],
            "Author Keywords": [
                "Wireless communication",
                "IoT",
                "honeypoint",
                "PLC",
                "data generation"
            ]
        },
        "title": "PHCG: PLC Honeypoint Communication Generator for Industrial IoT"
    },
    {
        "authors": [
            "Aditi Chakraborty",
            "Ashis Maity"
        ],
        "published_in": "Published in: IEEE Journal of Solid-State Circuits ( Early Access )",
        "date_of_publication": "07 June 2024",
        "doi": "10.1109/JSSC.2024.3406565",
        "publisher": "IEEE",
        "abstract": "This work presents a single-inductor multiple-output (SIMO) converter to achieve a fast transient response using a low-profile inductor for powering the command-directed Internet of Things (IoT) nodes. The multiple outputs are sequentially charged and regulated afterward by the proposed state-driven priority sequencing and the delay-adjusted fixed window (DAFW) hysteretic controller to keep the output transient ripple low. Also, the multiple outputs are serviced using the proposed constant current-peak sequential discontinuous conduction mode (DCM)-continuous conduction mode (CCM) (CCPS-DCM-CCM) operation to obtain zero cross-regulation and a fast transient response. To make a better trade-off between light load efficiency and fast transient response at the high load, the proposed converter operates in two different command-directed modes, namely CCPS-DCM switching at $\\sim$ 70 kHz for a load range of 10 $~\\mu$ A–1 mA and CCPS-DCM-CCM operating at $\\sim$ 3.1 MHz for a load range of 1–40 mA. The proposed SIMO converter is designed in a standard 180-nm CMOS technology to generate three regulated outputs of 0.9, 1.2, and 1.5 V. In the measured results, the converter shows an output transient ripple $<$ 1.1% and power efficiency of 76.3% at a load current of 10 $~\\mu$ A in each output with $L$ $=$ 470 nH $C_{O}$ $=$ 22 $~\\mu$ F.",
        "issn": {
            "Print ISSN": "0018-9200",
            "Electronic ISSN": "1558-173X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Inductors",
                "Switches",
                "Sequential analysis",
                "Transient analysis",
                "Transient response",
                "Internet of Things",
                "Switching frequency"
            ],
            "Author Keywords": [
                "Continuous conduction mode (CCM)",
                "dc–dc converter",
                "discontinuous conduction mode (DCM)",
                "hysteretic control",
                "Internet of Things (IoT)",
                "power management",
                "single-inductor multiple-outputs (SIMO) converter"
            ]
        },
        "title": "A Fast SIMO Converter for Command-Directed IoT Nodes With State-Driven Priority Sequencing and Delay-Adjusted Fixed Window Hysteretic Control Using Constant Current-Peak Sequential DCM-CCM Operation"
    },
    {
        "authors": [
            "Zhan Shi"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "11 October 2024",
        "doi": "10.1109/TCE.2024.3478767",
        "publisher": "IEEE",
        "abstract": "The swift advancement of internet of things (IoT) and consumer electronics along with edge computing offers a feasible solution to meet the stringent data processing requirements imposed by real-time monitoring of distributed solar photovoltaic (PV). For resource scheduling problems in terms of data processing, ant colony algorithm offers an effective solution. However, conventional ant colony suffers from poor convergence and path searching performances due to the lack of adaptability. Hence, we propose an edge-end collaborative multi-timescale resource scheduling algorithm to address these challenges. The large-timescale container selection is optimized by the proposed ant colony algorithm, which dynamically adjusts pheromone and heuristic factors to strike a balance between exploitation and exploration. The small-timescale channel allocation and power control are jointly optimized by integrating bilateral matching with British auction, which solves matching competition with low complexity and achieves queuing delay guarantee based on option combination and elimination. Results from simulations indicate that the algorithm put forward can diminish queuing delay and decrease energy consumption.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Containers",
                "Delays",
                "Internet of Things",
                "Consumer electronics",
                "Monitoring",
                "Channel allocation",
                "Logic gates",
                "Energy consumption",
                "Collaboration",
                "Data processing"
            ],
            "Author Keywords": [
                "IoT",
                "consumer electronics",
                "edge computing",
                "distributed PV monitoring",
                "improved ant colony",
                "auction matching",
                "resource scheduling"
            ]
        },
        "title": "Edge-End Collaborative Communication and Computing Resource Scheduling for IoT Empowered PV Monitoring"
    },
    {
        "authors": [
            "Kyungchang Jeong",
            "Hongseok Oh",
            "Yeongyu Lee",
            "Hanbit Seo",
            "Gyuchan Cho",
            "Jaemin Jeong",
            "Gyutae Park",
            "Jungseok Choi",
            "Young-Duk Seo",
            "Ji-Hoon Jeong",
            "Euijong Lee"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "16 September 2024",
        "doi": "10.1109/JIOT.2024.3461775",
        "publisher": "IEEE",
        "abstract": "Bees play a crucial role in human food production and ecosystem maintenance. However, they face a global crisis characterized by significant population decline, including colony collapse disorder, which threatens their survival. Therefore, the development of precision beekeeping IoT systems is pivotal for enhancing bee colony strength. This study analyzes and categorizes the research focusing on the colony strength into two main areas based on colony activity and threat detection. Research on colony activity aids beekeepers to effectively manage their hives by analyzing behaviors, such as internal status, swarming, and traffic. In threat detection, research focuses on identifying critical predators, such as hornets and varroa destructor that significantly affect colony survival. This study conducts a comprehensive literature review, thoroughly presenting findings on the evolution and diversification of methodologies aimed at enhancing colony strength. By delving into various innovative approaches and assessing their effectiveness, this review highlights key developments that have significantly contributed to improving the resilience of bee populations against emerging threats. Furthermore, this study identifies the limitations of current research and proposes future research directions focused on enhancing the accuracy of bee behavior analysis and threat detection to improve colony strength and productivity.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Surveys",
                "Monitoring",
                "Temperature sensors",
                "Artificial intelligence",
                "Temperature measurement",
                "Sensors",
                "Productivity"
            ],
            "Author Keywords": [
                "Precision Beekeeping",
                "IoT System",
                "AI System",
                "Honeybee Colony Strength",
                "Colony Activity",
                "Threat Analysis"
            ]
        },
        "title": "IoT and AI Systems for Enhancing Bee Colony Strength in Precision Beekeeping: A Survey and Future Research Directions"
    },
    {
        "authors": [
            "Maggie Ezzat Gaber Gendy",
            "Asanka Rathnayaka",
            "Stephanie J. Curtis",
            "Andrew James Stewardson",
            "Mehmet Rasit Yuce"
        ],
        "published_in": "Published in: IEEE Journal of Biomedical and Health Informatics ( Early Access )",
        "date_of_publication": "10 November 2023",
        "doi": "10.1109/JBHI.2023.3331943",
        "publisher": "IEEE",
        "abstract": "Contact tracing is an effective method for mitigating the infectious diseases spread and it played a crucial role in reducing COVID-19 outbreak. Since the pandemic, there has been an increased concern regarding people's health in hospital and office settings, as these limited air exchange spaces provide a conductive medium for virus spread. Various technologies were used to recognize close contacts autonomously, in addition, multiple machine learning attempts were carried out to determine proximity in contact tracing. This study, however, proposes a unique concept in contact tracing: forecasting future close contact prior to occurrence in order to regulate and control it rather than tracking past occurrences. For our research, we constructed a completely new real-life dataset that was collected during the pandemic in a hospital infectious ward (Alfred Hospital, Melbourne, Australia) utilizing a Bluetooth Low Energy (BLE) Internet of Things (IoT) system. Our prediction technique considers two types of environments: single transceiver environments and multiple transceivers settings, these transceivers record the nearby tags' BLE received signal strength indicator (RSSI) values. The system employs mathematical models and supervised machine learning (ML) algorithms to solve regression and classification problems for workers' pattern recognition within the environment. The output is compared using different metrics, such as efficiency, which reached more than 80%, root mean square errors and mean absolute errors which were as low as 2.4 and 1.2 respectively in some models.",
        "issn": {
            "Print ISSN": "2168-2194",
            "Electronic ISSN": "2168-2208"
        },
        "keywords": {
            "IEEE Keywords": [
                "Hospitals",
                "Radiofrequency identification",
                "Infectious diseases",
                "COVID-19",
                "Pandemics",
                "Indexes",
                "Global Positioning System"
            ],
            "Author Keywords": [
                "BLE",
                "classification",
                "contact tracing",
                "COVID-19",
                "hospital setting",
                "IoT",
                "pattern recognition/detection",
                "proximity detection",
                "regression",
                "RSSI",
                "supervised learning",
                "wearable tags"
            ]
        },
        "title": "Future Prediction of Close Contacts in IoT-based Contact Tracing System using a New Real-Life Dataset"
    },
    {
        "authors": [
            "Shao Fei Bo",
            "Jun-Hui Ou",
            "Pei Ming Wang",
            "Huaiguang Jiang",
            "Xiu Yin Zhang"
        ],
        "published_in": "Published in: IEEE Transactions on Circuits and Systems I: Regular Papers ( Early Access )",
        "date_of_publication": "05 November 2024",
        "doi": "10.1109/TCSI.2024.3487262",
        "publisher": "IEEE",
        "abstract": "The paper proposes a hybrid RF and wind energy harvester. It is constructed by structural and functional integration of the two dissimilar energy harvesting techniques, constituting a conformal design. The rectifying efficiency can be boosted by the hybrid power source excitation, thereby increasing DC output power compared to standalone power source. A fan-shaped omnidirectional antenna and a hybrid single shunt-diode rectifier are designed to realize energy receiving and rectifying, respectively. A prototype is implemented and measured. The receiving part can achieve 2.29-dBi peak gain and 0.92-dB non-roundness at 1.85 GHz. It can also work smoothly when the wind speed varies from 0 to 12 m/s. The RF-DC conversion efficiency at -20 dBm and AC-DC output voltage at 12 m/s are measured as 20% and 79 mV, respectively. When both RF and wind power sources are accessible, the hybrid DC output power of 1.0 uW can be obtained with -30-dBm RF power and 10m/s wind speed. Moreover, the output power at hybrid rectifying mode is higher than that of simply superimposed RF and wind energy. Efficiency gain of up to 182% can be achieved. The hybrid energy harvester is a good candidate to power the sensors in battery-free IoTs.",
        "issn": {
            "Print ISSN": "1549-8328",
            "Electronic ISSN": "1558-0806"
        },
        "keywords": {
            "IEEE Keywords": [
                "Radio frequency",
                "Wind energy",
                "Hybrid power systems",
                "Blades",
                "Rectifiers",
                "Antennas",
                "AC generators",
                "Wind turbines",
                "Rotors",
                "Fans"
            ],
            "Author Keywords": [
                "Hybrid energy harvester",
                "RF",
                "wind",
                "conformal rectenna",
                "IoT"
            ]
        },
        "title": "Battery-Free Hybrid Ambient RF and Wind Energy Harvester for Outdoor IoTs"
    },
    {
        "authors": [
            "Alla Ramakrishna",
            "Kranthi Kumar Singamaneni",
            "Guddeti Jagadeeswar Reddy",
            "K. Reddy Madhavi",
            "T. Venkatakrishnamoorthy"
        ],
        "published_in": "Published in: Tsinghua Science and Technology ( Early Access )",
        "date_of_publication": "01 October 2024",
        "doi": "10.26599/TST.2024.9010084",
        "publisher": "TUP",
        "abstract": "It is essential to ensure the safety of networks and devices that are wired together because of the rapid growth of the IoT and IIoT networks. To address the existing security challenges over the IoT while routing sensitive data, we proposed a novel QoS-based IoT security approach. Our model parades apparent characteristics in its unconventionality from accustomed routing protocols. Our approach proved that it is better than existing models in terms of efficiency, security, and privacy based on the conduction of a series of scrupulous tests and evaluations and noticed a better quality and performance over the IoT network while establishing connection and routing. To achieve better efficacy, we combined Lightweight lattice-based ABE with a novel Quantum Key Distribution model to overcome the cyber-attacks that were faced by most of the existing approaches with better computational overhead. This work presents an innovative method, integrating security and privacy by integrating QoS principles, intended to address the security and privacy issues while routing over the IoT networks. With our research, we have significantly improved IoT applications by introducing improvements of 24% in computational overhead, 18% in efficiency, and 14% in routing speed. These improvements allow devices to use less power and resources, support denser networks with better data reliability, and react faster to critical events. These developments directly help applications in smart homes, smart workplaces, and smart cities by making IoT systems more secure, economical, scalable, and energy efficient. These advancements directly affect applications in smart workplaces, smart homes, and smart cities, increasing the security, affordability, scalability, and energy efficiency of IoT systems. Our method helps to realise safer and more resilient IoT ecosystems across various application domains by enhancing the security and effectiveness of IoT networks.",
        "issn": {
            "Electronic ISSN": "1007-0214"
        },
        "keywords": {
            "IEEE Keywords": [
                "Quality of service",
                "Internet of Things",
                "Routing",
                "Security",
                "Routing protocols",
                "Reliability",
                "Encryption",
                "Data privacy",
                "Network security",
                "Measurement"
            ],
            "Author Keywords": [
                "adaptive routing protocols",
                "attribute-based encryption",
                "Internet of Things",
                "lightweight cryptographic techniques",
                "network security",
                "QoS",
                "QKD"
            ]
        },
        "title": "A novel QoS-based IoT network security approach with lightweight lattice-based quantum attribute-based encryption"
    },
    {
        "authors": [
            "Debbarni Sarkar",
            "Yogita",
            "Satyendra Singh Yadav",
            "Linga Reddy Cenkeramaddi",
            "Om Jee Pandey"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "06 September 2024",
        "doi": "10.1109/JIOT.2024.3455434",
        "publisher": "IEEE",
        "abstract": "In wireless networks, automatic modulation classification (AMC) is crucial for enabling intelligent signal demodulation, thereby enhancing the system’s adaptability across various applications. Concurrently, the rapid expansion of the Internet of Things (IoT) necessitates scalable network solutions with limited power consumption. Moreover, addressing the non-line-of-sight (NLoS) effects in IoT networks, intelligent reflecting surface (IRS) emerges as a promising, cost-effective technology. This paper introduces a novel transformer-based deep recurrent architecture (TDRA) for AMC, tailored for IRS-assisted IoT networks, which significantly improves IoT device performance in NLoS scenarios. In TDRA, the existing recurrent models, long-short-term memory (LSTM), and gated-recurrent-unit (GRU) are suitably revamped with a transformer-based approach and termed as transformer-based LSTM (T-LSTM) and transformer-based GRU (T-GRU). Numerical datasets are generated for IoT applications considering the seven widely used modulation types to train and test the proposed models. Comparative analysis with seven state-of-the-art deep learning models and five machine learning models for AMC demonstrates the superior performance of the proposed models across multiple metrics, including accuracy, R-squared-score, mean-squared-error, mean-absolute-error, precision, recall, and F1-score. Further, the proposed models exhibit notable improvements under various conditions, such as optimized and random IRS phase shifts, with and without IRS-assisted IoT networks, different modulation sequence lengths, and fading channels. Additionally, the time complexity and processing time of the proposed models have been studied to test their suitability for IoT devices. The simulation results indicate that the TDRA for AMC in IRS-assisted IoT networks achieves up to 87% higher accuracy compared to without IRS-assisted IoT networks. This significant enhancement underscores the potential of TDRA to revolut...",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Modulation",
                "Wireless communication",
                "Wireless networks",
                "Transformers",
                "Quality of experience",
                "Numerical models"
            ],
            "Author Keywords": [
                "Intelligent reflecting surface",
                "automatic modulation classification",
                "internet of things",
                "deep learning",
                "transformer-based deep recurrent architecture"
            ]
        },
        "title": "TDRA: Transformer Based Deep Recurrent Architecture for Automatic Modulation Classification (AMC) Pertinent to Intelligent Reflecting Surface Assisted Internet of Things (IoT) Networks"
    },
    {
        "authors": [
            "Mahsa Farahani",
            "Mohammad Amin Rashid",
            "Bardia Safaei"
        ],
        "published_in": "Published in: IEEE Internet of Things Magazine ( Early Access )",
        "date_of_publication": "16 September 2024",
        "doi": "10.1109/IOTM.001.2400090",
        "publisher": "IEEE",
        "abstract": "The growth of the Internet of Things (IoT) is expected to rapidly interconnect tens of billions of new devices, many of which will be connected to the Internet. These IoT devices span a wide range, from high-end devices that can utilize traditional Operating Systems (OSs) to low-end devices with stringent resource constraints, i.e., memory, processing power, and power supply. Effectively developing, deploying, and maintaining large-scale IoT systems requires appropriate OSs that can cater to the diverse needs of these resource-constrained IoT devices. Due to the significant role of OSs on the performance of emerging IoT applications, this study provides a detailed comparative study of the top practical IoT OSs over the past few years. We investigate various OSs from the unique perspective of their suitability for embedded devices and edge nodes in IoT applications, providing a comprehensive understanding of how their architectural features and design choices align with the requirements of resource-constrained IoT environments. Furthermore, by conducting a comprehensive set of real-world implementations, this comparative study is accompanied by a deep evaluation of IoT-based and non-IoT-based versions of the most widely-used OS, in terms of power consumption, CPU utilization, and memory usage. These evaluations give valuable insight into how various aspects of the OS design could affect its performance to be selected as the underlying embedded or edge devices OS.",
        "issn": {
            "Print ISSN": "2576-3180",
            "Electronic ISSN": "2576-3199"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Operating systems",
                "Linux",
                "Security",
                "Ecosystems",
                "Real-time systems",
                "Performance evaluation"
            ],
            "Author Keywords": []
        },
        "title": "From Kernel to Cloud: A Concise Comparative Study of Practical IoT Operating Systems"
    },
    {
        "authors": [
            "Lalit Kumar Baghel",
            "Radhika Raina",
            "Suman Kumar",
            "Luca Catarinucci"
        ],
        "published_in": "Published in: IEEE Journal of Radio Frequency Identification ( Early Access )",
        "date_of_publication": "06 November 2024",
        "doi": "10.1109/JRFID.2024.3488534",
        "publisher": "IEEE",
        "abstract": "Effective cold chain management is critical across various sectors to ensure the integrity of temperature-sensitive goods, ranging from pharmaceuticals to perishable produce. A key challenge within this domain is maintaining items within their required temperature range, typically between 2∘C to 8∘C, to prevent spoilage or loss of effectiveness. This paper introduces a cost-effective, integrated solution that combines sensors, controllers, and memory into a compact, power-efficient, and low-cost commercial Bluetooth-based temperature & humidity data logger. The proposed solution is particularly useful not only in safeguarding food and pharmaceuticals but also plays a crucial role in the specific context of vaccine storage, such as those for COVID-19, which demands rigorous temperature adherence to ensure efficacy during storage and transportation. Unlike existing solutions, the proposed solution is equipped with interactive algorithms that monitor and record real-time temperature & humidity data throughout the distribution chain. It features a groundbreaking seamless data logging capability, allowing for wireless data retrieval via Bluetooth-enabled devices such as mobile phones, computers, or laptops. The development and testing of the proposed solution have been conducted in our laboratory, ensuring end-to-end performance and efficiency that meet the stringent standards set by health organizations, including the World Health Organization (WHO). A comprehensive comparative analysis further validates the proposed design’s accuracy, cost-effectiveness, and power efficiency, demonstrating its potential to enhance cold chain management practices universally.",
        "issn": {
            "Electronic ISSN": "2469-7281"
        },
        "keywords": {
            "IEEE Keywords": [
                "Temperature sensors",
                "Monitoring",
                "Humidity",
                "Temperature distribution",
                "Vaccines",
                "Bluetooth",
                "Accuracy",
                "Radiofrequency identification",
                "Flash memories",
                "Sensors"
            ],
            "Author Keywords": [
                "Data logger",
                "cold chain monitoring",
                "temperature & humidity",
                "sensing and logging",
                "BLE",
                "IoT"
            ]
        },
        "title": "IoT Based Integrated Sensing and Logging Solution for Cold Chain Monitoring Applications"
    },
    {
        "authors": [
            "Akram Refaei",
            "Sebastien Genevey",
            "Yves Audet",
            "Yvon Savaria"
        ],
        "published_in": "Published in: IEEE Transactions on Microwave Theory and Techniques ( Early Access )",
        "date_of_publication": "19 September 2024",
        "doi": "10.1109/TMTT.2024.3456690",
        "publisher": "IEEE",
        "abstract": "The need for efficient and high-performance energy harvesting systems is rising to power modern wearable smart devices. Several energy sources can be harvested such as thermal, vibrational, and ambient radio frequency (RF). RF energy harvesters (RFEHs) are widely adopted as they wirelessly deliver power. This article proposes a new RFEH design based on the three-phase rectifier topology. The rectifier is integrated with a custom-designed phase shifter that can split received power equally and deliver three signals with a 120 $^{\\circ}$ phase shift at the same moment. Due to their low forward drop voltage and high sensitivity, the rectifier diodes are chosen to be Schottky diodes SMS7621-005LF from Skyworks. A prototype is fabricated on an RT/Duroid 5880 Laminates substrate with 0.005 in thickness to reduce the dielectric losses. The RF energy harvester shows promising results in the ISM band at a 435.6 MHz frequency. At 8 dBm available RF power, the prototype demonstrates a high efficiency of 56% end to end at 6 k $\\Omega$ load and 5.2 V output voltage. In addition, the system maintains an efficiency higher than 20% over a wide available input power range (IPR) of 28 dBm. The RFEH reports a 1 V sensitivity at $-$ 10 dBm. This system is ideal for supplying ambient sensor nodes and systems-on-chip (SoCs) in urban areas where RF electromagnetic waves are widely available.",
        "issn": {
            "Print ISSN": "0018-9480",
            "Electronic ISSN": "1557-9670"
        },
        "keywords": {
            "IEEE Keywords": [
                "Rectifiers",
                "Radio frequency",
                "Phase shifters",
                "Impedance",
                "Antenna arrays",
                "Internet of Things",
                "Schottky diodes"
            ],
            "Author Keywords": [
                "Internet of Things (IoTs)",
                "phase shifter",
                "power conversion efficiency (PCE)",
                "radio frequency energy harvesting",
                "Schottky diode",
                "sensitivity",
                "three-phase rectifier"
            ]
        },
        "title": "High-Efficiency Wide Input Power Range Three-Phase Radio Frequency Energy Harvester for IoT Applications"
    },
    {
        "authors": [
            "Neha Sharma",
            "Sumit Gautam",
            "Symeon Chatzinotas",
            "Björn Ottersten"
        ],
        "published_in": "Published in: IEEE Communications Letters ( Early Access )",
        "date_of_publication": "15 October 2024",
        "doi": "10.1109/LCOMM.2024.3481289",
        "publisher": "IEEE",
        "abstract": "This paper addresses two distinctly poised objectives, i.e., data rate and energy harvesting (EH) in Simultaneous Wireless Information and Power Transfer (SWIPT) systems with Reconfigurable Intelligent Surface (RIS) by tackling a weighted objective to maximize data rate, EH, and transmit power utilization for multi-antenna BS and multiple RIS-User scenarios. This approach optimizes power splitting (PS) ratio at the end-user and transmit power using an optimized practical phase-dependent amplitude model for each RIS element reflectivity. Fractional programming-based Dinkelbach and Quadratic transform-related algorithms are proposed and compared with Karush-Kuhn-Tucker (KKT) conditions based solutions. Optimized discrete phase shift (DPS) level has been sought. Numerical results show that deploying more RIS elements and placing them closer together enhances both information rate and EH, whereas it nearly saturates with increasing DPS levels.",
        "issn": {
            "Print ISSN": "1089-7798",
            "Electronic ISSN": "1558-2558"
        },
        "keywords": {
            "IEEE Keywords": [
                "Reconfigurable intelligent surfaces",
                "Simultaneous wireless information and power transfer",
                "Optimization",
                "Internet of Things",
                "Wireless communication",
                "Reflection",
                "Linear programming",
                "Energy efficiency",
                "Data models",
                "Transmitting antennas"
            ],
            "Author Keywords": [
                "Reconfigurable Intelligent Surfaces (RIS)",
                "Simultaneous Wireless Information and Power Transfer (SWIPT)",
                "Energy-Harvesting",
                "IoT",
                "Power-splitting (PS)",
                "Fractional Programming"
            ]
        },
        "title": "Fractional Programming based Optimization Techniques for RIS-assisted SWIPT-IoT system"
    },
    {
        "authors": [
            "Mengli Wei",
            "Wenwu Yu",
            "Duxin Chen"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "24 October 2024",
        "doi": "10.1109/JIOT.2024.3486122",
        "publisher": "IEEE",
        "abstract": "In Healthcare Internet of Things Networks (HIoTNs), safeguarding the sensitivity of patients’ Electric Healthcare Records (EHRs) is imperative, necessitating effective privacy protection while ensuring ample data for training. Decentralized Federated Learning (DFL), as a peer-to-peer distributed machine learning architecture, serves a pivotal role in preserving the data privacy of EHRs. However, two primary challenges in applying DFL to HIoTNs include safeguarding the privacy of EHRs at individual hospitals and optimizing communication resources among hospitals. This paper proposes a three-layer healthcare framework comprising a physical layer, a communication layer and an edge-encrypted layer to segregate hospital local data from interaction models. Moreover, a novel doubly accelerated DFL algorithm (AccDFL) is introduced to integrate DFL training and information interaction into the communication layer of HIoTNs. By employing the heavy-ball and Nesterov methods, AccDFL achieves double acceleration, ensuring both i-Differential Privacy (DP) and linear convergence. The theoretical analysis of AccDFL delves into the interaction among Distributed Gradient Tracking (DGT), DP, and accelerated mechanisms, presenting comprehensive convergence and privacy analyses to overcome the exponential increase in parameters brought by the acceleration and DP mechanisms. Experimental results with realistic non-IID grayscale and color medical datasets of different disease types affirm the significant advantages of AccDFL over other algorithms in terms of accuracy and communication efficiency.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Privacy",
                "Convergence",
                "Training",
                "Hospitals",
                "Internet of Things",
                "Data models",
                "Servers",
                "Peer-to-peer computing",
                "Federated learning",
                "Biomedical imaging"
            ],
            "Author Keywords": [
                "decentralized federated learning",
                "healthcare IoT networks",
                "accelerated learning",
                "differential privacy"
            ]
        },
        "title": "AccDFL: Accelerated Decentralized Federated Learning for Healthcare IoT Networks"
    },
    {
        "authors": [
            "Qingyang Ding",
            "Xiaofei Yue",
            "Qinnan Zhang",
            "Zehui Xiong",
            "Jinping Chang",
            "Hongwei Zheng"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "23 October 2024",
        "doi": "10.1109/JIOT.2024.3485208",
        "publisher": "IEEE",
        "abstract": "With the flourishing of the Agricultural Internet of Things (AIoT), analyzing large-volume sensor data has become a regular requirement for agricultural decision-making. Federated Learning (FL), which facilitates scattered AIoT devices to train models collaboratively, has gained significant attention. However, traditional FL poses challenges in AIoT scenarios, such as wide geo-distribution, heterogeneous data distribution, and high device risks. Existing works tend to be one-sided and remain unclear on how to tackle these issues thoroughly in AIoT. To fill the gap, we present, a double-layer blockchain-based FL framework, which enhances both learning efficiency and security for AIoT. The double-layer blockchain, coupled with a two-stage consensus algorithm, drives the hierarchical FL process to enable efficient and reliable agricultural knowledge-sharing. In addition, adopts an adaptive model aggregation algorithm to dynamically tune noise levels based on the model quality, further improving the learning security and model credibility. Finally, the extensive experimental results demonstrate that not only improves the model accuracy by up to 21.17% compared with the state-of-the-art baselines, but also enhances the privacy protection within an additional error of only 2.1%.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Blockchains",
                "Internet of Things",
                "Privacy",
                "Computational modeling",
                "Federated learning",
                "Data models",
                "Biological system modeling",
                "Adaptation models",
                "Accuracy",
                "Servers"
            ],
            "Author Keywords": [
                "Federated learning",
                "blockchain",
                "privacy protection",
                "knowledge-sharing",
                "agriculture IoT"
            ]
        },
        "title": "Bc2FL: Double-Layer Blockchain-Driven Federated Learning Framework for Agricultural IoT"
    },
    {
        "authors": [
            "Tung-Tso Tsai",
            "Han-Yu Lin",
            "Wei-Ning Huang",
            "Sachin Kumar",
            "Kadambari Agarwal",
            "Chien-Ming Chen"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "16 September 2024",
        "doi": "10.1109/TCE.2024.3460753",
        "publisher": "IEEE",
        "abstract": "In the realm of consumer Internet of Things environments, data related to personal health and medical records can be collected. However, as this healthcare data falls under personal privacy, it must undergo encryption procedures before being uploaded to the cloud to ensure data confidentiality. Additionally, there is a desire for the encrypted data uploaded to the cloud to be compared, enabling the timely detection of anomalous data. If there are issues with healthcare data, the cloud system can issue alerts. Indeed, there already exists a mechanism, namely identity-based signcryption with equality test (IBSCET), which can accomplish the desire by comparing whether two encrypted data contain the same message. However, IBSCET does not address the issue of user revocation, which is crucial in any system. To address this problem, we enhance the existing IBSCET to propose the first outsourced revocable IBSCET (OR-IBSCET) scheme. Under the bilinear Diffie-Hellman and the computational Diffie-Hellman assumptions, we also demonstrate that the proposed scheme possesses security of the indistinguishability under chosen ciphertext attacks, the existential unforgeability under chosen message attacks, and one-wayness under chosen ciphertext attacks.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Cloud computing",
                "Encryption",
                "Public key",
                "Medical services",
                "Servers",
                "Clouds",
                "Symbols"
            ],
            "Author Keywords": [
                "Revocation",
                "identity-based signcryption",
                "consumer IoT"
            ]
        },
        "title": "Anomaly Detection through Outsourced Revocable Identity-Based Signcryption With Equality Test for Sensitive Data in Consumer IoT Environments"
    },
    {
        "authors": [
            "Chenglong Tian",
            "Hankai Liu",
            "Xiao Shen",
            "Yongtao Ma",
            "Yuan Shen"
        ],
        "published_in": "Published in: IEEE Transactions on Wireless Communications ( Early Access )",
        "date_of_publication": "26 September 2024",
        "doi": "10.1109/TWC.2024.3464116",
        "publisher": "IEEE",
        "abstract": "In passive Internet of Things, existing synthetic aperture-based 3D localization methods face many challenges, such as high computational load, a large aperture of a virtual antenna array, and sensitivity to noise. To address these challenges, this paper develops a synthetic aperture scheme for integrated localization and navigation, which implements the localization algorithm with a trajectory generated by the navigation algorithm. The localization problem is formulated by multidimensional scaling, which exploits phase differences involving the spatial information between target tags and a virtual antenna array. The new formulation allows the system to provide an accurate location estimate with a large moving step and sparse virtual antenna array of narrow apertures. The navigation problem is formulated to decrease errors of distance differences. Moreover, a navigation criterion is established to determine the feasibility of a virtual antenna position based on phase measurements, and an efficient navigation algorithm is proposed to find such a feasible point. Extensive numerical results validate our theoretical analysis and the performance of the proposed scheme.",
        "issn": {
            "Print ISSN": "1536-1276",
            "Electronic ISSN": "1558-2248"
        },
        "keywords": {
            "IEEE Keywords": [
                "Location awareness",
                "Navigation",
                "Trajectory",
                "Three-dimensional displays",
                "Phase measurement",
                "Phased arrays",
                "Internet of Things"
            ],
            "Author Keywords": [
                "Passive Internet of Things (IoT)",
                "synthetic aperture",
                "virtual antenna array",
                "cooperative localization",
                "phase difference",
                "multidimensional scaling (MDS)"
            ]
        },
        "title": "A Synthetic Aperture Scheme for Integrated Localization and Navigation in Passive IoT"
    },
    {
        "authors": [
            "Eakkachai Klaithin",
            "Vissavakawn Matimapa-kay",
            "Wachira Daosud",
            "Yanisa Laoonguthai"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "07 October 2024",
        "doi": "10.1109/JSEN.2024.3469273",
        "publisher": "IEEE",
        "abstract": "A successful automatic control was developed using Arduino and ESP32 microcontrollers to operate a low-cost prototype bioreactor, which resulted in the effective production of biobutanol. The system maintained a temperature of 30.70 °C, an average pH of 6.61, and effectively reduced oxygen levels to 0% v/v within 130 seconds. The highest cell concentration was 1.26 × 10 8 CFU/ml, and GC-FID analysis showed acetone, butanol, and ethanol concentrations of 0.3925, 0.05304, and 1.1184 g/L, respectively. In addition, a sample collector was designed to collect samples with a precision of 10±1 ml and a time deviation of 2±13 seconds. In terms of the cloud system, NETPIE was used for device management. It can effectively display real-time conditions and control equipment as required. Google Sheets collected 43,200 data points for each parameter for data analysis and observation. Moreover, the Line application was applied for message notification when a sample was collected. The cost of the automated prototype bioreactor was 1093.61 USD for a 10 L production volume, and the automated sample was 210.99 USD for 24 tubes, with a maximum volume capacity of 15 ml. Therefore, both the automated bioreactor and the sample collector system were effective for control and monitoring, contributing to improving the biobutanol production process.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Temperature sensors",
                "Production",
                "Temperature control",
                "Monitoring",
                "Temperature distribution",
                "Process control",
                "Temperature measurement",
                "Microcontrollers",
                "Fermentation",
                "Microorganisms"
            ],
            "Author Keywords": [
                "IoT System-Based Arduino",
                "Automatic control",
                "Sample collector",
                "Biobutanol"
            ]
        },
        "title": "Effective an IoT-Based Arduino Design for Automated Bioreactor Control and Sample Collection in Biobutanol Production"
    },
    {
        "authors": [
            "Sutanu Ghosh",
            "Keshav Singh",
            "Haejoon Jung",
            "Chih-Peng Li",
            "Trung Q. Duong"
        ],
        "published_in": "Published in: IEEE Transactions on Cognitive Communications and Networking ( Early Access )",
        "date_of_publication": "05 August 2024",
        "doi": "10.1109/TCCN.2024.3438359",
        "publisher": "IEEE",
        "abstract": "In this paper, we analyze the performance of rate splitting multiple access (RSMA) technique for a multi-device communication system applying integrated sensing and communication (ISAC) to alleviate the problem of overlapping spectrum of radar signal and communication frequency bands. The system includes a cooperative access point (AP) which serves as a sensing node and a decode-and-forward (DF) relay to support the communication between a mobile device (MD) and multiple Internet-of-Things devices (IoDs). Assuming Nakagami fading channels, we provide an extensive analytical framework to evaluate the dual functionalities of the system considering various scenarios with different assumptions on blocklength, channel state information (CSI), and successive interference cancellation (SIC). In other words, we consider both infinite and finite blocklength transmissions under practical impairments including imperfect CSI and SIC. We investigate the outage probability (OP), and ergodic sum rate assuming infinite blocklength, while the block error rate (BLER), and goodput are analyzed in the finite blocklength regime. The closed-form and asymptotic expressions for the OP and BLER are presented. In addition, to evaluate the sensing performance, we derive the closed-form expressions of the false alarm and detection probabilities. Through the simulation results, we validate our analysis and delve into the impacts of various system parameters including transmit power, Nakagami shaping parameter, CSI error, SIC imperfection, the number of devices, and sensing threshold. Further, we observe that the proposed RSMA-based ISAC system provides higher ergodic sum rates compared to non-orthogonal multiple access (NOMA) both in the presence and absence of practical impairments.",
        "issn": {
            "Electronic ISSN": "2332-7731"
        },
        "keywords": {
            "IEEE Keywords": [
                "NOMA",
                "Integrated sensing and communication",
                "Interference cancellation",
                "Probability",
                "Power system reliability",
                "Resource management",
                "Performance evaluation"
            ],
            "Author Keywords": [
                "Rate splitting multiple access (RSMA)",
                "integrated sensing and communication (ISAC)",
                "cooperative communication",
                "non-orthogonal multiple access (NOMA)",
                "outage probability (OP)",
                "Internet-of-Things (IoT)"
            ]
        },
        "title": "On the Performance of Rate Splitting Multiple Access for ISAC in Device-to-Multi-Device IoT Communications"
    },
    {
        "authors": [
            "Aditya Singh",
            "Syed Danish",
            "Gaurav Prasad",
            "Sudhir Kumar"
        ],
        "published_in": "Published in: IEEE Transactions on Network Science and Engineering ( Early Access )",
        "date_of_publication": "09 October 2024",
        "doi": "10.1109/TNSE.2024.3478053",
        "publisher": "IEEE",
        "abstract": "Wi-Fi-based localization using Received Signal Strength (RSS) is widely adopted due to its cost-effectiveness and ubiquity. However, localization accuracy of RSS-based localization degrades due to random fluctuations from shadowing and multipath fading effects. Existing fading distributions like Rayleigh, $\\kappa - \\mu$ , and $\\alpha$ -KMS struggle to capture all factors contributing to fading. In contrast, the $\\alpha - \\eta - \\kappa - \\mu$ distribution offers the most generalized coverage of fading in literature. However, as fading distributions become more generalized, their computational demands also increases. This results in a trade-off between localization accuracy and complexity, which is undesirable for real-time localization. In this work, we propose a novel localization strategy utilizing the $\\alpha - \\eta - \\kappa - \\mu$ distribution combined with a novel approximation method that significantly reduces computational overhead while maintaining accuracy. Our proposed strategy effectively mitigates the trade-off between localization accuracy and complexity, outperforming existing state-of-the-art (SOTA) localization techniques on simulated and real-world testbeds. The proposed strategy achieves accurate localization with a speedup of 280 times over non-approximated methods. We validate its feasibility for real-time tasks on low-compute edge device Raspberry Pi Zero W, where it demonstrates fast and accurate localization, making it suitable for real-time edge applications.",
        "issn": {
            "Electronic ISSN": "2327-4697"
        },
        "keywords": {
            "IEEE Keywords": [
                "Location awareness",
                "Accuracy",
                "Real-time systems",
                "Rayleigh channels",
                "Computational modeling",
                "Maximum likelihood estimation",
                "Fingerprint recognition",
                "Fluctuations",
                "Wireless fidelity",
                "Smart devices"
            ],
            "Author Keywords": [
                "Localization",
                "Fading",
                "Edge Computing",
                "IoT"
            ]
        },
        "title": "A Unified $\\alpha -\\eta -\\kappa -\\mu$ Fading Model based Real-Time Localization on IoT Edge Devices"
    },
    {
        "authors": [
            "Filippo Campagnaro",
            "Matin Ghalkhani",
            "Riccardo Tumiati",
            "Federico Marin",
            "Matteo Dal Grande",
            "Alessandro Pozzebon",
            "Davide De Battisti",
            "Roberto Francescon",
            "Michele Zorzi"
        ],
        "published_in": "Published in: IEEE Journal of Oceanic Engineering ( Early Access )",
        "date_of_publication": "05 November 2024",
        "doi": "10.1109/JOE.2024.3459483",
        "publisher": "IEEE",
        "abstract": "The impact of global warming is felt worldwide, but coastal regions are particularly vulnerable, and face rising sea levels, increased seaquakes, and floods. In addition, the ecosystems of rivers, seas and lakes undergo significant changes due to climate change and pollutants. Thus, monitoring coastal areas, such as the Venice Lagoon (Italy), is of significant importance. This article presents a novel floating wireless sensor system for measuring water-related parameters. The core of the system is the SENSWICH platform, which integrates sensors to measure pH, electrical conductivity, turbidity, dissolved oxygen, and water level. The platform uses long range wide area network (LoRaWAN) connectivity for long-range data collection, which is then aggregated in the Amazon Web Service cloud and visualized using Grafana. While the proposed architecture can be applied to any large-scale coastal, lakeside or riverine monitoring context, it is specifically designed for the Venice Lagoon. This article presents results from two SENSWICH platforms: one in the Piovego river, Padova, which is a tributary of the Venice Lagoon, and one in Chioggia, in the lagoon itself. Together with these results, an analysis for a full-scale deployment in the whole Venice Lagoon is proposed. The results show that SENSWICH, despite being a low-cost platform compared to similar technologies available on the market, enables pervasive monitoring of large water surfaces. It provides reliable datasets over prolonged periods and significantly reduces human resource requirements. The proposed software framework for data aggregation is effective for managing large quantities of data acquired using a LoRaWAN network and is user-friendly through the graphic tool.",
        "issn": {
            "Print ISSN": "0364-9059",
            "Electronic ISSN": "1558-1691"
        },
        "keywords": {
            "IEEE Keywords": [
                "Biodiversity",
                "Climate change",
                "Internet of Things",
                "Wireless sensor networks",
                "Sea measurements",
                "Environmental monitoring",
                "Pollution measurement",
                "Rivers",
                "LoRaWAN",
                "Wide area networks",
                "Water resources"
            ],
            "Author Keywords": [
                "Biodiversity",
                "climate change",
                "IoT",
                "LoRa",
                "sensor networks",
                "WAN"
            ]
        },
        "title": "Monitoring the Venice Lagoon: An IoT Cloud-Based Sensor Network Approach"
    },
    {
        "authors": [
            "Alekhya Gorrela",
            "Nikumani Choudhury"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "06 November 2024",
        "doi": "10.1109/JIOT.2024.3492233",
        "publisher": "IEEE",
        "abstract": "For large-scale, remote, and harsh environments-based Industrial-IoT applications, the Long Range Wide Area Network (LoRaWAN) is a low-power protocol that enables reliable, robust, and long-range communication. A critical aspect of LoRaWAN is the Adaptive Data Rate (ADR), which allows end devices in IIoT applications to modify the data rate dynamically according to channel conditions. This guarantees efficient connectivity and prolongs the battery life of an end device. Data loss in harsh IIoT situations can be huge because the channel traffic tends to vary, which might include significant congestion and interference. There are several issues with the present ADR method, including its slow convergence time and lack of adaptability. Furthermore, in large IIoT applications, the ADR approach is overwhelming in high-traffic networks since it only considers link-level performance and assigns configuration parameters like Spreading Factor (SF) and Transmission Power (TP) to individual end devices. As a result, network performance could be improved. This paper proposes an effective Cluster-based ADR mechanism (Cluster-ADR) to minimize collisions and packet loss. The proposed Cluster-ADR clusters the end devices based on the estimated path loss. To address the aforementioned issues, the paper proposes an effective Cluster-based ADR mechanism (Cluster-ADR) by employing signal orthogonality to allocate SFs to minimize collisions and packet loss in dense LoRa-based IIoT networks. A clustering method based on measured path loss is also presented that clusters end devices, assigns different channels to each cluster, and allocates optimal SFs for each end device within the cluster. Additionally, this paper presents an analytical computation of the energy consumption and convergence time of Standard and the proposed ADR mechanisms using a Markov Model. The performance of Cluster-ADR is examined in terms of packet success rate, power consumption, and convergence time using simulati...",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Resource management",
                "LoRaWAN",
                "Interference",
                "Industrial Internet of Things",
                "Standards",
                "Performance evaluation",
                "Throughput",
                "Signal to noise ratio",
                "Convergence",
                "Computational modeling"
            ],
            "Author Keywords": [
                "LoRa",
                "LoRaWAN",
                "low-power wide-area networks (LPWANs)",
                "Adaptive Data Rate",
                "Industrial Internet of Things(IIoT)"
            ]
        },
        "title": "A Clustering-Based Adaptive Data Rate Technique for Industrial LoRaWAN-IoT Networks"
    },
    {
        "authors": [
            "Pietro Mercati",
            "Ganapati Bhat"
        ],
        "published_in": "Published in: IEEE Design & Test ( Early Access )",
        "date_of_publication": "29 July 2024",
        "doi": "10.1109/MDAT.2024.3432862",
        "publisher": "IEEE",
        "abstract": "Wearable and Internet of Things devices (IoT) are becoming ubiquitous in health applications, such as movement disorders, rehabilitation, and activity monitoring. Wearable IoT devices are enabling these applications through low-power sensors, processors, and machine learning algorithms. However, widespread adoption of wearable IoT devices has been limited due to challenges such as energy sustainability, sensor data shift during real-world usage, missing data, privacy, and security. Multiple health organizations have stipulated that resolving these challenges is crucial to bolster the adoption of wearable IoT devices. This paper reviews the primary challenges to wearable IoT devices and current state-of-the-art in addressing these challenges. Specifically, the goal of this paper is also to review state-of-the-art research across multiple challenges to wearable IoT devices, as opposed to focusing on a single area. We also highlight some future research directions and open problems to achieve widespread adoption of wearable IoT devices.",
        "issn": {
            "Print ISSN": "2168-2356",
            "Electronic ISSN": "2168-2364"
        },
        "keywords": {
            "IEEE Keywords": [
                "Biomedical monitoring",
                "Internet of Things",
                "Sensors",
                "Monitoring",
                "Circuits",
                "Wearable devices",
                "Wearable sensors"
            ],
            "Author Keywords": [
                "Wearable electronics",
                "health monitoring",
                "energy management",
                "design optimization",
                "privacy and security"
            ]
        },
        "title": "Self-Sustainable Wearable and Internet of Things (IoT) Devices for Health Monitoring: Opportunities and Challenges"
    },
    {
        "authors": [
            "Amudheesan Nakkeeran",
            "Prashant K. Wali",
            "Jyotsna Bapat",
            "Debabrata Das"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "11 September 2024",
        "doi": "10.1109/JIOT.2024.3458011",
        "publisher": "IEEE",
        "abstract": "Puncturing ongoing enhanced mobile broadband (eMBB) traffic to accommodate the stringent delay and reliability requirements of ultra-reliable low-latency communication (URLLC) Internet of Things (IoT) traffic has attracted a lot of attention recently. These puncturing decisions are based on the channel quality indicator (CQI) as seen by the eMBB/URLLC users, and any error/delay in the estimation/observation of the CQI is likely to adversely impact the reliability of URLLC-IoT traffic. Moreover, each puncturing decision incurs a signaling overhead in terms of the downlink control information for the URLLC-IoT devices, as well as puncturing indication for eMBB devices. In this work, we propose a static puncturing scheme which does not rely on CQI information. The reliability of URLLC-IoT transmissions is maintained by designing the system for lowest supported CQI. The deterministic nature of the proposed technique will allow a base station to schedule the eMBB traffic in such a way that system design goals like overall fairness and other QoE parameters for eMBB users can be optimized. The amount of URLLC-IoT traffic that can be served reliably at any given time is always limited, and we quantify this limit for the proposed scheme by numerically evaluating the maximum URLLC-IoT arrival rate that can be supported for any chosen level of reliability. Furthermore, we show that our puncturing scheme provides a deterministic and contained impact on eMBB throughput when compared to other schemes that puncture eMBB traffic greedily with respect to URLLC-IoT CQI or eMBB sum-throughput.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Ultra reliable low latency communication",
                "Reliability",
                "Downlink",
                "Delays",
                "Symbols",
                "Schedules",
                "Optimization"
            ],
            "Author Keywords": [
                "eMBB",
                "URLLC",
                "puncturing",
                "5G NR"
            ]
        },
        "title": "Novel Static Puncturing Scheme for Facilitating Ultra-Reliable Low-Latency IoT Communications"
    },
    {
        "authors": [
            "Luke Houben",
            "Thijs Terhoeve",
            "Savio Sciancalepore"
        ],
        "published_in": "Published in: IEEE Communications Magazine ( Early Access )",
        "date_of_publication": "17 June 2024",
        "doi": "10.1109/MCOM.002.2300789",
        "publisher": "IEEE",
        "abstract": "The manufacturer usage description (MUD) standard recently published by the IETF allows manufacturers of Internet of things (IoT) devices to equip their products with device specifications, that is, information about the expected network connections of the devices. Such data can be used to detect unauthorized behavior and mitigate attacks involving IoT devices. However, at the time of this writing, no previous work integrated security services based on MUD into constrained IoT networks, for example, the ones using the standardized protocol stack Thread. This article proposes MUDThread, a framework for integrating and managing security services into constrained Thread-based IoT networks using MUD-derived security specifications. Using MUDThread, IoT devices can provide MUD-related information at the join of the network using a standard-compliant extension of the Mesh Link Establishment protocol. At the same time, the MUD Manager, integrated into the edge border router of the network, can enforce MUD-based rules to stop unauthorized network traffic. We deploy a proofof- concept of our solution using actual nRF5340 and nRF 52833 IoT devices, and we experimentally verify its limited communication latency (0.012% more) and capability to detect both incoming and outgoing unauthorized network traffic during regular operations of a constrained IoT network.",
        "issn": {
            "Print ISSN": "0163-6804",
            "Electronic ISSN": "1558-1896"
        },
        "keywords": {
            "IEEE Keywords": [
                "Multiuser detection",
                "Internet of Things",
                "Instruction sets",
                "Security",
                "Local area networks",
                "Logic gates",
                "Uniform resource locators"
            ],
            "Author Keywords": []
        },
        "title": "MUDThread: Securing Constrained IoT Networks via Manufacturer Usage Descriptions"
    },
    {
        "authors": [
            "Wonmi Choi",
            "Yeonho Yoo",
            "Kyungwoon Lee",
            "Zhixiong Niu",
            "Peng Cheng",
            "Yongqiang Xiong",
            "Gyeongsik Yang",
            "Chuck Yoo"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "02 September 2024",
        "doi": "10.1109/JIOT.2024.3453410",
        "publisher": "IEEE",
        "abstract": "This paper explores the computing and communication overhead of network processing in IoT devices, focusing on containers, a major building block for edge computing. Our experiments reveal that containers on IoT devices suffer 2.6× higher CPU usage for SoftIRQ processing, 59% less network throughput, and 2× higher per-packet latency on average than native processes. While several existing studies enhance networking performance, they often sacrifice interoperability by requiring special hardware or modifying networking semantics or APIs. Thus, we design and implement a kernel networking accelerator, called SCON, that maintains interoperability, crucial for IoT devices. SCON addresses major bottlenecks in container networking through system-level profiling. We evaluate SCON with three types of IoT devices. On the Raspberry Pi 4, SCON reduces the latencies of major IoT application protocols (e.g., HTTP and MQTT) by ~10×, achieving a similar level of latency to the native process. Further analysis shows that SCON reduces CPU usage for SoftIRQ processing by ~26%. We also report similar improvements on the other two IoT devices. Our conclusion is that SCON is unique in significantly reducing the computing and communication overhead of container networking in IoT devices while maintaining interoperability. Furthermore, it works consistently across different types of devices, whether wired or wireless, and regardless of heavy or sporadic traffic.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Containers",
                "IP networks",
                "Performance evaluation",
                "Kernel",
                "Bridges",
                "Throughput"
            ],
            "Author Keywords": [
                "Container Virtualization",
                "Device Virtualization",
                "Efficient Communications and Networking",
                "Resource-Constrained Networks",
                "Real-Time Systems"
            ]
        },
        "title": "Intelligent Packet Processing for Performant Containers in IoT"
    },
    {
        "authors": [
            "Wensheng Su",
            "Zhenni Li",
            "Minrui Xu",
            "Jiawen Kang",
            "Dusit Niyato",
            "Shengli Xie"
        ],
        "published_in": "Published in: IEEE Transactions on Vehicular Technology ( Early Access )",
        "date_of_publication": "13 May 2024",
        "doi": "10.1109/TVT.2024.3399826",
        "publisher": "IEEE",
        "abstract": "Deep reinforcement learning (DRL) has shown remarkable success in complex autonomous driving scenarios. However, DRL models inevitably bring high memory consumption and computation, which hinders their wide deployment in resourcelimited autonomous driving devices. Structured Pruning has been recognized as a useful method to compress and accelerate DRL models, but it is still challenging to estimate the contribution of a parameter (i.e., neuron) to DRL models. In this paper, we introduce a novel dynamic structured pruning approach that gradually removes a DRL model's unimportant neurons during the training stage. Our method consists of two steps, i.e. training DRL models with a group sparse regularizer and removing unimportant neurons with a dynamic pruning threshold. To efficiently train the DRL model with a small number of important neurons, we employ a neuron-importance group sparse regularizer. In contrast to conventional regularizers, this regularizer imposes a penalty on redundant groups of neurons that do not significantly influence the output of the DRL model. Furthermore, we design a novel structured pruning strategy to dynamically determine the pruning threshold and gradually remove unimportant neurons with a binary mask. Therefore, our method can remove not only redundant groups of neurons of the DRL model but also achieve high and robust performance. Experimental results show that the proposed method is competitive with existing DRL pruning methods on discrete control environments (i.e., CartPole-v1 and LunarLander-v2) and MuJoCo continuous environments (i.e., Hopper-v3 and Walker2D-v3). Specifically, our method effectively compresses 93% neurons and 96% weights of the DRL model in four challenging DRL environments with slight accuracy degradation.",
        "issn": {
            "Print ISSN": "0018-9545",
            "Electronic ISSN": "1939-9359"
        },
        "keywords": {
            "IEEE Keywords": [
                "Neurons",
                "Training",
                "Autonomous vehicles",
                "Vehicle dynamics",
                "Computational modeling",
                "Laboratories",
                "Adaptation models"
            ],
            "Author Keywords": [
                "Dynamic Structured Pruning",
                "Deep Reinforcement Learning",
                "Model Compression",
                "Autonomous Driving"
            ]
        },
        "title": "Compressing Deep Reinforcement Learning Networks with a Dynamic Structured Pruning Method for Autonomous Driving"
    },
    {
        "authors": [
            "Taiheng Liu",
            "Guang-Zhong Cao",
            "Zhaoshui He",
            "Shengli Xie",
            "Xiuqin Deng"
        ],
        "published_in": "Published in: IEEE Transactions on Neural Networks and Learning Systems ( Early Access )",
        "date_of_publication": "22 May 2024",
        "doi": "10.1109/TNNLS.2024.3398654",
        "publisher": "IEEE",
        "abstract": "3-D lane detection is a challenging task due to the diversity of lanes, occlusion, dazzle light, and so on. Traditional methods usually use highly specialized handcrafted features and carefully designed postprocessing to detect them. However, these methods are based on strong assumptions and single modal so that they are easily scalable and have poor performance. In this article, a multimodal fusion network (MFNet) is proposed through using multihead nonlocal attention and feature pyramid for 3-D lane detection. It includes three parts: multihead deformable transformation (MDT) module, multidirectional attention feature pyramid fusion (MA-FPF) module, and top-view lane prediction (TLP) ones. First, MDT is presented to learn and mine multimodal features from RGB images, depth maps, and point cloud data (PCD) for achieving optimal lane feature extraction. Then, MA-FPF is designed to fuse multiscale features for presenting the vanish of lane features as the network deepens. Finally, TLP is developed to estimate 3-D lanes and predict their position. Experimental results on the 3-D lane synthetic and ONCE-3DLanes datasets demonstrate that the performance of the proposed MFNet outperforms the state-of-the-art methods in both qualitative and quantitative analyses and visual comparisons.",
        "issn": {
            "Print ISSN": "2162-237X",
            "Electronic ISSN": "2162-2388"
        },
        "keywords": {
            "IEEE Keywords": [
                "Lane detection",
                "Feature extraction",
                "Roads",
                "Point cloud compression",
                "Task analysis",
                "Semantics",
                "Shape"
            ],
            "Author Keywords": [
                "3-D lane detection",
                "deformable transformation",
                "feature pyramid",
                "multihead attention",
                "multimodal fusion"
            ]
        },
        "title": "Multimodal Fusion Network for 3-D Lane Detection"
    },
    {
        "authors": [
            "Jiana Liao",
            "Jinbo Wen",
            "Jiawen Kang",
            "Changyan Yi",
            "Yang Zhang",
            "Yutao Jiao",
            "Dusit Niyato",
            "Dong In Kim",
            "Shengli Xie"
        ],
        "published_in": "Published in: IEEE Transactions on Cognitive Communications and Networking ( Early Access )",
        "date_of_publication": "30 May 2024",
        "doi": "10.1109/TCCN.2024.3407067",
        "publisher": "IEEE",
        "abstract": "Web 3.0 is recognized as a pioneering paradigm that empowers users to securely oversee data without reliance on a centralized authority. Blockchains, as a core technology to realize Web 3.0, can facilitate decentralized and transparent data management. Nevertheless, the evolution of blockchain-enabled Web 3.0 is still in its nascent phase, grappling with challenges such as ensuring efficiency and reliability to enhance block propagation performance. In this paper, we design a Graph Attention Network (GAT)-based reliable block propagation optimization framework for blockchain-enabled Web 3.0. We first innovatively apply a data-freshness metric called age of block to measure block propagation efficiency in public blockchains. To achieve the reliability of block propagation, we introduce a reputation mechanism based on the subjective logic model, including the local and recommended opinions to calculate the miner reputation value. Moreover, considering that the GAT possesses the excellent ability to process graph-structured data, we utilize the GAT with reinforcement learning to obtain the optimal block propagation trajectory. Numerical results demonstrate that the proposed scheme exhibits the most outstanding block propagation efficiency and reliability compared with traditional routing mechanisms.",
        "issn": {
            "Electronic ISSN": "2332-7731"
        },
        "keywords": {
            "IEEE Keywords": [
                "Blockchains",
                "Semantic Web",
                "Reliability",
                "Optimization",
                "Mathematical models",
                "Measurement",
                "Web 2.0"
            ],
            "Author Keywords": [
                "Web 3.0",
                "block propagation",
                "age of block",
                "reputation",
                "graph attention network"
            ]
        },
        "title": "Graph Attention Network-Based Block Propagation With Optimal AoB and Reputation in Web 3.0"
    },
    {
        "authors": [
            "Weijun Sun",
            "Zhikun Jiang",
            "Yonghao Chen",
            "Jiaqing Li",
            "Chengbin Zhou",
            "Na Han"
        ],
        "published_in": "Published in: IEEE Transactions on Computational Social Systems ( Early Access )",
        "date_of_publication": "30 October 2024",
        "doi": "10.1109/TCSS.2024.3479188",
        "publisher": "IEEE",
        "abstract": "The graph-based multiview clustering has gained significant attention due to its effectiveness in representing complex relationships among multiview data for enhanced clustering. Among the previous graph-based methods, the multiview graph learning (or graph fusion) technique has rapidly emerged as a promising direction, which, however, still suffers from two critical limitations. First, most of previous methods adopt a single-level of graph fusion, which lack the ability to go from single-level graph fusion to multilevel (deep) graph fusion. Second, they generally focus on constructing an optimal unified graph but cannot fully investigate the correlations among multiple views. Therefore, it is difficult to establish a comprehensive and obvious graph structure. In light of this, this article presents a new multiview graph learning method called deep similarity graph fusion (DSGF) for the multiview clustering task, where three pathways are simultaneously leveraged to fuse multilevel similarity into a unified graph. Particularly, multilevel graph fusion is utilized to obtain a view-specific similarity graph for each view and then fuse these single-view graphs (via three levels of graph fusion) into a robust graph, which takes advantage of deeper consensus information between various similarity graphs and improves the quality of the learned graph for the final spectral clustering process. Extensive experiments are conducted on six real-world multiview datasets, which demonstrate the highly competitive clustering performance of DSGF in comparison with state-of-the- art methods.",
        "issn": {
            "Electronic ISSN": "2329-924X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Linear programming",
                "Clustering methods",
                "Sun",
                "Fuses",
                "Clustering algorithms",
                "Automation",
                "Semisupervised learning",
                "Optimization",
                "Manufacturing"
            ],
            "Author Keywords": [
                "Deep graph fusion",
                "multiview clustering",
                "similarity graph"
            ]
        },
        "title": "Deep Similarity Graph Fusion for Multiview Clustering"
    },
    {
        "authors": [
            "Guoliang Cheng",
            "Peichun Li",
            "Beihai Tan",
            "Rong Yu",
            "Yuan Wu",
            "Miao Pan"
        ],
        "published_in": "Published in: IEEE Transactions on Cognitive Communications and Networking ( Early Access )",
        "date_of_publication": "14 October 2024",
        "doi": "10.1109/TCCN.2024.3480045",
        "publisher": "IEEE",
        "abstract": "Efficient federated learning (FL) in mobile edge networks faces challenges due to energy-consuming on-device training and wireless transmission. Optimizing the neural network structures is an effective approach to achieving energy savings. In this paper, we present a Snowball FL training with expanding neural network structure, which starts with a small-sized submodel and gradually progresses to a full-sized model. To achieve this, we first design the submodel and embedding extraction schemes for fine-grained model structure expansion. We then investigate the joint minimization problem of the global training loss and system-wise energy consumption. After that, we decompose the optimization problem into a long-term model structure expansion subproblem and a single-round local resource allocation subproblem. Specifically, the former subproblem is transformed into a variational calculus problem by leveraging theoretical analysis of the convergence bound. The Euler-Lagrange method is used to derive the solution, where the optimal evolution strategy for the model structure exponentially increases with the global round (i.e., the Snowball effect). Meanwhile, the latter subproblem is solved by convex optimization to acquire the optimal computing frequency and transmission power. Experiments indicate that the proposed framework can save about 50% of energy consumption to achieve on-par accuracy against state-of-the-art algorithms.",
        "issn": {
            "Electronic ISSN": "2332-7731"
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Energy consumption",
                "Computational modeling",
                "Accuracy",
                "Costs",
                "Convergence",
                "Resource management",
                "Integrated circuit modeling",
                "Wireless communication",
                "Servers"
            ],
            "Author Keywords": [
                "Federated learning",
                "distributed computing",
                "resource management",
                "optimization methods"
            ]
        },
        "title": "Snowball Effect in Federated Learning: An Approach of Exponentially Expanding Structures for Optimizing the Training Efficiency"
    },
    {
        "authors": [
            "Biao Yang",
            "Junrui Zhu",
            "Zhitao Yu",
            "Fucheng Fan",
            "Xiaofeng Liu",
            "Rongrong Ni"
        ],
        "published_in": "Published in: IEEE Transactions on Automation Science and Engineering ( Early Access )",
        "date_of_publication": "12 February 2024",
        "doi": "10.1109/TASE.2024.3362980",
        "publisher": "IEEE",
        "abstract": "Conventional deep learning-based trajectory prediction methods always adopt offline training based on trajectory data collected in known scenes. Despite its high prediction accuracy, it is unable to process trajectory data acquired in real-time, making it non-trivial to adapt to unknown scenes. To mitigate the above problem, an online multi-source transfer learning-based pedestrian trajectory predictor, dubbed OMTL-PTP, is proposed to achieve fast adaptation of trajectory prediction. OMTL-PTP resorts to online transfer learning to transfer trajectory knowledge from multiple source domains to the target domain, enabling the model to learn from the new scene and continuously improve its trajectory prediction ability. Concretely, we propose several base learners with external memory modules to preserve source domain trajectory knowledge for online knowledge transfer. A multi-hop attention mechanism is introduced in each learner to handle the future uncertainty of generated trajectories. To fully utilize the knowledge from multiple source domains, OMTL-PTP leverages ensemble learning to transfer knowledge from multiple base learners in the source domains to the online learner and fine-tunes the online learner in the target domain. Specifically, all base learners are combined to update the online learner, improving its ability to process future arriving samples and adapt to unknown scenes quickly. Qualitative and quantitative evaluations on ETH/UCY indicate the effectiveness of OMTL-PTP in online learning, which is beneficial for deploying trajectory prediction methods on intelligent edge devices. The code will be released at https://github.com/zjrcczu/OMTL-PTP after acceptance. Note to Practitioners —This paper is motivated by the challenge of online sustained trajectory prediction for unmanned autonomous agents, but it also applies to other trajectory prediction tasks, such as intelligent monitoring. Existing approaches always collect trajectory data from different sc...",
        "issn": {
            "Print ISSN": "1545-5955",
            "Electronic ISSN": "1558-3783"
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Predictive models",
                "Pedestrians",
                "Hidden Markov models",
                "Adaptation models",
                "Data models",
                "Uncertainty"
            ],
            "Author Keywords": [
                "Trajectory prediction",
                "multi-hop attention",
                "online transfer learning",
                "ensemble learning",
                "memory network"
            ]
        },
        "title": "Fast Adaptation Trajectory Prediction Method Based on Online Multisource Transfer Learning"
    },
    {
        "authors": [
            "Chengkai Lou",
            "Fen Hou",
            "Bo Li",
            "Hongwei Ding"
        ],
        "published_in": "Published in: IEEE Transactions on Vehicular Technology ( Early Access )",
        "date_of_publication": "28 October 2024",
        "doi": "10.1109/TVT.2024.3483891",
        "publisher": "IEEE",
        "abstract": "In recent years, there has been rapid development in vehicular networks and autonomous driving. While vehicles of various intelligence levels are becoming more common on the road, most research overlooks the data distribution across different vehicles in multicast scenarios. Our aim is to allow different kinds of vehicles to receive the needed content in a multicast scenario and to fulfill certain freshness requirements. Although deep reinforcement learning (DRL) has been widely used to address this issue, it suffers from slow training convergence and unstable performance. Hence, this study proposes combining DRL algorithms with behavior cloning and action mask, leveraging prior knowledge and expert algorithms to enhance performance. Finally, the freshness of the data content is ensured for all kinds of vehicles and effective data transmission is achieved. The simulation results indicate a significant improvement in the training efficiency and performance in our proposed method, with 15.6% to 31.9% improvement in terms of effective traffic compared to other counterparts.",
        "issn": {
            "Print ISSN": "0018-9545",
            "Electronic ISSN": "1939-9359"
        },
        "keywords": {
            "IEEE Keywords": [
                "Resource management",
                "Optimization",
                "Vehicle dynamics",
                "Cloning",
                "Real-time systems",
                "Multicast algorithms",
                "Autonomous vehicles",
                "Quality of service",
                "Heuristic algorithms",
                "Dynamic scheduling"
            ],
            "Author Keywords": [
                "vehicular network",
                "deep reinforcement learning",
                "age of information",
                "multicast"
            ]
        },
        "title": "Deep Reinforcement Learning based Intelligent Resource Allocation in Hybrid Vehicle Scenario"
    },
    {
        "authors": [
            "Fang Liu",
            "Minxin Wang",
            "Yuanan Liu",
            "Wei Ni",
            "Abbas Jamalipour"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "29 October 2024",
        "doi": "10.1109/JIOT.2024.3487800",
        "publisher": "IEEE",
        "abstract": "Data traffic has grown exponentially with the rapid development of narrowband Internet of Things (NB-IoT) technology. Non-orthogonal multiple access (NOMA) and mobile edge computing (MEC) are essential technologies to enhance the performance of NB-IoT networks. This paper proposes a new hybrid NOMA offloading strategy, allowing an IoT device to execute NOMA with other devices at different periods until the task offloading is completed. An optimization problem is established to minimize the overall offloading delay. To find the solution to the problem, we first use the proposed offloading strategy to determine the pairing method and offloading order of the IoT devices. Then, we transform the delay optimization problem into the link rate maximization problem. Finally, the closed-form solution of the optimal power allocation scheme for each IoT device is derived according to the theoretical analysis and the Karush-Kuhn-Tucker (KKT) condition. The simulation results show that the proposed offloading strategy and power allocation scheme effectively reduce the overall offloading delay under different device numbers and data lengths, which exceeds the orthogonal multiple access (OMA) schemes, the pure NOMA scheme, and the iterative multi-user NOMA scheme in lowering offloading delay",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "NOMA",
                "Delays",
                "Resource management",
                "Internet of Things",
                "Servers",
                "Performance evaluation",
                "Optimization",
                "Decoding",
                "Complexity theory",
                "Solid modeling"
            ],
            "Author Keywords": [
                "Internet of Things",
                "mobile edge computing",
                "non-orthogonal multiple access",
                "delay minimization",
                "power allocation"
            ]
        },
        "title": "Hybrid NOMA Offloading for Delay-Sensitive Applications in MEC-Based NB-IoT Networks"
    },
    {
        "authors": [
            "Ding Xu",
            "Lingjie Duan",
            "Haitao Zhao",
            "Hongbo Zhu"
        ],
        "published_in": "Published in: IEEE Transactions on Vehicular Technology ( Early Access )",
        "date_of_publication": "19 September 2024",
        "doi": "10.1109/TVT.2024.3465036",
        "publisher": "IEEE",
        "abstract": "As a promising paradigm to support wireless connections among massive devices, the Internet of things (IoT) is facing several challenges. Among them, the energy supply to IoT devices, the weak computation ability of IoT devices, and the stringent latency requirement of IoT applications are three major challenges, which can be tackled by radio frequency energy harvesting, mobile edge computing (MEC), and short packet communications (SPC), respectively. In this paper, we investigate resource allocation in MEC-enabled wireless powered IoT (WP-IoT) networks with SPC, and formulate the problem of optimizing the computation frequency, the packet length and the packet error rate, targeting maximizing the sum effective computation throughput. Since the problem is difficult to solve in general, we first simplify the problem by analyzing its properties, then design an efficient algorithm to obtain a suboptimal solution in an iterative manner based on the bisection method, the block coordinate descent method, the successive convex approximation method, and the majorization-minimization method. Simulation results confirm the effectiveness of the proposed algorithm. Particularly, it is shown that the proposed algorithm is of low complexity and achieves the performance close to that of the optimal exhaustive search, while also significantly outperforms other benchmark algorithms in existing literature.",
        "issn": {
            "Print ISSN": "0018-9545",
            "Electronic ISSN": "1939-9359"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Resource management",
                "Wireless communication",
                "Throughput",
                "Servers",
                "Error analysis",
                "Optimization"
            ],
            "Author Keywords": [
                "Internet of things",
                "radio frequency energy harvesting",
                "mobile edge computing",
                "short packet communications"
            ]
        },
        "title": "Effective Computation Throughput Maximization for MEC-Enabled WP-IoT Networks With Short Packet Communications"
    },
    {
        "authors": [
            "Nguyen Huu Sang",
            "Nguyen Duc Hai",
            "Nguyen Duc Duy Anh",
            "Nguyen Cong Luong",
            "Van-Dinh Nguyen",
            "Shimin Gong",
            "Dusit Niyato",
            "Dong In Kim"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "20 September 2024",
        "doi": "10.1109/JIOT.2024.3464646",
        "publisher": "IEEE",
        "abstract": "In this work, we consider the integration of energy harvesting and semantic communication strategies in resource-constrained Internet of Things (IoT) systems. The system empowers IoT devices to harvest energy from a base station, utilizing this harvested energy for the extraction and transmission of semantic information (e.g. scene graphs). To maximize the total transmission of image data or scene graphs to the central station, we formulate a comprehensive problem that jointly optimizes the energy harvesting duration, original image selection, transmit power, and channel allocation to IoT devices. The challenges arising from the dynamic environments and uncertain system parameters are effectively tackled by policy-based deep reinforcement learning algorithms, i.e., advantage actor-critic (A2C) and proximal policy optimization (PPO). Simulation results are implemented on the real dataset clearly showing the superior performance achieved by our proposed algorithms compared to baseline schemes. Notably, our approach enables IoT devices to transmit a greater number of original images and scene graphs with increased triplets to the central station, as highlighted in the simulation outcomes. This phenomenon showcases the potential of our strategy to enhance the capabilities of IoT systems in dynamic environments.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Semantics",
                "Energy harvesting",
                "Optimization",
                "Base stations",
                "Resource management",
                "Channel allocation"
            ],
            "Author Keywords": [
                "Semantic communication",
                "channel allocation",
                "deep reinforcement learning",
                "energy harvesting",
                "power control"
            ]
        },
        "title": "Wireless Power Transfer Meets Semantic Communication For Resource-Constrained IoT Networks: A Joint Transmission Mode Selection and Resource Management Approach"
    },
    {
        "authors": [
            "Tong Niu",
            "Yaqiu Liu",
            "Qingfeng Li",
            "Qichi Bao"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "16 October 2024",
        "doi": "10.1109/ACCESS.2024.3481496",
        "publisher": "IEEE",
        "abstract": "As the Internet of Things (IoT) continues to evolve, its inherent diversity and rapid iteration pose challenges for the development and deployment of malicious traffic detection systems. This study aims to develop a scalable detection architecture that can accurately identify malicious traffic in IoT environments, while protecting user privacy. We propose a deep-learning-based model that is capable of training and updating within an IoT setting. The payload of the data packets is encoded to enhance the feature extraction capability of the model. The model is then trained using federated learning/edge computing to ensure data privacy. The final model parameters are stored in an IPFS file storage system with the corresponding hash value stored in the blockchain, ensuring the correctness of the model parameters during expansion. Experiments were conducted in a Docker environment using the CIC IoT Dataset 2022 and CIC IoT Dataset 2023. The results demonstrate that the proposed architecture achieves a pre-training accuracy of 98.6% for known abnormal traffic and 79% for unknown malicious traffic. After a few rounds of training, the accuracies improved to 99.5% and 89.4%. The model also exhibited robust performance during expansion.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Data models",
                "Computational modeling",
                "Servers",
                "Accuracy",
                "Training",
                "Image edge detection",
                "Feature extraction",
                "Data privacy",
                "Telecommunication traffic",
                "Blockchains"
            ],
            "Author Keywords": [
                "Blockchain",
                "Internet of Things",
                "Malicious traffic detection",
                "Docker"
            ]
        },
        "title": "An Easily Scalable Docker-based Privacy-Preserving Malicious Traffic Detection Architecture for IoT Environments"
    },
    {
        "authors": [
            "Priit Roosipuu",
            "Ivar Annus",
            "Alar Kuusik",
            "Nils Kändler",
            "Muhammad Mahtab Alam",
            "Ivo Müürsepp"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "11 October 2024",
        "doi": "10.1109/ACCESS.2024.3478733",
        "publisher": "IEEE",
        "abstract": "Narrow-band Internet of Things (NB-IoT) and LTE CAT-M are part of low-power wide-area network (LPWAN) access technologies that provide long-range and energy-efficient network access to IoT devices. In this study, we conducted empirical measurements on both NB-IoT and LTE CAT-M networks inside a street manhole. Street manholes are an integral part of smart city structures, where sensor implementation is important. Our study demonstrates how to derive the network parameters from system information (SI) messages. More precisely, as we measured three commercial operators, we show the parameters and their influence on enhanced coverage level (ECL) repetitions. We give special consideration to signal strength and quality measurements at different locations inside the manhole. Our results indicate that when designing sensor networks for urban infrastructure monitoring, NB-IoT and LTE CAT-M should have different coverage margins in the implementation. Robust and low cost NB-IoT construction leads to the consideration of up to 2.4 dB higher coverage margins compared to LTE CAT-M. In this work, we analyze various environmental conditions that are relevant when evaluating and understanding underground signal propagation.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Long Term Evolution",
                "Antenna measurements",
                "Noise measurement",
                "Internet of Things",
                "Soil measurements",
                "Low-power wide area networks",
                "Environmental monitoring",
                "Loss measurement",
                "Wavelength measurement",
                "Downlink",
                "Radiowave propagation",
                "Wireless sensor networks",
                "Water resources"
            ],
            "Author Keywords": [
                "Downlink",
                "Internet of things",
                "LTE",
                "Radiowave propagation",
                "Wireless sensor networks"
            ]
        },
        "title": "Empirical Evaluation of NB-IoT and CAT-M Coverage for Underground Water System"
    },
    {
        "authors": [
            "Mohammadali Mohammadi",
            "Hien Quoc Ngo",
            "Michail Matthaiou"
        ],
        "published_in": "Published in: IEEE Transactions on Communications ( Early Access )",
        "date_of_publication": "22 July 2024",
        "doi": "10.1109/TCOMM.2024.3432455",
        "publisher": "IEEE",
        "abstract": "We investigate reconfigurable intelligent surface (RIS)-assisted simultaneous wireless information and power transfer (SWIPT) Internet of Things (IoT) networks, where energy-limited IoT devices are overlaid with cellular information users (IUs). IoT devices are wirelessly powered by a RIS-assisted massive multiple-input multiple-output (MIMO) base station (BS), which is simultaneously serving a group of IUs. By leveraging a two-timescale transmission scheme, precoding at the BS is developed based on the instantaneous channel state information (CSI), while the passive beamforming at the RIS is adapted to the slowly-changing statistical CSI. We derive closed-form expressions for the achievable spectral efficiency of the IUs and average harvested energy at the IoT devices, taking the channel estimation errors and pilot contamination into account. Then, a non-convex max-min fairness optimization problem is formulated subject to the power budget at the BS and individual quality of service requirements of IUs, where the transmit power levels at the BS and passive RIS reflection coefficients are jointly optimized. Our simulation results show that the average harvested energy at the IoT devices can be improved by 132% with the proposed resource allocation algorithm. Interestingly, IoT devices benefit from the pilot contamination, leading to a potential doubling of the harvested energy in certain network configurations.",
        "issn": {
            "Print ISSN": "0090-6778",
            "Electronic ISSN": "1558-0857"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Reconfigurable intelligent surfaces",
                "Massive MIMO",
                "Wireless communication",
                "Optimization",
                "Simultaneous wireless information and power transfer",
                "Precoding"
            ],
            "Author Keywords": [
                "Massive multiple-input multiple-output (MIMO)",
                "reconfigurable intelligent surface (RIS)",
                "simultaneous wireless information and power transfer (SWIPT)"
            ]
        },
        "title": "Phase-Shift and Transmit Power Optimization for RIS-Aided Massive MIMO SWIPT IoT Networks"
    },
    {
        "authors": [
            "Xin Wang",
            "Zhongwei Wan",
            "Arvin Hekmati",
            "Mingyu Zong",
            "Samiul Alam",
            "Mi Zhang",
            "Bhaskar Krishnamachari"
        ],
        "published_in": "Published in: IEEE Internet Computing ( Early Access )",
        "date_of_publication": "16 August 2024",
        "doi": "10.1109/MIC.2024.3443169",
        "publisher": "IEEE",
        "abstract": "Advancements in Generative AI hold immense promise to push Internet of Things (IoT) to the next level. In this article, we share our vision on IoT in the era of Generative AI. We discuss some of the most important applications of Generative AI in IoT-related domains. We also identify some of the most critical challenges and discuss current gaps as well as promising opportunities on enabling Generative AI for IoT. We hope this article can inspire new research on IoT in the era of Generative AI.",
        "issn": {
            "Print ISSN": "1089-7801",
            "Electronic ISSN": "1941-0131"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Generative AI",
                "Data models",
                "Adaptation models",
                "Robots",
                "Computational modeling",
                "Autonomous vehicles"
            ],
            "Author Keywords": []
        },
        "title": "IoT in the Era of Generative AI: Vision and Challenges"
    },
    {
        "authors": [
            "Awaneesh Kumar Yadav",
            "Manoj Misra",
            "Pradumn Kumar Pandey",
            "Pasika Ranaweera",
            "Madhusanka Liyanage",
            "Neeraj Kumar"
        ],
        "published_in": "Published in: IEEE Transactions on Dependable and Secure Computing ( Early Access )",
        "date_of_publication": "15 April 2024",
        "doi": "10.1109/TDSC.2024.3388467",
        "publisher": "IEEE",
        "abstract": "The plethora of Internet of Things (IoT) devices and their diversified requirements have opted to design security mechanisms that cover all major security requirements. Wireless Local Area Networks (WLANs) is the most common network domains where IoT devices are launched, particularly because of its easy availability. Security, in other words authentication however, remains to be a major constriction for IoT-WLAN deployments. Though there are IoT based authentication protocols prevailing, such protocols are either prone to threats such as perfect forward secrecy violations, insider with database access attack, traceability attack, stolen device attack, ephemeral secret leakage, or they consume excessive computational and communication resources that result in an unprecedented burden for the IoT system. This paper presents an Extensible Authentication Protocol (EAP) based mechanism for IoT devices deployed in a WLAN that addresses the above security issues and achieves cost-effectiveness. Validation follows an informal and formal approaches (using GNY and BAN logic, and Scyther verification tool) for the proposed protocol, demonstrating its robustness. Our performance analysis shows that the proposed protocol is lightweight and more secure in contrast to the state-of-the-art solutions. In addition, performance of the proposed protocol subjected to unknown attacks is investigated, which deduces that the proposed protocol has less overhead under unknown attacks than its competitors. A prototype of the protocol has been developed to demonstrate its feasibility and accuracy.",
        "issn": {
            "Print ISSN": "1545-5971",
            "Electronic ISSN": "1941-0018"
        },
        "keywords": {
            "IEEE Keywords": [
                "Authentication",
                "Protocols",
                "Wireless LAN",
                "Security",
                "Protection",
                "Internet of Things",
                "Servers"
            ],
            "Author Keywords": [
                "Authentication",
                "Extensible Authentication Protocol (EAP)",
                "Formal Verification",
                "Network Security",
                "Wireless Local Area Network (WLAN)"
            ]
        },
        "title": "A Secure Authentication Protocol for IoT-WLAN using EAP framework"
    },
    {
        "authors": [
            "Akhilesh Kumar Singh",
            "Fru Ngwa Fru Junior",
            "Ngu Leonel Mainsah",
            "Bande Abdoul-Rahmane"
        ],
        "published_in": "Published in: IEEE Transactions on AgriFood Electronics ( Early Access )",
        "date_of_publication": "02 October 2024",
        "doi": "10.1109/TAFE.2024.3454644",
        "publisher": "IEEE",
        "abstract": "This article presents an in-depth exploration of multifaceted efforts in agricultural research aimed at addressing the unpredictable nature of crop production and related processes, including the demonstration of data collection and its application. This research focuses on leveraging current technologies and devising sustainable solutions to mitigate uncertainties attributed to natural climatic conditions and infectious agents. The central theme of this review centers around the utilization of Internet of things sensors for data collection, cloud software for data processing, and the integration of diverse machine learning algorithms for data analysis. The objective is to advance insights into the application of these technologies in agriculture and their potential to enhance crop yield and sustainability. The article comprehensively explores the technological landscape and the levels at which current research is being conducted, shedding light on machine learning algorithms, their specific functions, and comparative analysis of each algorithm based on different use cases. Furthermore, the article presents an implementation approach that integrates sensors and machine learning. Its primary application is to predict the type of crop to produce based on nutrient levels detected by the sensors.",
        "issn": {
            "Electronic ISSN": "2771-9529"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Cloud computing",
                "Precision agriculture",
                "Data collection",
                "Wireless sensor networks",
                "Wireless fidelity",
                "Wireless communication",
                "Zigbee",
                "Internet of Things",
                "Reliability"
            ],
            "Author Keywords": [
                "Cloud processing",
                "Internet of Things (IoT)",
                "machine learning (ML)",
                "precision agriculture",
                "wireless sensor network (WSN)"
            ]
        },
        "title": "Enabling Data Collection and Analysis for Precision Agriculture in Smart Farms"
    },
    {
        "authors": [
            "Liang Zhang",
            "Jie Guo",
            "Tongyu Ding",
            "Chong-Zhi Han",
            "Pengzheng Xu"
        ],
        "published_in": "Published in: IEEE Antennas and Wireless Propagation Letters ( Early Access )",
        "date_of_publication": "22 July 2024",
        "doi": "10.1109/LAWP.2024.3432089",
        "publisher": "IEEE",
        "abstract": "This paper proposes a monostatic backscatter communication system with enhanced isolation between excitation and receiver. This enables a mechanism akin to simultaneous co-frequency full-duplex operation, allowing the receiver to effectively amplify and quantify the weak reflected backscatter signals, subsequently demodulating them to extract valid data. Compared to RFID systems, which also employ monostatic backscatter communication, this approach can substantially extend the effective communication range. Furthermore, the exciting source and receiver are integrated in the same system, which makes the base-station system's framework compatible with a common WiFi router. Meanwhile, the communication power consumption of the remote node is significantly decreased",
        "issn": {
            "Print ISSN": "1536-1225",
            "Electronic ISSN": "1548-5757"
        },
        "keywords": {
            "IEEE Keywords": [
                "Backscatter",
                "Optimization",
                "Receivers",
                "Voltage measurement",
                "Reflection",
                "Surface impedance",
                "Antenna measurements"
            ],
            "Author Keywords": [
                "Monostatic backscatter communication",
                "Internet-of-Things",
                "reconfigurable intelligent surface",
                "near field isolation"
            ]
        },
        "title": "Design and Implementation of a Tunable Metasurface Enhanced Monostatic Backscatter Communication System for IoT"
    },
    {
        "authors": [
            "Lei Zheng",
            "Juanjuan Ren",
            "Yong Liu",
            "Qingchun Chen"
        ],
        "published_in": "Published in: IEEE Wireless Communications Letters ( Early Access )",
        "date_of_publication": "15 October 2024",
        "doi": "10.1109/LWC.2024.3481059",
        "publisher": "IEEE",
        "abstract": "Motivated by age of information (AoI), we introduce the age of leaked information (AoLI) to characterize the freshness of information eavesdropped by an unintended receiver (UR). To ensure that the intended receiver acquires the fresh information while preventing UR from overhearing, a multi-objective optimization problem is formulated to jointly minimize the time-averaged AoI and maximize the time-averaged AoLI. By modeling their evolution as Markov chains, the theoretical AoI and AoLI are analyzed to derive the closed-form optimal transmit and jamming power. Simulation results unveil the inherent tradeoff between the AoI and AoLI.",
        "issn": {
            "Print ISSN": "2162-2337",
            "Electronic ISSN": "2162-2345"
        },
        "keywords": {
            "IEEE Keywords": [
                "Jamming",
                "Optimization",
                "Internet of Things",
                "Information age",
                "Receivers",
                "Steady-state",
                "Interference cancellation",
                "Information leakage",
                "Wireless communication",
                "Throughput"
            ],
            "Author Keywords": [
                "Age of information",
                "age of leaked information",
                "statistical channel state information"
            ]
        },
        "title": "Analysis and Optimization of Age of Information and Age of Leaked Information for IoT Networks"
    },
    {
        "authors": [
            "Hongwei Cui",
            "Yuyang Du",
            "Qun Yang",
            "Yulin Shao",
            "Soung Chang Liew"
        ],
        "published_in": "Published in: IEEE Communications Magazine ( Early Access )",
        "date_of_publication": "27 September 2024",
        "doi": "10.1109/MCOM.002.2400106",
        "publisher": "IEEE",
        "abstract": "Task-oriented communications are an important element in future intelligent IoT systems. Existing IoT systems, however, are limited in their capacity to handle complex tasks, particularly in their interactions with humans to accomplish these tasks. In this article, we present LLMind, a large language model-based (LLM-based), task-oriented AI agent framework that enables effective collaboration among IoT devices, with humans communicating high-level verbal instructions, to perform complex tasks. Inspired by the functional specialization theory of the brain, our framework integrates an LLM with domain-specific AI modules, enhancing its capabilities. Complex tasks, which may involve collaborations of multiple domain-specific AI modules and IoT devices, are executed through a control script generated by the LLM using a Language- Code transformation approach, which first converts language descriptions to an intermediate finite-state machine (FSM) before final precise transformation to code. Furthermore, the framework incorporates a novel experience accumulation mechanism to enhance response speed and effectiveness, allowing the framework to evolve and become progressively sophisticated through continuing user and machine interactions.",
        "issn": {
            "Print ISSN": "0163-6804",
            "Electronic ISSN": "1558-1896"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Artificial intelligence",
                "Codes",
                "Collaboration",
                "Reliability",
                "Performance evaluation",
                "Transforms",
                "Social networking (online)",
                "Process control",
                "Planning"
            ],
            "Author Keywords": []
        },
        "title": "LLMind: Orchestrating AI and IoT with LLM for Complex Task Execution"
    },
    {
        "authors": [
            "Xiangyun Meng",
            "Xuanli Wu",
            "Ziyi Xie",
            "Tingting Zhang",
            "Tao Xu"
        ],
        "published_in": "Published in: IEEE Transactions on Green Communications and Networking ( Early Access )",
        "date_of_publication": "15 May 2024",
        "doi": "10.1109/TGCN.2024.3401107",
        "publisher": "IEEE",
        "abstract": "Unmanned aerial vehicles (UAVs) enable flexible data collection from Internet of Things (IoT) nodes in remote areas, but the data of IoT nodes face security threats. In the proposed data collection strategy based on a double cluster head (CH) framework, we exploit the inter-user interference (IUI) of uplink non-orthogonal multiple access (NOMA) to improve the security of IoT nodes. Specifically, inter-CH interference in NOMA is used as jamming signals to hide confidential data. Then a CH selection scheme is designed to alleviate the unbalanced energy consumption among member nodes in a cluster. Based on the CH selection scheme, we maximize the secrecy energy efficiency (SEE) via joint optimization of power, time scheduling, and trajectory. Due to the highly coupled variables and non-convex constraints, an alternating optimization method is used to decouple the original problem into subproblems and they are solved iteratively. In each iteration, Dinkelbach’s method is used to tackle the fractional objective function; the successive convex approximation technique is used to transform the non-convex subproblems into convex forms. In numerical simulations, our proposed data collection strategy shows effectiveness in improving SEE and hindering wiretapping. Furthermore, the proposed CH selection scheme efficiently extends the lifetime of energy-constrained IoT nodes.",
        "issn": {
            "Electronic ISSN": "2473-2400"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "NOMA",
                "Autonomous aerial vehicles",
                "Trajectory",
                "Interference",
                "Energy consumption",
                "Wireless networks"
            ],
            "Author Keywords": [
                "Internet of Things",
                "secrecy energy efficiency",
                "non-orthogonal multiple access (NOMA)",
                "physical layer security (PLS)",
                "inter-user interference"
            ]
        },
        "title": "Secure Resource Allocation and Trajectory Design for UAV-Assisted IoT With Double Cluster Head"
    },
    {
        "authors": [
            "Abrar Bin Sarawar",
            "A. S. M. Badrudduza",
            "Md. Ibrahim",
            "Imran Shafique Ansari",
            "Heejung Yu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "27 September 2024",
        "doi": "10.1109/JIOT.2024.3469731",
        "publisher": "IEEE",
        "abstract": "In the sixth-generation (6G) Internet of Things (IoT) networks, unmanned aerial vehicle (UAV)-mounted base stations and reconfigurable intelligent surfaces (RISs) have been considered key technologies to enhance coverage, flexibility, and security in non-terrestrial networks (NTNs). In addition to aerial networks enabled by NTN technologies, the integration of underwater networks with 6G IoT can be considered one of the most innovative challenges in future IoT. Along with such trends in IoT, this study investigates the secrecy performance of IoT networks that integrate radio frequency (RF) UAV-based NTNs and underwater optical wireless communication (UOWC) with RISs. Under three potential eavesdropping scenarios, i.e., overhearing of RF signal, UOWC signal, and both signals, we derive closed-form expressions for secrecy performance metrics, including average secrecy capacity, secrecy outage probability, probability of strictly positive secrecy capacity, and effective secrecy throughput. Extensive numerical analyses and Monte Carlo simulations elucidate the impact of system parameters such as fading severity, the number of RIS reflecting elements, underwater turbulence, pointing errors, and detection techniques on system security. The findings offer comprehensive design guidelines for network deployment aiming to enhance secrecy performance and ensure secure communication in diverse and challenging environments.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Radio frequency",
                "Internet of Things",
                "Eavesdropping",
                "Security",
                "Wireless communication",
                "Autonomous aerial vehicles",
                "Reconfigurable intelligent surfaces",
                "Measurement",
                "Fading channels",
                "6G mobile communication"
            ],
            "Author Keywords": [
                "Reconfigurable intelligent surface",
                "unmanned aerial vehicle",
                "non-terrestrial network",
                "underwater optical wireless communication",
                "physical layer security"
            ]
        },
        "title": "Secrecy Performance Analysis of Integrated RF-UOWC IoT Networks Enabled by UAV and Underwater-RIS"
    },
    {
        "authors": [
            "Xidi Qu",
            "Shengling Wang",
            "Kun Li",
            "Jianhui Huang",
            "Xiuzhen Cheng"
        ],
        "published_in": "Published in: IEEE Transactions on Mobile Computing ( Early Access )",
        "date_of_publication": "19 September 2024",
        "doi": "10.1109/TMC.2024.3464494",
        "publisher": "IEEE",
        "abstract": "The integration of directed acyclic graph (DAG)- based blockchain and Internet of Things (IoT) aims at improving the efficiency of data storage. However, if massive IoT data are not placed in an organized way, the search and usage of the data for upper-level applications can be burdensome, since they have to examine the data block by block, which also increases the difficulty of data verification, affecting consensus efficiency. To maintain the high throughput advantage of DAGbased blockchain applied in IoT and improve the data analysis efficiency, we propose a novel consensus mechanism named TidyBlock, including the transaction collation mechanism for block generation and the block selection mechanism for verification. The first mechanism can tidy up scattered transactions before they are packaged into blocks, while the second one can collate blocks to facilitate verification, realizing a two-layer collation of IoT data so as to increase analysis efficiency of upper-level IoT applications. Additionally, the second mechanism can provide a self-driven incentive for rational participants to follow the first one in case they are reluctant to do extra collation work. Theoretical analysis is provided to demonstrate the validity of our proposed algorithms by formal methods. Extensive simulations based on synthetic data verify the rationality and effectiveness of the proposed mechanisms.",
        "issn": {
            "Print ISSN": "1536-1233",
            "Electronic ISSN": "1558-0660"
        },
        "keywords": {
            "IEEE Keywords": [
                "Blockchains",
                "Internet of Things",
                "Consensus protocol",
                "Throughput",
                "Memory",
                "Clustering algorithms",
                "Sorting"
            ],
            "Author Keywords": [
                "Blockchain",
                "Internet of Things",
                "consensus mechanism",
                "formal methods"
            ]
        },
        "title": "TidyBlock: A Novel Consensus Mechanism for DAG-based Blockchain in IoT"
    },
    {
        "authors": [
            "Han Zhang",
            "Qian Wang",
            "Xiaoli Zhang",
            "Yi He",
            "Bo Tang",
            "Qi Li"
        ],
        "published_in": "Published in: IEEE Communications Magazine ( Early Access )",
        "date_of_publication": "17 June 2024",
        "doi": "10.1109/MCOM.001.2300390",
        "publisher": "IEEE",
        "abstract": "Internet of things (IoT) networks allow cross-device interactions to achieve various intelligent applications, for example, smart homes and smart commercial spaces. However, cross-device interactions are often protected by inadequate authorization mechanisms, making them susceptible to various attacks, including connection-based attacks, application impersonation attacks, and so on. In this article, we propose a zero-trust IoT network architecture, OUTSIDE, designed to provide fine-grained authorization for IoT applications. It achieves the application-level authorization at the network layer by encoding the capability information of applications into verifiable tokens. Meanwhile, it enables a zero-trust service for per-packet verification, ensuring that every packet is sent by an authorized application with proper access privileges. Particularly, our architecture is versatile and compatible with various IoT protocols. We prototype and deploy OUTSIDE in Raspberry Pis and ESP32 microcontrollers running over the constrained application protocol (CoAP). The experimental results show that our architecture incurs negligible performance degradation.",
        "issn": {
            "Print ISSN": "0163-6804",
            "Electronic ISSN": "1558-1896"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Authorization",
                "Servers",
                "Permission",
                "Protocols",
                "Object recognition",
                "Impersonation attacks"
            ],
            "Author Keywords": []
        },
        "title": "Toward Zero-Trust IoT Networks via Per-Packet Authorization"
    },
    {
        "authors": [
            "Xingchi Chen",
            "Bo Yi",
            "Qing Li",
            "Fa Zhu",
            "Yingpu Nian",
            "Achyut Shankar",
            "Michele Nappi",
            "Amr Tolba"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "10 October 2024",
        "doi": "10.1109/JIOT.2024.3477494",
        "publisher": "IEEE",
        "abstract": "Beyond 5G-enabled Edge Computing Networking (ECN) will further deploy computing and communication resources to the edge of the networks. Then, edge service demands for Internet of Things (IoT) applications are becoming more and more diverse, while the corresponding routing service capability is limited and not flexible enough to deal with the demands of ECN, which then leads to reducing the inherent routing capability of ECN. It becomes extremely difficult for ECN to support diversified demands and provide diverse IoT applications quickly and flexibly. In this paper, we propose a novel and customized deep routing mechanism for IoT applications in ECN, in which the network slicing and deep learning methods are jointly applied and leveraged. Firstly, we design a new ECN architecture that formulates four kinds of network slices to cope with various IoT scenarios, which are eMBB, uRLLC, mMTTC, and backup slices. Secondly, using these slices, we can customize the ECN environment flexibly, based on which we propose the corresponding routing method for the purpose of fast and efficient service delivery. In particular, the mapping between network slices and the infrastructure is established with the object of maximizing the resource utilization. Then, the routing is designed and customized by using the deep learning model. Lastly, the experimental results show that the deep customized mechanism designed in this paper can reduce the average loss rate of the model, decrease the average delay, as well as improve the average resource utilization compared with the existing studies.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Routing",
                "Internet of Things",
                "Network slicing",
                "Edge computing",
                "Deep learning",
                "Resource management",
                "Computer architecture",
                "Neural networks",
                "Load modeling",
                "Delays"
            ],
            "Author Keywords": [
                "Edge computing",
                "Internet of Things applications",
                "network slicing",
                "deep learning",
                "customized routing"
            ]
        },
        "title": "Deep Customized Network Slicing and Efficient Routing for IoT Applications in B5G-Enabled Edge Computing Networks"
    },
    {
        "authors": [
            "Byung Moo Lee"
        ],
        "published_in": "Published in: IEEE Transactions on Mobile Computing ( Early Access )",
        "date_of_publication": "28 October 2024",
        "doi": "10.1109/TMC.2024.3486712",
        "publisher": "IEEE",
        "abstract": "Massive MIMO technology offers a promising solution for supporting the simultaneous connectivity of a large number of ultra high-density massive IoT devices. However, due to limited resources, effective channel estimation becomes a challenge. One approach is to reuse the orthogonal reference signal (ORS) in a repetitive manner, while another option is to employ random non-orthogonal reference signals (NORS) for distributed massive IoT devices. In order to enhance performance in the face of high RS congestion from massive IoT devices, the strategic utilization of several critical resources becomes imperative. In this paper, we delve into the methodology of resource management to effectively address the severe high-density scenario of massive IoT devices. Specifically, we examine the system bandwidth, which proves particularly advantageous in bandwidth-limited environments. Exploiting the spatial domain, we leverage the distinctive features of massive MIMO to enable simultaneous parallel transmission by increasing the number of service antennas at the base station (BS). Furthermore, we explore the potential benefits of sectorization, a technique that involves dividing a circular cell into multiple sectors, thereby reducing RS congestion. Nevertheless, it is crucial to acknowledge that increasing these resources may entail certain trade-offs and could potentially have adverse effects on overall system performance. To gain comprehensive insights, we conduct a thorough performance analysis under various scenarios, aiming to identify key characteristics that can facilitate the optimal operation of massive MIMO in high-density IoT environments. Building upon our findings, we devise an algorithm that efficiently manages resources, ultimately leading to improved system performance.",
        "issn": {
            "Print ISSN": "1536-1233",
            "Electronic ISSN": "1558-0660"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Bandwidth",
                "Massive MIMO",
                "Antennas",
                "Signal to noise ratio",
                "Transmitting antennas",
                "System performance",
                "Resource management",
                "Mobile computing",
                "Channel estimation"
            ],
            "Author Keywords": [
                "Internet of things",
                "massive MIMO",
                "resources"
            ]
        },
        "title": "Efficient Resource Management for Massive MIMO in High-Density Massive IoT Networks"
    },
    {
        "authors": [
            "Yipeng Wang",
            "Xintong Zhang",
            "Yingxu Lai",
            "Zijian Zhao",
            "Yongjian Deng"
        ],
        "published_in": "Published in: IEEE Transactions on Cognitive Communications and Networking ( Early Access )",
        "date_of_publication": "08 July 2024",
        "doi": "10.1109/TCCN.2024.3424888",
        "publisher": "IEEE",
        "abstract": "This paper concerns the detection of Distributed Denial of Service (DDoS) attacks in network traffic generated by Internet of Things (IoT) devices in smart home environments. The detection of DDoS attacks is crucial for IoT network security, as such attacks can disrupt the availability of essential services. In particular, due to the growing popularity of smart homes and the emergence of malicious software that compromises devices, home IoT devices have become susceptible to botnet infections capable of launching DDoS attacks. With the development of artificial intelligence technology, many advanced methods have been proposed that show promising performance in detecting DDoS attacks. However, there is still a need for improvement in their generalizability and detection efficiency. In this paper, we propose Hifoots, a highly efficient IoT DDoS attack detection scheme, aiming to achieve high detection robustness and detection efficiency. Hifoots builts upon our key observation that DDoS attacks can be detected by examining the group behavior of all flows over a given time interval. We evaluated Hifoots on five complex DDoS attack scenarios. The experimental results demonstrate that Hifoots outperforms the detection performance of existing state-of-the-art methods and offers an improvement in detection efficiency that is up to 12 times better, along with stronger generalizability compared to the state-of-the-art methods.",
        "issn": {
            "Electronic ISSN": "2332-7731"
        },
        "keywords": {
            "IEEE Keywords": [
                "Denial-of-service attack",
                "Feature extraction",
                "Computer crime",
                "Training",
                "Internet of Things",
                "Telecommunication traffic",
                "Smart homes"
            ],
            "Author Keywords": [
                "Internet of Things",
                "Distributed Denial of Service",
                "Deep Learning",
                "Machine Learning",
                "Network Security"
            ]
        },
        "title": "Hifoots: A Highly Efficient DDoS Attack Detection Scheme Deployed in Smart IoT Homes"
    },
    {
        "authors": [
            "Yan Wang",
            "Rongen Dong",
            "Feng Shu",
            "Wei Gao",
            "Qi Zhang",
            "Jiajia Liu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "11 September 2024",
        "doi": "10.1109/JIOT.2024.3458096",
        "publisher": "IEEE",
        "abstract": "In this paper, channel estimation of an active intelligent reflecting surface (IRS) aided uplink Internet of Things (IoT) network is investigated. Firstly, the least square (LS) estimators for the direct channel and the cascaded channel are presented, respectively. The corresponding mean square errors (MSE) of channel estimators are derived. Subsequently, in order to evaluate the influence of adjusting the transmit power at the IoT devices or the reflected power at the active IRS on Sum-MSE performance, two situations are considered. In the first case, under the total power sum constraint of the IoT devices and active IRS, the closed-form expression of the optimal power allocation factor is derived. In the second case, when the transmit power at the IoT devices is fixed, there exists an optimal reflective power at active IRS. To further improve the estimation performance, the convolutional neural network (CNN)-based direct channel estimation (CDCE) algorithm and the CNN-based cascaded channel estimation (CCCE) algorithm are designed. Finally, simulation results demonstrate the existence of an optimal power allocation strategy that minimizes the Sum-MSE, and further validate the superiority of the proposed CDCE / CCCE algorithms over their respective traditional LS and minimum mean square error (MMSE) baselines.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Channel estimation",
                "Uplink",
                "Signal processing algorithms",
                "Resource management",
                "Estimation",
                "Performance evaluation"
            ],
            "Author Keywords": [
                "Active intelligent reflecting surface",
                "Internet of Things",
                "channel estimation",
                "power optimization",
                "deep learning"
            ]
        },
        "title": "Power Optimization and Deep Learning for Channel Estimation of Active IRS-Aided IoT"
    },
    {
        "authors": [
            "Van-Linh Nguyen",
            "Hao-Ping Tsai",
            "Hyundong Shin",
            "Trung Q. Duong"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "12 September 2024",
        "doi": "10.1109/JIOT.2024.3459015",
        "publisher": "IEEE",
        "abstract": "Network intrusion detection systems (NIDS) are vital for identifying security attacks and predicting early invasion attempts, which is essential for protecting the Internet. Recently, Deep learning (DL) has made significant achievements in enhancing intrusion detection accuracy. Nevertheless, the practical implementation of high-complexity DL models is limited by the constrained computational capabilities of the Internet of Things (IoT) devices, e.g., home routers and IoT gateways. This article introduces a novel NIDS approach explicitly tailored for IoT networks, leveraging a lightweight deep learning model. During the data preprocessing phase, we use a spatially enriched data conversion technique to decrease the dimensionality of high-dimensional raw traffic variables. This helps to offset the problem of increased model complexity. Furthermore, when spatial relationships often exist in the data, we can simplify the learning architecture by utilizing state-of-the-art vision transformer techniques in the computer vision field that can substantially reduce model complexity. The experimental results indicate that the proposed method achieves outstanding accuracy up to 99.57% with high-volume traffic input. Moreover, the proposed method reaches substantial reductions in learnable parameters by 55.35% and 82.07%, along with a remarkable decrease in floating point operations (FLOPs) by 93.56% and 99.28% compared to existing studies. The outstanding achievement highlights the proposed method’s ability to balance model complexity and accuracy performance, making it extremely appropriate for deployment on IoT gateways with limited resources.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Feature extraction",
                "Computational modeling",
                "Complexity theory",
                "Data models",
                "Accuracy",
                "Intrusion detection"
            ],
            "Author Keywords": [
                "Intrusion Detection System",
                "Vision Transformer",
                "Internet of Things security"
            ]
        },
        "title": "Spatial Data Transformation and Vision Learning For Elevating Intrusion Detection in IoT Networks"
    },
    {
        "authors": [
            "Kaijun Leng",
            "Cheng-Feng Wu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "26 June 2024",
        "doi": "10.1109/JIOT.2024.3419440",
        "publisher": "IEEE",
        "abstract": "Under the trend of continuous expansion of urban areas and increasing number of vehicles, the traditional transportation system has been unable to cope with various complex situations, and the problem of insufficient information exchange has emerged. This study is based on the cooperative vehicle-infrastructure system (CVIS) of the Internet of Things (IoT)-enhanced intelligent model, aiming to solve the problem of insufficient information exchange in the existing intelligent transportation system under dynamic traffic conditions. Thus, power management, intelligent driving ability, and driving comfort and safety can be improved. Through the application of IoT technology, a CVIS is established, which enables efficient information exchange between vehicles and infrastructure by constructing advanced IoT architectures, and utilizes reinforcement learning models to optimize system responses and driving strategies. The experimental results show that the system can maintain an effective accelerated response of the vehicle, even under heavy loads (up to 2000 kg) and complicated road conditions, with the fastest response time of 15.9 seconds. In addition, the system remarkably improves driving stability, especially in tests conducted on slippery roads and uphill segments. Regarding safety and security, the system achieved a commendable command recognition accuracy of about 100%. Moreover, integrated intelligent algorithms help reduce overall energy consumption while maintaining system performance. These results highlight the critical role of IoT technology in promoting the responsiveness and safety of intelligent vehicle systems.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Real-time systems",
                "Data models",
                "Safety",
                "Analytical models",
                "Roads",
                "Predictive models",
                "Internet of Things"
            ],
            "Author Keywords": [
                "Internet of Things technology",
                "CVIS control",
                "intelligent model vehicle",
                "power management",
                "intelligent driving"
            ]
        },
        "title": "Traffic Management Optimization via Iot-Enhanced Cooperative Vehicle-Infrastructure Systems"
    },
    {
        "authors": [
            "Yulong Gao",
            "Wenxuan Feng",
            "Liang He",
            "Mianxiong Dong"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "12 November 2024",
        "doi": "10.1109/JIOT.2024.3496732",
        "publisher": "IEEE",
        "abstract": "Quantum computing increases the security risks of data in intelligent Internet of Things (IoT) based on traditional cryptography. Multivariate Public Key Cryptography has the security advantage of resisting quantum computing and Rainbow is an important research focus in it. However, the Rainbow algorithms secret keys are too large to suitable for resource-constrained IoT system. We propose an efficient Rainbow signature scheme based on circulant and Toeplitz matrices for intelligent IoT. In our scheme, the variable matrixes of polynomials have special forms constructed from circulant and Toeplitz matrices. Every variable matrix of central maps and public key polynomials is divided into four submatrices. Three submatrices of variable matrixes are generated using seeds randomly chosen. The fourth submatrix is generated by an affine map and three submatrices. Therefore, the public key consists of seeds and variables in the fourth submatrices, rather than all variables of polynomials. Then, the correctness of our scheme has been proved and we provide a security analysis. More specifically, it’s proved that the scheme can resist five attacks against Rainbow, such as Direct attacks, Unbalanced oil vinegar attacks, MinRank attacks, HighRank attacks, and Rainbow-band-separation attacks. At last, according to the experimental results, our system’s public key sizes are 97.96% smaller and private key size are 92.12% than the key sizes of the standard Rainbow scheme. Additionally, compare with other Rainbow-like schemes, the comparison analysis and experimental results show that our scheme has less communication costs and small key size than those in the similar literatures for IoT.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Security",
                "Polynomials",
                "Cryptography",
                "Encryption",
                "Oils",
                "Public key cryptography",
                "Resists",
                "Research and development",
                "Privacy"
            ],
            "Author Keywords": [
                "Internet of Things",
                "Rainbow",
                "multivariate public key cryptography",
                "post-quantum cryptography",
                "data security"
            ]
        },
        "title": "Novel and Efficient Rainbow Signature Scheme Based on Circulant and Toeplitz Matrices for Intelligent IoT"
    },
    {
        "authors": [
            "Ashu Taneja",
            "Shalli Rani"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "20 June 2024",
        "doi": "10.1109/TCE.2024.3417524",
        "publisher": "IEEE",
        "abstract": "The emerging consumer electronics are integrated with internet-of-things (IoT) for better user experience. But these IoT nodes consume energy for massive user interaction and are soon energy drained out because of limited battery life. The power limitation of network devices is the main challenge. This paper presents a sustainable solution to this problem by offering a digital twin powered IoT network. With the assistance of digital twin, the virtual network is tested for operational node power consumption. The massive network nodes harvest energy from the access points (APs) transmission using different models. Further, resource control is enabled through a proposed clustering algorithm which associates APs to nodes with minimum interference from neighboring nodes and interfering APs. The paper evaluates the digital twin empowered approach for maximum power harvested and per user spectral efficiency under different pilot length, AP count and number of IoT nodes. The impact of introducing uniform power control on the system performance is also discussed. It is observed that maximum per user spectral efficiency improves by 14.02% for K=50 and 9.79% for K=10 with uniform power control. Next, the comparison with other system models are presented under different clustering approaches. In the end, the application of digital twin in the health management of consumer electronics is also discussed as a use case scenario.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Digital twins",
                "Consumer electronics",
                "Internet of Things",
                "Spectral efficiency",
                "Power control",
                "Energy harvesting",
                "Antennas"
            ],
            "Author Keywords": [
                "digital twin",
                "power harvested",
                "network sustainability",
                "consumer electronics",
                "spectral efficiency"
            ]
        },
        "title": "Digital Twin Empowered Approach for Sustainable IoT in Consumer Electronics Health: A Use Case"
    },
    {
        "authors": [
            "Naeem Ali Askar",
            "Adib Habbal"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "09 September 2024",
        "doi": "10.1109/ACCESS.2024.3456669",
        "publisher": "IEEE",
        "abstract": "Internet of Things technology is seeing huge success in various fields. Smart objects combined with Internet connection have become an essential part of every aspect of human life. The way people interact with the things around them has inevitably changed. The IoT system presents many challenges. Devices are heterogeneous and limited in energy and memory. The applications used require a continuous and stable connection to transmit information effectively. This results in high energy consumption. Named Data Networking (NDN) is a promising networking concept. Unlike traditional networking, it’s a data-driven model. It uses names to identify and retrieve data instead of device addresses. NDN provides a simple and efficient forwarding mechanism which makes it suitable for IOT communication. In this paper, we proposed forwarding strategy based on reinforcement learning for NDN-based IoT communications. The proposal integrates the reinforcement learning algorithm in the path selection strategy to optimize the overall energy consumption and extend the network lifetime. This research consists of two schemes firstly provide the complexities and dynamic nature of real-world IoT environments, finally, enhance the interest forward strategy. Our proposed research is implemented in ndnSIM and compared with state of the-art IOT-NDN forwarding strategies. The obtained results show clearly the effectiveness and robustness of our solution which outperforms the benchmarked methods in terms of energy consumption, network lifetime, retrieval time, and satisfactory rates.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Storms",
                "Q-learning",
                "Energy consumption",
                "Ad hoc networks",
                "Wireless communication",
                "Security"
            ],
            "Author Keywords": [
                "Internet of Things",
                "Named Data networking",
                "forwarding strategy",
                "machine learning",
                "Q-learning algorithm",
                "efficient energy consumption"
            ]
        },
        "title": "RLEAFS: Reinforcement Learning based Energy Aware Forwarding Strategy for NDN based IoT Networks"
    },
    {
        "authors": [
            "Qingming Wang",
            "Xiao Liang",
            "Hua Zhang",
            "Linghui Ge"
        ],
        "published_in": "Published in: IEEE Transactions on Green Communications and Networking ( Early Access )",
        "date_of_publication": "10 July 2024",
        "doi": "10.1109/TGCN.2024.3425848",
        "publisher": "IEEE",
        "abstract": "Integrated satellite and terrestrial network (ISTN) is a potential technology to achieve ubiquitous and reliable broadband communication for Internet of Things (IoT) devices. Timely delivery of information updates represents a pivotal metric in IoT networks. However, due to the limited satellite transmission resources and the huge propagation delay caused by long distance from satellites to the Earth, ISTN faces great challenges in ensuring such freshness. Moreover, energy efficiency (EE) is also a crucial factor in ISTN with multiple antennas serving multiple users. In this research, we incorporate Age of Information (AoI) as a metric to ensure the information freshness of all IoT devices and design an AoI-aware EE resource allocation scheme. We maximize the system average EE by jointly optimizing the beamforming of Low Earth Orbit (LEO) satellites and base station (BS), and scheduling while maintaining the maximum AoI constraints of all IoT devices. To solve such a difficult problem, a Lyapunov drift-plus-penalty approach is leveraged to transform the original dynamic resource allocation problem into a deterministic problem, which is efficiently solved by an alternating optimization. Compared with existing schemes, our proposed scheme achieves the highest average EE. Our simulations also verify the tradeoff between average EE and information freshness.",
        "issn": {
            "Electronic ISSN": "2473-2400"
        },
        "keywords": {
            "IEEE Keywords": [
                "Satellites",
                "Internet of Things",
                "Low earth orbit satellites",
                "Resource management",
                "Optimization",
                "Array signal processing",
                "Satellite antennas"
            ],
            "Author Keywords": [
                "Integrated satellite and terrestrial network",
                "resource allocation",
                "energy efficiency",
                "Age of Information"
            ]
        },
        "title": "AoI-Aware Energy Efficiency Resource Allocation for Integrated Satellite-Terrestrial IoT Networks"
    },
    {
        "authors": [
            "Mehak Basharat",
            "Muhammad Naeem",
            "Asad M. Khattak",
            "Alagan Anpalagan"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "07 August 2024",
        "doi": "10.1109/JIOT.2024.3440061",
        "publisher": "IEEE",
        "abstract": "We investigate digital twin-assisted task offloading in unmanned aerial vehicle (UAV)–mobile edge computing (UAV-MEC) networks with energy harvesting. Digital twin technology leverages a real-time simulated environment to optimize UAV-MEC networks. Considering unpredictable MEC environments and low-power Internet of things (IoT) devices, we propose a digital twin-assisted task offloading scheme in UAV-MEC networks with energy harvesting. The goal is to minimize latency and maximize the number of associated IoT devices by optimizing UAV placement and IoT device association. The constraints on computing, caching, energy harvesting, latency and maximum number of IoT devices a UAV can serve are considered. To solve the formulated problem, we employ a branch-and-bound algorithm to obtain optimal results. We also solve the optimization problem using relaxed heuristic algorithm. In addition, we propose a difference of convex penalty-based algorithm to solve the problem with reduced computational complexity. This approach provide efficient alternatives to obtain near-optimal solution. Through extensive simulations, we demonstrate the effectiveness of the proposed algorithm and validate the benefits of leveraging digital twin technology in UAV-MEC networks with energy harvesting.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Task analysis",
                "Digital twins",
                "Autonomous aerial vehicles",
                "Energy harvesting",
                "Computational modeling",
                "Real-time systems"
            ],
            "Author Keywords": [
                "Digital twin",
                "energy harvesting",
                "mobile edge computing",
                "task offloading",
                "unmanned aerial vehicles"
            ]
        },
        "title": "Digital Twin-Assisted Task Offloading in UAV-MEC Networks With Energy Harvesting for IoT Devices"
    },
    {
        "authors": [
            "ZiYao Chen",
            "Jialiang Peng",
            "Jiawen Kang",
            "Dusit Niyato"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "11 September 2024",
        "doi": "10.1109/JIOT.2024.3457871",
        "publisher": "IEEE",
        "abstract": "Federated Learning (FL) has emerged as a crucial approach in the realm of distributed machine learning, providing a framework for training models on decentralized data while preserving data privacy. This paradigm has established itself as an effective solution for deploying artificial intelligence technology in scenarios associated with the Internet of Things (IoT). Despite its potential, FL faces encounters several challenges, particularly the limited computational and communication capabilities of some local clients, which can hinder further advancement. Such constraints limit the effective implementation and utilization of Deep Neural Networks (DNNs) with numerous parameters on IoT devices. Our study tackles this issue by utilizing Group Lasso for model sparsification and pruning, aimed at lowering the computational and communication demands on IoT devices. Moreover, this paper proposes a Group Lasso-enabled FL model pruning strategy specifically tailored for IoT, designed to reduce the size of model parameters, and provides theoretical guarantees of FL convergence. Empirical analysis across multiple models and datasets demonstrates that our method effectively halved the parameters in fully connected layers during federated training. This substantial reduction is achieved with minimal impact on accuracy, thus preserving the integrity of model performance and providing a competitive edge over existing methodologies.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Computational modeling",
                "Training",
                "Data models",
                "Federated learning",
                "Neurons",
                "Performance evaluation"
            ],
            "Author Keywords": [
                "Internet of Things",
                "Federated learning",
                "Group Lasso",
                "Model pruning"
            ]
        },
        "title": "FedGroup-Prune: IoT Device Amicable and Training-Efficient Federated Learning via Combined Group Lasso Sparse Model Pruning"
    },
    {
        "authors": [
            "Dingde Jiang",
            "Zhihao Wang",
            "Xinhui Liu",
            "Qi Xu",
            "Tao Zou",
            "Ruyun Zhang",
            "Lizhuang Tan",
            "Peiying Zhang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "14 October 2024",
        "doi": "10.1109/JIOT.2024.3468209",
        "publisher": "IEEE",
        "abstract": "Non-Terrestrial Networks (NTNs) enabled Internet of Things (IoT) extends connectivity to remote and underserved areas, enhances network reliability and coverage, and supports diverse IoT applications in challenging environments such as rural, maritime, and disaster-stricken regions. As an emerging and fast-evolving IoT scheme, NTN-enabled IoT requires extensive evaluation to ensure effective deployment in real-world scenarios, such as connectivity, performance, and security evaluation. Since conducting testing in remote and diverse environments is logistically challenging and costly, we propose a Generative Artificial Intelligence (GAI)-based synthetic traffic generation framework that facilitates comprehensive traffic analysis and performance evaluation. The proposed framework employs a GAI model to learn the traffic pattern and generate synthetic traffic from historical data. Our approach includes an embedding-based model for representing network flow attributes and a Conditional Generative Adversarial Network (CGAN) for generating traffic flows. Considering both source-destination information and statistical features achieves more comprehensive characterization of traffic flows. Finally, the simulation results demonstrate that the proposed approach can generate high quality traffic that conforms to real data distribution and shows obvious difference between multiple applications.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Testing",
                "Telecommunication traffic",
                "Generative adversarial networks",
                "Vectors",
                "Performance evaluation",
                "Vocabulary",
                "Training",
                "Traffic control",
                "Load modeling"
            ],
            "Author Keywords": [
                "Non-Terrestrial Networks",
                "Internet of Things",
                "Generative AI",
                "Word Embedding",
                "Traffic Generation"
            ]
        },
        "title": "Towards Synthetic Network Traffic Generating in NTN-Enabled IoT: A Generative AI"
    },
    {
        "authors": [
            "Zhihe Gao",
            "Yufang Li",
            "Zhe Chen",
            "Muhammad Asif",
            "Lingwei Xu",
            "Xingwang Li",
            "T. Aaron Gulliver"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "24 June 2024",
        "doi": "10.1109/TCE.2024.3418103",
        "publisher": "IEEE",
        "abstract": "In the swift evolution of 5G cellular communication technology and Internet of Things (IoT), the consumer electronics market is booming. Consumer IoT has become an emerging industry. However, the development of the consumer IoT is subject to limited spectrum resources. Hence, this study suggests a smart spectrum sensing approach for consumer IoT based on GAN-GRU-YOLO. First, a Continuous Wavelet Transform (CWT) is used to capture frequency domain information from the received signals. A frequency domain feature matrix is constructed and then converted to a signal spectrogram to improve data diversity and enhance sensing. GAN is used to learn the signal spectrogram to generate more realistic synthetic data to achieve data enhancement and improve the classification performance of the overall model. Then, a two-branch GRU-YOLO network is employed to learn the signal characteristics in the time and frequency domains. The upper branch captures local feature information in the frequency domain and the YOLOv5 network captures high-level features. A combination of GRU and CNN in the lower branch extracts features from the data time series to ensure information continuity. Finally, the branch outputs are fused for further processing. The GAN-GRU-YOLO network has high generalization ability and efficiency. Compared with other methods, the proposed approach has a lower false alarm probability (Pf) and a higher detection probability (Pd). At a signal-to-noise (SNR) ratio of -15 dB, the Pd is 11% to 65% higher and the Pf is 25% to 61% lower than the ResNet, MobileNet, Transformer and YOLOv6 algorithms.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Continuous wavelet transforms",
                "YOLO",
                "Internet of Things",
                "Frequency-domain analysis",
                "Feature extraction",
                "Consumer electronics"
            ],
            "Author Keywords": [
                "Consumer Internet of Things",
                "Spectrum Sensing",
                "CWT",
                "GAN",
                "GRU",
                "YOLOv5"
            ]
        },
        "title": "Intelligent Spectrum Sensing of Consumer IoT Based on GAN-GRU-YOLO"
    },
    {
        "authors": [
            "Ning Chen",
            "Tie Qiu",
            "Weisheng Si",
            "Dapeng Oliver Wu"
        ],
        "published_in": "Published in: IEEE Transactions on Mobile Computing ( Early Access )",
        "date_of_publication": "05 November 2024",
        "doi": "10.1109/TMC.2024.3492002",
        "publisher": "IEEE",
        "abstract": "Robust Topology is a key prerequisite to providing consistent connectivity for highly dynamic Internet-of-Things (IoT) applications that are suffering node failures. In this paper, we present a two-step approach to organizing the most robust IoT topology. Firstly, we propose a novel robustness metric denoted as $I$ , which is based on network motifs and is specifically designed to sensitively analyze the dynamic changes in topology resulting from node failures. Secondly, we introduce a Distributed duAl-layer collaborative competition optimization strategy based on Motif density (DAiMo). This strategy significantly expands the search space for optimal solutions and facilitates the identification of the optimal IoT topology. We utilize the motif density concept in the collaborative optimization process to efficiently search for the optimal topology. To support our approach, extensive mathematical proofs are provided to demonstrate the advantages of the metric $I$ in effectively perceiving changes in IoT topology and to establish the convergence of the DAiMo algorithm. Finally, we conduct comprehensive performance evaluations of DAiMo and investigate the influence of network motifs on the resilience and reliability of IoT topologies. Experimental results clearly indicate that the proposed method outperforms existing state-of-the-art topology optimization methods in terms of enhancing network robustness.",
        "issn": {
            "Print ISSN": "1536-1233",
            "Electronic ISSN": "1558-0660"
        },
        "keywords": {
            "IEEE Keywords": [
                "Topology",
                "Network topology",
                "Robustness",
                "Network motifs",
                "Measurement",
                "Optimization",
                "Internet of Things",
                "Convergence",
                "Heuristic algorithms",
                "Couplings"
            ],
            "Author Keywords": [
                "Internet-of-Things",
                "Robust Networking",
                "Network Motifs",
                "Topology Optimization",
                "Distributed Optimization"
            ]
        },
        "title": "DAiMo: Motif Density Enhances Topology Robustness for Highly Dynamic Scale-free IoT"
    },
    {
        "authors": [
            "Pengxiang Qin",
            "Dongyang Xu",
            "Lei Liu",
            "Mianxiong Dong",
            "Shahid Mumtaz",
            "Mohsen Guizani"
        ],
        "published_in": "Published in: IEEE Transactions on Network Science and Engineering ( Early Access )",
        "date_of_publication": "16 October 2024",
        "doi": "10.1109/TNSE.2024.3481630",
        "publisher": "IEEE",
        "abstract": "Low earth orbit (LEO) satellite networks have emerged as a promising field for distributed Internet of Things (IoT) devices, particularly in latency-tolerant applications. Federated learning (FL) is implemented in LEO satellite IoT networks to preserve data privacy and facilitate machine learning (ML). However, the user who spends the longest time significantly hampers FL efficiency and degrades the Quality-of-Service (QoS), potentially leading to irreparable damage. To address this challenge, we propose a joint data allocation and server selection strategy based on long short-term memory (LSTM) with parallelized FL in LEO satellite IoT networks. Herein, data-parallel learning is utilized, allowing multiple users to collaboratively train ML networks to minimize latency. Moreover, server selection takes into account signal propagation delays as well as traffic loads forecasted by an LSTM network, thereby improving the efficiency even further. Specifically, the strategies are formulated as optimization problems and tackled using a line search sequential quadratic programming (SQP) method and a multiple-objective particle swarm optimization (MOPSO) algorithm. Simulation results show the effectiveness of the proposed strategy in reducing total latency and enhancing the efficiency of FL in LEO satellite IoT networks compared to the alternatives.",
        "issn": {
            "Electronic ISSN": "2327-4697"
        },
        "keywords": {
            "IEEE Keywords": [
                "Satellites",
                "Low earth orbit satellites",
                "Internet of Things",
                "Servers",
                "Training",
                "Resource management",
                "Data models",
                "Load modeling",
                "Telecommunication traffic",
                "Data privacy"
            ],
            "Author Keywords": [
                "Satellite communication networks",
                "low earth orbit",
                "Internet of Things",
                "federated learning"
            ]
        },
        "title": "Joint Data Allocation and LSTM-Based Server Selection With Parallelized Federated Learning in LEO Satellite IoT Networks"
    },
    {
        "authors": [
            "Gleiston Guerrero-Ulloa",
            "Duval Carvajal-Suárez",
            "Paulo Novais",
            "Miguel J. Hornos",
            "Carlos Rodríguez-Domínguez"
        ],
        "published_in": "Published in: IEEE Software ( Early Access )",
        "date_of_publication": "17 October 2024",
        "doi": "10.1109/MS.2024.3479880",
        "publisher": "IEEE",
        "abstract": "TDDT4IoTS is a comprehensive tool designed to streamline the development of IoT systems by integrating both device design and software application development within a test-driven framework. Its test-driven development approach enhances the reliability and efficiency of IoT systems by ensuring early error detection and facilitating the integration of diverse technologies. By addressing critical challenges such as interoperability and standardization, TDDT4IoTS empowers developers to overcome key obstacles in IoT adoption, ensuring scalable, robust, and feature-rich applications. The tool’s strong usability, validated through a System Usability Scale (SUS) evaluation, along with future enhancements like Artificial Intelligence (AI)-driven automation for keyword marking and expanded platform support, underscores its significant contribution to accelerating IoT innovation and development.",
        "issn": {
            "Print ISSN": "0740-7459",
            "Electronic ISSN": "1937-4194"
        },
        "keywords": {
            "IEEE Keywords": [
                "Unified modeling language",
                "Internet of Things",
                "Object oriented modeling",
                "Java",
                "Codes",
                "Servers",
                "Interoperability",
                "Computer architecture",
                "Computational modeling",
                "Business"
            ],
            "Author Keywords": []
        },
        "title": "Test-Driven Development Tool for IoT Systems"
    },
    {
        "authors": [
            "Aroosa Hameed",
            "John Violos",
            "Nina Santi",
            "Aris Leivadeas",
            "Nathalie Mitton"
        ],
        "published_in": "Published in: IEEE Transactions on Network and Service Management ( Early Access )",
        "date_of_publication": "07 November 2024",
        "doi": "10.1109/TNSM.2024.3493758",
        "publisher": "IEEE",
        "abstract": "Internet of Things (IoT) applications generate tremendous amounts of data streams which are characterized by varying Quality of Service (QoS) indicators. These indicators need to be accurately estimated in order to appropriately schedule the computational and communication resources of the access and Edge networks. Nonetheless, such types of IoT data may be produced at irregular time instances, while suffering from varying network conditions and from the mobility patterns of the edge devices. At the same time, the multipurpose nature of IoT networks may facilitate the co-existence of diverse applications, which however may need to be analyzed separately for confidentiality reasons. Hence, in this paper, we aim to forecast time series data of key QoS metrics, such as throughput, delay, packet delivery and loss ratio, under different network configuration settings. Additionally, to secure data ownership while performing the QoS forecasting, we propose the FeDerated Temporal Sparse Transformer (FeD-TST) framework, which allows local clients to train their local models with their own QoS dataset for each network configuration; subsequently, an associated global model can be updated through the aggregation of the local models. In particular, three IoT applications are deployed in a real testbed under eight different network configurations with varying parameters including the mobility of the gateways, the transmission power and the channel frequency. The results obtained indicate that our proposed approach is more accurate than the identified state-of-the-art solutions.",
        "issn": {
            "Electronic ISSN": "1932-4537"
        },
        "keywords": {
            "IEEE Keywords": [
                "Quality of service",
                "Internet of Things",
                "Data models",
                "Throughput",
                "Predictive models",
                "Forecasting",
                "Transformers",
                "Delays",
                "Biological system modeling",
                "Accuracy"
            ],
            "Author Keywords": [
                "Internet of Things",
                "QoS forecasting",
                "Edge Computing",
                "Federated Learning"
            ]
        },
        "title": "FeD-TST: Federated Temporal Sparse Transformers for QoS prediction in Dynamic IoT Networks"
    },
    {
        "authors": [
            "Prantaneel Debnath",
            "Anshul Gusain",
            "Parth Sharma",
            "Pyari Mohan Pradhan"
        ],
        "published_in": "Published in: IEEE Embedded Systems Letters ( Early Access )",
        "date_of_publication": "03 October 2024",
        "doi": "10.1109/LES.2024.3473017",
        "publisher": "IEEE",
        "abstract": "As the demand for edge computing applications continues to rise, the need for efficient training of resource-constrained devices becomes paramount. This letter proposes MQTT-based implementation of distributed estimation strategies in the context of the Internet of Things (IoT), namely incremental, consensus and diffusion strategies. The use of Raspberry Pi Pico W in the emulation environment is motivated by its advanced capability, while the MQTT data protocol is employed to address the constraints associated with conventional HTTP/HTTPs protocols. Synchronisation in an IoT network is achieved by the integration of a novel methodology that entails the use of the wait-for-slowest (WFS) protocol and MQTT protocol. Furthermore, the development of a GUI supported by the Django application allows for adjusting parameters in distributed strategies through the HTTP REST API, along with SQLite. The results acquired from hardware experiments exhibit a strong correlation between the mean-square performance achieved from simulation studies. The distributed estimation strategy is compared with state-of-the art centralized and non-cooperation estimation strategies, demonstrating its superior performance. In addition, a study is conducted on the resilience of these IoT networks in the face of several network threats, such as node failure and model poisoning attacks. A theoretical analysis is provided to explain the relationship between the number of iterations and node failure.",
        "issn": {
            "Print ISSN": "1943-0663",
            "Electronic ISSN": "1943-0671"
        },
        "keywords": {
            "IEEE Keywords": [
                "Protocols",
                "Estimation",
                "Synchronization",
                "Real-time systems",
                "Training",
                "Steady-state",
                "Graphical user interfaces",
                "Distributed databases",
                "Hardware",
                "Costs"
            ],
            "Author Keywords": [
                "IoT",
                "MQTT",
                "wait-for-slowest",
                "distributed adaptive algorithms",
                "Raspberry Pi Pico W",
                "model poisoning attack"
            ]
        },
        "title": "MQTT Based Adaptive Estimation Over Distributed Network Using Raspberry Pi Pico W"
    },
    {
        "authors": [
            "Mai Le",
            "Thien Huynh-The",
            "Tan Do-Duy",
            "Thai-Hoc Vu",
            "Won-Joo Hwang",
            "Quoc-Viet Pham"
        ],
        "published_in": "Published in: IEEE Communications Surveys & Tutorials ( Early Access )",
        "date_of_publication": "12 July 2024",
        "doi": "10.1109/COMST.2024.3427324",
        "publisher": "IEEE",
        "abstract": "The emergence of new services and applications in emerging wireless networks (e.g., beyond 5G and 6G) has shown a growing demand for the usage of artificial intelligence (AI) in the Internet of Things (IoTs). However, the proliferation of massive IoT connections and the availability of computing resources distributed across future IoT systems have strongly demanded the development of distributed AI for better IoT services and applications. Therefore, existing AI-enabled IoT systems can be enhanced by implementing distributed machine learning (aka distributed learning) approaches. This work aims to provide a comprehensive survey on distributed learning for IoT services and applications in emerging networks. In particular, we first provide a background of machine learning and present a preliminary to typical distributed learning approaches, such as federated learning, multi-agent reinforcement learning, and distributed inference. Then, we provide an extensive review of distributed learning for critical IoT services (e.g., data sharing and computation offloading, localization, mobile crowdsensing, and security and privacy) and IoT applications (e.g., smart healthcare, smart grid, autonomous vehicle, aerial IoT networks, and smart industry). From the reviewed literature, we also present critical challenges of distributed learning for IoT and propose several promising solutions and research directions in this emerging area.",
        "issn": {
            "Electronic ISSN": "1553-877X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Artificial intelligence",
                "Distance learning",
                "Computer aided instruction",
                "Distributed databases",
                "Servers",
                "6G mobile communication"
            ],
            "Author Keywords": [
                "6G",
                "Artificial Intelligence",
                "Distributed Learning",
                "Federated Learning",
                "Internet-of-Things",
                "IoT Services",
                "Machine Learning",
                "Reinforcement Learning",
                "Vertical Applications"
            ]
        },
        "title": "Applications of Distributed Machine Learning for the Internet-of-Things: A Comprehensive Survey"
    },
    {
        "authors": [
            "Mingchuan Zhang",
            "Wei Quan",
            "Nan Cheng",
            "Qingtao Wu",
            "Junlong Zhu",
            "Ruijuan Zheng",
            "Keqin Li"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "28 May 2019",
        "doi": "10.1109/JIOT.2019.2919562",
        "publisher": "IEEE",
        "abstract": "Many problems in Internet of Things (IoT) can be cast as distributed optimization problems. For this reason, this paper considers a distributed online constrained optimization problem in IoT, where the local objective functions change with time. In order to solve this problem, distributed projected gradient descent methods are employed frequently. However, the computation of the projection operators is prohibitive in high-dimensional constrained optimization problem. To address the issue, we propose a distributed online learning algorithm based on the conditional gradient method over IoT systems, which avoids the costly projection steps. Moreover, when the local objective functions are strongly convex, we show that the regret bound of O(T) is achieved, where T is a time horizon. When the local objective functions are potentially non-convex, we also show that the algorithm converges to some stationary points at rate of O(T). In addition, we present simulation experiments to confirm the theoretical results.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Optimization",
                "Linear programming",
                "Convex functions",
                "Internet of Things",
                "Standards",
                "Convergence",
                "Machine learning algorithms"
            ],
            "Author Keywords": [
                "Conditional gradient methods",
                "high dimensional constrained optimization",
                "online learning",
                "regret bound."
            ]
        },
        "title": "Distributed Conditional Gradient Online Learning for IoT Optimization"
    },
    {
        "authors": [
            "Yaozong Li",
            "Xiong Luo",
            "Wenbing Zhao",
            "Honghao Gao"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "11 June 2024",
        "doi": "10.1109/TCE.2024.3412957",
        "publisher": "IEEE",
        "abstract": "Recently, the concept of smart cities has provided new insights for efficient utilization of urban resources, while posing significant security challenges in the Internet of Things (IoT) consumer electronics. Sharding technology is a representative approach to solving the scalability problems of blockchain systems used in security-enabled smart cities. However, considering high scalability requirements from the IoT consumer electronics, this strategy may lead to higher security risks for a single shard, and the worst-performing shard will become the performance bottleneck with the increase of the number of shards. To address it, we propose a reputation-based stable blockchain sharding scheme. After analyzing the faulty nodes and malicious behaviors in blockchain system, the reputation-based stability evaluation criteria is accordingly established. Then, a stable sharding strategy that can be dynamically adjusted according to the current state of blockchain system, is realized to replace the original random sharding strategy. Specifically, to handle the dynamic and large-scale characteristics of IoT consumer electronics in smart cities, the deep reinforcement learning (DRL) method is incorporated into training the sharding strategy and optimizing the system performance. The simulation results show that our scheme can effectively improve antirisk capability of the sharded blockchain system, achieving a high transaction throughput performance.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Blockchains",
                "Sharding",
                "Smart cities",
                "Security",
                "Consumer electronics",
                "Peer-to-peer computing",
                "Throughput"
            ],
            "Author Keywords": [
                "Blockchain",
                "smart cities",
                "deep reinforcement learning (DRL)",
                "sharding strategy"
            ]
        },
        "title": "Reputation-Based Stable Blockchain Sharding Scheme for Smart Cities With IoT Consumer Electronics: A Deep Reinforcement Learning Approach"
    },
    {
        "authors": [
            "Mazin Abed Mohammed",
            "Abdullah Lakhan",
            "Karrar Hameed Abdulkareem",
            "Muhammet Deveci",
            "Ashit Kumar Dutta",
            "Sajida Memon",
            "Haydar Abdulameer Marhoon",
            "Radek Martinek"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "04 April 2024",
        "doi": "10.1109/TCE.2024.3384455",
        "publisher": "IEEE",
        "abstract": "The number of people with kidney disease rises every day for many reasons. Many existing machine-learning-enabled mechanisms for processing kidney disease suffer from long delays and consume much more resources during processing. In this paper, the study shows how federated and reinforcement learning schemes can be used to develop the best delay scheme. The scheme must optimize both the internal and external states of reinforcement learning and the federated learning fog cloud network. This work presents the Adaptive Federated Reinforcement Learning-Enabled System (AFRLS) for Internet of Things (IoT) consumers’ kidney disease image processing. The main relationship between IoT consumers and kidney image is that the data is collected from different IoT consumer sources, such as ultrasound and X-rays in healthcare clinics. In healthcare applications, kidney urinary tasks reduce the time it takes to preprocess federated learning datasets for training and testing and run them on different fog and cloud nodes. AFRLS decides the scheduling on other nodes and improves constraints based on the decision tree. Based on the simulation results, AFRLS is a new strategy that reduces the time tasks need to be delayed compared to other machine learning methods used in fog cloud networks. The AFRLS improved the delay among nodes by 55%, the delay among internal states by 40%, and the training and testing delay by 51%.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Kidney",
                "Diseases",
                "Delays",
                "Cloud computing",
                "Training",
                "Task analysis",
                "Testing"
            ],
            "Author Keywords": [
                "Federated Learning",
                "Reinforcement Learning",
                "Fog",
                "Cloud",
                "Kidney Disease Prediction",
                "Medical Images"
            ]
        },
        "title": "Federated-Reinforcement Learning-Assisted IoT Consumers System for Kidney Disease Images"
    },
    {
        "authors": [
            "Jie Huang",
            "Tao Yu",
            "Chinmay Chakraborty",
            "Fan Yang",
            "Xianzhi Lai",
            "Abdullah Alharbi",
            "Keping Yu"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "27 June 2024",
        "doi": "10.1109/TCE.2024.3419784",
        "publisher": "IEEE",
        "abstract": "Unmanned aerial vehicles (UAVs) play a crucial role in remote environmental applications of the Low-Power Internet of Things (IoT) network. However, the interaction between a large number of consumer electronics products integrated with TinyML (CEP-TML) may lead to serious interference and overlap issues in Low-Power IoT networks, which still pose challenges to the lifespan of UAVs. To address this issue, this paper proposes an interference hypergraph-based energy harvesting resource allocation (IH-EHRA) algorithm in UAV-assisted CEP-TML communication (UAV-CEPC-TML) networks, aimed at ensuring effective resource management in overlapping interference scenarios. Firstly, we established an interference hypergraph model to analyze the types and relationships of interference in a Low-Power IoT network, to reduce interference and optimize spectrum resource utilization. Then, in the overlapping interference scenario, we propose an EH optimization model with imperfect channel state information (CSI), aiming to maximize network throughput while improving the service life of UAVs. Finally, we optimize the model into two subproblems and apply learning theory to reduce the impact of imperfect CSI to obtain the optimal solution to the problem. The simulation results show that the algorithm can effectively ensure the EH requirements of UAVs, and improve the interference efficiency and throughput of the network.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Interference",
                "Resource management",
                "Autonomous aerial vehicles",
                "Internet of Things",
                "Device-to-device communication",
                "Throughput",
                "Wireless communication"
            ],
            "Author Keywords": [
                "Low-Power Internet of Things",
                "TinyML consumer electronics",
                "interference hypergraph",
                "UAV",
                "energy harvesting"
            ]
        },
        "title": "An Energy Harvesting Algorithm for UAV-Assisted TinyML Consumer Electronic in Low-Power IoT Networks"
    },
    {
        "authors": [
            "Ikram Ud Din",
            "Ahmad Almogren",
            "Joel J. P. C. Rodrigues",
            "Ayman Altameem"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "03 July 2024",
        "doi": "10.1109/TCE.2024.3422788",
        "publisher": "IEEE",
        "abstract": "This study investigates the integration and utilization of diverse data forms within consumer electronics, with a particular emphasis on Internet of Things (IoT) technologies. We introduce innovative data fusion techniques designed to enhance decision-making precision in IoT-enabled consumer electronics. While our findings are relevant not only for assessing consumer electronics for commercial purposes but also for individual use, the present study focuses on the commercial aspect of consumption. In this paper, we discuss and demonstrate how these techniques can be employed in wearables, smart home systems and in other IoT deployments and draw out their implications on user engagement and more specific services. Our findings show a substantial increase in accuracy when using video data posts to perform sentimental analysis with an average of 83% followed by the text with an average of 78 % and by audio with an average of 65 %. Emotion recognition accuracy also varied, with the model performing best for surprise (85%) and happiness (80%). In addition, it found that the CNN model is slightly superior to the LSTM model in terms of accuracy (68% vs. 78%), F1-score (0. 62 vs. 0. 77). These results prove that composite data analysis is efficient in improving decision-making processes. Finally, the study looks at the ethical concerns that could be relevant to the recommended site, especially with regard to data privacy and protection. From our analysis, we clearly see great improvements in decision making phases, the areas that could be explored in future concerning the prospects of multimodal data in consumer electronics.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Consumer electronics",
                "Feature extraction",
                "Decision making",
                "Data integration",
                "Complexity theory",
                "Internet of Things",
                "Ethics"
            ],
            "Author Keywords": [
                "Integrative Multimodal Data",
                "Consumer Electronics",
                "Advanced Data Fusion",
                "Machine Learning Applications",
                "Decision-Making"
            ]
        },
        "title": "Advancing Secure and Privacy-Preserved Decision-Making in IoT-Enabled Consumer Electronics via Multimodal Data Fusion"
    },
    {
        "authors": [
            "Mengyuan Xu",
            "Qingwei Jiang",
            "Mingqing Liu",
            "Xinhe Wang",
            "Shuaifan Xia",
            "Mingliang Xiong",
            "Qingwen Liu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "23 August 2024",
        "doi": "10.1109/JIOT.2024.3449141",
        "publisher": "IEEE",
        "abstract": "In the rapidly evolving field of the Internet of Things (IoT), simplifying infrastructure and deployment is highly competitive. This study introduces a novel approach using an individual-source resonant beam (RB) for passive 3D positioning in IoT scenarios, excelling in efficient system design and signal utilization. The proposed solution simultaneously exploits the frequency structure and spatial distribution of RB to estimate position parameters, facilitating a compact design for both the base station (BS) and the mobile target (MT). The BS incorporates an individual-source RB transmitter, while the MT is equipped with a passive (electricity-free) reflector. Specifically, for extracting the distance information from RB, we develop theoretical models to illustrate the frequency attributes and associated structural variations of RB due to MT’s movement. Regarding angle estimation, we deduce the relationship between the RB field distribution and incident angles. We also conduct simulations to validate the entire positioning process. The results indicate the positioning accuracy can reach approximately 2cm over a distance from 1.7m to 2.5m. This technique sets a new benchmark for precise and efficient 3D positioning, making it suitable for various IoT scenarios, such as smart homes, industrial automation, and asset tracking.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Estimation",
                "Three-dimensional displays",
                "Location awareness",
                "Frequency estimation",
                "Light emitting diodes",
                "Resonant frequency",
                "Accuracy"
            ],
            "Author Keywords": [
                "Angle estimation",
                "Distance estimation",
                "Individual Resonant-Beam Source",
                "Indoor localization",
                "Passive positioning",
                "Resonant Beam"
            ]
        },
        "title": "Individual-Source Resonant Beam Enabled 3D Positioning for IoT Scenarios"
    },
    {
        "authors": [
            "Mohammad Manzurul Islam",
            "Gour Karmakar",
            "Joarder Kamruzzaman",
            "Manzur Murshed",
            "Abdullahi Chowdhury"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "12 September 2024",
        "doi": "10.1109/JIOT.2024.3459477",
        "publisher": "IEEE",
        "abstract": "Image sensors deployed in the Internet of Things (IoT) generate vast volumes of digital images. These images may be subject to deliberate alteration, compromising their trustworthiness. Estimating the trustworthiness of this image data is crucial for many applications; however, this aspect has not been adequately explored in the existing literature. In this paper, we propose a robust and real-time trust estimation framework for IoT image data, leveraging numeric data generated from other types of sensors deployed in the same area of interest (AoI). The theoretical model was developed using statistical approaches, and Shannons entropy was employed to measure the uncertainty associated with sensor readings during a specific event. Later, we applied Dempster-Shafer theory (DST) of combination to fuse information collected from image as well as numeric data-generating sensors where both types of sensors were observing the same event in the same AoI concomitantly. To evaluate the proposed framework, we implemented an IoT testbed using LoRa sensor nodes, edge devices, a LoRaWAN gateway, the Things Network (TTN), and a data analytics server. The testbed was used to collect observation data of a fire event using image and temperature sensors in an indoor residential setup in different conditions. Consequently, eight datasets (four authentic and four hacked) were built, each containing both image and temperature data readings under various scenarios. The proposed trust framework accurately estimated the trust score of images (91% overall accuracy) across all datasets and outperformed existing trust models.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Internet of Things",
                "Image sensors",
                "Data models",
                "Correlation",
                "Uncertainty",
                "Temperature sensors"
            ],
            "Author Keywords": [
                "Internet of Things",
                "trust",
                "trustworthiness",
                "image sensor data",
                "temperature sensor data",
                "multimodal fusion"
            ]
        },
        "title": "Trustworthiness of IoT Images Leveraging With Other Modal Sensor’s Data"
    },
    {
        "authors": [
            "Weidong Li",
            "Bo Zhao",
            "Lingzi Zhu",
            "Yixuan Wang",
            "Qian Zhong",
            "Shui Yu"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "23 August 2024",
        "doi": "10.1109/JSEN.2024.3445576",
        "publisher": "IEEE",
        "abstract": "In edge computing, an Internet of Things (IoT) node may employ container-based virtualization to manage and process data collected by sensors. Compared to cloud computing, containers on edge computing nodes have more components to handle computing tasks. This makes it more challenging to protect the integrity of components within containers. In this paper, we propose the Trusted Container for Edge Computing (TCEC) solution grounded in Trusted Computing (TC) designed to address this challenge. TCEC consists of two crucial parts: the Trusted Agent (TA) and the Virtual Trusted Platform Module (vTPM) Manager. The TA is distributed within each container, facilitating the effortless collection of component information requiring integrity protection. Subsequently, this information will be forwarded to the vTPM Manager. The vTPM Manager safeguards the integrity of critical information through integrity measurements and stores the results in the vTPM of the specified container. We have modified the Measurement Log (ML) format to identify faulty components quickly. TCEC operates on the Linux kernel and is compatible with various container engines without requiring virtualization architecture for IoT device modifications. Experimental results demonstrate the effectiveness and efficiency of TCEC in a Docker-based prototype. It also reveals that existing container protection schemes relying on TC are not directly applicable to an open IoT platform. The codes are available at https://github.com/chrisli1995/paper/tree/main/TCEC.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Containers",
                "Internet of Things",
                "Security",
                "Edge computing",
                "Kernel",
                "Protection",
                "Hardware"
            ],
            "Author Keywords": [
                "Container",
                "Trusted Computing",
                "Edge Computing",
                "Internet of Things",
                "Docker"
            ]
        },
        "title": "TCEC: Integrity Protection for Containers by Trusted Chip on IoT Edge Computing Nodes"
    },
    {
        "authors": [
            "Naoto Watanabe",
            "Taku Yamazaki",
            "Takumi Miyoshi",
            "Ryo Yamamoto",
            "Masataka Nakahara",
            "Norihiro Okui",
            "Ayumu Kubota"
        ],
        "published_in": "Published in: IEICE Transactions on Communications ( Early Access )",
        "date_of_publication": "16 September 2024",
        "doi": "10.23919/transcom.2024EBT0002",
        "publisher": "IEICE",
        "abstract": "With the growth of internet of things (IoT) devices, cyberattacks, such as distributed denial of service, that exploit vulnerable devices infected with malware have increased. Therefore, vendors and users must keep their device firmware updated to eliminate vulnerabilities and quickly handle unknown cyberattacks. However, it is difficult for both vendors and users to continually keep the devices safe because vendors must provide updates quickly and the users must continuously manage the conditions of all deployed devices. Therefore, to ensure security, it is necessary for a system to adapt autonomously to changes in cyberattacks. In addition, it is important to consider network-side security that detects and filters anomalous traffic at the gateway to comprehensively protect those devices. This paper proposes a self-adaptive anomaly detection system for IoT traffic, including unknown attacks. The proposed system comprises a honeypot server and a gateway. The honeypot server continuously captures traffic and adaptively generates an anomaly detection model using real-time captured traffic. Thereafter, the gateway uses the generated model to detect anomalous traffic. Thus, the proposed system can adapt to unknown attacks to reflect pattern changes in anomalous traffic based on real-time captured traffic. Three experiments were conducted to evaluate the proposed system: a virtual experiment using pre-captured traffic from various regions across the world, a demonstration experiment using real-time captured traffic, and a virtual experiment using a public dataset containing the traffic generated by malware. The results of all experiments showed that the detection model with the dynamic update method achieved higher accuracy for traffic anomaly detection than the pre-generated detection model. The experimental results indicate that a system adaptable in real time to evolving cyberattacks is a novel approach for ensuring the comprehensive security of IoT devices against bo...",
        "issn": {
            "Electronic ISSN": "1745-1345",
            "Print ISSN": "0916-8516"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Anomaly detection",
                "Logic gates",
                "Computer crime",
                "Adaptation models",
                "Servers",
                "Real-time systems"
            ],
            "Author Keywords": [
                "internet of things",
                "machine learning",
                "honeypot",
                "traffic anomaly detection"
            ]
        },
        "title": "Self-adaptive traffic anomaly detection system for IoT smart home environments"
    },
    {
        "authors": [
            "Arun Kumar Sangaiah",
            "Jayakrishnan Anandakrishnan",
            "Venkatesan Meenakshisundaram",
            "Mohd Amiruddin Abd Rahman",
            "Padmapriya Arumugam",
            "Mrinali Das"
        ],
        "published_in": "Published in: IEEE Internet of Things Magazine ( Early Access )",
        "date_of_publication": "16 September 2024",
        "doi": "10.1109/IOTM.001.2400027",
        "publisher": "IEEE",
        "abstract": "Precision agriculture significantly boosts socio-economic growth and national productivity through monitoring accurate periodic biomass and biophysical traits. Numerous Internet of Things (IoT) and Unmanned Aerial Vehicles (UAVs) connected sensors can facilitate the automated collection of these traits, even in adverse conditions. This article introduces SmartAgri-Net (SA-Net), a decision support system that utilizes real-time multi-sensor and multi-temporal data from Edge- IoT-UAV sensors. The SA-Net-Biomass Estimation Framework (SA-BEF) consisting of Occlusion Reconstruction Module (ORM), 3D-2D Transfer Block (3-2DTB), Attention-based Biomass Estimation Block (ABE) approximates biomass from Light Detection and Ranging (LiDAR) 3D-Point clouds. The SA-Net-TCN-Prediction Framework (SA-TPF) implements Temporal Convolution Neural Network (TCN) for predictive analytics over the derived biomass and aggregated biophysical data from IoT sensors to perform decision-making. Finally, we propose engineering and deploying SA-Net recommendation support for smartphone applications.",
        "issn": {
            "Print ISSN": "2576-3180",
            "Electronic ISSN": "2576-3199"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Biomass",
                "Laser radar",
                "Biosensors",
                "Crops",
                "Autonomous aerial vehicles",
                "Three-dimensional displays"
            ],
            "Author Keywords": []
        },
        "title": "Edge-IoT-UAV Adaptation Toward Precision Agriculture Using 3D-LiDAR Point Clouds"
    },
    {
        "authors": [
            "Boutaina Jebari",
            "Khalil Ibrahmi",
            "Mounir Ghogho",
            "Hamidou Tembine"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "11 November 2024",
        "doi": "10.1109/JIOT.2024.3495563",
        "publisher": "IEEE",
        "abstract": "Integrating blockchain technology into the Internet-of-Things (IoT) has revolutionized industries, enabling decentralized and reliable management of systems, while improving both efficiency and security. However, a key challenge for blockchain-based IoT solutions is ensuring the accuracy of data fed into the blockchain, known as the \"blockchain oracle problem.\" This work addresses this challenge by proposing the BIVO system (Blockchain Information Verification Oracles), a blockchain-based decentralized oracle for IoT networks. The system utilizes a reputation and voting mechanism suitable for both crowdsourced and semi-controlled environments. We also model the weighted voting mechanism as a stochastic game and conduct stress tests to analyze the system’s expected accuracy and cumulative payoffs under various conditions. Our findings indicate that the system achieves higher accuracy compared to non-weighted voting approaches. In semi-controlled environments, the system demonstrates resilience against up to 64% of adversarial nodes. However, under the worst conditions, malicious nodes need to control no more than 36% of the network to benefit from malicious behavior. Additionally, we implemented a prototype of the BIVO system and deployed it on both a local blockchain simulator and the public Ethereum testnet Sepolia to evaluate the cost and feasibility of blockchain integration.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Blockchains",
                "Accuracy",
                "Soft sensors",
                "Games",
                "Reliability",
                "Internet of Things",
                "Smart contracts",
                "Costs",
                "Stochastic processes",
                "Resilience"
            ],
            "Author Keywords": [
                "Blockchain",
                "Oracles",
                "Game-Theory",
                "Internet-of-Things",
                "Smart Contracts",
                "Decentralization"
            ]
        },
        "title": "BIVO -A Decentralized Oracle Solution for Data Authenticity in Blockchain-Based IoT Networks"
    },
    {
        "authors": [
            "Junwei Sun",
            "Yuhan Cao",
            "Yi Yue",
            "Yan Wang",
            "Yanfeng Wang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "21 October 2024",
        "doi": "10.1109/JIOT.2024.3484396",
        "publisher": "IEEE",
        "abstract": "The operational integrity and rotational accuracy of bearings are critical in maintaining the reliability of precision devices within IoT systems. In order to improve the efficiency and accuracy of bearing fault diagnosis, a portable advanced bearing fault diagnosis model for IoT applications is proposed. It leverages a novel LSTM neural network architecture augmented with memristor technology for enhanced computational efficiency. In this work, a hardware neural network capable of running LSTM is designed, enabling low-power, fast and parallel computation. The circuit comprises three modules: the weight calculation module, the activation function module and the output modul. The weights of the neural network are optimized and adjusted using the double-population jackal optimization algorithm. This algorithm performs convex lens imaging on the jackal population, applies reverse learning, and divides them into elite and ordinary jackals based on fitness values. It integrates the whale algorithm and cosine algorithm to strengthening the optimization ability of the jackal algorithm. Finally, the model is validated using the dataset from Paderborn University (PU). The results indicate that the accuracy of the model exceeds 96% for all four fault types. The findings underscore the potential of this model in powering the next generation of portable diagnostic tools for consumer electronics within the IoT framework.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Long short term memory",
                "Logic gates",
                "Hardware",
                "Memristors",
                "Internet of Things",
                "Integrated circuit modeling",
                "Neural networks",
                "Computational modeling",
                "Fault diagnosis",
                "Feature extraction"
            ],
            "Author Keywords": [
                "Memristor",
                "LSTM neural network",
                "Golden Jackal Optimization Algorithm",
                "Bearing fault diagnosis"
            ]
        },
        "title": "Memristor-Based Long and Short-Term Memory Network Models for Optimal Prediction in the IoT"
    },
    {
        "authors": [
            "Khan A",
            "Mishra S",
            "Pandey S",
            "Sardar T",
            "Mishra A",
            "Mudgal M",
            "Singhai S"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "30 October 2024",
        "doi": "10.1109/JIOT.2024.3488290",
        "publisher": "IEEE",
        "abstract": "Structural Health Monitoring (SHM) is a modern and innovative method used to assess the integrity, safety, and performance of structures without causing any damage. The need for SHM is particularly critical for infrastructure such as dams, where timely detection of damage allows for the implementation of remedial safety measures. Structural integrity can be compromised by events such as earthquakes, floods, and other disasters, which create significant disturbances in man-made constructions. The longevity of a structure is influenced by construction methods and the quality of materials used. Researchers have developed various techniques to evaluate the lifespan of constructed structures and propose safety measures. Recent advancements in technology, including the Internet of Things (IoT), Artificial Intelligence (AI), and wireless integrated sensor devices, have revolutionized the field of SHM. IoT-based wireless sensors monitor different structural parameters and transmit data to cloud-based storage systems in real-time. AI techniques, particularly machine learning, analyze this data to predict potential issues and provide actionable insights to user agencies, researchers, and administrative bodies such as Dam Monitoring Centers (DMCs). The ability of these technologies to monitor dam behavior in real-time has garnered significant attention. This study reviews current research on real-time SHM of dams using AI and IoT cloud-based sensor techniques, aiming to support researchers and practitioners in advancing SHM methodologies.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Dams",
                "Monitoring",
                "Artificial intelligence",
                "Wireless sensor networks",
                "Wireless communication",
                "Internet of Things",
                "Cloud computing",
                "Temperature sensors",
                "Vibrations",
                "Temperature measurement"
            ],
            "Author Keywords": [
                "Dam",
                "Structural Health Monitoring",
                "Internet of Things",
                "Artificial Intelligence",
                "Wireless Sensor Devices"
            ]
        },
        "title": "A Critical Review of IoT-Based Structural Health Monitoring for Dams"
    },
    {
        "authors": [
            "Xiongbing Xiao",
            "Xiumin Wang",
            "Weiwei Lin"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "09 August 2024",
        "doi": "10.1109/TCE.2024.3440406",
        "publisher": "IEEE",
        "abstract": "Unmanned Aerial Vehicles (UAVs) have recently received considerable attention in Internet of Things (IoT), because of their flexible deployment and extendable collection coverage. To collect data timely, the trajectory of the UAV should be intelligently planned. However, existing works mainly focus on the trajectory planning of a single UAV, ignoring the consideration of multiple UAVs. Although multiple UAVs greatly enhance the timeliness of data collection, they also pose challenges to UAVs collaboration and coordination. To address this issue, this paper formulates a joint multi-UAVs trajectory planning and data collection problem as a Mixed Integer Non-Linear Programming (MINLP), aiming at minimizing the Age of Information (AoI) and energy consumption. Due to the difficulty of the problem and the dynamic environment of IoT system, we reformulate it as a Markov Decision Process (MDP), and design a Deep Reinforcement Learning (DRL) approach to obtain the trajectory planning of the UAVs. Based on this, a deterministic data collection decision is made with a minimum cost bipartite matching in an auxiliary graph. Theoretical analysis shows that the designed deterministic data collection algorithm achieves the optimal data collection decision with the minimum weighted sum of the AoI and IoT devices’ energy consumption. Finally, simulations are conducted to confirm the efficiency of the proposed algorithms.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Autonomous aerial vehicles",
                "Data collection",
                "Sensors",
                "Trajectory planning",
                "Trajectory",
                "Task analysis"
            ],
            "Author Keywords": [
                "Internet of things",
                "age of information",
                "unmanned aerial vehicle (UAV)",
                "trajectory planning"
            ]
        },
        "title": "Joint AoI-Aware UAVs Trajectory Planning and Data Collection in UAV-Based IoT Systems: A Deep Reinforcement Learning Approach"
    },
    {
        "authors": [
            "Lisu Yu",
            "Biao Li",
            "Yuanzhi Yao",
            "Zhen Wang",
            "Zipeng Li",
            "Zhicheng Dong",
            "Donghong Cai"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "11 September 2024",
        "doi": "10.1109/JIOT.2024.3457837",
        "publisher": "IEEE",
        "abstract": "In highly interconnected large-scale event and other Internet of Things (IoT) device-intensive scenarios, traditional terrestrial base stations have difficulty meeting the requirements of IoT devices for network speed and security, and have exacerbated carbon pollution. To this end, a blockchain-enabled UAVs-assisted mobile edge computing system is introduced to enhance communication efficiency and ensure the privacy of IoT devices. In this system, the Byzantine consensus algorithm is applied in the blockchain. Considering the pollution of reducing carbon dioxide emissions, a strategy for jointly optimizing the flight trajectories of UAVs, task offloading scheduling, and MEC computing resource allocation is formulated to minimize the system’s carbon emissions and time delay while meeting MEC and blockchain computing tasks. However, due to the coupling of variables, this problem is very complex. Therefore, the original problem is decoupled into multiple sub-problems, and the block coordinate descent method (BCD) and successive convex approximation method (SCA) are used for solving. Specifically, the UAV flight trajectories, task offloading scheduling, and MEC computing resource allocation are alternately optimized until convergence. Simulation results verify the effectiveness and good performance of the proposed algorithm in this paper.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Autonomous aerial vehicles",
                "Blockchains",
                "Carbon dioxide",
                "Security",
                "Resource management",
                "Trajectory"
            ],
            "Author Keywords": [
                "Unmanned aerial vehicle (UAV) communication",
                "mobile edge computing (MEC)",
                "blockchain",
                "trajectory optimization",
                "carbon emission"
            ]
        },
        "title": "Efficient and Emission-Reducing Blockchain-Enabled Multi-UAV Assisted MEC System in IoT Networks"
    },
    {
        "authors": [
            "Sahar Rezagholi Lalani",
            "Bardia Safaei",
            "Amir Mahdi Hosseini Monazzah",
            "Hossein Taghizadeh",
            "Jörg Henkel",
            "Alireza Ejlali"
        ],
        "published_in": "Published in: IEEE Transactions on Green Communications and Networking ( Early Access )",
        "date_of_publication": "10 May 2024",
        "doi": "10.1109/TGCN.2024.3399455",
        "publisher": "IEEE",
        "abstract": "Resource-limited mobile IoT networks are a dynamic, and uncertain wireless communicating system. In such systems, the standard RPL routing protocol cannot select long-lasting communication links due to not employing mobility-aware metrics, e.g., direction and speed of movements. While several classical heuristic approaches exist to improve PDR in RPL-based mobile networks, their solutions cannot adapt to alterations of the mobile topology. Hence, in this paper, by mapping the routing problem in mobile and resource-limited networks into an infinite-time horizon MDP, an energy-aware and reliable RPL-based routing mechanism based on Q-learning is proposed to improve PDR in mobile IoT networks. This routing mechanism, which is called QUERA, utilizes mobility and quality-aware metrics, including Time-to-Reside (TTR), ETX, and RSSI. Furthermore, QUERA probes and maintains stable candidates based on its neighbor table management policy. These two aspects mitigate the need for retransmissions due to packet loss leading to less energy dissipation. According to evaluations, QUERA improves energy consumption by up to 50% against the state-of-the-art. The efficiency of QUERA is also evaluated in terms of power distribution diagram, which shows significant improvement in the lifetime of IoT devices. It has also been observed that QUERA improves PDR in mobile networks by up to 12%.",
        "issn": {
            "Electronic ISSN": "2473-2400"
        },
        "keywords": {
            "IEEE Keywords": [
                "Routing",
                "Internet of Things",
                "Linear programming",
                "Q-learning",
                "Standards",
                "Reliability",
                "Topology"
            ],
            "Author Keywords": [
                "Internet of Things",
                "Embedded Devices",
                "Routing",
                "RPL",
                "Mobility",
                "Machine Learning",
                "Energy Efficiency",
                "Reliability",
                "PDR",
                "Neighbor Table Management"
            ]
        },
        "title": "QUERA: Q-Learning RPL Routing Mechanism to Establish Energy Efficient and Reliable Communications in Mobile IoT Networks"
    },
    {
        "authors": [
            "Tie Qiu",
            "Jingchen Sun",
            "Ning Chen",
            "Songwei Zhang",
            "Weisheng Si",
            "Xingwei Wang"
        ],
        "published_in": "Published in: IEEE Transactions on Computers ( Early Access )",
        "date_of_publication": "23 September 2024",
        "doi": "10.1109/TC.2024.3465934",
        "publisher": "IEEE",
        "abstract": "With the scale of the Internet of Things (IoT) system growing constantly, node failures frequently occur due to device malfunctions or cyberattacks. Existing robust network generation methods utilize heuristic algorithms or neural network approaches to optimize the initial topology. These methods do not explore the core of topology robustness, namely how edges are allocated to each node in the topology. As a result, these methods use massive iterative processes to optimize the initial topology, leading to substantial time overhead when the scale of the topology is large. We examine various robust networks and observe that uniform degree distribution is the core of topology robustness. Consequently, we propose a novel UNIformity driven robusT topologY generation scheme (UNITY) for IoT systems to prevent the node degree from becoming excessively high or low, thereby balancing node degrees. Comprehensive experimental results demonstrate that networks generated with UNITY have an “olive-like” topology consisting of a substantial number of medium-degree nodes and possess strong robustness against both random node failures and targeted attacks. This promising result indicates that the UNITY makes a significant advancement in designing robust IoT systems.",
        "issn": {
            "Print ISSN": "0018-9340",
            "Electronic ISSN": "1557-9956"
        },
        "keywords": {
            "IEEE Keywords": [
                "Topology",
                "Network topology",
                "Robustness",
                "Feature extraction",
                "Internet of Things",
                "Computers",
                "Vectors"
            ],
            "Author Keywords": [
                "Internet of Things System",
                "Topology Generation",
                "Network Robustness"
            ]
        },
        "title": "Olive-like Networking: A Uniformity Driven Robust Topology Generation Scheme for IoT System"
    },
    {
        "authors": [
            "Jinguo Li",
            "Yun Ni",
            "Jin Zhang",
            "Jie Yu",
            "Yin He"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "09 September 2024",
        "doi": "10.1109/JIOT.2024.3456134",
        "publisher": "IEEE",
        "abstract": "Internet of Things (IoT) devices generate vast amounts of real-time data across diverse sectors, offering lucrative opportunities in the data trading market. This facilitates the conversion of raw data into valuable products and services, resulting in significant economic and social benefits. To issue data security problems in trading mechanisms, several solutions based on local differential privacy (LDP) provide lightweight methods for privacy preservation and efficient data exchange. However, most solutions lack effective mechanisms for longitudinal data. Moreover, LDP needs to address the data quality problem in IoT. At last, the pricing of privacy-preserving longitudinal data remains unresolved. To address these problems, we propose a Privacy-Preserving Data Trading (PPDT) scheme for IoT in this paper. Specifically, to guarantee the security of longitudinal data, we utilize two perturbation techniques to accommodate data owners with varying privacy preferences. To enhance data availability, we devise a binary tree-based aggregation algorithm combined with a weighted average strategy and maximum likelihood estimation. Additionally, we derive an optimal contract that considers different levels of privacy preservation and data trading prices. In scenarios with incomplete information, the contract can provide appropriate incentives to the involved parties. Finally, we demonstrate the efficiency and effectiveness of the proposed data trading scheme through theoretical analysis and extensive experiments.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Pricing",
                "Data models",
                "Internet of Things",
                "Differential privacy",
                "Security",
                "Blockchains",
                "Sensors"
            ],
            "Author Keywords": [
                "Internet of Things",
                "Privacy-preserving",
                "Data trading",
                "Local differential privacy",
                "Longitudinal data"
            ]
        },
        "title": "A Contract-Based Privacy-Preserving Longitudinal Data Trading Mechanism for IoT"
    },
    {
        "authors": [
            "Jianfei Sun",
            "Yangyang Bao",
            "Weidong Qiu",
            "Rongxing Lu",
            "Songnian Zhang",
            "Yunguo Guan",
            "Xiaochun Cheng"
        ],
        "published_in": "Published in: IEEE Transactions on Dependable and Secure Computing ( Early Access )",
        "date_of_publication": "23 July 2024",
        "doi": "10.1109/TDSC.2024.3432650",
        "publisher": "IEEE",
        "abstract": "The cloud-edge computing model has been expected to play a revolutionary role in promoting the quality of future generation large-scale Internet of Things (IoT) services. However, security and privacy in data sharing remain crucial issues hindering the success of cloud-edge IoT services. While some solutions based on attribute-based encryption (ABE) have been proposed to address these issues, they still face practical challenges such as attribute privacy leakage, resource-constrained devices, dynamic user groups, inflexible and inefficient service response. To address these challenges, this paper proposes a privacy-preserving fine-grained data sharing scheme with dynamic service (PF2DS), which implements access control by calculating the inner product between an attribute vector and an access vector. PF2DS is also capable of providing dynamic user group services through an efficient and indirect user revocation mechanism that periodically updates the key-embedded leaf nodes. Building on PF2DS, edge-assisted PF2DS (EPF2DS) delegates most of the operations to the edge device, which facilitates the performance of resource-constrained IoT devices. EPF2DS also supports efficient and asynchronous keyword search over the ciphertexts stored in the cloud. We demonstrate the security by the rigorous security proof. Both theoretical comparisons and experimental simulations demonstrate the practicality and superiority of our schemes over existing works.",
        "issn": {
            "Print ISSN": "1545-5971",
            "Electronic ISSN": "1941-0018"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Data privacy",
                "Security",
                "Privacy",
                "Cloud computing",
                "Vectors",
                "Encryption"
            ],
            "Author Keywords": [
                "Attribute-based access control",
                "cloud edge computing",
                "Internet of Things",
                "keyword search"
            ]
        },
        "title": "Privacy-preserving Fine-grained Data Sharing with Dynamic Service for the Cloud-edge IoT"
    },
    {
        "authors": [
            "Sunyeop Kim",
            "Myoungsu Shin",
            "Seonkyu Kim",
            "Hanbeom Shin",
            "Insung Kim",
            "Donggeun Kwon",
            "Dongjae Lee",
            "Seonggyeom Kim",
            "Deukjo Hong",
            "Jaechul Sung",
            "Seokhie Hong"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "05 November 2024",
        "doi": "10.1109/JIOT.2024.3491138",
        "publisher": "IEEE",
        "abstract": "Shadow is a block cipher for IoT Nodes proposed in the IEEE IoT Journal in 2021. The primary design principle of Shadow is the adoption of a variant 4-branch Feistel structure to ensure a fast diffusion. We refer to this structure as the Shadow structure and prove that it is almost identical to the Feistel structure, which invalidates the design principle. We also present a new structural distinguisher that can distinguish the Shadow structure from a random permutation with only two plaintext/ciphertext pairs. Additionally, we demonstrate that the key-recovery attacks utilizing the impossible differential proposed by Liu et al. in the Cybersecurity Journal in 2023 and the integral characteristic proposed by Mirzaie et al. in the IEEE IoT Journal are infeasible. Instead, we extend our distinguisher to a key-recovery attack using only one plaintext/ciphertext pair by exploiting the key schedule. Moreover, upon investigating Shadow’s round function, we observe that only specific forms of monomials can appear in the ciphertext, leading to an integral distinguisher involving four plaintext/ciphertext pairs. Notably, the algebraic degree does not exceed 12 for Shadow-32 and 20 for Shadow-64, regardless of the number of rounds used. Our results show that Shadow is highly vulnerable to algebraic attacks, emphasizing the need for careful consideration of algebraic attacks when incorporating AND, rotation, and XOR operations in cipher design.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Ciphers",
                "Boolean functions",
                "Internet of Things",
                "Upper bound",
                "Schedules",
                "Real-time systems",
                "Polynomials",
                "Encryption",
                "Complexity theory",
                "Wireless communication"
            ],
            "Author Keywords": [
                "Block cipher",
                "algebraic attack",
                "cube attack"
            ]
        },
        "title": "Redefining Security in Shadow Cipher for IoT Nodes: New Full-Round Practical Distinguisher and the Infeasibility of Key-Recovery Attacks"
    },
    {
        "authors": [
            "Muhammad Baqer Mollah",
            "Md Abul Kalam Azad",
            "Yinghui Zhang"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "01 August 2024",
        "doi": "10.1109/TCE.2024.3436825",
        "publisher": "IEEE",
        "abstract": "Smart devices are considered as an integral part of Internet of Things (IoT), have an aim to make a dynamic network to exchange information, collect data, analysis, and make optimal decisions in an autonomous way to achieve more efficient, automatic, and economical services. Message dissemination among these smart devices allows adding new features, sending updated instructions, alerts or safety messages, informing the pricing information or billing amount, incentives, and installing security patches. On one hand, such message disseminations are directly beneficial to the all parties involved in the IoT system. On the other hand, due to remote procedure, smart devices, vendors, and other involved authorities might have to meet a number of security, privacy, and performance related concerns while disseminating messages among targeted devices. To this end, in this paper, we design STarEdgeChain, a security and privacy aware targeted message dissemination in IoT to show how blockchain along with advanced cryptographic techniques are devoted to address such concerns. In fact, the STarEdgeChain employs a permissioned blockchain assisted edge computing in order to expedite a single signcrypted message dissemination among targeted groups of devices, at the same time avoiding the dependency of utilizing multiple unicasting approaches. Finally, we develop a software prototype of STarEdgeChain and show it’s practicability for smart devices.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Security",
                "Blockchains",
                "Privacy",
                "Internet of Things",
                "Edge computing",
                "Smart devices",
                "Generators"
            ],
            "Author Keywords": [
                "Blockchain",
                "edge computing",
                "Internet of Things",
                "message dissemination",
                "privacy",
                "security",
                "smart device"
            ]
        },
        "title": "Secure Targeted Message Dissemination in IoT Using Blockchain Enabled Edge Computing"
    },
    {
        "authors": [
            "Jailsingh Bhookya"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "08 October 2024",
        "doi": "10.1109/ACCESS.2024.3476322",
        "publisher": "IEEE",
        "abstract": "The article presents a new optimization algorithm called the hybrid MFO-WOA algorithm, which is designed to tackle global optimization problems and optimize FOPID/PID controller designs. The integration of IoT applications for experimental validation is also discussed. Compared to the existing MFO and WOA algorithms, the proposed algorithm demonstrates faster convergence. To evaluate its efficiency, the algorithm is tested on 23 benchmark functions, and statistical parameters are used to describe its performance qualitatively. The results indicate that the proposed algorithm is competitive and can be considered state-of-the-art. The study also includes a comparison of the efficiency of the hybrid MFO-WOA controller with other controller designs for tuning controller parameters. The results of the FOPID/PID controller design are analyzed and compared. The next step involves implementing the PID controller on the ESP32 microcontroller module using the proposed algorithm. An Android-based application using the Blynk framework is developed for monitoring and controlling plants via IoT. The ESP32 microcontroller serves as the IoT gateway, and the controller and interface software are developed using the Arduino IDE. Based on the comparative findings of various case studies, it is evident that the proposed hybrid MFO-WOA algorithm-based controller design outperforms other approaches in terms of performance.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Optimization models",
                "Internet of Things",
                "Whale optimization algorithms",
                "Heuristic algorithms",
                "Approximation algorithms",
                "PD control",
                "Algorithm design and theory",
                "Microcontrollers",
                "Benchmark testing"
            ],
            "Author Keywords": [
                "Optimization",
                "Microcontroller",
                "FOPID Controller",
                "Moth-Flame Optimization",
                "Whale Optimization algorithm",
                "Internet of Things",
                "Liquid Level Control",
                "PID Controller"
            ]
        },
        "title": "A New Hybrid MFO-WO Algorithm and its Application to Design of FOPID/PID Controller for IoT Applications"
    },
    {
        "authors": [
            "Khai-Duy Nguyen",
            "Tuan-Kiet Dang",
            "Binh Kieu-Do-Nguyen",
            "Duc-Hung Le",
            "Cong-Kha Pham",
            "Trong-Thuc Hoang"
        ],
        "published_in": "Published in: IEEE Transactions on Circuits and Systems II: Express Briefs ( Early Access )",
        "date_of_publication": "18 October 2024",
        "doi": "10.1109/TCSII.2024.3483214",
        "publisher": "IEEE",
        "abstract": "The number of IoT devices has grown significantly in recent years, and edge computing in IoT is considered a new and growing trend in the technology industry. While cryptography is widely used to enhance the security of IoT devices, it also carries limitations such as resource constraints or latency. Therefore, lightweight cryptography (LWC) balances commensurate resource usage and maintaining security while minimizing system costs. The ASCON stands out among the LWC algorithms as a potential target for implementation and cryptoanalysis. It provides authenticated encryption with associated data (AEAD) and hashing functionalities in many variants, aiming for various applications. In this brief, we present an implementation of Ascon cryptography as a peripheral of a RISC-V System-on-a-Chip (SoC). The ASCON crypto core occupies 1,424 LUTs in FPGA and 17.4 kGE in 180nm CMOS technology while achieving 417 Gbits/J energy efficiency at a supply voltage of 1.0V and frequency of 2 MHz.",
        "issn": {
            "Print ISSN": "1549-7747",
            "Electronic ISSN": "1558-3791"
        },
        "keywords": {
            "IEEE Keywords": [
                "Ciphers",
                "Internet of Things",
                "Field programmable gate arrays",
                "Hardware",
                "Encryption",
                "Cryptography",
                "Wireless sensor networks",
                "Table lookup",
                "Throughput",
                "Hash functions"
            ],
            "Author Keywords": [
                "RISC-V",
                "lightweight",
                "ASCON",
                "ASIC"
            ]
        },
        "title": "ASIC Implementation of ASCON Lightweight Cryptography for IoT Applications"
    },
    {
        "authors": [
            "Xuanxia Yao",
            "Jinyuan Zhou",
            "Xiaojiang Du",
            "Shurong Zhang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "06 September 2024",
        "doi": "10.1109/JIOT.2024.3456553",
        "publisher": "IEEE",
        "abstract": "Nowadays, we are living in an open network environment with varieties of smart devices, which makes individual privacy face unprecedented threats. For one thing, a plenty of sensitive information may be gathered without the owners knowledge. For the other, the Internet of Things (IoT) based services and various intelligent applications require a large amount of perceptual data. And in practice, these data are usually encrypted and stored in storage providers like cloud for security and cost saving. To fully harness the productivity value of data and protect privacy, ciphertext-policy attribute-based encryption (CP-ABE) is widely used. Nevertheless, most existing CP-ABE schemes cannot work well for IoT because of the heavy overhead and the open and distributed environment. To lower the cost, a lightweight ciphertext-policy attribute-based encryption scheme without pairing is proposed and proved in the set-selective mode. Both the theoretical analysis and experiments show its advantages in computation, communication and storage overhead. For flexible access control in IoT, we attempt to employ the Masked Authenticated Message (MAM) mechanism of the IOTA to manage authorization for our CP-ABE scheme. Comparisons with similar schemes show that it can overcome the low throughput and monetary cost in other distributed ledger based access control schemes.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Access control",
                "Internet of Things",
                "Encryption",
                "Distributed ledger",
                "Data privacy",
                "Privacy",
                "Blockchains"
            ],
            "Author Keywords": [
                "CP-ABE",
                "IOTA",
                "Access Control",
                "Privacy Preserving"
            ]
        },
        "title": "A CP-ABE and IOTA based Lightweight Sensitive Data Access Control Scheme for IoT"
    },
    {
        "authors": [
            "Yu’e Jiang",
            "Yutong Wang",
            "Haiqin Wu",
            "Yiliang Liu",
            "Langtao Hu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "05 November 2024",
        "doi": "10.1109/JIOT.2024.3491431",
        "publisher": "IEEE",
        "abstract": "This paper proposes an energy-efficient covert offloading scheme for blockchain-enabled IoT, allowing sensors to upload tasks undetected by adversaries while ensuring satisfaction in paid computation offloading. Covert communication conceals the existence of transmitted signals or links. However, existing schemes primarily rely on artificial noise (AN) or wireless channel uncertainty, resulting in low covert rates for IoT offloading scenarios. Additionally, blockchain-enabled IoT, being value-oriented, necessitates consideration of sensors’ satisfaction during covert offloading. To tackle these challenges, the proposed scheme combines the adversary’s channel estimation errors with AN to enhance the covert rate, while also matching sensors’ satisfaction with the computation resources of mobile edge servers. Notably, a closed-form expression of the average minimum error detection probability is derived to maximize the effective covert rate. Furthermore, an integrated algorithm combining the Kuhn-Munkres (KM) algorithm with two bubble sort algorithms is designed to minimize energy consumption. Both analytical and simulation results demonstrate that the proposed scheme significantly reduces energy consumption compared to existing solutions.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Sensors",
                "Noise",
                "Channel estimation",
                "Blockchains",
                "Security",
                "Energy consumption",
                "Resource management",
                "Estimation error",
                "Servers"
            ],
            "Author Keywords": [
                "Internet of Things",
                "Blockchain",
                "Covert Communications",
                "Artificial Noise",
                "Computation offloading"
            ]
        },
        "title": "Energy-Efficient Covert Offloading in Blockchain-Enabled IoT: Joint Artificial Noise and Computation Resource Allocation"
    },
    {
        "authors": [
            "Gasim Alandjani"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "11 November 2024",
        "doi": "10.1109/ACCESS.2024.3495635",
        "publisher": "IEEE",
        "abstract": "Despite the widespread applicability of malware detection on the Internet of Things (IoT) and edge platforms, efficient malware detection on edge devices is significantly challenging due to hardware limitations. This study proposes a novel edge-friendly framework for learning malware detection by incorporating quantization awareness. The proposed framework incorporates a lightweight deep neural network leveraging long-distance pixel dependency, spatial-asymmetric attention, and normalization-free multi-head attention (MHA) block to refine salient features. Additionally, the proposed framework introduces an efficient quantization-aware training (QAT) strategy to optimize the proposed model for edge-friendly INT8 precision. The practicality of the proposed model has been validated through deployment on ARM-based hardware and extensive evaluations on standard x64 hardware. Experimental results show that the proposed method significantly outperforms existing malware detection methods in numerous precision modes on both platforms. The proposed method achieved 99.36% accuracy on the MalImg dataset and 98.41% on the IoT malware dataset with FP16 precision at 662 fps. With INT8 precision, it achieved 95.62% accuracy on the MalImg dataset and 97.36% on the IoT malware dataset, maintaining 1197 fps on a low-power edge device. To our knowledge, this is the first work in the open literature demonstrating QAT’s practicality for malware detection on edge platforms.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Malware",
                "Accuracy",
                "Image edge detection",
                "Quantization (signal)",
                "Training",
                "Feature extraction",
                "Internet of Things",
                "Hardware",
                "Performance evaluation",
                "Deep learning"
            ],
            "Author Keywords": [
                "Quantization aware training",
                "malware classification",
                "deep learning",
                "attention",
                "edge device"
            ]
        },
        "title": "Optimizing Malware Detection for IoT and Edge Environments with Quantization Awareness"
    },
    {
        "authors": [
            "Riheng Jia",
            "Qiyong Fu",
            "Zhonglong Zheng",
            "Guanglin Zhang",
            "Minglu Li"
        ],
        "published_in": "Published in: IEEE/ACM Transactions on Networking ( Early Access )",
        "date_of_publication": "02 September 2024",
        "doi": "10.1109/TNET.2024.3450489",
        "publisher": "IEEE",
        "abstract": "In this work, we study the problem of dispatching multiple unmanned aerial vehicles (UAVs) for data collection in internet of things (IoT), where each UAV departs from its start point, visits some IoT devices for data collection and returns to its destination point. Considering the UAV’s limited onboard energy and the time required to collect data from all IoT devices, it is essential to appropriately assign the data collection task for each UAV, such that none of the dispatched UAVs consumes excessive energy and the maximum task completion time among all UAVs is minimized. To optimize those two conflicting objectives, we focus on minimizing the maximum task completion time and the maximum energy consumption among all UAVs, by jointly designing the flight trajectory, hovering positions for data collection and flight speed of each UAV. We formulate this problem as a multi-objective optimization problem with the aim of obtaining a set of Pareto-optimal solutions in terms of time or energy dominance. Due to the NP-hardness and complexity of the formulated problem, we propose a multi-strategy multi-objective ant colony optimization algorithm (MSMOACO), which is developed based on a constrained ant colony optimization algorithm with a fitnessguided mutation strategy and an adaptive hovering strategy being delicately incorporated, to solve the problem. To accommodate the practical scenario, we also design a novel geometry-based collision avoidance strategy to reduce the possibility of collisions among UAVs. Extensive evaluations validate the effectiveness and superiority of the proposed MSMOACO, compared with previous approaches.",
        "issn": {
            "Print ISSN": "1063-6692",
            "Electronic ISSN": "1558-2566"
        },
        "keywords": {
            "IEEE Keywords": [
                "Autonomous aerial vehicles",
                "Internet of Things",
                "Optimization",
                "Task analysis",
                "Data collection",
                "Trajectory",
                "Energy consumption"
            ],
            "Author Keywords": [
                "Multi-UAV network",
                "multi-objective optimization",
                "cooperative trajectory planning",
                "speed control"
            ]
        },
        "title": "Energy and Time Trade-Off Optimization for Multi-UAV Enabled Data Collection of IoT Devices"
    },
    {
        "authors": [
            "Zihan Jiang",
            "Qi Chen",
            "Zhihong Deng",
            "He Zhang"
        ],
        "published_in": "Published in: IEEE Transactions on Network and Service Management ( Early Access )",
        "date_of_publication": "17 September 2024",
        "doi": "10.1109/TNSM.2024.3462813",
        "publisher": "IEEE",
        "abstract": "Recently, blockchain has become a crucial technology for addressing security concerns in the Internet of Things (IoT). However, the substantial storage requirements of blockchain present a major obstacle to integrating IoT with blockchain. This paper introduces a novel coded blockchain architecture called locally repairable blockchain (LRB) to reduce the storage costs for IoT devices. Our architecture employs a node selection allocation algorithm to determine encoding parameters and local repair groups based on the blockchain’s current state, enhancing system availability. We also present a new coding process, GELRC, which combines group coding exchange methods with locally repairable codes, significantly reducing encoding complexity. GELRC facilitates low-cost local repair and improves fault tolerance. Furthermore, we introduce a specialized Vandermonde matrix for designing the local codes of LRB, enhancing the scalability of existing coded blockchains. Experimental results demonstrate that our architecture outperforms previous coded blockchain solutions with greater fault tolerance, improved single-point repair capabilities, lower coding complexity, and particualrly, eliminates the need for re-encoding when new nodes are added.",
        "issn": {
            "Electronic ISSN": "1932-4537"
        },
        "keywords": {
            "IEEE Keywords": [
                "Blockchains",
                "Encoding",
                "Codes",
                "Maintenance engineering",
                "Internet of Things",
                "Decoding",
                "Resource management"
            ],
            "Author Keywords": [
                "Blockchain",
                "storage optimization",
                "locally repairable code",
                "node selection and allocation",
                "group coding exchange"
            ]
        },
        "title": "LRB: Locally Repairable Blockchain for IoT Integration"
    },
    {
        "authors": [
            "Salman Khan",
            "Ibrar Ali Shah",
            "Khursheed Aurangzeb",
            "Shabir Ahmad",
            "Javed Ali Khan",
            "Muhammad Shahid Anwar"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "11 June 2024",
        "doi": "10.1109/JIOT.2024.3403003",
        "publisher": "IEEE",
        "abstract": "In the n-tier framework, data generated by sensors requires immediate execution. The processing elements need powerful resources to entertain incoming requests. Fog computing, unlike cloud computing, provides low latency for real-time applications. However, data generated by real-time Internet of Things (IoT) devices significantly impacts fog devices. The data generated must be processed by fog devices with quick response time, minimum delay, and energy consumption and send it back to the end-users with high reliability and success rate. However, devices fail due to damage or internal state of a fog device which measures incorrectly or causes destruction which badly affects the overall system performance. The end-to-end transmission requests from IoT devices require immediate response with minimal delay, execution cost, and energy consumption in spite the occurrence of fog devices failure. In this article, we propose a novel energy efficient task scheduling algorithm based on reactive fault tolerance in an n-tier fog computing framework for IoT applications to enhance the overall fog computing performance. In case of fog device failure, the assigned task is rescheduled to other executable fog nodes without further delay. The proposed framework is based on modified particle swarm optimization and is designed and evaluated in iFogSim. The main objective of the proposed technique is to reduce energy consumption, latency, network bandwidth utilization and increase system reliability and success rate. Several experiments have been carried out by taking a maximum of 10 iterations based on which it is concluded that the proposed technique reduces energy consumption by 3%, latency by 5%, network bandwidth utilization by 3% and increases the system reliability by 2% and success rate by 8%.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Fault tolerant systems",
                "Fault tolerance",
                "Task analysis",
                "Internet of Things",
                "Costs",
                "Cloud computing",
                "Reliability"
            ],
            "Author Keywords": [
                "Fog computing",
                "fault tolerance",
                "energy efficient",
                "Internet of Things"
            ]
        },
        "title": "Energy Efficient Task Scheduling Using Fault Tolerance Technique for IoT Applications in Fog Computing Environment"
    },
    {
        "authors": [
            "Zijing Wang",
            "Mihai-Alin Badiu",
            "Justin P. Coon"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/JIOT.2024.3489774",
        "publisher": "IEEE",
        "abstract": "Internet of Things (IoT) is an emerging next-generation technology in the fourth industrial revolution. The industrial IoT is required to transmit the collected data in a timely manner to support real-time monitoring, control and automation. In such systems, the timeliness of information is very important, and meanwhile, different physical processes have different requirements on the accuracy of timeliness. However, existing performance metrics, such as the age of information (AoI), are unable to fully evaluate the timeliness of information with heterogeneous physical processes. Recently, we proposed an information-theoretic metric named the “value of information\" (VoI) to measure the usefulness of information in the context of a heterogeneous and noisy environment. In this work, we study a joint path planning of the mobile robot and user scheduling optimisation problem in industrial IoT networks with the aim of maximising the minimum VoI among all users under mobility and communication constraints. We formulate this optimisation problem as a Markov decision process, and propose a reinforcement learning-based algorithm to find the VoI-aware mobility and communication strategy efficiently. Through numerical results, we show that the proposed method can capture the impact of data freshness, inherent correlation characteristics of underlying data sources and noise on the usefulness of information. Compared with the existing AoI-aware strategy, the proposed VoI-aware strategy achieves better performance by exploiting the heterogeneity of data sources especially when the wireless resource is limited.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [],
            "Author Keywords": []
        },
        "title": "Optimal Mobility and Communication Strategy to Maximize the Value of Information in IoT Networks"
    },
    {
        "authors": [
            "Mai Hassan",
            "Hesham G. Moussa",
            "Pin-Han Ho",
            "Limei Peng"
        ],
        "published_in": "Published in: IEEE Transactions on Communications ( Early Access )",
        "date_of_publication": "05 July 2024",
        "doi": "10.1109/TCOMM.2024.3424233",
        "publisher": "IEEE",
        "abstract": "This paper explores the usage of low-voltage Power-Line Communication (PLC) links for Enhanced Common Public Radio Interface (eCPRI)-based front-hauling in 5G IoT indoor mobile coverage environments, using a split Centralized Radio Access Network (C-RAN) architecture. This research aims to analyze how parameters such as wireless IoT device count, bandwidth, and transmission technology affect the delay performance of the proposed system. To achieve this goal, we develop detailed mathematical models that draw insights from queuing theory, stochastic geometry, and Markov models. Extensive system-level simulations verify these models’ accuracy, and the analytical results cover radio and access delay performance. We validate the system’s efficiency in supporting IoT indoor cellular applications and assess the feasibility of the proposed PLC-based fronthauling system, considering the strict delay requirements of the eCPRI standard.",
        "issn": {
            "Print ISSN": "0090-6778",
            "Electronic ISSN": "1558-0857"
        },
        "keywords": {
            "IEEE Keywords": [
                "Delays",
                "Internet of Things",
                "Optical fiber cables",
                "5G mobile communication",
                "Optical fibers",
                "Optical fiber networks",
                "Optical fiber devices"
            ],
            "Author Keywords": [
                "Internet of things",
                "Stochastic geometry",
                "Hybrid automatic repeated request",
                "Power Line communications"
            ]
        },
        "title": "On Power-line based Front-hauling for IoT Cellular Indoor Communications"
    },
    {
        "authors": [
            "Thockchom Birjit Singha",
            "Roy Paily Palathinkal",
            "Shaik Rafi Ahamed"
        ],
        "published_in": "Published in: IEEE Embedded Systems Letters ( Early Access )",
        "date_of_publication": "10 October 2024",
        "doi": "10.1109/LES.2024.3478070",
        "publisher": "IEEE",
        "abstract": "Side Channel Attacks (SCA) have rendered Internet of Things (IoT)-based devices unsafe despite employing Advanced Encryption Standard (AES) as the cryptographic algorithm. Additional circuitry, called countermeasures, is used to protect AES against the attacks at the cost of huge area and power overheads. The attacks are performed on SubBytes round operation of AES, which comprises of 16 S-boxes. This work makes a novel attempt to boost the intrinsic security of an unprotected AES by analysing four smallest Composite Field Arithmetic (CFA)-based S-boxes available in literature, Circuit Minimization Team (CMT), Canright, Maximov and Masoleh with Look-Up Table (LUT)-based S-box as a reference. This work proposes an AES design which is unprotected but with enhanced security. The designer can aim higher security by adding smaller countermeasure protective schemes before incorporating into IoT devices. A novel three-dimensional hardware analysis, namely hardware resources, hardware complexity/linearity and hardware security, are performed which demonstrates that lesser gate equivalent (GE) and linear gates of Masoleh S-box offer the highest security. Upon evaluation on Side-channel Attack Standard Evaluation BOard (SASEBO), all the hardware security metrics favored Masoleh S-box, depicting nearly 94× gain in security and 80 reduction in area with respect to other unprotected designs.",
        "issn": {
            "Print ISSN": "1943-0663",
            "Electronic ISSN": "1943-0671"
        },
        "keywords": {
            "IEEE Keywords": [
                "Logic gates",
                "Security",
                "Internet of Things",
                "Germanium",
                "Signal to noise ratio",
                "Field programmable gate arrays",
                "Table lookup",
                "Switches",
                "Standards",
                "Resilience"
            ],
            "Author Keywords": [
                "IoT",
                "AES",
                "S-box",
                "Correlational Power Analysis (CPA) attacks",
                "intrinsic security",
                "MTD",
                "SNR",
                "MI",
                "TVLA"
            ]
        },
        "title": "Analysis of S-box Hardware Resources to Improve AES Intrinsic Security Against Power Attacks"
    },
    {
        "authors": [
            "Zhao Li",
            "Lijuan Zhang",
            "Chengyu Liu",
            "Siwei Le",
            "Jie Chen",
            "Kang G. Shin",
            "Zheng Yan",
            "Jia Liu"
        ],
        "published_in": "Published in: IEEE Transactions on Mobile Computing ( Early Access )",
        "date_of_publication": "25 September 2024",
        "doi": "10.1109/TMC.2024.3467339",
        "publisher": "IEEE",
        "abstract": "With the rapid development of wireless communication technologies, Internet of Things (IoT) has emerged as one of the most important application scenarios. Due to the high density of IoT devices and the limited spectrum resources, along with the miniaturization and sustainability requirements of these devices, the development of low-cost interference management (IM) methods has become crucial for widespread use of IoT. Interference has long been known to harm network performance. Since a desired signal can be distorted by interference, and thus be incorrectly decoded at the destination, we argue that interference can also be transformed intentionally to extract the desired data from interfering signal(s). Based on this observation, we propose Interference ReCycling (IRC) for the IoT. Under IRC, a recycling signal is generated using the interference a victim IoT device is subjected to, and then sent by the device's associated gateway. Under the influence of the recycling signal, the desired data of the interfered/victim IoT transmission-pair can be recovered from the interference at the IoT device. We also show that the interfered user's spectral efficiency (SE) with IRC can be optimized further by properly distributing the transmit power used for the desired signal's transmission and the recycling signal. We validate the feasibility of IRC by implementing the method on the Universal Software Radio Peripheral (USRP) platform. Our theoretical analysis, experimental and numerical evaluation have shown that the proposed IRC can fully exploit interference, and hence can significantly improve the SE of the victim IoT device compared to other existing IM methods.",
        "issn": {
            "Print ISSN": "1536-1233",
            "Electronic ISSN": "1558-0660"
        },
        "keywords": {
            "IEEE Keywords": [
                "Interference",
                "Internet of Things",
                "Recycling",
                "Data communication",
                "Wireless communication",
                "Wireless fidelity",
                "Performance evaluation"
            ],
            "Author Keywords": [
                "Interference management",
                "IoT",
                "power allocation",
                "signal processing",
                "spectral efficiency"
            ]
        },
        "title": "Interference Recycling: Effective Utilization of Interference for Enhancing Data Transmission"
    },
    {
        "authors": [
            "Ryo Harada",
            "Wasiu O. Popoola"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "06 September 2024",
        "doi": "10.1109/JIOT.2024.3455773",
        "publisher": "IEEE",
        "abstract": "This paper presents the performance analysis of angular diversity receivers (ADRs) and proposes a novel ADR system for visible light communication (VLC) in Internet of Things (IoT) sensor networks. The numerical analysis applying the combining techniques indicates that any combination of ADR types and diversity techniques can enhance performance. Therefore, a frustum-type ADR is implemented in the experimental demonstration with the equal gain combining (EGC) for signal-to-noise (SNR) enhancement. The ADR and the transceiver’s analog front end (AFE) are designed and prototyped. The proposed ADR system shows a 4 dB increase in received SNR under two access points. The bit error rate (BER) is also improved, and the link coverage is more than doubled. Additionally, to show the feasibility of the IoT application, a real-time sensor network system using the ADR is demonstrated. This experimental duplex system indicates the benefits in a practical scenario by showing packet error rate (PER) and response time.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Lenses",
                "Visible light communication",
                "Internet of Things",
                "Diversity reception",
                "Vectors",
                "Prototypes",
                "Light emitting diodes"
            ],
            "Author Keywords": [
                "Angular diversity receiver (ADR)",
                "bidirectional",
                "receive diversity",
                "visible light communication (VLC)"
            ]
        },
        "title": "Study, Design and Implementation of VLC With Angular Diversity Receiver for IoT Systems"
    },
    {
        "authors": [
            "Haneul Ko",
            "Hongrok Choi",
            "Sangheon Pack"
        ],
        "published_in": "Published in: IEEE Transactions on Sustainable Computing ( Early Access )",
        "date_of_publication": "13 August 2024",
        "doi": "10.1109/TSUSC.2024.3442918",
        "publisher": "IEEE",
        "abstract": "Energy harvesting Internet of Things (IoT) devices are capable of sensing only intermittent and coarse-grained data due to sleep scheduling; therefore, we develop a restoration mechanism (e.g., probabilistic matrix factorization (PMF)) that exploits spatial and temporal correlations of data to build up an environmental monitoring system. However, even with a well-designed restoration mechanism, a high accuracy of the environmental map cannot be achieved if an appropriate sleep scheduling of IoT devices is not incorporated (e.g., if IoT devices at necessary locations are in sleep mode or are not involved in restoration due to their insufficient energy). In this paper, we propose a restoration-aware sleep scheduling (RASS) framework for energy harvesting IoT-based environmental monitoring systems. Here, RASS involves customized deep reinforcement learning (DRL) considering the restoration mechanism, using which the controller performs sleep scheduling to achieve high accuracy of the restored environmental map while avoiding energy outage of IoT devices. The evaluation results demonstrate that RASS can achieve an environmental map with 5% or a lower difference from the actual values and fair energy consumption among IoT devices.",
        "issn": {
            "Electronic ISSN": "2377-3782"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Accuracy",
                "Correlation",
                "Energy harvesting",
                "Temperature sensors",
                "Environmental monitoring",
                "Spatiotemporal phenomena"
            ],
            "Author Keywords": [
                "Energy harvesting",
                "environmental monitoring",
                "internet of things (IoT)",
                "reinforcement learning",
                "spatiotemporal"
            ]
        },
        "title": "Restoration-Aware Sleep Scheduling Framework in Energy Harvesting Internet of Things: A Deep Reinforcement Learning Approach"
    },
    {
        "authors": [
            "Yi He",
            "Yanzhong Zhang",
            "Che Wu",
            "Meng Yang",
            "Weidong Xu",
            "Haiyang Wan",
            "Zhuyun Chen"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "29 October 2024",
        "doi": "10.1109/ACCESS.2024.3487832",
        "publisher": "IEEE",
        "abstract": "An Internet of Things (IoT) platform is a software architecture that enables the connection, management, and analysis of IoT devices, sensors, and data. It provides a centralized system for IoT devices to interact with each other and with the cloud, facilitating the collection, processing, and analysis of data from these devices. However, in the automotive manufacturing industry, traditional Internet of Things (IoT) platforms are facing challenges such as bottleneck issues due to business volume growth and system challenges. To address these challenges, we propose a design methodology for an IoT platform based on microservices. The platform’s modules are divided into front end, database, security, and operation maintenance architecture, all effectively designed. Through practical applications, the platform enables interconnections between different information systems, production status monitoring, efficiency management, performance evaluation, energy consumption analysis, quality detection, and equipment asset evaluation. Finally, a data-driven deep learning algorithm, named Long Short-Term Memory Neural Network (LSTM) is developed for the state recognition of the industrial robot based on the Intelligent data services platform, which validate the effectiveness of the constructed IoT platforms. This platform offers advantages in extendibility, reusability, and provides methods for upgrading, expanding functions, and maintaining industrial IoT platforms in the discrete manufacturing industry.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Microservice architectures",
                "Industrial Internet of Things",
                "Computer architecture",
                "Maintenance",
                "Manufacturing",
                "Business",
                "Security",
                "Databases",
                "Monitoring",
                "Automotive engineering",
                "Long short term memory"
            ],
            "Author Keywords": [
                "Automobile manufacturing",
                "IoT platform",
                "Microservices",
                "Deep learning",
                "Long Short-Term Memory Recurrent Neural Network (LSTM)",
                "State recognition"
            ]
        },
        "title": "Architecture Design and Application of IIoT Platform in Automobile Manufacturing based on Microservices and Deep learning Techniques"
    },
    {
        "authors": [
            "Shi Dong",
            "Longhui Shu",
            "Qinyu Xia",
            "Joarder Kamruzzaman",
            "Yuanjun Xia",
            "Tao Peng"
        ],
        "published_in": "Published in: IEEE Transactions on Services Computing ( Early Access )",
        "date_of_publication": "21 August 2024",
        "doi": "10.1109/TSC.2024.3440013",
        "publisher": "IEEE",
        "abstract": "In recent years, the Internet of Things (IoT) has penetrated all aspects of our lives through smart cities, health, industries and others that are related topeople's livelihood. With the increasing number of IoT devices, more and more personal information is exposed in the network space, which inevitably brings some network security problems. Due to the diversity and heterogeneity of IoT devices, identification of such devices in the complex IoT environments remains a major challenge. Existing deep learning-based device identification methods achieve identification of IoT devices by automatically extracting device traffic features, but usually only single modal features of device traffic are considered, which cannot achieve all-around characterization features of communication traffic and affect the identification results. Therefore, we propose an identification method, termed DMRMTT, that employs a Deep convolutional maxout network and MTT model (Multiple Time-series Transformers) to automatically extract the spatial and temporal features of IoT communication session fingerprints and perform further fusion using the structure of the residual, which makes up for the limitations of the existing methods for studying device traffic. This method can improve the characterization of device traffic behaviour and achieve a more accurate identification of IoT devices. Its efficacy is experimentally validated by using two publicly availbale datasets and compared with existing methods. Results show that our method outperforms other methods in widely used performance metrics and achieves 99.82% identification accuracy, demonstrating its superiority and usefulness in IoT device identification.",
        "issn": {
            "Electronic ISSN": "1939-1374"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Object recognition",
                "Feature extraction",
                "Accuracy",
                "Fingerprint recognition",
                "Protocols",
                "Computational modeling"
            ],
            "Author Keywords": [
                "IoT device classification",
                "deeplearning",
                "Maxout network",
                "Transformer",
                "residual connections"
            ]
        },
        "title": "Device Identification Method for Internet of Things Based on Spatial-Temporal Feature Residuals"
    },
    {
        "authors": [
            "Muhammad Nawaz Khan",
            "Haseeb Ur Rahman",
            "Tariq Hussain",
            "Bailin Yang",
            "Saeed Mian Qaisar"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "06 June 2024",
        "doi": "10.1109/TCE.2024.3410300",
        "publisher": "IEEE",
        "abstract": "The emergence of smart and embedded devices and the adaptation of new technologies with the Internet of Things have entirely changed online shopping with a new ecommerce paradigm. However, while the accessibility of IoT services has many advantages, it also creates hazards to customerrelated data. Due to pervasive e-commerce services, anyone can intercept and be compromised, creating great security and privacy concerns. To address these security challenges and to provide lightweight authentication for all entities, we introduce a “Lightweight Mutual Authentication (LMA) Scheme for Connected Devices in IoT”. The proposed scheme uses automatic and mutual authentication for all entities, employing a distributed approach within a server-based architecture. It is lightweight because it provides a secure way of using e-services with fewer steps, and it is automotive because the entities automatically authenticate each other. The LMA is formally validated in TCL, and the experimental results show that it decreases computation cost by about 56%, increases throughput by about 33.3%, and communication cost remains the same as the average of the other three schemes. In evaluation, the results demonstrate that the presence of the LMA leads to a 20-millisecond increase in delay and a 2% decrease in PDR for 100 devices.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Authentication",
                "Security",
                "Protocols",
                "Consumer electronics",
                "Servers",
                "Electronic commerce"
            ],
            "Author Keywords": [
                "Authentication",
                "Internet of Things",
                "Security Mechanisms",
                "Electronics Devices",
                "lightweights"
            ]
        },
        "title": "Enabling Trust in Automotive IoT: Lightweight Mutual Authentication Scheme for Electronic Connected Devices in Internet of Things"
    },
    {
        "authors": [
            "Claudio Savaglio",
            "Vincenzo Barbuto",
            "Fabrizio Mangione",
            "Giancarlo Fortino"
        ],
        "published_in": "Published in: IEEE Internet of Things Magazine ( Early Access )",
        "date_of_publication": "16 August 2024",
        "doi": "10.1109/IOTM.001.2400035",
        "publisher": "IEEE",
        "abstract": "Digital Twins (DTs) are software replicas that not only mirrors physical entities but can also proactively predict, control, optimize and simulate their behavior. Born in the manufacturing sector, this concept after an initial hype stayed untouched for decades. The rise of Internet of Things (IoT) and Artificial Intelligence (AI) enabled DT, respectively, to exchange real-world data and to fully exploit it for fulfilling its own goals. Very recently, Generative AI (Gen-AI) methods started being sporadically applied to DT in different contexts and with different targets. After studying the literature, in this article we provide a definition for the Generative DT (GDT) which embraces main distinctive aspects and potential of current and future Gen- AI-aided DTs. In particular, we first disclose the role of Gen-AI in conciliating the model- and the data-driven approach for the development of DTs. Then, we analyze the added value of main Gen-AI architectures for maximizing the performance of DTs operating in the IoT domain and deployed in the edge-cloud continuum. Finally, we illustrate the potential of a GDT in emblematic Smart City scenarios through a use case involving the prediction of vehicles' trajectories when, due to uncontrolled events, only partial information is accessible. The outlined solution conciliates accuracy and explainability in the trajectory prediction with overall system robustness and effectiveness.",
        "issn": {
            "Print ISSN": "2576-3180",
            "Electronic ISSN": "2576-3199"
        },
        "keywords": {
            "IEEE Keywords": [
                "Artificial intelligence",
                "Data models",
                "Solid modeling",
                "Computational modeling",
                "Real-time systems",
                "Industrial Internet of Things",
                "Adaptation models"
            ],
            "Author Keywords": []
        },
        "title": "Generative Digital Twins: A Novel Approach in the IoT Edge-Cloud Continuum"
    },
    {
        "authors": [
            "Hongyan Dui",
            "Yulu Zhang",
            "Xinmin Wu",
            "Liudong Xing"
        ],
        "published_in": "Published in: IEEE Transactions on Reliability ( Early Access )",
        "date_of_publication": "19 August 2024",
        "doi": "10.1109/TR.2024.3428870",
        "publisher": "IEEE",
        "abstract": "Offshore wind farm is one of the most promising applications in the Internet of Things (IoT), due to being energy-renewable and resources-unlimited. However, the reliability monitoring and maintenance models of power equipment based on communication paths and sensors are still immature in the smart offshore wind farm (SOWF). Based on the hierarchical architecture and end-to-end communication, a dynamic reliability assessment model (DRAM) is proposed for SOWFs. First, based on the IoT hierarchy, a four-stage network is developed to represent the relationship or dependencies between diverse devices in a complex SOWF. Second, a two-layer DRAM with forward monitoring (FM) and lateral protection (LP) is proposed. The FM encompasses a sensor network-based state-monitoring phase (monitoring weather data like temperature and wind speed), and a data-monitoring phase (monitoring the reliability-related data like reception power and data processing speed). The LP includes a signal-protection mode (LP-I) ensuring that virtual machines read the data and issue protection orders before turbine failures to minimize losses, and a radius-maintenance model (LP-II) performing maintenance of the failed turbine nodes. Simulation results show that the optimal maintenance strategy based on DRAM outperforms the benchmark maintenance method for traditional wind grids.",
        "issn": {
            "Print ISSN": "0018-9529",
            "Electronic ISSN": "1558-1721"
        },
        "keywords": {
            "IEEE Keywords": [
                "Reliability",
                "Maintenance",
                "Monitoring",
                "Substations",
                "Wind farms",
                "Frequency modulation",
                "Random access memory"
            ],
            "Author Keywords": [
                "Dynamic reliability assessment model",
                "end-to-end communication",
                "hierarchical architecture",
                "maintenance",
                "smart offshore wind farm"
            ]
        },
        "title": "Dynamic Reliability Assessment Model for IoT-Enabled Smart Offshore Wind Farm"
    },
    {
        "authors": [
            "Xiang Ma",
            "Haijian Sun",
            "Rose Qingyang Hu",
            "Yi Qian"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "30 October 2024",
        "doi": "10.1109/JIOT.2024.3488377",
        "publisher": "IEEE",
        "abstract": "Federated learning (FL) has emerged as a distributed machine learning (ML) technique that can protect local data privacy for participating clients and improve system efficiency. Instead of sharing raw data, FL exchanges intermediate learning parameters, such as gradients, among clients. This article presents an efficient wireless communication approach tailored for FL parameter transmission, especially for Internet of Things (IoT) devices, to facilitate model aggregation. Our study considers practical wireless channels that can lead to random bit errors, substantially affecting FL performance. Motivated by empirical gradient value distribution, we introduce a novel received bit masking method that confines received gradient values within prescribed limits. Moreover, given the intrinsic error resilience of ML gradients, our approach enables the delivery of approximate gradient values with errors without resorting to extensive error correction coding or retransmission. This strategy reduces computational overhead at both the transmitter and the receiver and minimizes communication latency. Consequently, our scheme is particularly well-suited for resource-constrained IoT devices. Our simulations demonstrate that our proposed scheme can effectively mitigate random bit errors in FL performance, achieving similar learning objectives but with the 50% air time required by existing methods involving error correction and retransmission.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Wireless communication",
                "Computational modeling",
                "Internet of Things",
                "Servers",
                "Forward error correction",
                "Resilience",
                "Receivers",
                "Propagation losses",
                "Communication system security",
                "Stochastic processes"
            ],
            "Author Keywords": [
                "Approximate communication",
                "federated learning",
                "lossy wireless communication",
                "gradient model updates",
                "forward error correction (FEC)"
            ]
        },
        "title": "Approximate Wireless Communication for Lossy Gradient Updates in IoT Federated Learning"
    },
    {
        "authors": [
            "Quanze Liu",
            "Yong Liu",
            "Qian Meng",
            "Tianyi Yu"
        ],
        "published_in": "Published in: IEEE Transactions on Network Science and Engineering ( Early Access )",
        "date_of_publication": "28 October 2024",
        "doi": "10.1109/TNSE.2024.3487355",
        "publisher": "IEEE",
        "abstract": "By integrating Software-Defined Networking (SDN), Software-Defined Internet of Things (SD-IoT) simplifies network configuration while enhancing controllability. The expansion of the IoT scale has led to the emergence of the multiple controller architecture. However, it introduces the challenge of controller load imbalances. Existing schemes primarily focus on dynamic switch migration. Nonetheless, conventional strategies use real-time network information for load measurement and selection of candidate switches, which reduces load balancing performance due to inaccurate load measurement. Moreover, existing approaches struggle to balance load balancing rate and migration cost when selecting the target controllers. Therefore, we propose the controller load balancing based on load prediction (CLB-LP) scheme, which uses historical load data to predict future load, thereby avoiding unnecessary switch migrations. Additionally, we introduce a switch selection algorithm that combines load prediction and migration probability to select candidate switches, effectively improving load balancing performance. Furthermore, we present a target controller selection algorithm based on the Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS), which improves the load balancing rate while reducing migration cost. Finally, we evaluate the effectiveness of CLB-LP, and compared to existing schemes, its load balancing rate and response time are 29.4% higher and 28.5% lower, respectively.",
        "issn": {
            "Electronic ISSN": "2327-4697"
        },
        "keywords": {
            "IEEE Keywords": [
                "Switches",
                "Control systems",
                "Load management",
                "Internet of Things",
                "Computer architecture",
                "Reliability",
                "Real-time systems",
                "Prediction algorithms",
                "Process control",
                "Costs"
            ],
            "Author Keywords": [
                "Load Balancing",
                "SDN",
                "Switch Migration",
                "Load Prediction"
            ]
        },
        "title": "CLB-LP: Controller Load Balancing Based on Load Prediction Using Deep Learning for Software-Defined IoT Networks"
    },
    {
        "authors": [
            "Alaa Awad Abdellatif",
            "Khaled Shaban",
            "Ahmed Massoud"
        ],
        "published_in": "Published in: IEEE Transactions on Network Science and Engineering ( Early Access )",
        "date_of_publication": "21 October 2024",
        "doi": "10.1109/TNSE.2024.3483295",
        "publisher": "IEEE",
        "abstract": "Supervised Machine Learning (ML) models require large amounts of labeled data for training. However, this becomes challenging when dealing with resource- and networkconstrained Internet of Things (IoT) devices that collect data. Furthermore, in scenarios where the acquired data is fastchanging and highly temporal, continuous and online learning becomes necessary. In this paper, we address the problem of efficiently training ML models using data from IoT nodes. We specifically focus on two aspects: i) selecting the nodes that provide data for the re/training, and ii) determining the optimal amounts of data to be acquired from these nodes, considering network and time constraints, while minimizing learning errors. To tackle this optimization problem, we propose ONDS: an Optimum Node and Data Selection algorithm with linear complexity in the worst-case. ONDS offers a modelagnostic solution applicable to different data modalities and ML architectures. To evaluate the performance of ONDS, we conduct experiments using various models and real-world datasets. The results demonstrate the effectiveness of ONDS, as it outperforms existing alternatives in both classification and regression tasks.",
        "issn": {
            "Electronic ISSN": "2327-4697"
        },
        "keywords": {
            "IEEE Keywords": [
                "Data models",
                "Internet of Things",
                "Computational modeling",
                "Distributed databases",
                "Training",
                "Performance evaluation",
                "Costs",
                "Accuracy",
                "Optimization",
                "Resource management"
            ],
            "Author Keywords": [
                "Machine learning models",
                "distributed sources and data selection",
                "Internet of things",
                "online machine learning",
                "classification",
                "regression"
            ]
        },
        "title": "ONDS: Optimum Node and Data Selection from Constrained IoT for Efficient Online Learning"
    },
    {
        "authors": [
            "Xiuwen Fu",
            "Mingyuan Ren",
            "Xiangwei Liu"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "08 November 2024",
        "doi": "10.1109/JSEN.2024.3490857",
        "publisher": "IEEE",
        "abstract": "Unmanned aerial vehicle (UAV)-enabled Internet of things (IoT) systems are considered an effective solution for maritime environmental monitoring. However, in the maritime data collection scenarios, the coverage of maritime sensor network is significantly larger and changes over time. This makes existing UAV-enabled IoT solutions unable to meet the performance requirements of maritime environmental monitoring, especially in terms of data transmission latency. Therefore, we design a UAV-enabled maritime Internet of things (MIoT) data collection architecture with constrained aerial base stations. Based on this architecture, we propose a UAV-enabled MIoT collaborative data collection (UMCDC) scheme. In this scheme, we first introduce a distributed adaptive large neighborhood search algorithm to generate flight trajectories of UAVs. On this basis, we present a diffusion-based relay pairing method and a centripetal-based flight trajectory assignment method to establish relay pairing relationships between UAVs and assign flight trajectories to UAVs, respectively. Finally, a re-selection method is developed to extend the lifetime of the maritime sensor network by optimizing the hover points of UAVs and data collection nodes of subnets. Extensive experimental results have shown that the UMCDC scheme can achieve low data transmission latency and significantly extend the lifetime of the maritime sensor network.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Data collection",
                "Sensors",
                "Trajectory",
                "Autonomous aerial vehicles",
                "Sea surface",
                "Relays",
                "Internet of Things",
                "Collaboration",
                "Base stations",
                "Systems architecture"
            ],
            "Author Keywords": [
                "unmanned aerial vehicle",
                "maritime Internet of things",
                "transmission latency",
                "data collection",
                "flight trajectory",
                "sensor network"
            ]
        },
        "title": "Collaborative Data Collection in UAV-Enabled Maritime IoT with Constrained Aerial Base Stations"
    },
    {
        "authors": [
            "Takahiro Sasaki",
            "Yukihiro Kamiya"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "30 September 2024",
        "doi": "10.1109/ACCESS.2024.3470225",
        "publisher": "IEEE",
        "abstract": "Non-contact vital sensing has increased its importance associated with the Internet of Things (IoT) concept. Although it creates opportunities to conceive new services, data collection using narrow-band wireless links is crucial. Edge computing architecture enables us to solve this problem even though it increases the power consumption in sensor nodes. Successful implementation of edge computing in the IoT system requires a power-efficient implementation of algorithms for data analysis and hardware configuration. This paper proposes to modify the average magnitude difference function (AMDF), which has been employed in pitch extractions of speech signals, and to apply it to the estimation of the respiration period measured by a Doppler sensor. In addition, a multi-stage search algorithm is proposed to be associated with an efficient hardware configuration. The performance of the algorithm and the implementation using a small-scale processor is verified through computer simulations and experiments.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Doppler effect",
                "Monitoring",
                "Hardware",
                "Sensors",
                "Power demand",
                "Internet of Things",
                "Heart beat",
                "Estimation",
                "Edge computing",
                "Wireless sensor networks"
            ],
            "Author Keywords": [
                "Doppler sensing",
                "vital signs",
                "AMDF",
                "edge computing",
                "parameter estimation"
            ]
        },
        "title": "Application of AMDF for Vital Sensing and Its Implementation Toward IoT Edge Computing"
    },
    {
        "authors": [
            "M. Kumari Kala",
            "M. Priya"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "18 October 2024",
        "doi": "10.1109/ACCESS.2024.3483437",
        "publisher": "IEEE",
        "abstract": "The Internet of Things transmits data collected from remote sensors and shares public and private data through the Internet or other communication networks. The IoT healthcare data uses decentralized cloud security for the sharing and storage of healthcare data. Privacy and security are critical components for managing private data and personalization. Due to the Big Data environment in healthcare data processing the sensitive data collection from Electronic Personal Healthcare Records poses challenges and difficulties. The unauthorized policies allow adversaries to obtain data and keys for sharing healthcare records. So Sensitive information in personal healthcare records needs to be highly protected in the medical field. The excessive insensitivity of the current privacy regulations causes delays in complexity and poor performance. Exposure-risk response interactions are necessary to calculate the burden of environmental ailments. To tackle this issue, this paper introduces Blockchain-based Decentralized Secure Sharing using the Shuffled Random Starvation Link Encryption algorithm to protect the PHR-sensitive records in the smart IoT. Initially, the proposed Quasi Sensitive Attribute Identification method is used to analyse and categorize the sensitive cases of PHR. Then encrypt the sensitive attributes using the novel Shuffled Random Starvation Link Encryption algorithm to protect the user’s private data after the Key Authentication Policy is used to verify the master node key aggregation and peer-end verification to share information between users. As a result, the proposed system outperforms previous approaches in reliability and security.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Medical services",
                "Security",
                "Blockchains",
                "Internet of Things",
                "Medical diagnostic imaging",
                "Data privacy",
                "Cryptography",
                "Privacy",
                "Electronic medical records",
                "Organizations"
            ],
            "Author Keywords": [
                "Blockchain",
                "Electronic Healthcare Records",
                "Healthcare Management",
                "Internet of Things",
                "Key Authentication Policy",
                "Personal Healthcare Records",
                "Privacy",
                "Sensitive Terms",
                "Security"
            ]
        },
        "title": "Smart IoT-Blockchain Security to secure Sensitive Personal Medical Data using Shuffled Random Starvation Link Encryption"
    },
    {
        "authors": [
            "Tao Qian",
            "Mingyu Fang",
            "Yongxu Zhu",
            "Zeyu Liang",
            "Yuxiong Huang",
            "Qinran Hu",
            "Zaijun Wu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "30 September 2024",
        "doi": "10.1109/JIOT.2024.3469954",
        "publisher": "IEEE",
        "abstract": "Managing energy usage flexibility has been identified as an effective way to coordinate Internet of Things (IoT) technologies and transition to a low-carbon future. However, the various models of these flexibilities may not be fully understood, and some may even be characterized by black-box neural networks, such as those used for electric vehicles (EVs) charging. In this paper, we propose a novel framework called augmented shadow-price deep reinforcement learning (ASP-DRL) for the online, distributed management of IoT under multiple sources of uncertainties, including renewable energy sources (RES), wholesale electricity prices, and EV behavior patterns. To address these challenges, the proposed framework combines scheduling mechanisms for neural networks and optimization models to maximize total social welfare. Within the ASP-DRL framework, the policy network adaptively learns about system uncertainties and delivers actions to different distributed entities, either to form augmented objective functions for optimization models or to guide neural networks. We also present a distribution correction algorithm that enhances the vanilla soft actor-critic method with attention-based maximal corrective feedback, resulting in faster convergence and better performance. Our numerical studies demonstrate the superiority of the proposed ASP-DRL framework compared to conventional deep reinforcement learning (DRL) and optimization-based approaches.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Uncertainty",
                "Neural networks",
                "Internet of Things",
                "Optimization models",
                "Electric vehicle charging",
                "Stochastic processes",
                "Energy management",
                "Microgrids",
                "Renewable energy sources",
                "Electricity"
            ],
            "Author Keywords": [
                "Deep reinforcement learning",
                "electric vehicles",
                "energy management",
                "Internet of Things",
                "neural networks"
            ]
        },
        "title": "ASP - DRL: A Novel Framework for Unifying IoTs Energy Usage Flexibilities Characterized by Neural Networks and Optimization Models"
    },
    {
        "authors": [
            "Makhduma F Saiyed",
            "Irfan Al-Anbagi",
            "M. Shamim Hossain"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "16 October 2024",
        "doi": "10.1109/TCE.2024.3482092",
        "publisher": "IEEE",
        "abstract": "The integration of Internet of Things (IoT) in consumer environments enhances convenience and security while increasing Human-Computer Interaction (HCI). However, this increased interactivity has also increased the vulnerability of Consumer IoT (CIoT) networks to cyber threats, mainly Distributed Denial of Service (DDoS) attacks. The DDoS attacks, which vary in volume, present substantial challenges to these networks’ operational integrity and customer trust. This paper introduces the Artificial Intelligence (AI)-driven (ADEPT) system that utilizes explainable and optimized deep-ensemble learning with pruning for DDoS detection. The system uses attention-based ensemble DL for DDoS detection, combining Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks. To address the resource constraints of edge devices in CIoT networks, the system uses Differential Evolution (DE)-based pruning and quantization techniques, optimizing the model for efficient deployment on edge nodes while preserving high performance. An HCI interface is designed to allow network administrators and researchers to engage with the system through dynamic visualizations, facilitating complex data interpretation and empowering administrators to refine detection strategies. The interface, integrating SHapley Additive exPlanations (SHAP) and risk assessment, enhances model transparency and interpretability, highlighting the synergy of HCI and AI. The ADEPT system is evaluated using an experimental testbed and CIoT datasets and has demonstrated over 90% accuracy in detecting high-and low-volume DDoS attacks.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Denial-of-service attack",
                "Computational modeling",
                "Quantization (signal)",
                "Accuracy",
                "Internet of Things",
                "Adaptation models",
                "Convolutional neural networks",
                "Optimization",
                "Intrusion detection",
                "Image edge detection"
            ],
            "Author Keywords": [
                "DDoS",
                "CNN",
                "Ensemble Learning",
                "Differential Evolution",
                "CIoT",
                "LSTM",
                "Pruning",
                "Quantization",
                "Security",
                "SHAP"
            ]
        },
        "title": "Interactive and Explainable Optimized Learning for DDoS Detection in Consumer IoT Networks"
    },
    {
        "authors": [
            "Nadia Abdolkhani",
            "Nada Abdel Khalek",
            "Walaa Hamouda"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "10 September 2024",
        "doi": "10.1109/JIOT.2024.3457012",
        "publisher": "IEEE",
        "abstract": "In the evolving landscape of the Internet of Things (IoT), integrating cognitive radio (CR) has become a practical solution to address the challenge of spectrum scarcity, leading to the development of cognitive IoT (CIoT). However, the vulnerability of radio communications makes radio jamming attacks a key concern in CIoT networks. In this paper, we introduce a novel deep reinforcement learning (DRL) approach designed to optimize throughput and extend network lifetime of an energy-constrained CIoT system under jamming attacks. This DRL framework equips a CIoT device with the autonomy to manage energy harvesting (EH) and data transmission, while also regulating its transmit power to respect spectrum-sharing constraints. We formulate the optimization problem under various constraints, and we model the CIoT device’s interactions within the channel as a model-free Markov decision process (MDP). The MDP serves as a foundation to develop a double deep Q-network (DDQN), designed to help the CIoT agent learn the optimal communication policy to navigate challenges such as dynamic channel occupancy, jamming attacks, and channel fading while achieving its goal. Additionally, we introduce a variant of the upper confidence bound (UCB) algorithm, named UCB-IA, which enhances the CIoT network’s ability to efficiently navigate jamming attacks within the channel. The proposed DRL algorithm does not rely on prior knowledge and uses locally observable information such as channel occupancy, jamming activity, channel gain, and energy arrival to make decisions. Extensive simulations prove that our proposed DRL algorithm that utilizes the UCB-IA strategy surpasses existing benchmarks, allowing for a more adaptive, energy-efficient, and secure spectrum sharing in CIoT networks.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Jamming",
                "Internet of Things",
                "Heuristic algorithms",
                "Power control",
                "Games",
                "Throughput",
                "Radio frequency"
            ],
            "Author Keywords": [
                "Cognitive Internet of Things",
                "deep reinforcement learning",
                "upper confidence bound",
                "energy harvesting",
                "jamming attacks"
            ]
        },
        "title": "Deep Reinforcement Learning for EH-Enabled Cognitive-IoT Under Jamming Attacks"
    },
    {
        "authors": [
            "Altaf Hussain",
            "Wajahat Akbar",
            "Tariq Hussain",
            "Ali Kashif Bashir",
            "Maryam M. Al Dabel",
            "Farman Ali",
            "Bailin Yang"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "16 August 2024",
        "doi": "10.1109/TCE.2024.3444824",
        "publisher": "IEEE",
        "abstract": "In the increasingly digitized world, the privacy and security of sensitive data shared via IoT devices are paramount. Traditional privacy-preserving methods like k-anonymity and ldiversity are becoming outdated due to technological advancements. In addition, data owners often worry about misuse and unauthorized access to their personal information. To address this, we propose a secure data-sharing framework that uses local differential privacy (LDP) within a permissioned blockchain, enhanced by federated learning (FL) in a zero-trust environment. To further protect sensitive data shared by IoT devices, we use the Interplanetary File System (IPFS) and cryptographic hash functions to create unique digital fingerprints for files. We mainly evaluate our system based on latency, throughput, privacy accuracy, and transaction efficiency, comparing the performance to a benchmark model. The experimental results show that the proposed system outperforms its counterpart in terms of latency, throughput, and transaction efficiency. The proposed model achieved a lower average latency of 4.0 seconds compared to the benchmark model’s 5.3 seconds. In terms of throughput, the proposed model achieved a higher throughput of 10.53 TPS (transactions per second) compared to the benchmark model’s 8 TPS. Furthermore, the proposed system achieves 85% accuracy, whereas the counterpart achieves only 49%.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Blockchains",
                "Security",
                "Data models",
                "Internet of Things",
                "Medical services",
                "Privacy",
                "Federated learning"
            ],
            "Author Keywords": [
                "Blockchain",
                "Differential Privacy",
                "Federated Learning",
                "Internet of Things",
                "Zero Trust Securitys"
            ]
        },
        "title": "Ensuring Zero Trust IoT Data Privacy: Differential Privacy in Blockchain using Federated Learning"
    },
    {
        "authors": [
            "Weili Wang",
            "Omid Abbasi",
            "Halim Yanikomeroglu",
            "Chengchao Liang",
            "Lun Tang",
            "Qianbin Chen"
        ],
        "published_in": "Published in: IEEE Network ( Early Access )",
        "date_of_publication": "02 January 2024",
        "doi": "10.1109/MNET.2023.3349309",
        "publisher": "IEEE",
        "abstract": "Vertical heterogeneous networks (VHetNets) and artificial intelligence (AI) play critical roles in 6G and beyond networks. This article presents an AI-native VHetNets architecture to enable the synergy of VHetNets and AI, thereby supporting varieties of AI services while facilitating the intelligent network management. Anomaly detection stands as an essential AI service across various applications in Internet of Things (IoT), including intrusion detection, state monitoring, analysis of device activities, and security supervision. In this article, we first discuss the possibilities of VHetNets used for distributed AI model training to provide the anomaly detection service for ubiquitous IoT, i.e., VHetNets for AI. After that, we study the application of AI approaches in helping implement the intelligent network management functionalities for VHetNets, i.e., AI for VHetNets, whose aim is to facilitate the efficient implementation of the anomaly detection service. Finally, a case study is presented to demonstrate the efficiency and effectiveness of the proposed AI-native VHetNets-enabled anomaly detection framework.",
        "issn": {
            "Print ISSN": "0890-8044",
            "Electronic ISSN": "1558-156X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Artificial intelligence",
                "Internet of Things",
                "Sensors",
                "Data models",
                "Autonomous aerial vehicles",
                "Anomaly detection",
                "Training"
            ],
            "Author Keywords": []
        },
        "title": "VHetNets for AI and AI for VHetNets: An Anomaly Detection Case Study for Ubiquitous IoT"
    },
    {
        "authors": [
            "Junchao Fan",
            "Xiaolin Chang",
            "Jelena Mišić",
            "Vojislav B. Mišić",
            "Tong Yang",
            "Yanwei Gong"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "23 August 2024",
        "doi": "10.1109/JIOT.2024.3448537",
        "publisher": "IEEE",
        "abstract": "Unmanned aerial vehicles (UAVs) are being broadly employed to assist in efficient data collection for IoT networks. Studies have been conducted to ensure the effectiveness and safety of UAVs in the data collection process. However, they only considered part of the challenges of energy consumption, collision avoidance, and mobility of IoT devices. In this paper, we study a UAV path planning optimization problem for UAV-assisted data collection to maximize the amount of collected data. Different from these existing works, this optimization problem not only considers all these challenges, but also considers the kinematic and communication constraints. Moreover, in this problem, the duration required for the UAV to complete the mission is unknown, makes it more challenging to solve this problem through traditional optimization methods. We thus formulate the problem as a partially observable Markov decision process (POMDP) with a continuous action space and propose a proximal policy optimization-based algorithm to address it. Experiment results demonstrate that our algorithm has significant advantages over other baseline algorithms in terms of success rate, data collection rate, and collision rate.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Data collection",
                "Internet of Things",
                "Autonomous aerial vehicles",
                "Trajectory",
                "Optimization",
                "Heuristic algorithms",
                "Collision avoidance"
            ],
            "Author Keywords": [
                "data collection",
                "deep reinforcement learning",
                "mobile device",
                "path planning",
                "unmanned aerial vehicle"
            ]
        },
        "title": "Energy-Constrained Safe Path Planning for UAV-Assisted Data Collection of Mobile IoT Devices"
    },
    {
        "authors": [
            "Deema Abdal Hafeth",
            "Mohammed Al-khafajiy",
            "Stefanos Kollias"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "05 November 2024",
        "doi": "10.1109/JIOT.2024.3492066",
        "publisher": "IEEE",
        "abstract": "The emergence of Edge Computing has shifted the processing capabilities in proximity to the Internet of Things data sources, offering solutions to latency and bandwidth constraints applications. This shift complements Cloud Computing, especially in handling real-time data processing and enhancing processing. Image processing, particularly image captioning for smart monitoring systems, benefits greatly from this synergy. Image captioning plays a crucial role in understanding visual data. While early methods excelled in encoder-decoder frameworks and attention mechanisms, they often overlooked semantic representations which are essential for comprehensive image understanding. To address this gap, we introduce the EdgeScan framework, leveraging Edge Computing for image analysis and semantic feature extractions closer to data sources. EdgeScan integrates visual and semantic features to create more informative and enriched image captions that enhance image captioning accuracy. The EdgeScan image captioning model architecture is capable of i) learning the salient image region’s specific feature representation, and ii) co-embedding visual attention and semantic attributes in one space for feature fusion. This improves models’ ability to interpret and respond to data in a meaningful way, which is particularly valuable for IoT applications that require a deep understanding of the semantics of diverse and constantly changing data for efficient operation. Extensive experiments were conducted on the MS-COCO dataset to demonstrate the superiority of EdgeScan in both quantitative and qualitative performance, achieving highest CIDEr score of 120.9, as well as notable scores of 78.6 for BLEU@1 and 57.7 for ROUGE metrics, promising advancements in IoT-driven image understanding and competitiveness against the state-of-the-art.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Cloud computing",
                "Semantics",
                "Internet of Things",
                "Computational modeling",
                "Edge computing",
                "Visualization",
                "Data models",
                "Feature extraction",
                "Data processing"
            ],
            "Author Keywords": [
                "Edge Computing",
                "Image Captioning",
                "Internet of Things",
                "Semantic Attentions"
            ]
        },
        "title": "EdgeScan for IoT Contextual Understanding With Edge Computing and Image Captioning"
    },
    {
        "authors": [
            "Amit Kumar Bhuyan",
            "Hrishikesh Dutta",
            "Subir Biswas"
        ],
        "published_in": "Published in: IEEE Transactions on Emerging Topics in Computational Intelligence ( Early Access )",
        "date_of_publication": "25 October 2024",
        "doi": "10.1109/TETCI.2024.3482855",
        "publisher": "IEEE",
        "abstract": "This paper presents a computationally efficient and distributed speaker diarization framework for networked IoT-style audio devices. The work proposes a Federated Learning model which can identify the participants in a conversation without the requirement of a large audio database for training. An unsupervised online update mechanism is proposed for the Federated Learning model which depends on cosine similarity of speaker embeddings. Moreover, the proposed diarization system solves the problem of speaker change detection via. unsupervised segmentation techniques using Hotelling's t-squared Statistic and Bayesian Information Criterion. In this new approach, speaker change detection is biased around detected quasi-silences, which reduces the severity of the trade-off between the missed detection and false detection rates. Additionally, the computational overhead due to frame-by-frame identification of speakers is reduced via. unsupervised clustering of speech segments. The results demonstrate the effectiveness of the proposed training method in the presence of non-IID speech data. It also shows a considerable improvement in the reduction of false and missed detection at the segmentation stage, while reducing the computational overhead. Improved accuracy and reduced computational cost makes the mechanism suitable for real-time speaker diarization across a distributed IoT audio network.",
        "issn": {
            "Electronic ISSN": "2471-285X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Mel frequency cepstral coefficient",
                "Computational modeling",
                "Accuracy",
                "Oral communication",
                "Federated learning",
                "Training",
                "Bayes methods",
                "Feature extraction",
                "Data models",
                "Computational intelligence"
            ],
            "Author Keywords": [
                "Unsupervised Learning",
                "Bayesian methods",
                "federated learning",
                "distributed processing",
                "Hotelling's t-squared statistic",
                "Bayesian information criterion",
                "cepstral analysis"
            ]
        },
        "title": "Unsupervised Speaker Diarization in Distributed IoT Networks Using Federated Learning"
    },
    {
        "authors": [
            "Lin Chen",
            "Yuxiang Chen",
            "Wei Liang",
            "Xiong Li",
            "Kuan-Ching Li",
            "Jin Wang",
            "Naixue Xiong"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "26 September 2024",
        "doi": "10.1109/JIOT.2024.3468733",
        "publisher": "IEEE",
        "abstract": "With the swift advancement of the Internet of Things (IoT) and Artificial Intelligence (AI), various technologies have been integrated into wearable medical health devices, improving users’ awareness of their physical states and enabling the analysis of a greater amount of human data. However, these sensitive pieces of information are prone to tampering or theft during storage and transmission, posing security risks. In this article, we propose a multi-attribute sketch secure data sharing scheme for IoT wearable medical devices based on blockchain (MASS). We introduce a multi-attribute sketch storage method that stores the encrypted hash of health data transmitted by medical wearable devices on the blockchain. This work also designs a Ciphertext-Policy Attribute-Based Encryption (CPABE) access control mechanism that effectively addresses the secure sharing of data from wearable medical devices among healthcare professionals. Experimental findings indicate that with the rise in the number of medical health data documents, the costs associated with index generation and search time decrease by 55.3% and 10.83%, respectively. Additionally, as the frequency of data access increases, there is a 13.5% reduction in encryption time, and the implementation of multi-attribute sketches results in a 24.8% and 11.3% reduction in index generation and search times, respectively.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Medical diagnostic imaging",
                "Blockchains",
                "Medical devices",
                "Wearable devices",
                "Access control",
                "Data privacy",
                "Cryptography"
            ],
            "Author Keywords": [
                "Blockchain",
                "Health Data",
                "Multi-Attribute Sketch",
                "CP-ABE",
                "Secure Data Sharing"
            ]
        },
        "title": "MASS: A Multi-Attribute Sketch Secure Data Sharing Scheme for IoT Wearable Medical Devices Based on Blockchain"
    },
    {
        "authors": [
            "Timothy J. Pierson",
            "Cesar Arguello",
            "Beatrice Perez",
            "Wondimu Zegeye",
            "Kevin Kornegay",
            "Carl A. Gunter",
            "David Kotz"
        ],
        "published_in": "Published in: IEEE Security & Privacy ( Early Access )",
        "date_of_publication": "07 May 2024",
        "doi": "10.1109/MSEC.2024.3386467",
        "publisher": "IEEE",
        "abstract": "Internet of Things (IoT) devices left behind when a home is sold create security and privacy concerns for both prior and new residents. We envision a specialized “building inspector for IoT” to help securely facilitate transfer of the home.",
        "issn": {
            "Print ISSN": "1540-7993",
            "Electronic ISSN": "1558-4046"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Security",
                "Task analysis",
                "Privacy",
                "Smart homes",
                "Smart devices",
                "Wireless fidelity"
            ],
            "Author Keywords": []
        },
        "title": "We Need a “Building Inspector for IoT” When Smart Homes Are Sold"
    },
    {
        "authors": [
            "Tao Liu",
            "Shengli Pan",
            "Peng Li"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/JIOT.2024.3482292",
        "publisher": "IEEE",
        "abstract": "Graph neural networks (GNNs) have shown great success in IoT applications, but many IoT scenarios further involve evolving graph data over time. Streaming Graph Learning (SGL) tackles the issue by updating GNN models continuously, incorporating significant historical node data for tasks like node classification. However, existing research on SGL often neglects memory limitations during historical node selection, and efficient distributed training is challenging due to the coupling of node selection, placement, and parallelization. This paper proposes an efficient distributed SGL system that optimizes node selection and storage in multi-GPU environments, while reducing communication overhead to accelerate training. Extensive evaluations demonstrate the effectiveness of the approach.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Graph neural networks",
                "Internet of Things",
                "Data models",
                "Optimization",
                "Aggregates",
                "Memory management",
                "Backpropagation",
                "Toy manufacturing industry",
                "Termination of employment"
            ],
            "Author Keywords": [
                "SGL",
                "Graph neural network",
                "Distributed training",
                "Continual graph learning"
            ]
        },
        "title": "Streaming Graph Learning in IoT with Storage Optimization and Communication Reduction"
    },
    {
        "authors": [
            "Lun Tang",
            "Zhoulin Pu",
            "Qiang Hou",
            "Dongxu Fang",
            "Qianbin Chen"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "08 October 2024",
        "doi": "10.1109/JIOT.2024.3476112",
        "publisher": "IEEE",
        "abstract": "With the rapid development of the Internet of Things (IoT), many IoT devices are accessing the network. However, existing networks cannot fully meet the strict and diverse requirements for delay and reliability in delay-sensitive services. Dynamic changes in service requests and the states of service nodes cause a lack of guaranteed end-to-end (E2E) network slicing delay determinism. To address this issue, we propose a Digital Twin (DT)-assisted network slicing resource allocation scheme. By integrating DT and network slicing, we first construct a DT-assisted E2E network architecture, and construct the base and mapping models in the proposed architecture. Secondly, we use the Stochastic Network Calculus (SNC) theory to analyze the E2E delay violation probability and characterize the relationship between delay and service reliability under given traffic arrival distributions and delay constraints. Then, we construct a joint resource allocation problem of time-frequency, computation, storage, and bandwidth resources to maximize the utility of the InP while guaranteeing the deterministic delay. Furthermore, a multi-agent deep reinforcement learning algorithm in a distributed architecture is used to solve the complex optimization problem, achieving efficient network resource allocation. Simulation results demonstrate that the proposed resource allocation scheme meets the requirements for deterministic delay and enhances system utility.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Delays",
                "Network slicing",
                "Resource management",
                "Industrial Internet of Things",
                "Optimization",
                "Deep reinforcement learning",
                "Real-time systems",
                "Quality of service",
                "Indium phosphide",
                "III-V semiconductor materials"
            ],
            "Author Keywords": [
                "Network slicing",
                "digital twin (DT)",
                "stochastic network calculus",
                "deep reinforcement learning",
                "resource allocation"
            ]
        },
        "title": "Deterministic Delay of Digital Twin-Assisted End-to-End Network Slicing in Industrial IoT via Multiagent Deep Reinforcement Learning"
    },
    {
        "authors": [
            "Subhranshu Sekhar Tripathy",
            "Manisha Guduri",
            "Chinmay Chakraborty",
            "Sujit Bebortta",
            "Subhendu Kumar Pani",
            "Sabyasachi Mukhopadhyay"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "05 July 2024",
        "doi": "10.1109/TCE.2024.3424189",
        "publisher": "IEEE",
        "abstract": "A prominent use case of consumer electronics-based Internet of Things (IoT) applications, focused on smart cities, is connected devices that enable cities to optimize their operations via access to high volumes of sensitive data. Yet, these devices commonly utilize public channels for data access and sharing, requiring consistent communication protocols and an Intrusion Detection System (IDS) with the aid of AI. However, most of them involve high computation and communication costs. They are not fully reliable, either. Also, AI-based IDS solutions are viewed as black boxes because they cannot justify their decisions. To resolve these issues, we have proposed a framework based on explainable artificial intelligence (XAI) for securing consumer IoT applications in smart cities. At the beginning of the protocol execution, the participants exchange authenticated data through the blockchain-based AKA procedure. Meanwhile, we adopt the Python-based Shapley Additive Explanation (SHAP) framework to explain and interpret the core features guiding decision-making. The working model of this framework depicts its validation with recent benchmark methods.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Servers",
                "Authentication",
                "Smart cities",
                "Security",
                "Computational modeling",
                "Consumer electronics"
            ],
            "Author Keywords": [
                "Explainable AI",
                "Consumer Electronic Devices",
                "Internet of Things",
                "Fog Computing",
                "Deep Reinforcement Learning",
                "Task Offloading",
                "Energy Efficiency",
                "Latency"
            ]
        },
        "title": "An Adaptive Explainable AI Framework for Securing Consumer Electronics-Based IoT Applications in Fog-Cloud Infrastructure"
    },
    {
        "authors": [
            "Charalampos Eleftheriadis",
            "Georgios Karakonstantis"
        ],
        "published_in": "Published in: IEEE Transactions on Biomedical Circuits and Systems ( Early Access )",
        "date_of_publication": "29 May 2024",
        "doi": "10.1109/TBCAS.2024.3406520",
        "publisher": "IEEE",
        "abstract": "Power spectral analysis (PSA) is one of the most popular and insightful methods, currently employed in several biomedical applications, aiming to identify and monitor various health related conditions. Among the most common applications of PSA is heart rate variability (HRV) analysis, which allows the extraction of further insights compared with conventional time-domain methods. Unfortunately, existing PSA approaches exhibit high computational complexity, hindering their execution on power-constrained embedded internet of things (IoT) devices. Such IoT devices are increasingly used for monitoring various conditions mainly by processing the input signals in the less complex time-domain. In this paper, a new low-complexity PSA system based on fast Gaussian gridding (FGG) is proposed, which can be used to calculate the Lomb-Scargle periodogram (LSP) of a non-uniformly spaced RR tachogram. The proposed approach is implemented on a popular ARM Cortex-M4 based embedded system, which is widely used in common wearables, and compared with conventional LSP-based approaches. Utilizing this experimental setup, a meticulous analysis is performed in terms of power, performance and quality under different operational settings, such as the total input/output samples, precision of computations, computer arithmetic (floating/fixed-point), and clock frequency. The experimental results show that the proposed FGG-based LSP approach, when specifically optimized for the targeted embedded device, outperforms existing approaches by up-to 92.99% and 91.70% in terms of energy consumption and total execution time respectively, with minimal accuracy loss.",
        "issn": {
            "Print ISSN": "1932-4545",
            "Electronic ISSN": "1940-9990"
        },
        "keywords": {
            "IEEE Keywords": [
                "Heart rate variability",
                "Time-domain analysis",
                "Electrocardiography",
                "Internet of Things",
                "Estimation",
                "Performance evaluation",
                "Monitoring"
            ],
            "Author Keywords": [
                "Power spectral analysis (PSA)",
                "Lomb-Scargle periodogram (LSP)",
                "fast Gaussian gridding (FGG)",
                "embedded system evaluation"
            ]
        },
        "title": "Energy-Efficient Spectral Analysis of ECGs on Resource Constrained IoT Devices"
    },
    {
        "authors": [
            "Deyu Luo",
            "Gang Sun",
            "Hongfang Yu",
            "Mohsen Guizani"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "25 October 2024",
        "doi": "10.1109/JIOT.2024.3486331",
        "publisher": "IEEE",
        "abstract": "In the Internet of Things (IoT), blockchain-based cross-domain authentication schemes can effectively establish trust and share data across different administrative domainss. However, current blockchain-based cross-domain authentication solutions often overlook dynamic node participation challenges, which is crucial for the flexible IoT environment. In this study, we propose a blockchain-based cross-domain authentication scheme that supports dynamic node participation, allowing administrative domainss to freely join or leave under legal conditions. Firstly, we introduce an efficient blockchain-based cross-domain authentication framework. Secondly, we propose a blockchain consensus algorithm that supports dynamic node participation to serve the aforementioned framework. Specifically, inspired by the Transmission Control Protocol (TCP) protocol’s piggybacking strategy, this algorithm integrates the process of node joining and leaving into the regular consensus flow to enhance efficiency. To further improve the algorithm’s efficiency, we designed compressed block and parallel chain structures to increase bandwidth utilization and throughput. Detailed correctness proofs demonstrate the algorithm’s security. Extensive experiments have been conducted to show that our scheme increases throughput by approximately 8x compared to existing approaches.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Authentication",
                "Blockchains",
                "Internet of Things",
                "Heuristic algorithms",
                "Security",
                "Servers",
                "Public key",
                "Consensus algorithm",
                "Information sharing",
                "Protocols"
            ],
            "Author Keywords": [
                "Internet of Things",
                "blockchain",
                "cross-domain authentication",
                "dynamic",
                "parallelization",
                "efficiency improvement"
            ]
        },
        "title": "Blockchain-Based Cross-Domain Authentication With Dynamic Domain Participation in IoT"
    },
    {
        "authors": [
            "Osama Al-Khaleel",
            "Selçuk Baktir",
            "Mohammad Al-Khaleel",
            "Alptekin Küpçü"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "11 November 2024",
        "doi": "10.1109/ACCESS.2024.3495995",
        "publisher": "IEEE",
        "abstract": "Elliptic Curve Cryptography (ECC) provides high security with shorter key sizes and higher performance. Internet of Things (IoT) is the interconnected network where a vast number of connected devices require high security to ensure data integrity and privacy. Field programmable gate arrays (FPGAs) are configurable hardware fabrics enhancing the performance of applications. We propose two FPGA-based high performance and lightweight ECC processors for IoT devices. One of the ECC processor is serial with less area and the other is parallel with higher speed. The proposed ECC processors use Edwards curves and perform frequency domain multiplication utilizing FPGA-embedded digital signal processors (DSPs) or lookup tables. The proposed ECC processors work over prime characteristic finite fields of the form GF ((2 n -1) n ). Three such prime characteristic finite fields are investigated for each processor: n = 13, 17 and 19, providing the key lengths of 169, 289 and 361 bits, respectively. Synthesis results, targeting different FPGAs, show that the proposed ECC processors outperform similar existing ECC processors in terms of area and speed. For example, over Virtex5 FPGA, compared to the lowest area ECC implementations in the literature over the same finite fields, our serial ECC processor without DSPs occupies 19.07% to 26.06% less slices while being 2.17× to 3.57× faster. Compared to the fastest ECC implementations, our parallel ECC processor without DSPs occupies 20.42% to 27.38% less slices while being 1.76× to 2.22× faster, and our parallel ECC processor that use DSPs is 1.93× to 2.50× faster.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Elliptic curve cryptography",
                "Galois fields",
                "Elliptic curves",
                "Field programmable gate arrays",
                "Internet of Things",
                "Cryptography",
                "Frequency-domain analysis",
                "Performance evaluation",
                "Hardware",
                "Software"
            ],
            "Author Keywords": [
                "Elliptic Curve Cryptography",
                "Edwards Curve",
                "embedded multiplier",
                "FPGA",
                "discrete Fourier transform",
                "modular multiplication"
            ]
        },
        "title": "Efficient ECC Processor Designs for IoT Using Edwards Curves and Exploiting FPGA Embedded Components"
    },
    {
        "authors": [
            "Chaitanya Thuppari",
            "Srikanth Jannu",
            "Damodar Reddy Edla",
            "Anand Kumar Mishra",
            "Debjani Ghosh",
            "Ankit Vidyarthi",
            "Daniel Gavilanes Aray"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "01 October 2024",
        "doi": "10.1109/TCE.2024.3471897",
        "publisher": "IEEE",
        "abstract": "Continuous monitoring is crucial for the early detection of mine fires in underground coal mines (UCMs). Internet of Things (IoT), is widely used for continuous monitoring of the environment of UCMs. However, coverage and connectivity issues among the nodes in (UCM) pose challenges for sustained monitoring and communication. To address this, the strategy involves partitioning the UCM using designated target points. The key idea is to confirm that nodes adequately cover these predetermined targets, ensuring comprehensive coverage and connectivity throughout the entire UCM. This study addresses the challenges of node coverage and connectivity in UCMs by proposing a novel k-coverage and m-connectivity model using the Fish Swarm Optimization (FSO) algorithm. The methodology involves partitioning the UCM into designated target points and deploying nodes to ensure comprehensive coverage and connectivity. Sensor nodes gather local environmental data, which is then processed to predict potential fire hazards using a logistic regression model (LRM). The FSO algorithm optimizes node deployment by achieving k-coverage, where each target point is covered by at least k nodes, and m-connectivity, ensuring robust communication routes even when some connections fail. A fitness function is formulated to minimize node count, maximize coverage, and maintain connectivity. The proposed method demonstrates high accuracy and efficiency in simulations, outperforming traditional linear regression and Naive Bayes models with an accuracy of 98% in fire prediction. This approach is scalable and can be adapted for various Consumer IoT applications to enhance safety and operational efficiency.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Fuel processing industries",
                "Fish",
                "Predictive models",
                "Prediction algorithms",
                "Monitoring",
                "Visualization",
                "Coal mining",
                "Relays",
                "Particle swarm optimization",
                "Internet of Things"
            ],
            "Author Keywords": [
                "Coal mines",
                "fish swarm optimization",
                "Logistic regression",
                "Internet of things",
                "Fire prediction"
            ]
        },
        "title": "A Novel k-Coverage and m-Connectivity based Fire Prediction Model for Consumer IoT Using Fish Swarm Optimization and Logistic Regression Model"
    },
    {
        "authors": [
            "Ting Li",
            "Yinlong Liu",
            "Tao Ouyang",
            "Hangsheng Zhang",
            "Kai Yang",
            "Xu Zhang"
        ],
        "published_in": "Published in: IEEE Transactions on Mobile Computing ( Early Access )",
        "date_of_publication": "17 September 2024",
        "doi": "10.1109/TMC.2024.3462731",
        "publisher": "IEEE",
        "abstract": "To bridge the gap of conventional single-hop task offloading schemes in infrastructure-free scenarios, multi-hop task offloading schemes for IoT devices in Mobile Edge Computing (MEC) are desired to jointly optimize task offloading decisions and routing paths. In this paper, we investigate a hierarchical multi-hop edge computing framework and propose a joint Task Offloading and Relay Selection (TORS) scheme. It considers realtime computation at each relay node and employs directional searches to facilitate the task execution and results reporting at the fastest speed. However, finding the optimal TORS solution is a formidable challenge due to the time-varying network environments, the strong interdependence of decision sets across different time slots, and the high computational complexity. To address these challenges, we first leverage Lyapunov optimization to transform the stochastic TORS problem into a deterministic per-slot block problem, avoiding the need for extensive system prior knowledge. Subsequently, we propose a Soft Actor-Critic (SAC)-based algorithm, SAC-TORS, to find a satisfactory TORS solution with minimal computational complexity in a distributed manner. Accordingly, each IoT device can independently make self-determined and directional decisions with observable network information. Through extensive experiments, we demonstrate that the SAC-TORS outperforms state-of-the-art solutions, achieving performance improvements of up to 66%.",
        "issn": {
            "Print ISSN": "1536-1233",
            "Electronic ISSN": "1558-0660"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Spread spectrum communication",
                "Relays",
                "Routing",
                "Heuristic algorithms",
                "Delays",
                "Optimization"
            ],
            "Author Keywords": [
                "Deep reinforcement learning",
                "lyapunov optimization",
                "mobile edge computing",
                "relay",
                "task offloading"
            ]
        },
        "title": "Multi-Hop Task Offloading and Relay Selection for IoT Devices in Mobile Edge Computing"
    },
    {
        "authors": [
            "Ateeq Ur Rehman",
            "Mahnoor Farooq",
            "Fazlullah Khan",
            "Gautam Srivastava",
            "Rakan Ameen Aldmour",
            "Ryan Alturki",
            "Bandar Alshawi"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "10 September 2024",
        "doi": "10.1109/JIOT.2024.3457372",
        "publisher": "IEEE",
        "abstract": "Federated Learning (FL) has emerged as a pivotal technology for the Internet of Things (IoT) that models distributed client data without compromising privacy. The IoT-based wearable generates data and FL running on a private edge performing Human Activity Recognition (HAR). In this paper, we proposed a novel technique to protect sensitive data during the training process and ensure the confidentiality of model updates before transmission to the edge server. The proposed technique integrates the El-Gamal encryption technique for data protection, and the FL process is rigorously optimized using Pruning, Quantization, and Network Slicing. Pruning removes redundant connections, which reduces model complexity and communication delays. On the other hand, Quantization decreases the bit precision of model parameters, and Network Slicing strategically allocates resources solely for FL resulting in low latency and optimal bandwidth utilization. The results are evaluated in terms of accuracy and communication overhead, which is highly required in real-world applications. Furthermore, the HAR system within PEC shows better results by achieving an accuracy of 99% at 300 epochs that outperformed existing Machine Learning (ML) algorithms.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Human activity recognition",
                "Training",
                "Privacy",
                "Servers",
                "Data privacy",
                "Quantization (signal)",
                "Data models"
            ],
            "Author Keywords": [
                "Internet of Things",
                "Federated Learning",
                "Private Edge Computing",
                "Human Activity Recognition",
                "El-Gamal",
                "Quantization",
                "Pruning",
                "Network Slicing",
                "Privacy Protection"
            ]
        },
        "title": "FEDge-HAR: An Optimized Private Mobile Edge-Enabled IoT Paradigm for Privacy of Human Activity Recognition"
    },
    {
        "authors": [
            "Jundong Feng",
            "Jia Ai",
            "Fangjie Li",
            "Junchao Wang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "21 August 2024",
        "doi": "10.1109/JIOT.2024.3446813",
        "publisher": "IEEE",
        "abstract": "Chaotic encryption is applied as a lightweight symmetric cryptography in Internet of Things (IoT). In symmetric encryption, system security depends on the randomness of the encryption sequence. However, the chaotic algorithm can’t be efficiently implemented with high precision in IoT resource-constrained circumstance. The cyclic effect of the algorithm under limited precision makes the sequence randomness not enough, while the exponential enhance in overhead by increasing the precision is not satisfy the lightweight goal. Therefore, a reconfigurable chaotic encryption ASIC with dynamic precision is proposed. Initially, a reconfigurable chaotic combination strategy based on multidimensional perturbations is proposed to improve sequence randomness. Secondly, a reconfigurable architecture with resource sharing and specified computation units are designed to effectively reduce the overhead of the algorithm hardware implementation. In addition, dynamic precision architecture is employed to provide trade-off between precision and power consumption. Finally, the security results demonstrate that the scheme achieves satisfactory various metrics. The synthesis results by using 65 nm CMOS technology shows that its area is only 11.8k μm2, with 179.6 μW power and 11.2 pJ/bit energy efficiency at 50 MHz, which achieves lower overhead compared to the existing hybrid chaotic encryption designs.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Encryption",
                "Hardware",
                "Internet of Things",
                "Ciphers",
                "Chaotic communication",
                "Perturbation methods",
                "Heuristic algorithms"
            ],
            "Author Keywords": [
                "Reconfigurable",
                "Dynamic Precision",
                "Chaotic Encryption",
                "ASIC"
            ]
        },
        "title": "A 11.2 pJ/bit Reconfigurable Dynamic Chaotic Encryption ASIC for IoT"
    },
    {
        "authors": [
            "Alian Yu",
            "Jian Kang",
            "Wei Jiang",
            "Dan Lin"
        ],
        "published_in": "Published in: IEEE Transactions on Dependable and Secure Computing ( Early Access )",
        "date_of_publication": "22 April 2024",
        "doi": "10.1109/TDSC.2024.3392262",
        "publisher": "IEEE",
        "abstract": "With the continuous advancement of sensing, networking, controlling, and computing technologies, there is a growing number of IoT (Internet of Things) devices emerging that are expected to integrate into public infrastructure in the near future. However, the deployment of these smart devices in public venues presents new challenges for existing access control mechanisms, particularly in terms of efficiency. To address these challenges, we have developed a highly efficient and scalable access control mechanism that enables automatic and fine-grained access control management while incurring low overhead in large-scale settings. Our mechanism includes a dual-hierarchy access control structure and associated information retrieval algorithms, which we have used to develop a large-scale IoT device access control system called FACT+. FACT+ overcomes the efficiency issues of granting and inquiring access control status over millions of devices in pervasive environments. Additionally, our system offers a pay-and-consume scheme and plug-and-play device management for convenient adoption by service providers. We have conducted extensive experiments to demonstrate the practicality, effectiveness, and efficiency of our access control mechanism.",
        "issn": {
            "Print ISSN": "1545-5971",
            "Electronic ISSN": "1941-0018"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Access control",
                "Biological system modeling",
                "Authorization",
                "Task analysis",
                "Standards",
                "Scalability"
            ],
            "Author Keywords": [
                "Scalable Access Control",
                "Large Scale",
                "Internet of Things"
            ]
        },
        "title": "Highly Efficient and Scalable Access Control Mechanism for IoT Devices in Pervasive Environments"
    },
    {
        "authors": [
            "Ying Gao",
            "Yue Ge",
            "Jianting Ning",
            "Jie Ma",
            "Xiaofeng Chen"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "10 September 2024",
        "doi": "10.1109/JIOT.2024.3457270",
        "publisher": "IEEE",
        "abstract": "The Internet of Things (IoT) boom has enabled massive data collection in cloud servers. Therefore, access efficiency and data privacy in cloud storage services have become a significant concern. Data and users are hierarchical in IoT applications, which require fine-grained multi-level access control. Additionally, achieving public verification to resist the malicious server and clients is indispensable. Aiming at the challenge above, we propose a new forward private multi-level dynamic searchable symmetric encryption (DSSE) scheme called Peony, employing multi-level linked lists and constrained pseudorandom function, which is more efficient and secure. Then, we introduce a cryptographic primitive named multi-level symmetric revocable encryption (MSRE), and we give a general method for constructing a novel forward and type-II backward-private multi-level DSSE scheme Peony++ based on MSRE. Further, we design the multi-level digests and utilize the smart contract as a trusted platform to support public verification for Peony++. Theoretical analysis and experimental evaluations show that Peony achieves higher security and reduces search time by an average of 35.81% compared to the state-of-the-art multi-level DSSE scheme. To the best of our knowledge, Peony++ is the only multi-level searchable encryption currently available that can achieve forward and type-II backward privacy, all while balancing efficiency and functionality.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Privacy",
                "Encryption",
                "Servers",
                "Smart contracts",
                "Security",
                "Cloud computing",
                "Internet of Things"
            ],
            "Author Keywords": [
                "Access control",
                "dynamic searchable symmetric encryption",
                "forward and backward privacy",
                "smart contract",
                "verification"
            ]
        },
        "title": "Verifiable Multi-Level Dynamic Searchable Encryption With Forward and Backward Privacy in Cloud-Assisted IoT"
    },
    {
        "authors": [
            "Gabriel Augusto David",
            "Paulo Monteiro de Carvalho Monson",
            "Cristiano Soares",
            "Pedro de Oliveira Conceição",
            "Paulo Roberto de Aguiar",
            "Alessandro Simeone"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "21 August 2024",
        "doi": "10.1109/JIOT.2024.3447579",
        "publisher": "IEEE",
        "abstract": "Industrial demand forecasting is crucial for managing stock levels, financial revenue projections, logistics, and efficient production operations. However, there are notable gaps in the literature, particularly in the research that explores the potential of intelligent monitoring systems using Internet of Things (IoT) devices in combination with industrial production forecasting algorithms. This gap is particularly evident in scenarios characterized by seasonal data and limited historical samples. This research work presents an affordable method for monitoring and forecasting industrial production demand, which involves developing an IoT device and applying deep learning algorithms to an industrial production line. In the proposed methodology, an edge device, known as a datalogger, collects and transmits production data to a cloud server obtained through photoelectric laser beam sensors and the PLC of a filling machine. The data is subsequently retrieved from the cloud database for analysis and production demand forecasting. This work compares the performance of three distinct neural networks–sxLSTM, GRU, and SCINet-in the context of time series forecasting. The evaluation employs metrics such as root mean squared error (RMSE), mean absolute error (MAE), and mean absolute percentage error (MAPE). Through this framework, it was possible to achieve to forecast production demand with error rates of 9.08 (RMSE), 7.67 (MAE), and 1.86 (MAPE), due to the SCINet neural network, even in scenarios with seasonal samples and limited historical data from an industrial filling line.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Production",
                "Computer architecture",
                "Predictive models",
                "Monitoring",
                "Time series analysis",
                "Long short term memory",
                "Demand forecasting"
            ],
            "Author Keywords": [
                "industrial internet of things",
                "time-series",
                "deep learning",
                "forecasting",
                "SCINet"
            ]
        },
        "title": "IoT-Driven Deep Learning for Enhanced Industrial Production Forecasting"
    },
    {
        "authors": [
            "Zhendong Song",
            "Tao Gong",
            "Menglin Xie",
            "Jinda Luo",
            "Thippa Reddy Gadekallu",
            "Mohammed Amoon",
            "Chien-Ming Chen",
            "Saru Kumari",
            "Ning Liu"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "12 September 2024",
        "doi": "10.1109/TCE.2024.3446803",
        "publisher": "IEEE",
        "abstract": "The integration of wearable healthcare devices and fog computing has emerged as a promising paradigm for providing personalized and efficient healthcare solutions. However, the security and privacy concerns associated with the sensitive health data transmitted in such systems pose significant challenges. This paper proposes a secure and efficient fog-assisted wearable healthcare Internet of Things (IoT) system, which leverages physical layer security techniques and quantum-inspired resource allocation to enhance the confidentiality and performance of the system. First, we present a system model for a secure fog-assisted wearable healthcare IoT system, incorporating wearable healthcare devices, fog computing servers, and potential eavesdroppers. Next, we formulate an optimization problem with the goal of maximizing the secrecy rate of the system by allocating both spectral and computing resources jointly. Ultimately, we develop a bean optimization method influenced by quantum principles to address the issue of allocating resources collaboratively. The experimental findings show the efficacy of the suggested system and resource allocation strategy regarding secrecy rate performance under various scenarios.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Medical services",
                "Biomedical monitoring",
                "Wearable devices",
                "Servers",
                "Resource management",
                "Optimization",
                "Edge computing"
            ],
            "Author Keywords": [
                "Wearable healthcare devices",
                "Internet of Things",
                "physical layer security",
                "quantum-inspired optimization"
            ]
        },
        "title": "Secure and Efficient Fog-Assisted Quantum-Inspired Wearable Healthcare Consumer Electronics IoT System"
    },
    {
        "authors": [
            "Sundeep Kumar",
            "Viren Sharma",
            "Ashwani Sharma"
        ],
        "published_in": "Published in: IEEE Sensors Letters ( Early Access )",
        "date_of_publication": "16 October 2024",
        "doi": "10.1109/LSENS.2024.3482409",
        "publisher": "IEEE",
        "abstract": "This letter proposes a simultaneous wireless information and power transfer (SWIPT) module capable of operating a battery-less ultra-low-power wireless sensor node (WSN). This module features a miniaturized circular patch antenna that utilizes a proximity-coupled feed to enable wireless information transfer (WIT) in the 5 GHz Wi-Fi band (5.15 GHz − 5.825 GHz). The WIT operation can be powered through wireless power transfer (WPT) to the proposed module using a capacitive coupled feed. The WPT is realized using a dedicated RF shower, which transmits RF energy in 4.9 GHz-5.1 GHz band. The different frequency bands are chosen for WIT and WPT to reduce the mutual coupling effect. Moreover, the use of orthogonal-polarized feeds for WIT and WPT further enhances the isolation between information and power signals. The design incorporates two conjugate matched shunt Schottky diodes, a stepped impedance low pass filter, and a parallel DC combining network for full-wave rectification (FWR) of the incident RF waves. Therefore, the proposed module ensures high power conversion efficiency (PCE) with a smaller footprint making it an ideal candidate for implementing battery-less WSN operation for IoT applications.",
        "issn": {
            "Electronic ISSN": "2475-1472"
        },
        "keywords": {
            "IEEE Keywords": [
                "Radio frequency",
                "Simultaneous wireless information and power transfer",
                "Feeds",
                "Sensors",
                "Wireless sensor networks",
                "Impedance",
                "Antennas",
                "Inductors",
                "Signal to noise ratio",
                "Schottky diodes"
            ],
            "Author Keywords": [
                "Dual-band",
                "WIT",
                "WPT",
                "FWR",
                "IoT",
                "SWIPT",
                "shunted Schottky diodes",
                "cross-polarized"
            ]
        },
        "title": "SWIPT Module for Battery Less Ultra Low Power Sensor Nodes"
    },
    {
        "authors": [
            "Linta Antony",
            "Abel Zandamela",
            "Nicola Marchetti",
            "Adam Narbudowicz"
        ],
        "published_in": "Published in: IEEE Antennas and Wireless Propagation Letters ( Early Access )",
        "date_of_publication": "09 August 2024",
        "doi": "10.1109/LAWP.2024.3441513",
        "publisher": "IEEE",
        "abstract": "Phased arrays are pivotal for determining the angle of arrival (AoA) of signals. However, they are impractical for size and weight-constrained platforms. Recently developed spherical modes (SM) multiport antennas offer a compact beamforming solution due to their ability to generate multiple radiating modes within a confined volume. This work exploits this methodology by introducing a sparse sensing (SS) technique to generate additional virtual SM, increasing the degrees of freedom (DoF) of AoA detection. It is demonstrated that SS with multiple signal classification (MUSIC), and estimation of signal parameters via rotational invariance technique (ESPRIT) algorithms can identify up to six uncorrelated sources with a five-port SM beamforming antenna, resulting in the number of simultanously detected sources being greater than the number of ports used. The technique allows for increased accuracy, computational efficiency, and DoF for AoA estimation, highlighting its potential for internet of things (IoT) applications.",
        "issn": {
            "Print ISSN": "1536-1225",
            "Electronic ISSN": "1548-5757"
        },
        "keywords": {
            "IEEE Keywords": [
                "Antennas",
                "Estimation",
                "Receiving antennas",
                "Vectors",
                "Multiple signal classification",
                "Covariance matrices",
                "Adaptive arrays"
            ],
            "Author Keywords": [
                "AoA estimation",
                "ESPRIT",
                "MIMO",
                "MUSIC",
                "Small IoT platforms",
                "Sparse sensing",
                "Spherical modes"
            ]
        },
        "title": "Virtual Spherical Modes for AoA Estimation with Small Sub-Wavelength Antennas"
    },
    {
        "authors": [
            "Vishnu Padmakumar",
            "Titu Mary Ignatius",
            "Thockchom Birjit Singha",
            "Roy Paily Palathinkal",
            "Shaik Rafi Ahamed"
        ],
        "published_in": "Published in: IEEE Embedded Systems Letters ( Early Access )",
        "date_of_publication": "27 June 2024",
        "doi": "10.1109/LES.2024.3420226",
        "publisher": "IEEE",
        "abstract": "Advanced Encryption Standard’s (AES) vulnerabilities surfaced with Power Side Channel Attacks (PSCA). Enhancing security by adding extra countermeasure circuitry introduces significant hardware overheads, which are impractical for resource-constrained Internet of Things (IoT) edge devices. This study proposes an alternative approach, focusing on the AES design itself to enable lightweight countermeasures. Targeting the SubBytes round operation as the vulnerable point, the operation is split across different clock cycles to minimize side-channel information leakage. We investigated 12-clock, 22-clock, 42-clock, 82-clock, and 162-clock AES designs among which the 82-clock version stands out as the optimal choice, providing efficient hardware resource utilization. Evaluation using hardware security metrics, such as Measurements To Disclose (MTD) and Signal-to-Noise Ratio (SNR), confirms its superior security and reduced information leakage compared to other designs. Power traces for attacks are generated on both Application Specific Integrated Circuit (ASIC) and Field Programmable Gate Array (FPGA) platforms, maintaining a consistent 16 MHz design frequency with traces sampled at 1 GSa/s.",
        "issn": {
            "Print ISSN": "1943-0663",
            "Electronic ISSN": "1943-0671"
        },
        "keywords": {
            "IEEE Keywords": [
                "Clocks",
                "Security",
                "Signal to noise ratio",
                "Field programmable gate arrays",
                "Throughput",
                "Resilience",
                "Information leakage"
            ],
            "Author Keywords": [
                "IoT",
                "PSCA",
                "AES",
                "SubBytes",
                "Countermeasure",
                "intrinsic resilience",
                "CPA",
                "MTD",
                "SNR",
                "ASIC",
                "FPGA"
            ]
        },
        "title": "Boosting AES Intrinsic Resilience Using Split SubBytes Round Function Against Power Attacks"
    },
    {
        "authors": [
            "Nitinkumar Shingari",
            "Beenu Mago"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "17 June 2024",
        "doi": "10.1109/TCE.2024.3415157",
        "publisher": "IEEE",
        "abstract": "With the growing dependence of consumer devices on the Internet of Things (IoT), the need for cybersecurity has increased to combat common injection and functional replica threats. Inadequate ports in the consumer electronics network render these devices vulnerable to cyber assaults, requiring strong security measures. This paper presents a new method, the Beta Control Access Scheme (β-CAS), to strengthen closed IoT-consumer electronics loops. Using a different access control (β), this system efficiently fixes corrupted electronic circuits by blocking illegal access and restoring normal control using IoT records. The complex procedure requirements and control flow are carefully observed and guided by dual-layer neural networks. The first layer aims to reduce unwanted access and rapidly terminates the initial control (α) when unlawful access is detected. The next processing layer retrieves the most recent control logs from the IoT and then executes an access control reset using the main goal’s reverse time. Authentication in this approach depends on traditional key/fingerprint verification for setting up and restoring control of electronic devices. The robust computational procedures for security validation and record retrieval are made possible with IoT, delivering a complete and efficient cybersecurity architecture.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Security",
                "Internet of Things",
                "Consumer electronics",
                "Authentication",
                "Computer security",
                "Smart homes",
                "Blockchains"
            ],
            "Author Keywords": [
                "Cybersecurity",
                "Consumer electronics",
                "Beta Control Access Scheme",
                "Injection attacks",
                "Security validation",
                "IoT logs"
            ]
        },
        "title": "Securing Consumer Electronics Device Loops in the Internet of Things Through β Control Access Scheme"
    },
    {
        "authors": [
            "Aryan Rana",
            "Deepika Gautam",
            "Pankaj Kumar",
            "Ashok Kumar Das"
        ],
        "published_in": "Published in: IEEE Communications Surveys & Tutorials ( Early Access )",
        "date_of_publication": "04 July 2024",
        "doi": "10.1109/COMST.2024.3423477",
        "publisher": "IEEE",
        "abstract": "The Internet of Things (IoT) has profoundly impacted today’s world. Recent technological advancements in the Internet and Information Communication (ICT) have paved the way for revolutionary technologies, making life easier for humankind. IoT has found applications in almost every sector, enabling the connectivity of digital objects to the Internet. This technology has undergone significant advancements, leading to the emergence of a new paradigm called the Internet of Nano Things (IoNT). IoNT represents the latest evolution of IoT and has garnered attention from researchers and scientists. By combining nanotechnology with IoT, IoNT offers a more advanced paradigm than its generic counterpart, potentially reshaping the current IoT landscape. However, these limitless benefits come with challenges that persist in both IoT and the emerging IoNT ecosystem. Moreover, additional challenges need to be addressed due to the miniature nature and novelty of the IoNT ecosystem. The lack of research in the field of IoNT has motivated us to undertake this study. Our research aims to highlight the security and privacy challenges specific to IoNT and nano communication. Furthermore, we examine important architectures related to IoNT, nanoscale communication, real-life applications of IoNT, as well as simulators and testbeds useful for IoNT. We also address challenges associated with the incorporation of novel technologies, primarily Artificial Intelligence (AI), into IoNT. To achieve this, we provide a detailed review of existing works related to the IoNT ecosystem, emphasizing security and privacy concerns. Additionally, we explore the opportunities and future directions for IoNT, serving as a reference for further research.",
        "issn": {
            "Electronic ISSN": "1553-877X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Security",
                "Nanoscale devices",
                "Surveys",
                "Internet of Things",
                "Privacy",
                "Artificial intelligence",
                "Tutorials"
            ],
            "Author Keywords": [
                "Internet of Things (IoT)",
                "Internet of Nano Things (IoNT)",
                "Internet and Information Communication (ICT)",
                "Nanoscale Communication",
                "Security",
                "Privacy",
                "Artificial Intelligence (AI)"
            ]
        },
        "title": "Architectures, Benefits, Security and Privacy Issues of Internet of Nano Things: A Comprehensive Survey, Opportunities and Research Challenges"
    },
    {
        "authors": [
            "Geetha Anbazhagan",
            "M. Maragatharajan",
            "Tai-hoon Kim"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "18 June 2024",
        "doi": "10.1109/TCE.2024.3415757",
        "publisher": "IEEE",
        "abstract": "Mobile consumer devices have become essential components of modern society, but their reliance on rechargeable batteries creates issues with overcharging, overdischarging, and battery health. The demand for effective and privacy-aware battery management becomes even more obvious in view of the fast changing environment of Smart IoT devices. Reliable power supplies and predicted battery performance are essential for the seamless integration of these smart IoT devices into daily life. This research offers a novel recurrent neural network (RNN) based training method for battery management systems in mobile consumer electronics and smart IoT devices. This approach addresses the heterogeneous ecosystem of smart IoT devices by allowing model training across decentralized data sources while protecting user data privacy by Federated Split Learning (FSL). Sharing extensive model parameters from these devices is restricted by FSL’s emphasis on privacy. To overcome this, it is suggested to break the RNN model into smaller sub-networks, each of which would be trained using data segments from various consumer electronics products and intelligent IoT sensors. Following their transmission to a federated server for aggregation, these sub-networks help advance mobile technology, consumer electronics, and the larger Smart IoT ecosystem by producing a more reliable RNN model for battery performance prediction.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Batteries",
                "Cloud computing",
                "Internet of Things",
                "State of charge",
                "Consumer electronics",
                "Data models",
                "Voltage"
            ],
            "Author Keywords": [
                "IoT",
                "Smart Energy Management",
                "Split Learning",
                "Smart Phone"
            ]
        },
        "title": "Smart Battery Management System for Mobile Consumer Electronics based on Federal Split Learning Model"
    },
    {
        "authors": [
            "Siva Sai",
            "Vinay Chamola"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "26 June 2024",
        "doi": "10.1109/TCE.2024.3418963",
        "publisher": "IEEE",
        "abstract": "Distributed machine learning in healthcare has great potential in training models to learn from the patients’ data distributed across different medical institutions. Recently, there has been a significant surge in research works applying federated learning(FL), a popular distributed machine learning technique in the healthcare sector. However, FL faces challenges like communication overhead and scalability to low-resource devices like Internet of Things(IoT) nodes. Addressing these issues, we propose a Blockchain-enabled split learning framework with a novel client selection algorithm for collaborative learning in healthcare. In the split learning model, the neural network is trained between the server and the clients, and the forward and backward propagation steps to update the weights happen in a collaborative way. The Blockchain platform serves the functions of decentralized model governance, decentralized identity and access management, incentive management, and client selection governance in the proposed framework. We proposed a comprehensive client selection algorithm incorporating several client features like deadline strictness, resource availability, data utility, model utility, etc. The experimental results show that the proposed split learning model achieves better results than the federated learning and cloud-centric machine learning models. Further, we also provide a hardware implementation for the proposed framework to gauge its real-world deployment feasibility.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Servers",
                "Data models",
                "Computational modeling",
                "Medical services",
                "Federated learning",
                "Neural networks",
                "Blockchains"
            ],
            "Author Keywords": [
                "Split Learning",
                "Blockchain",
                "Client selection"
            ]
        },
        "title": "A Blockchain-Enabled Split Learning Framework With a Novel Client Selection Method for Collaborative Learning in Smart Healthcare"
    },
    {
        "authors": [
            "Bowen Gu",
            "Dong Li",
            "Haiyang Ding",
            "Gongpu Wang",
            "Chintha Tellambura"
        ],
        "published_in": "Published in: IEEE Communications Surveys & Tutorials ( Early Access )",
        "date_of_publication": "31 July 2024",
        "doi": "10.1109/COMST.2024.3436082",
        "publisher": "IEEE",
        "abstract": "As the Internet of Things (IoT) advances by leaps and bounds, a multitude of devices are becoming interconnected, marking the onset of an era where everything is connected. While this growth opens up opportunities for novel products and applications, it also leads to increased energy reliance on IoT devices, creating a significant bottleneck that hinders sustainable progress. Backscatter communication (BackCom), as a low-power and passive communication technology, emerges as one of the promising solutions to this energy impasse by reducing the manufacturing cost and energy consumption for IoT devices. However, BackCom systems also face some challenges, such as complex interference environments, including the direct-link interference (DLI) and the mutual interference (MI) between tags, which severely disrupt the efficiency of BackCom networks. Moreover, the double-path fading is another major issue that leads to a degraded system performance. To fully unleash the potential of BackComs, the purpose of this paper is to furnish a comprehensive review of existing solutions with a focus on addressing these challenges, offering an insightful analysis and comparison of various strategies. Specifically, we begin by introducing the preliminaries for BackCom, including its history, operating mechanisms, main architectures, etc., providing a foundational understanding of this field. Then, we delve into fundamental issues related to BackCom systems, such as solutions for the DLI, the MI, and the double-path fading. This paper thoroughly provides state-of-the-art advances for each case, particularly highlighting how the latest innovations in theoretical approaches and system design can strategically address these challenges. Finally, we explore emerging trends and challenges in BackComs by forecasting potential technological advancements and providing insights and guidelines for navigating the intricate landscape of future communication needs in a rapidly evolving IoT ecosystem.",
        "issn": {
            "Electronic ISSN": "1553-877X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Radio frequency",
                "Backscatter",
                "Batteries",
                "Surveys",
                "Fading channels",
                "Tutorials"
            ],
            "Author Keywords": [
                "IoT",
                "backscatter communication",
                "performance enhancement",
                "direct-link interference",
                "mutual interference",
                "double-path fading"
            ]
        },
        "title": "Breaking the Interference and Fading Gridlock in Backscatter Communications: State-of-the-Art, Design Challenges, and Future Directions"
    },
    {
        "authors": [
            "Tianyu Tu",
            "Zhili He",
            "Zhigao Zheng",
            "Zimu Zheng",
            "Jiawei Jiang",
            "Yili Gong",
            "Chuang Hu",
            "Dazhao Cheng"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "06 May 2024",
        "doi": "10.1109/JIOT.2024.3396282",
        "publisher": "IEEE",
        "abstract": "With the rapid development of the Internet of Things (IoT), IoT devices find applications in various domains. The data generated by these devices is utilized for analysis and services, especially in the field of Artificial Intelligence (AI) applied to IoT, known as Artificial Intelligence of Things (AIoT). The enhancement of edge device computing power in the IoT has led to the emergence of research areas like edge-cloud synergy AI theories and application services. In the context of lifelong learning and real-time processes in AIoT edge-cloud synergy services, addressing unseen tasks becomes crucial. Unseen tasks arise when inference requests from edge devices involve models not present in the cloud’s model repository. Addressing these challenges involves generating data to either augment small sample problems or alter the data distribution for heterogeneous sample issues. As the application of large language models (LLMs) for data generation gains traction, challenges emerge in the context of AIoT edge-cloud synergy services. Firstly, fine-tuning LLMs with heterogeneous data exacerbates model bias issues. Secondly, the substantial data requirements for training LLMs pose a contradiction. Lastly, the involvement of manual annotation in LLM-based data generation introduces complexity and cost. This paper proposes a framework Seafarer to these challenges using Generative Adversarial Networks and Self-taught Learning. Seafarer avoids model bias, reduces data requirements, and eliminates the need for manual annotation. The design demonstrates effectiveness theoretically and is validated on the Cityscapes dataset, achieving an 80% reduction in training loss and improved validation loss stability.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Task analysis",
                "Artificial intelligence",
                "Internet of Things",
                "Training",
                "Cloud computing",
                "Manuals",
                "Data mining"
            ],
            "Author Keywords": [
                "Edge-Cloud Synergy AI",
                "Lifelong Learning",
                "Internet of Things (IoT)",
                "Artificial Intelligence of Things (AIoT)"
            ]
        },
        "title": "Towards Lifelong Unseen Task Processing With a Lightweight Unlabeled Data Schema for AIoT"
    },
    {
        "authors": [
            "Khatereh Nadali",
            "Neeraj Kumar Maurya",
            "Patrick McEvoy",
            "Max J. Ammann"
        ],
        "published_in": "Published in: IEEE Open Journal of Antennas and Propagation ( Early Access )",
        "date_of_publication": "30 May 2024",
        "doi": "10.1109/OJAP.2024.3407050",
        "publisher": "IEEE",
        "abstract": "The growing prevalence of IoT devices has heightened the focus on minimizing energy consumption in low-power electronic systems. Ambient radio frequency (RF) energy harvesting allows self-sustaining electronic devices to operate in IoT networks without cables, batteries, or allocated energy sources. This paper presents an innovative printed dual-band dual-polarized antenna designed for ambient RF energy harvesting in IoT applications. The proposed antenna targets efficient RF energy conversion from the most prevalent cellular frequency bands (sub-1 GHz and 1800 MHz) and sub-700 MHz Digital Terrestrial Television. This antenna is a significant advancement in the field, boasting a wide circularly polarized 610-968 MHz bandwidth and a linearly polarized bandwidth covering the GSM/4G 1800 MHz band. The antenna’s integration with a rectifier circuit demonstrates its ability to convert ambient RF energy into electrical power, regardless of the source antenna orientation. This integrated antenna development significantly reduces battery dependency in low-power IoT devices, paving the way for more sustainable and versatile wireless networks. Specifically, this antenna is crafted to achieve broad CP and adequate impedance bandwidths across targeted frequencies, crucial for the diverse requirements of ambient RF energy harvesting. Such a design meets our primary objectives of capturing RF energy effectively, with its omnidirectional pattern enabling it to collect ambient RF signals from different polarizations and directions.",
        "issn": {
            "Electronic ISSN": "2637-6431"
        },
        "keywords": {
            "IEEE Keywords": [
                "Bandwidth",
                "Antennas",
                "Radio frequency",
                "Energy harvesting",
                "Antenna feeds",
                "Internet of Things",
                "Slot antennas"
            ],
            "Author Keywords": [
                "Circular polarization (CP)",
                "wideband antenna",
                "slot antenna",
                "RF energy harvesting",
                "Internet of Things (IoT)",
                "rectifier circuit",
                "power conversion efficiency (PCE)"
            ]
        },
        "title": "A Dual-Band Dual-Polarized Antenna for Harvesting in Cellular and Digital Terrestrial Television Bands"
    },
    {
        "authors": [
            "Xinming Li",
            "Meng Li",
            "Jiawei Gu",
            "Yanxue Wang",
            "Jiachi Yao",
            "Jianbo Feng"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "19 September 2024",
        "doi": "10.1109/JIOT.2024.3463718",
        "publisher": "IEEE",
        "abstract": "In intelligent fault diagnosis for construction machinery, robust and precise detection of out-of-distribution (OOD) data is crucial for enhancing operational efficiency and reducing downtime. This paper introduces the Energy-driven Graph Neural Out-of-Distribution (EGN-OOD) detector, a novel framework designed to address the complexities of OOD data in dynamic Internet of Things (IoT) environments. By integrating graph neural networks with energy-based models, our approach captures intricate fault correlations and improves the accuracy of fault diagnosis. The EGN-OOD framework uses the Maximal Information Coefficient to transform sensor-acquired vibration data, typical in IoT applications, into graph representations. This conversion produces an adjacency matrix that outlines the nonlinear interactions among different fault types. Additionally, the framework includes an energy score-based OOD detection module that redefines classifier logits to create an energy function, enabling precise differentiation between in-distribution (ID) and OOD data. To enhance model robustness in semi-supervised settings, a propagation mechanism-based energy score update scheme is implemented, iteratively refining energy values within the graph. Empirical validation on a framework for monitoring mechanical equipment bearing wear demonstrates the EGN-OOD framework’s exceptional ability to detect and diagnose various fault conditions. This validation confirms the framework’s robust generalization capabilities and precision in fault detection and underscores its integration within IoT infrastructures, facilitating smarter diagnostic processes. The results provide substantial technical support for the intelligent diagnosis of construction machinery, advancing IoT-driven solutions for sustainable and intelligent construction practices.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Data models",
                "Machinery",
                "Deep learning",
                "Internet of Things",
                "Fault diagnosis",
                "Training",
                "Reliability"
            ],
            "Author Keywords": [
                "Out-of-distribution",
                "Energy-driven detection",
                "Graph neural networks",
                "Construction machinery",
                "IoT-driven"
            ]
        },
        "title": "Energy-Propagation Graph Neural Networks for Enhanced Out-of-Distribution Fault Analysis in Intelligent Construction Machinery Systems"
    },
    {
        "authors": [
            "Muhammad Noaman Zahid",
            "Zhu Gaofeng",
            "Touseef Sadiq",
            "Hameed ur Rehman",
            "Muhammad Shahid Anwar"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "21 August 2024",
        "doi": "10.1109/ACCESS.2024.3446994",
        "publisher": "IEEE",
        "abstract": "The demand for low-cost healthcare solutions has been increasing and the use of Internet of Things (IoT) devices is becoming more common. IoT devices have the potential to monitor vital health parameters of patients remotely, providing early detection of health problems and reducing medical costs. Chipless RFID sensors, combined with IoT technology, offer cost-effective solutions for healthcare applications. These sensors have significance for remote data collection, object tracking, authentication, and security. In recent years, there has been a growing attention on the design and development of chipless sensors for monitoring health. This paper provides an extensive review of chipless RFID sensors, including their functioning, fabrication techniques, and classifications. The paper further addresses the various smart sensing materials used in chipless RFID sensors, along with challenges and future trends. The IoT-based health monitoring system permits patients to be tracked 24/7 and provides rapid health consultations from doctors in distant places. The technology also allows guardians or family members to remotely monitor patients, which is regarded as one of the most significant benefits of saving human lives. Overall, this review article provides a comprehensive investigation of the potential advantages of chipless RFID sensors and IoT-based systems for healthcare monitoring, paving the way for cost-effective, easily accessible, and reliable healthcare solutions.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Chipless RFID",
                "Medical services",
                "Radiofrequency identification",
                "Sensor phenomena and characterization",
                "Internet of Things",
                "Biomedical monitoring",
                "Tagging",
                "Wearable devices"
            ],
            "Author Keywords": [
                "healthcare",
                "health monitoring",
                "internet of things (IoT)",
                "non-contact sensing",
                "radio frequency identification (RFID)",
                "wearable tags"
            ]
        },
        "title": "A Comprehensive Study of Chipless RFID Sensors for Healthcare Applications"
    },
    {
        "authors": [
            "Yimeng Gao",
            "Tianqi Zhou",
            "Wenying Zheng",
            "Huijie Yang",
            "Tao Zhang"
        ],
        "published_in": "Published in: IEEE Transactions on Industrial Informatics ( Early Access )",
        "date_of_publication": "30 August 2024",
        "doi": "10.1109/TII.2024.3414443",
        "publisher": "IEEE",
        "abstract": "Recently, Industrial Internet of Things (IoT) has experienced significant growth and has garnered widespread interest. With industrial IoT, remote users can link and manage various sensing devices and gather instantaneous data from devices, thereby improving efficiency and productivity in industrial settings. However, challenges with existing authentication protocols in the Industrial IoT environment have been identified. To address these shortcomings, we propose a high-availability authentication and key agreement protocol for IoT-based devices, which reduces computational and communication overheads. In addition, our protocol offers privacy protection to users as the data message is transmitted by the users' pseudonym, helping to conceal their real identity. Furthermore, we also consider device dynamic updates in our proposed protocol. Security proof and analysis are provided to prove that our proposed protocol can withstand known attacks. In addition, performance analysis shows that our proposed protocol is a more efficient option than other related protocols.",
        "issn": {
            "Print ISSN": "1551-3203",
            "Electronic ISSN": "1941-0050"
        },
        "keywords": {
            "IEEE Keywords": [
                "Protocols",
                "Authentication",
                "Security",
                "Fifth Industrial Revolution",
                "Industrial Internet of Things",
                "Wireless sensor networks",
                "Sensors"
            ],
            "Author Keywords": [
                "Authentication and key (AKA) agreement",
                "device dynamic updates",
                "industrial Internet of Things (IoT)",
                "privacy protection"
            ]
        },
        "title": "High-Availability Authentication and Key Agreement for Internet of Things-Based Devices in Industry 5.0"
    },
    {
        "authors": [
            "Juan Luis Herrera",
            "Javier Berrocal",
            "Stefano Forti",
            "Antonio Brogi",
            "Juan Manuel Murillo"
        ],
        "published_in": "Published in: IEEE Transactions on Services Computing ( Early Access )",
        "date_of_publication": "28 August 2024",
        "doi": "10.1109/TSC.2024.3451239",
        "publisher": "IEEE",
        "abstract": "The advent of the Internet of Things has increased the interest in automating mission-critical processes from domains such as smart cities. These applications' stringent Quality of Service (QoS) requirements motivate their deployment through the Cloud-IoT Continuum, which requires solving the NP-hard problem of placing the application's services onto the infrastructure's devices. Moreover, as the infrastructure and application change over time, the placement needs to continuously adapt to these changes to maintain an acceptable QoS. While continuous reasoning techniques have enabled the creation of tools for these scenarios, they can have some trouble finding a feasible adaptation for abrupt and sharp changes, requiring non-adaptive techniques in those cases. Furthermore, for scenarios with smoother changes, it would be desirable to have faster algorithms to perform this placement. To explore the trade-off of effectiveness and execution times of different methods while ensuring that an application placement is found, we propose Multi-Layered Continuous Reasoning (MLCR) as an autonomic framework to adapt application placements through multiple continuous reasoning-based methods. We also present an MLCR prototype based on three methods: Faustum, MigDADO, and ConDADO. An evaluation in a realistic use case shows that MLCR is faster than traditional methods for application placement and maintains an acceptable QoS.",
        "issn": {
            "Electronic ISSN": "1939-1374"
        },
        "keywords": {
            "IEEE Keywords": [
                "Microservice architectures",
                "Quality of service",
                "Cognition",
                "Cloud computing",
                "Internet of Things",
                "Costs",
                "Smart cities"
            ],
            "Author Keywords": [
                "Internet of Things",
                "Fog Computing",
                "Quality of Service",
                "Continuous Reasoning",
                "Service Placement"
            ]
        },
        "title": "Multi-Layered Continuous Reasoning for Cloud-IoT Application Management"
    },
    {
        "authors": [
            "Matthieu Magnant",
            "Bertrand Le Gal",
            "Guillaume FerrÉ",
            "Florian Collard"
        ],
        "published_in": "Published in: IEEE Transactions on Aerospace and Electronic Systems ( Early Access )",
        "date_of_publication": "24 July 2024",
        "doi": "10.1109/TAES.2024.3431508",
        "publisher": "IEEE",
        "abstract": "The Internet of Things (IoT) has undergone a significant transformation with the introduction of low-Earth orbit (LEO) satellites as gateways. This transformation fulfills a long-standing promise of IoT, which is to enable the connectivity of objects regardless of their geographical location on Earth. Various physical communication layers have exhibited the sensitivity required for such connections. In this paper, we present a solution designed for IoT network operators employing LEO satellites to detect various uplink IoT communication technologies that share a common preamble. Our proposed approach has been implemented on three ARM cores, specifically the Cortex-A9 and Cortex-A53, which are integrated into AMD Zynq and Zynq UltraScale+ based platforms designed to meet spatial constraints. Our experiments confirm that the proposed approach exhibits real-time capabilities, even when executed on these lower-end processor targets, consuming only approximately 10% of the CPU time. Experiments were performed on both synthetic data and real traffic recordings from Eutelsat's ELO2 payload hosted in the Loft Orbital's Yam-3 LEO nanosatellite, and showed promising results.",
        "issn": {
            "Print ISSN": "0018-9251",
            "Electronic ISSN": "1557-9603"
        },
        "keywords": {
            "IEEE Keywords": [
                "Satellites",
                "Chirp",
                "Bandwidth",
                "Low earth orbit satellites",
                "Symbols",
                "Recording",
                "Orbits"
            ],
            "Author Keywords": [
                "IoT",
                "LPWAN",
                "satellite communications",
                "Low-Earth Orbit",
                "chirp signals",
                "ARM",
                "ELO2"
            ]
        },
        "title": "Implementation of a Low Earth Orbit Satellite Chirped Preamble Detector"
    },
    {
        "authors": [
            "Heng Li",
            "Ying Chen",
            "Kaixin Li",
            "Yaozong Yang",
            "Jiwei Huang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "19 August 2024",
        "doi": "10.1109/JIOT.2024.3438772",
        "publisher": "IEEE",
        "abstract": "With the swift progress of Internet of Things (IoT) technologies, the number of IoT devices has grown exponentially, leading to an increasing demand for computational power and system stability. Mobile Edge Computing (MEC) is a powerful solution that allows IoT devices to offload data to the edge for computing. In situations involving disasters or complex terrains, establishing ground-based stations may be challenging in providing computational services. Edge computing frameworks built with unmanned aerial vehicles (UAVs) and high-altitude platforms (HAPs) can provide airborne computational services for IoT devices situated in environments with disasters or complex terrains. In this paper, we design a three-tier framework consisting of ground users (GUs), UAVs, and HAP, offering MEC services for GUs. Considering the randomness and dynamism of task arrivals and the wireless communication quality of devices, we propose an algorithm supporting Non-Orthogonal Multiple Access (NOMA) communication in aerial access networks. The objective of the algorithm is to reduce the overall energy consumption of the system while ensuring system stability. Employing stochastic optimization techniques, we convert the task offloading and resource allocation problem into several parallel solvable sub-problems. We also provide a theoretical analysis of the algorithm. Through a series of comparative experiments, we demonstrate the feasibility and effectiveness of our proposed Dynamic Energy-Efficient Computation Offloading (DEECO) algorithm.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Task analysis",
                "Autonomous aerial vehicles",
                "Servers",
                "Internet of Things",
                "NOMA",
                "Computational modeling",
                "Atmospheric modeling"
            ],
            "Author Keywords": [
                "Mobile Edge Computing (MEC)",
                "Internet of Things (IoT)",
                "Dynamic Task Offloading",
                "Non-orthogonal Multiple Access (NOMA)",
                "Unmanned Aerial Vehicle (UAV)"
            ]
        },
        "title": "Dynamic Energy-Efficient Computation Offloading in NOMA-Enabled Air-Ground Integrated Edge Computing"
    },
    {
        "authors": [
            "Khouloud Eledlebi",
            "Ahmed Alzubaidi",
            "Ernesto Damiani",
            "Deepak Puthal",
            "Victor Mateu",
            "Mohammed Jamal Zemerly",
            "Yousof Al-Hammadi",
            "Chan Yeob Yeun"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "31 May 2024",
        "doi": "10.1109/JIOT.2024.3408031",
        "publisher": "IEEE",
        "abstract": "The rapid integration of IoT devices into everyday decision-making processes underscores the need for continuous user authentication and data integrity checking during network communication, all while minimizing energy consumption to extend device lifespan. This paper introduces the Bio-Integrated Hybrid TESLA protocol, which is a fully symmetric and energy-efficient authentication protocol designed for resource-constrained IoT devices. Based on the Hybrid TLI-lTESLA protocol, this innovative solution prioritizes high cybersecurity levels and minimal computational requirements for continuous authentication. An innovative advancement involves eliminating the public cryptography process during the synchronization stage of TESLA protocols. Instead, biometric authentication through distorted fingerprint and EEG templates is employed, to establish a non-shared symmetric session key, utilized only once. Furthermore, neither the key nor the original biometric templates are transmitted over the network, ensuring user identity preservation and effectively resolving the key distribution challenge inherent in symmetric cryptography. By offloading intensive tasks to servers and avoiding the storage or transmission of biometric data, the proposed approach conserves IoT device energy and enhances cybersecurity. Simulation analyses and cybersecurity assessments demonstrate successful synchronization, privacy preservation, and low computational demands compared to existing protocols, making the Bio-Integrated Hybrid TESLA protocol a significant advancement in IoT authentication.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Authentication",
                "Protocols",
                "Biometrics (access control)",
                "Computer security",
                "Cryptography",
                "Servers"
            ],
            "Author Keywords": [
                "Biometric Authentication",
                "Cybersecurity",
                "Energy Preservation",
                "Hybrid TESLA protocol",
                "IoT",
                "Symmetric Cryptography"
            ]
        },
        "title": "Bio-Integrated Hybrid TESLA: A Fully Symmetric Lightweight Authentication Protocol"
    },
    {
        "authors": [
            "Wajahat Ali",
            "Ikram Ud Din",
            "Ahmad Almogren",
            "Joel J. P. C. Rodrigues"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "03 June 2024",
        "doi": "10.1109/TCE.2024.3409069",
        "publisher": "IEEE",
        "abstract": "This study explores the fusion of Internet of Things (IoT) and advanced imaging technologies in the field of poultry farming, aiming to enhance both farm management and animal welfare. We investigate the application of IoT in a poultry farm environment, focusing on real-time monitoring of chickens for disease detection while ensuring data privacy during automated decision-making processes. Our approach introduces an innovative, cost-effective, and sustainable solution for the industry, leveraging the capabilities of YOLOv5 and ResNet50 algorithms for efficient disease detection and classification in poultry. This integration of IoT with advanced imaging techniques not only demonstrates a high accuracy rate of 99% in disease diagnosis but also shows a significant improvement in overall poultry health by 35%. Our findings underscore the potential of combining IoT and next-generation imaging technologies in consumer electronics to enhance disease management in poultry farming.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Diseases",
                "Monitoring",
                "Feature extraction",
                "YOLO",
                "Residual neural networks",
                "Mathematical models",
                "Imaging"
            ],
            "Author Keywords": [
                "Advanced Imaging Techniques",
                "IoT in Agriculture",
                "Poultry Health Monitoring",
                "ResNet50",
                "Smart Farming Technologies",
                "Disease Detection Algorithms",
                "YOLOv5"
            ]
        },
        "title": "Poultry Health Monitoring with Advanced Imaging: Towards Next-Generation Agricultural Applications in Consumer Electronics"
    },
    {
        "authors": [
            "Junnan Pan",
            "Yun Li",
            "Rong Chai",
            "Shichao Xia",
            "Linli Zuo"
        ],
        "published_in": "Published in: IEEE Transactions on Cognitive Communications and Networking ( Early Access )",
        "date_of_publication": "11 June 2024",
        "doi": "10.1109/TCCN.2024.3412073",
        "publisher": "IEEE",
        "abstract": "This paper investigates the planning of Unmanned aerial vehicles (UAVs) trajectory in UAV-assisted Internet of Things (IoT) networks with a massive number of IoT devices (IoTDs). Existing UAV-assisted IoT network data collection schemes mostly focus on optimizing energy consumption and data collection throughput, while neglecting the temporal value of data collection. With the assistance of the age of information (AoI), the average AoI of data collected by the UAV from IoTDs is minimized to enhance information freshness. To strike a balance between trajectory planning and information freshness, a two-stage artificial intelligence (AI) algorithm is proposed in this paper. Firstly, to tackle the issue of prolonged flight time caused by the UAV sequentially collecting data from IoTDs, an improved clustering algorithm is introduced to determine the cluster centers of IoTDs. Secondly, considering that the UAV lacks prior knowledge of the IoT network environment, the AoI minimization problem is reformulated as a Markov decision process (MDP). A neural network algorithm based on twindelayed deep deterministic policy gradient (TD3) is employed to optimize UAV trajectory. Simulation results show that the proposed algorithm is superior to the benchmark algorithms, particularly in scenarios involving a massive number of IoTDs.",
        "issn": {
            "Electronic ISSN": "2332-7731"
        },
        "keywords": {
            "IEEE Keywords": [
                "Autonomous aerial vehicles",
                "Trajectory",
                "Trajectory planning",
                "Internet of Things",
                "Clustering algorithms",
                "Quality of service",
                "Real-time systems"
            ],
            "Author Keywords": [
                "Unmanned aerial vehicle (UAV)",
                "Age of information (AoI)",
                "Internet of Things (IoT)",
                "deep reinforcement learning (DRL)",
                "trajectory planning"
            ]
        },
        "title": "Age of Information Aware Trajectory Planning of UAV"
    },
    {
        "authors": [
            "Xiao Chen",
            "Letian Sha",
            "Jincheng Wang",
            "Fu Xiao",
            "Jiankuo Dong"
        ],
        "published_in": "Published in: IEEE Transactions on Industrial Informatics ( Early Access )",
        "date_of_publication": "28 October 2024",
        "doi": "10.1109/TII.2024.3477563",
        "publisher": "IEEE",
        "abstract": "The rapid development of Industrial Internet of Things (IIoT) has raised wider concerns for security of IoT devices. Command injection (CI) vulnerabilities, prevalent in IoT devices, pose a severe risk for remote code execution. Traditional static detection methods suffer from high overhead and imprecision due to symbolic execution. Popular binary code similarity detection (BCSD) methods rely on Control Flow Graphs (CFGs) with redundant structures, resulting in low efficiency and accuracy. In addition, they struggle with cross-function issues. In this paper, we propose SFO-CID , a novel structural feature optimization based command injection vulnerability discovery model for IoT devices. Through backward taint analysis, all CFGs of suspicious CI vulnerabilities within the target binary file are precisely obtained. A large amount of code unrelated to vulnerabilities is removed, and cross-function issues are covered, significantly optimizing the structural features of original CFGs. Neural networks generate embedding vectors for optimized CFGs, transforming CI vulnerability detection into a vector similarity comparison. A wealth of semantic information within the code context is automatically and efficiently captured, improving the accuracy of vulnerability detection. We collect real-world cross-platform IoT firmware as data sources for tests. Experiments show that SFO-CID outperforms popular BCSD methods, such as Gemini , IoTSeeker , and FIT , achieving the highest accuracy of 88.67 $\\%$ in vulnerability detection. Compared to existing state-of-the-art static analysis methods, like KARONTE and SaTC , SFO-CID attains the highest precision at 88.43 $\\%$ and F1-score at 86.29 $\\%$ , and is less time-consuming. Until now, 8 high-risk unknown vulnerabilities have been discovered, including 5 cross-function cases, and corresponding CVE IDs were assigned.",
        "issn": {
            "Print ISSN": "1551-3203",
            "Electronic ISSN": "1941-0050"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Binary codes",
                "Accuracy",
                "Feature extraction",
                "Vectors",
                "Security",
                "Semantics",
                "Optimization",
                "Informatics",
                "Assembly"
            ],
            "Author Keywords": [
                "Backward taint analysis",
                "command injection vulnerability",
                "Internet of Things (IoT) devices",
                "neural network"
            ]
        },
        "title": "SFO-CID: Structural Feature Optimization Based Command Injection Vulnerability Discovery for Internet of Things"
    },
    {
        "authors": [
            "Daojing He",
            "Weiwen Huang",
            "Lei Chen",
            "Sammy Chan"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "26 September 2024",
        "doi": "10.1109/JIOT.2024.3468451",
        "publisher": "IEEE",
        "abstract": "The application of random numbers is essential in the Internet of Things (IoT), ranging from traditional data encryption functions to secure and trustworthy technologies for intelligent IoT devices. Due to their unique advantages of flexibility and convenience, software random number generators (SRNGs) are widely used in various computational applications within IoT. The research focus is on the quality, efficiency, and structural security of the output random sequences. However, there is no completely unified structural standard for SRNGs. For instance, even widely used generators such as the Linux generator have certain deficiencies in the quality or security of their random sequence outputs. This paper proposes a more secure and efficient software random number generator, namely, SESRNG. Firstly, a dual entropy pool system is constructed using a circular shift register connected to a ring aggregation pool. This system collects multiple system entropy sources in two rounds, with Shannon entropy estimation applied for online entropy estimation of the source data. Next, we utilize dual chaotic systems as extension functions to iteratively compute the entropy source data. In the designed deterministic random number generator structure, the SHA256 algorithm is used as a post-processing function to hash the key parameters of the internal state, resulting in the final random sequence. We used three well-known test suites–ENT, NIST, and the “Information Security Technology Randomness Test Methods for Binary Sequences”:–to evaluate the performance of the software random number generator. The results show that SESRNG can provide high-quality random numbers that meet the needs of various IoT applications.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Entropy",
                "Generators",
                "Security",
                "Internet of Things",
                "Software",
                "Random sequences",
                "Linux"
            ],
            "Author Keywords": [
                "Internet of Things (IoT)",
                "SRNG",
                "security",
                "Efficiency",
                "Chaotic Systems"
            ]
        },
        "title": "A Secure and Efficient Software Random Number Generator Applicable to the Internet of Things"
    },
    {
        "authors": [
            "Lingfeng Shen",
            "Huanran Zhang",
            "Ying Cui",
            "Xiaomin Mu",
            "Xiang Cheng",
            "Ning Wang"
        ],
        "published_in": "Published in: IEEE Sensors Letters ( Early Access )",
        "date_of_publication": "28 October 2024",
        "doi": "10.1109/LSENS.2024.3487009",
        "publisher": "IEEE",
        "abstract": "The design of timely data collection for a machine-type communication (MTC) network by unmanned-aerial-vehicle (UAV) platform is investigated. The ground-based MTC devices (MTCDs) are clustered for efficient service, and the UAV station's deployment in the three-dimensional (3-D) space is optimized. The corresponding mission time minimization problem is formulated as a coupled mixed-integer non-linear program (MINLP). For tractability, the original problem is decomposed into two subproblems respectively dealing with clustering-hovering optimization and inter-cluster UAV traveling path minimization. An alternating clustering-hovering optimization (ACH) and ant colony optimization (ACO) solution approach is proposed accordingly. Simulations are conducted to validate the superiority of the proposed ACH-ACO scheme over the scheme based on $k$ -means clustering.",
        "issn": {
            "Electronic ISSN": "2475-1472"
        },
        "keywords": {
            "IEEE Keywords": [
                "Autonomous aerial vehicles",
                "Data collection",
                "Optimization",
                "Trajectory",
                "Sensors",
                "Nonlinear optics",
                "Minimization",
                "Line-of-sight propagation",
                "Internet of Things",
                "Indexes"
            ],
            "Author Keywords": [
                "UAV-assisted data collection",
                "clustering",
                "mission time minimization",
                "UAV 3-D deployment"
            ]
        },
        "title": "Joint Clustering and 3-D UAV Deployment for Delay-Aware UAV-Enabled MTC Data Collection Networks"
    },
    {
        "authors": [
            "Osamah Ibrahim Khalaf",
            "Rajesh Natarajan",
            "Natesh Mahadev",
            "Prasanna Ranjith Christodoss",
            "Thangarasu Nainan",
            "Carlos Andrés Tavera Romero",
            "Ghaida Muttashar Abdulsahib"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "19 September 2022",
        "doi": "10.1109/ACCESS.2022.3207751",
        "publisher": "IEEE",
        "abstract": "In the remote healthcare industry data analytics denotes the computerization of collection, processing, and exploring complicated data to acquire finer perceptions and validate healthcare practitioners to produce familiar decisions. Healthcare basics in the modern age are vital challenges specifically in developing countries owing to the shortfall of difficult hospitals and medical professionals. As fuzzy systems have reformed several areas of work, health has also made the most of it. In this paper, the purpose of the study is to introduce a novel and intelligent remote healthcare system based on modern technologies like the Internet of things (IoT) and Neutrosophic fuzzy systems to ensure precise data analysis with lesser time and energy consumption. In this study, a novel method called, Blinder Oaxaca-based Shapiro Wilk Neutrosophic Fuzzy (BO-SWNF) data analytics for remote healthcare is designed. Data collection is performed with the WESAD dataset. Duplicated data are eliminated by Blinder Oaxaca Linear Regression-based Preprocessing model. With the application of the Blinder Oaxaca function, energy efficiency is enhanced. Finally, the Shapiro Wilk Neutrosophic Fuzzy algorithm is applied for ensuring robust data analysis. The experimental results of the proposed BO-SWNF envisage the data for finer comprehension of attribute distribution. The result is conducted by using PYHTON application to analyze stress detection with the WESAD dataset. The proposed BO-SWNF method achieved an overall accurate data analysis of 12% with minimum time ensuring 56%improvement and minimizing energy consumption by 54%.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Medical services",
                "Data analysis",
                "Decision making",
                "Data models",
                "Analytical models",
                "Time factors",
                "Forecasting",
                "Energy efficiency",
                "Internet of Things",
                "Fuzzy systems",
                "Linear regression"
            ],
            "Author Keywords": [
                "Blinder Oaxaca",
                "Energy Efficiency",
                "Internet of Things",
                "Linear Regression",
                "Neutrosophic Fuzzy",
                "Shapiro Wilk"
            ]
        },
        "title": "Blinder Oaxaca and Wilk Neutrosophic Fuzzy Set-based IoT Sensor Communication for Remote Healthcare Analysis"
    },
    {
        "authors": [
            "Elena Garcia",
            "Aurora Andújar",
            "Joan L. Pijoan",
            "Jaume Anguera"
        ],
        "published_in": "Published in: IEEE Open Journal of Antennas and Propagation ( Early Access )",
        "date_of_publication": "29 January 2024",
        "doi": "10.1109/OJAP.2024.3359183",
        "publisher": "IEEE",
        "abstract": "The increase in the IoT (Internet of Things), such as trackers and sensors, puts pressure on device dimensions and performance. This is translated to the antenna system since it has to be both efficient and provide operation across different frequency bands. However, these antennas must also be small enough to fit into the limited space available in wireless devices. To address this challenge, a reconfigurable architecture with a single SP4T (Single-Pole 4-Throw) operating at 698 MHz -960 MHz and 1710 MHz -2170 MHz is proposed, designed, and built embedding a 30 mm x 3 mm x 1 mm (0.07λ) non-resonant element called an antenna booster element. This approach does not require antennas with complex geometric shapes to achieve multiband behavior. Instead, they rely on a multiband matching network to achieve efficient operation across multiple frequency bands. This design approach is easier, faster, and simpler than creating a new antenna for every device. Additionally, the reconfigurable matching network allows an easy configuration and optimization of frequency bands. The analysis shows the advantages of the reconfigurable solution compared to a non-reconfigurable one for a 75 mm x 60 mm printed circuit board (PCB).",
        "issn": {
            "Electronic ISSN": "2637-6431"
        },
        "keywords": {
            "IEEE Keywords": [
                "Antennas",
                "Bandwidth",
                "Impedance",
                "Resonant frequency",
                "PIN photodiodes",
                "Capacitors",
                "Matched filters"
            ],
            "Author Keywords": [
                "Small",
                "multiband",
                "reconfigurable antennas",
                "matching networks",
                "antenna boosters"
            ]
        },
        "title": "Reconfigurable Antenna Booster System for Multiband Operation in IoT Devices With an SP4T Switch"
    },
    {
        "authors": [
            "Hugo De Oliveira",
            "Megumi Kaneko",
            "Lila Boukhatem"
        ],
        "published_in": "Published in: IEEE Vehicular Technology Magazine ( Early Access )",
        "date_of_publication": "20 September 2024",
        "doi": "10.1109/MVT.2024.3451191",
        "publisher": "IEEE",
        "abstract": "The soaring number of connected Internet of Things (IoT) devices and their extreme quality of service (QoS) demands in terms of rate, delay, and reliability pose multiple issues for beyond-5G (B5G) and 6G systems. As devices, applications, and mobile interfaces become increasingly diverse, optimizing the utilization of the scarce spectrum will be ever more challenging. Mathematical optimization techniques alone appear insufficient to provide viable solutions for such complex networks in real time.",
        "issn": {
            "Print ISSN": "1556-6072",
            "Electronic ISSN": "1556-6080"
        },
        "keywords": {
            "IEEE Keywords": [
                "Privacy",
                "Wireless communication",
                "Training",
                "Performance evaluation",
                "Industrial Internet of Things",
                "Delays",
                "Data privacy"
            ],
            "Author Keywords": []
        },
        "title": "Federated Multiagent Deep Reinforcement Learning for Intelligent IoT Wireless Communications: Overview and Challenges"
    },
    {
        "authors": [
            "Mengna Liu",
            "Xu Cheng",
            "Fan Shi",
            "Xiufeng Liu",
            "Hongning Dai",
            "Shengyong Chen"
        ],
        "published_in": "Published in: IEEE Transactions on Sustainable Computing ( Early Access )",
        "date_of_publication": "12 January 2024",
        "doi": "10.1109/TSUSC.2024.3353183",
        "publisher": "IEEE",
        "abstract": "Sea State Estimation (SSE) is essential for Internet of Things (IoT)-enabled autonomous ships, which rely on favorable sea conditions for safe and efficient navigation. Traditional methods, such as wave buoys and radars, are costly, less accurate, and lack real-time capability. Model-driven methods, based on physical models of ship dynamics, are impractical due to wave randomness. Data-driven methods are limited by the data imbalance problem, as some sea states are more frequent and observable than others. To overcome these challenges, we propose a novel data-driven approach for SSE based on ship motion data. Our approach consists of three main components: a data preprocessing module, a parallel convolution feature extractor, and a theoretical-ensured distance-based classifier. The data preprocessing module aims to enhance the data quality and reduce sensor noise. The parallel convolution feature extractor uses a kernel-varying convolutional structure to capture distinctive features. The distance-based classifier learns representative prototypes for each sea state and assigns a sample to the nearest prototype based on a distance metric. The efficiency of our model is validated through experiments on two SSE datasets and the UEA archive, encompassing thirty multivariate time series classification tasks. The results reveal the generalizability and robustness of our approach.",
        "issn": {
            "Electronic ISSN": "2377-3782"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sea state",
                "Feature extraction",
                "Marine vehicles",
                "Prototypes",
                "Data models",
                "Convolution",
                "Estimation"
            ],
            "Author Keywords": [
                "Autonomous ship",
                "class imbalance",
                "kernel-varying convolution",
                "prototype classifier",
                "sea state estimation"
            ]
        },
        "title": "A Prototype-Empowered Kernel-Varying Convolutional Model for Imbalanced Sea State Estimation in IoT-Enabled Autonomous Ship"
    },
    {
        "authors": [
            "Xiaoxing Qiu",
            "Chenchen Fu",
            "Weiwei Wu",
            "Zining Zhou",
            "Sujunjie Sun",
            "Yuanyuan Song",
            "Song Han"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "26 June 2024",
        "doi": "10.1109/TCE.2024.3419128",
        "publisher": "IEEE",
        "abstract": "Introducing mobile edge devices in IoT systems for real-time data retrieval can reduce energy consumption and human interaction, and thus has attracted significant research attentions in recent years. In the meanwhile, leveraging mobile edge device(s) to charge the sensor devices in proximity through RF-based Wireless Energy Transfer (WET) technologies can provide controllable and stable energy supply, and is being increasingly deployed in the field. Based on these two recent trends, this work aims to formulate and solve the joint device charging and fresh data retrieval problem where a speed-adjustable mobile edge device judiciously powers a set of sensor devices in emergent energy shortage through WET while retrieving designated data items through multi-hop transmissions. To this end, an efficient 3-phase method for Joint Charging and Retrieving (3JCR) is proposed. 3JCR first identifies the sensor devices to be charged and develops the moving trajectory of the mobile edge device. It then applies a max-flow min-cost based clustering scheme to determine the routing path of the required data items to be transmitted to the mobile edge device, and finally constructs the data retrieval schedule and adjusts the speed of the mobile edge device in the run time. Our extensive experimental results show that 3JCR outperforms all state-of-the-art methods in terms of the number of feasibly retrieved data items, while achieving similar node survival rate.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Task analysis",
                "Real-time systems",
                "Trajectory",
                "Data integrity",
                "Performance evaluation",
                "Consumer electronics",
                "Wireless communication"
            ],
            "Author Keywords": [
                "Real-time data retrieval",
                "RF energy harvesting",
                "Speed-adjustable mobile edge device"
            ]
        },
        "title": "Joint Device Charging and Fresh Data Retrieval With Mobile Edge Device in Wireless-Powered IoT Systems"
    },
    {
        "authors": [
            "Chaitanya Thuppari",
            "Srikanth Jannu",
            "Damodar Reddy Edla",
            "Ankit Vidyarthi",
            "Krishna Kant Agarwal",
            "Ahmed Alkhayyat"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "07 October 2024",
        "doi": "10.1109/TCE.2024.3475393",
        "publisher": "IEEE",
        "abstract": "With the proliferation of Industrial Internet of Things (IIoT) applications, the demand for energy efficient compression and clustering algorithms has escalated along with reliable routing algorithms. This paper proposes a novel approach: an energy aware compression based cluster and routing algorithms leveraging efficient tiny machine learning (TinyML) model using Aquila optimization for enhanced performance in IIoT environments. This approach aims to mitigate the challenges of energy and quality aware compression and energy consumption often encountered in IIoTs within underground coal mines. By dynamically forming clusters of IoT devices based on their proximity and data compression, our approach distributes the data traffic evenly across the network, thus preventing congestion and prolonging network lifetime. The integration of Aquila optimization further refines the routing decisions, optimizing the network’s overall performance. Through extensive simulations and comparative analysis, proposed method demonstrates superior performance metrics, including network lifetime by 62.3%, a 66% reduction in energy consumption and a 41% improvement in live nodes compared to existing methods. Additionally, our algorithms show 21% improvement in packet delivery ratio and increase network stability, making them particularly effective in challenging environments such as underground coal mines. This research contributes to the advancement of efficient routing techniques adapted for the demanding requirements of IIoT deployments, promising a more robust and sustainable IIoT.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Routing",
                "Optimization",
                "Tiny machine learning",
                "Energy consumption",
                "Clustering algorithms",
                "Performance evaluation",
                "Mathematical models",
                "Industrial Internet of Things",
                "Energy efficiency",
                "Heuristic algorithms"
            ],
            "Author Keywords": [
                "Energy compression",
                "TinyML",
                "clustering",
                "routing",
                "Aquila optimization",
                "Internet of Things"
            ]
        },
        "title": "Energy-Aware Compression and Consumption Algorithms for Efficient TinyML Model Using Aquila Optimization in Industrial IoT"
    },
    {
        "authors": [
            "Haitham Al-Obiedollah",
            "Haythem A. Bany Salameh",
            "Elhadj Benkhelifa"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "29 July 2024",
        "doi": "10.1109/TCE.2024.3434596",
        "publisher": "IEEE",
        "abstract": "Intelligent Reflecting Surface (IRS) has recently been combined with cutting-edge technologies to meet the demanding requirements of six-generation (6G)-based IoT consumer electronics (CE) communication systems. This paper considers IRS-assisted hybrid orthogonal frequency division multiple access (OFDMA) and non-orthogonal multiple access (NOMA) systems under proactive jamming attacks. Jamming severely affects the performance of CE devices, reducing data rates, increasing packet loss, and significantly reducing communication reliability. A jamming-aware fairness-oriented design is proposed to overcome such attacks and maintain fairness between CE devices. Specifically, the fairness index (FI) is maximized under relevant constraints, including secure transmission requirements and transmission power constraints. However, due to the non-convex and fractional nature of the proposed jamming-aware FI optimization framework, an iterative algorithm is developed to solve the problem and evaluate the optimization parameters, namely the IRS phase reflection coefficients and the per-user allocated power level (i.e., CE device). To validate the effectiveness of the proposed jamming-aware FI maximization framework, its performance is compared with a set of benchmarks. The simulation results demonstrate its superiority in ensuring fairness among users and providing secure jamming-resistant communication in IRS-assisted OFDMA-NOMA CE-based systems.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Jamming",
                "NOMA",
                "Resource management",
                "Signal to noise ratio",
                "Quality of service",
                "Performance evaluation",
                "Consumer electronics"
            ],
            "Author Keywords": [
                "Consumer electronics",
                "Intelligent reflecting surface (IRS)",
                "Jamming",
                "NOMA",
                "OFDMA",
                "Fairness"
            ]
        },
        "title": "Jamming-Resilient Fairness-Oriented Resource Allocation Technique for IRS-Assisted NOMA 6G-Enabled IoT Networks"
    },
    {
        "authors": [
            "Mohammad Hatami",
            "Markus Leinonen",
            "Marian Codreanu"
        ],
        "published_in": "Published in: IEEE Transactions on Green Communications and Networking ( Early Access )",
        "date_of_publication": "21 October 2024",
        "doi": "10.1109/TGCN.2024.3484132",
        "publisher": "IEEE",
        "abstract": "We study status updating under inexact knowledge about the battery levels of the energy harvesting sensors in an IoT network, where users make on-demand requests to a cache-enabled edge node to send updates about various random processes monitored by the sensors. To serve the request(s), the edge node either commands the corresponding sensor to send an update or uses the aged data from the cache. We find a control policy that minimizes the average on-demand AoI subject to per-slot energy harvesting constraints under partial battery knowledge at the edge node. Namely, the edge node is informed about sensors’ battery levels only via received status updates, leading to uncertainty about the battery levels for the decision-making. We model the problem as a POMDP which is then reformulated as an equivalent belief-MDP. The belief-MDP in its original form is difficult to solve due to the infinite belief space. However, by exploiting a specific pattern in the evolution of beliefs, we truncate the belief space and develop a dynamic programming algorithm to obtain an optimal policy. Moreover, we address a multi-sensor setup under a transmission limitation for which we develop an asymptotically optimal algorithm. Simulation results assess the performance of the proposed methods.",
        "issn": {
            "Electronic ISSN": "2473-2400"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Batteries",
                "Monitoring",
                "Wireless communication",
                "Optimal scheduling",
                "Knowledge engineering",
                "Energy harvesting",
                "Temperature sensors",
                "Sensor systems",
                "Sensor phenomena and characterization"
            ],
            "Author Keywords": [
                "Age of information (AoI)",
                "energy harvesting (EH)",
                "partially observable Markov decision process (POMDP)"
            ]
        },
        "title": "Status Updating Under Partial Battery Knowledge in Energy Harvesting IoT Networks"
    },
    {
        "authors": [
            "Weizhi Meng",
            "Wenjuan Li",
            "Andrei Nicolae Calugar"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "23 September 2024",
        "doi": "10.1109/JIOT.2024.3465891",
        "publisher": "IEEE",
        "abstract": "Internet of Things (IoT) devices such as smartphones have become important to people’s everyday usage, especially the number of smartphone shipment has surpassed six billion and is forecast to further grow. The smartphone security is the top priority as people may store various sensitive information on these devices. Currently, phone unlock patterns, e.g., Android unlock patterns, are one of the main protection methods to protect smartphones from unauthorized access. However, many research studies have revealed that cyber-attackers can easily compromise this type of unlock mechanism, i.e., learning the pattern from the touch residue. In this work, we advocate that an additional security layer should be added to enhance the security of Android unlock patterns, and thus develop a touch movement-based unlock mechanism via blockchain-enabled artificial neural networks (ANNs), named BANN-TMGuard, which can examine the biometric features of a user’s touch movement as well as the input pattern. Further, BANN-TMGuard adopts blockchain technology to secure the robustness and reliability when building the ANN models. In the evaluation, we perform a user study with 100 participants in the aspects of authentication accuracy, time consumption and user feedback. As compared with similar schemes, our BANN-TMGuard demonstrates better results and is preferred by most participants in the user study.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Smart phones",
                "Authentication",
                "Security",
                "Internet of Things",
                "Blockchains",
                "Touch sensitive screens",
                "Heart beat"
            ],
            "Author Keywords": [
                "Smartphone Security",
                "Blockchain technology",
                "Smartphone lock",
                "User authentication",
                "Touch movement"
            ]
        },
        "title": "BANN-TMGuard: Towards Touch Movement-Based Screen Unlock Patterns via Blockchain-enabled Artificial Neural Networks on IoT Devices"
    },
    {
        "authors": [
            "Zilin Song",
            "Kwan-Wu Chin",
            "Changlin Yang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "24 September 2024",
        "doi": "10.1109/JIOT.2024.3466959",
        "publisher": "IEEE",
        "abstract": "This paper considers maximizing coverage quality in Internet of Things (IoT) networks using Unnamed Aerial Vehicles (UAVs) to augment the link from solar-powered devices to a sink/gateway. Specifically, it aims to jointly optimize the assignment of UAVs to hovering points or a charging station, time in which devices monitor targets, and the amount of data transmitted by devices. These quantities are optimized over a given planning horizon using a Mixed Integer Linear Program (MILP). Further, this paper presents a heuristic method named Decoupled Energy Aware Algorithm (DEAA) to optimize the said quantities. In addition, it outlines a Model Predictive Control (MPC) approach that only requires current and historical energy arrivals information of devices. The simulation results showed that DEAA and MPC achieved 80.58% and 61.19% of the optimal results computed by MILP.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Autonomous aerial vehicles",
                "Relays",
                "Internet of Things",
                "Trajectory",
                "Planning",
                "Sensors",
                "Laser beams"
            ],
            "Author Keywords": [
                "Receding Horizon",
                "Samples",
                "Drones",
                "Renewable Energy",
                "Matching"
            ]
        },
        "title": "Optimizing Targets Coverage Quality in UAVs-Aided IoT Networks"
    },
    {
        "authors": [
            "Huilong Fan",
            "Chongxiang Sun",
            "Jun Long",
            "Ling Li",
            "Yakun Huo",
            "Shangpeng Wang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "30 May 2024",
        "doi": "10.1109/JIOT.2024.3407123",
        "publisher": "IEEE",
        "abstract": "Multi-satellite intelligent computing, aimed at autonomously generating resource allocation schemes without reliance on ground control, is a crucial technology for the intelligent and integrated Internet of Things (IoT) from space to ground. Existing work focuses on ground-based computation for scheduling schemes, and while some recent studies have adopted graph computing and distributed computing strategies on satellites, they have overlooked the frequent changes in satellite network topologies and the coupling relationships between various types of resources. The robustness advantage of game theory offers a possibility for reliable computation in resource allocation. We introduce a multi-satellite intelligent negotiation mechanism based on cooperative game theory, which for the first time, considers dynamic and autonomous negotiation methods for satellite agents with various types of resources. Subsequently, we propose an intelligent computing algorithm for resource allocation based on convex games and design heuristic strategies for adaptively updating resource allocation schemes. Our simulation results indicate that this method outperforms benchmark algorithms in different scenarios, reducing time consumption by at least 0.151%.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Satellites",
                "Resource management",
                "Task analysis",
                "Dynamic scheduling",
                "Games",
                "Satellite broadcasting",
                "Internet of Things"
            ],
            "Author Keywords": [
                "Game Theory",
                "Internet of Things",
                "Inter-satellite Negotiation",
                "Resource Allocation"
            ]
        },
        "title": "Graph-Driven Resource Allocation Strategies in Satellite IoT: A Cooperative Game Theoretic Approach"
    },
    {
        "authors": [
            "Ali Akbar Siddique",
            "Wad Ghaban",
            "Amer Aljaedi",
            "Faisal Saeed",
            "Mohammad S. Alshehri",
            "Ahmed Alkhayyat",
            "Hussain Mobarak Albarakati"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "12 August 2024",
        "doi": "10.1109/TCE.2024.3441934",
        "publisher": "IEEE",
        "abstract": "Global security is a matter of critical concern that requires adoption of advanced monitoring technologies. Efficient surveillance systems comprise extensive camera networks across large areas to ensure comprehensive coverage. However, the large volume of data generated by these networks poses challenges for traditional storage and computational resources. This paper presents an innovative video compression technique that focuses on optimizing data management in visual surveillance systems by selectively masking temporal information between frames. This technique introduces a specially designed adaptive masking filter, which hides the undetectable motion in video sequences and enhances video compression. The introduced masking technique uses an adaptive masking parameter ’q’ to improve frame prediction or to compensate for the masked temporal activity during decoding and achieves over 30% bit-rate reduction compared to the standard video encoding schemes, such as H.264/AVC. Moreover, the introduced technique also reduces the computational demands while keeping the quality of the output. This can be evidenced by a Peak Signal to Noise Ratio (PSNR) of 33.67 dB and a Structural Similarity Index (SSIM) of 92.7 in a traffic video sequence. The proposed technique holds the potential to be used in efficient IoT-driven video surveillance systems to process video frames efficiently without compromising quality.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Streaming media",
                "Image coding",
                "Visualization",
                "Video sequences",
                "Consumer electronics",
                "Cameras",
                "Internet of Things"
            ],
            "Author Keywords": [
                "Intra-Frame coding",
                "Temporal Masking",
                "Adaptive Motion Compensation",
                "Video Surveillance",
                "Imaging Technology"
            ]
        },
        "title": "IoT-Driven Visual Surveillance: Temporal Masking for Adaptive Motion Compensation in Imaging Technology"
    },
    {
        "authors": [
            "Mohsen Denden",
            "Mahdi Jemmali",
            "Wadii Boulila",
            "Mukesh Soni",
            "Faheem Khan",
            "Jawad Ahmad"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "14 June 2024",
        "doi": "10.1109/TCE.2024.3414929",
        "publisher": "IEEE",
        "abstract": "Edge computing emerges as a pivotal model in the era of next-generation consumer electronics and the emerging challenges of multimodal data-driven decision-making. Specifically, edge computing offers an open computing architecture for the vast and diverse consumer multimodal data generated by mobile computing and Internet of Things (IoT) technologies. While edge computing is instrumental in optimizing latency and bandwidth control in processing consumer multimodal data, the viability of employing edge resources is complicated by high service costs and the complexities of managing multimodal data diversity. This study introduces an innovative optimization method for distributing multimodal data on edge storage, considering both the I/O (input/output) speed and the overall distribution cost. The core part of our approach is the deployment of intelligent algorithms that ensure equitable data distribution across storage servers, thus eliminating unused space and reducing extra costs. Given the complexity of this NP-hard (non-deterministic polynomial-time) challenge, our study reveals a unique model incorporating an edge-broker component in combination with novel algorithms. The proposed algorithms aim to harmonize data distribution and reduce resource allocation expenses in a multimodal edge environment. Our proposed approach achieves excellent results, highlighting the efficacy of the proposed algorithms in several parameters such as makespan, cost, multimodal data security, and total processing time. Empirical tests reveal that the BCA (Best Clustering Algorithm) performs best, achieving a minimum load balancing rate of 92.2%, an average variance of 0.04, and an average run time of 0.56 seconds.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Costs",
                "Internet of Things",
                "Servers",
                "Clustering algorithms",
                "Scheduling",
                "Edge computing",
                "Task analysis"
            ],
            "Author Keywords": [
                "Consumer Cost",
                "edge workflow",
                "clustering",
                "randomization",
                "load balancing"
            ]
        },
        "title": "Clustering-Based Resource Management for Consumer Cost Optimization in IoT Edge Computing Environments"
    },
    {
        "authors": [
            "Sana Sharif",
            "Sherali Zeadally",
            "Waleed Ejaz"
        ],
        "published_in": "Published in: IEEE Internet of Things Magazine ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/IOTM.001.2400045",
        "publisher": "IEEE",
        "abstract": "We investigate how generative Artificial Intelligence (AI) can be used to optimize resources in Unmanned Aerial Vehicle (UAV)-assisted Internet of Things (IoT) networks. In particular, generative AI models for real-time decision-making have been used in public safety scenarios. This work describes how generative AI models can improve resource management within UAV-assisted networks. Furthermore, this work presents generative AI in UAV-assisted networks to demonstrate its practical applications and highlight its broader capabilities. We demonstrate a real-life case study for public safety, demonstrating how generative AI can enhance real-time decision-making and improve training datasets. By leveraging generative AI in UAV-assisted networks, we can design more intelligent, adaptive, and efficient ecosystems to meet the evolving demands of wireless networks and diverse applications. Finally, we discuss challenges and future research directions associated with generative AI for resource optimization in UAV-assisted networks.",
        "issn": {
            "Print ISSN": "2576-3180",
            "Electronic ISSN": "2576-3199"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Generative AI",
                "Artificial intelligence",
                "Biological system modeling",
                "Optimization",
                "Data models",
                "Autonomous aerial vehicles",
                "Training",
                "Real-time systems",
                "Generative adversarial networks"
            ],
            "Author Keywords": []
        },
        "title": "Resource Optimization in UAV-Assisted IoT Networks: The Role of Generative AI"
    },
    {
        "authors": [
            "Baochao Chen",
            "Xiulong Liu",
            "Hao Xu",
            "Sheng Chen",
            "Keqiu Li"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "21 October 2024",
        "doi": "10.1109/JIOT.2024.3483898",
        "publisher": "IEEE",
        "abstract": "Blockchain, with its immutability and decentralization, drives innovation in finance and supply chain, but the growing data volume makes storing complete ledger replicas impractical for users, especially in the resource-constrained Internet of Thing (IoT) scenarios. Existing solutions focus on nodes storing only a partial ledger to alleviate storage burdens. Nonetheless, these approaches prioritize storage optimization by minimizing the query cost and lack control over storage cost. Furthermore, these approaches overlook the relationships between network users, thus failing to fully measure the future query cost. Thus, this paper proposes BSSN, a Blockchain Storage technology based on Social Networks. The combined use of storage cost and query cost is introduced for the first time to formulate the Node Allocation Optimization (NAO) problem, and the Multi-population Genetic Ant Colony (MGAC) algorithm will be employed to derive node allocation strategies. Specifically, we address three technical challenges. (i) To predict the transactions that nodes will participate in in the future, we employ the social ties to obtain the access frequencies among users. (ii) To strike a balance between the storage cost and query cost, we jointly model the two costs as a multi-objective optimization problem to formulate the NAO problem. (iii) To solve the NP-hard NAO problem, we use MGAC algorithm, where the storage and query populations collaboratively search for solutions based on four operations. Extensive experiments indicate that compared with existing work, BSSN can reduce the average query cost to 67% with its adjustable storage cost, ensuring a balanced data storage among users.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Costs",
                "Blockchains",
                "Internet of Things",
                "Optimization",
                "Scalability",
                "Memory",
                "Sharding",
                "Social networking (online)",
                "Resource management",
                "Encoding"
            ],
            "Author Keywords": [
                "Blockchain",
                "storage cost",
                "query cost",
                "social tie"
            ]
        },
        "title": "BSSN: Enabling Adjustable Blockchain Storage for Resource-Constrained IoT Scenarios"
    },
    {
        "authors": [
            "Xiaonan Wang",
            "Yimin Lu",
            "Demin Gao"
        ],
        "published_in": "Published in: IEEE Transactions on Reliability ( Early Access )",
        "date_of_publication": "15 October 2024",
        "doi": "10.1109/TR.2024.3471529",
        "publisher": "IEEE",
        "abstract": "The underwater Internet of Things (UIoT) is becoming a significant means to explore marine areas, and compared to IoT, UIoT is deployed in a three-dimension marine space characterized by water depth. In this article, we propose a reliable data forwarding framework for UIoT, and aim to leverage the named data networking to reduce delays and costs of underwater data forwarding. The main idea behind this framework is to integrate the push-based data forwarding mode with the pull-based mode to realize in-networking caching and aggregation. That is, the push-based mode implements in-networking caching to shorten distances between users and marine data while the pull-based mode exploits aggregation to share data among users from the closest provider through one data forwarding process. The experimental results show that the framework achieves the above objectives.",
        "issn": {
            "Print ISSN": "0018-9529",
            "Electronic ISSN": "1558-1721"
        },
        "keywords": {
            "IEEE Keywords": [
                "Costs",
                "Delays",
                "Information sharing",
                "Reliability",
                "Temperature sensors",
                "Temperature distribution",
                "Symbols",
                "Surveillance",
                "Probes",
                "Measurement"
            ],
            "Author Keywords": [
                "Data pulling",
                "data pushing",
                "information-centric",
                "underwater Internet of Things (UIoT)"
            ]
        },
        "title": "Reliable Data Forwarding for Information-Centric Underwater IoT"
    },
    {
        "authors": [
            "Arijit Roy",
            "Salil Kashyap",
            "Ratnajit Bhattacharjee"
        ],
        "published_in": "Published in: IEEE Transactions on Green Communications and Networking ( Early Access )",
        "date_of_publication": "19 April 2024",
        "doi": "10.1109/TGCN.2024.3391270",
        "publisher": "IEEE",
        "abstract": "Wireless energy transfer (WET) is a promising method to extend the operation time of sensors in energy-constrained wireless networks. Specially, for the low-power applications such as the Internet-of-Things (IoT) and machine-to-machine communications, most of the existing works have so far focused on using fixed-frequency waveforms for WET. In this paper, we investigate the potential of superposed chirp waveforms for downlink (DL) WET from a multi-antenna access point (AP) to a group of sensors over orthogonal subbands while satisfying the peak power constraint. To this end, we first propose the general design of superposed chirp waveforms and establish key properties required to optimize WET. We derive novel closed-form analytical expressions using order statistics for average received energy based on DL WET via superposed chirps and via fixed-frequency waveforms over subbands selected independently for each sensor based on their estimated channel gain, and evaluate average harvested energy (HE) considering both linear and nonlinear energy harvesting models. For both superposed chirps and fixed-frequency based DL-WET, we then derive max-min optimal power control coefficients in closed-form to ensure that the sensors placed at different distances from the AP receive the same amount of energy. As a benchmark, we present the corresponding analysis considering perfect channel knowledge. Through our analytical and numerical results, for the considered setup, we prove and elucidate that superposed chirp-based WET over select subbands and under max-min power control provides an improvement of 40% in average HE performance as compared to multisine waveforms consisting of a set of fixed-frequency cosine signals, and extends the operating range of energy transfer by about 17.5% over fixed-frequency waveforms.",
        "issn": {
            "Electronic ISSN": "2473-2400"
        },
        "keywords": {
            "IEEE Keywords": [
                "Chirp",
                "Sensors",
                "Wireless sensor networks",
                "Sensor systems",
                "Channel estimation",
                "Wireless communication",
                "Minimax techniques"
            ],
            "Author Keywords": [
                "Chirp modulation",
                "signal design",
                "wireless energy transfer",
                "multiuser channels",
                "internet-of-things"
            ]
        },
        "title": "Wireless Energy Transfer Using Chirp Waveform With Joint Subband Selection and Max-Min Power Control for IoT Networks"
    },
    {
        "authors": [
            "Weiqi Liu",
            "Mohammad Arif Hossain",
            "Nirwan Ansari"
        ],
        "published_in": "Published in: IEEE Transactions on Cognitive Communications and Networking ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/TCCN.2024.3490779",
        "publisher": "IEEE",
        "abstract": "A scheme for edge computing-enabled offloading in a digital twin (DT) enabled heterogeneous network (HetNet) of multi-services IoT devices (IDs) is proposed. This scheme optimizes the association and handover of IDs, offloading ratio, and resource allocation considering the number of IDs, deadline requirements, and resource capacities. The objective is to enhance future generation networks by considering the ID movement, diverse ID requests, and network heterogeneity. We formulate the problem as Joint ID assOciatIon, offloadiNg ratio, Wireless bandwidth and computIng reSource allocation, and digital twin placEment (JOINWISE), aiming to minimize the task completion time of all IDs while considering ID movement. Since JOINWISE is a mixed-integer nonlinear problem, we decompose it into two sub-problems: the ID Association (IDA) problem and the offloading Ratio, DT plAcement, bandwiDth and computIng resource allOcation (RADIO) problem. IDA can be solved by mapping it to a multi-dimensional multiple knapsacks problem. Due to the non-convexity, high dimension of decision variables, and dynamic HetNet environment of RADIO, we propose a deep deterministic policy gradient (DDPG) based reinforcement learning method to iteratively solve the two sub-problems. Simulation results have confirmed the effectiveness of our proposed scheme in tackling the JOINWISE problem.",
        "issn": {
            "Electronic ISSN": "2332-7731"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Computational modeling",
                "Wireless communication",
                "Servers",
                "Resource management",
                "Digital twins",
                "Base stations",
                "Quality of service",
                "Multi-access edge computing",
                "Autonomous aerial vehicles"
            ],
            "Author Keywords": [
                "edge computing",
                "digital twin",
                "wireless communication",
                "HetNet"
            ]
        },
        "title": "Mobile Edge Computing for Multi-Services Digital Twin-Enabled IoT Heterogeneous Networks"
    },
    {
        "authors": [
            "Rajesh Kumar Chaudhary",
            "Ravinder Kumar",
            "Khursheed Aurangzeb",
            "Jatin Bedi",
            "Muhammad Shahid Anwar",
            "Ahyoung Choi"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "11 October 2024",
        "doi": "10.1109/TCE.2024.3478349",
        "publisher": "IEEE",
        "abstract": "Consumer Internet of Things (CIoT) interconnects multiple devices over internet, like smartphones, wearables, and smart gadgets to simplify tasks and provide convenience. However, it encounters obstacles such as privacy apprehensions arising from data aggregation, security flaws, interoperability discrepancies. Federated learning (FL) mitigates these issues by localizing data, reducing privacy risks, and securing IoT networks. It stores and updates learnt models on a central server, which is necessary for CIoT networks. Nonetheless, its application confronts problems like non-independent and identically distributed (non-IID) data, communication efficiency, and privacy issues. According to recent research, training models using non-IID data have detrimental influence on performance, convergence, and overall model quality in FL. Moreover, traditional FL approaches, including clustered federated learning (CFL), have problems with client training and fixed hyperparameter use. This paper introduces new approach, Artificial Bee Colony Clustered Federated Learning (ABC-CFL). ABC-CFL uses Density-based spatial clustering of applications with noise (DBSCAN) to cluster client devices based on training hyperparameters, followed by hyperparameter optimization for each cluster to better suit each cluster using artificial bee colony algorithm. This method outperforms static hyperparameter utilization problems and improves model performance and communication efficiency in CFL as demonstrated by experiments on the CIFAR-10, MNIST and CelebA datasets.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Federated learning",
                "Data models",
                "Training",
                "Performance evaluation",
                "Data privacy",
                "Internet of Things",
                "Clustering algorithms",
                "Accuracy",
                "Artificial bee colony algorithm",
                "Servers"
            ],
            "Author Keywords": [
                "CIoT",
                "Clustered federated learning",
                "Clustering",
                "DBSCAN",
                "Federated learning",
                "Artificial bee colony",
                "Non-IID"
            ]
        },
        "title": "Enhancing Clustered Federated Learning using Artificial Bee Colony Optimization Algorithm for Consumer IoT Devices"
    },
    {
        "authors": [
            "Liu Fu",
            "Meng Ma",
            "Zhi Zhai"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "21 August 2024",
        "doi": "10.1109/JIOT.2024.3446570",
        "publisher": "IEEE",
        "abstract": "Anomaly detection plays an important role in ensuring the safety and reliability of complex Internet of Things (IOT) systems through analysis of time-series data. Generally, real-world signals exhibit intrinsic non-stationary characteristics, resulting in a significant challenge the context of anomaly detection for time-series data. In this study, a novel Wavelet-Koopman predictor is proposed for time-series data modeling and anomaly detection underlying non-stationary dynamics of complex systems. The proposed wavelet-Koopman decomposes non-stationary data into time variant and time invariant components and processes them separately using different neural koopman operators. The operation of Wavelet decomposition avoids the impact of non-stationary temporal data on prediction results so the proposed Wavelet-Koopman is able to model and predict non-stationary time series data, which lays the foundation for the implementation of anomaly detection. Several experiments are conducted on common datasets and anomaly detection of liquid rocket engines are realized. The experimental results demonstrate that the proposed method outperforms several recent methods.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Time series analysis",
                "Anomaly detection",
                "Hidden Markov models",
                "Predictive models",
                "Wavelet analysis",
                "Data models",
                "Internet of Things"
            ],
            "Author Keywords": [
                "Anomaly Detection",
                "Time series prediction",
                "Deep learning"
            ]
        },
        "title": "Deep Koopman Predictors for Anomaly Detection of Complex IOT Systems With Time Series Data"
    },
    {
        "authors": [
            "Soumya Ranjan Biswal",
            "Tanmoy Roy Choudhury",
            "Subhendu Bikash Santra",
            "Babita Panda",
            "Subhrajyoti Mishra",
            "Sanjeevikumar Padmanaban"
        ],
        "published_in": "Published in: IEEE Journal of Emerging and Selected Topics in Industrial Electronics ( Early Access )",
        "date_of_publication": "09 July 2024",
        "doi": "10.1109/JESTIE.2024.3425670",
        "publisher": "IEEE",
        "abstract": "Automated greenhouse gases are essential for sustainable development and food security. PV powered greenhouse with physical sensors-based control using Internet of Things (IoT) needs high initial investment and operational cost. This also needs significant installed storage capacity. In the proposed solution, the dependency on physical sensors like temperature, humidity, soil moisture sensors, etc. are eliminated due to the application of eXtreme Gradient Boosting (XGBoost) based machine learning (ML) algorithm. The training and testing of ML algorithm are performed with one-year physical data (approx. 50k @10min interval) from greenhouse which provides accurate mapping (Temperature MAPE: 1.51%, R2: 0.9785 and Humidity MAPE: 1.68%, R2: 0.9867) between predicted and sensor data. Also, a novel priority-based demand side management (DSM) scheme is implemented which includes load shifting which reduces the requirement of installed PV and storage capacity. A reduction of 63.27% storage capacity is possible with proposed control approach. ML algorithm is programmed using Python language and implemented in Raspberry Pi-3B+ SBC. For physical verification of proposed control unit, a laboratory-based prototype is developed with PV emulator (1.5 kW), programmable electronic load box, and relay unit controlled through Arduino UNO, Raspberry Pi-3B+ SBC, ESP-32 Combo unit.",
        "issn": {
            "Print ISSN": "2687-9735",
            "Electronic ISSN": "2687-9743"
        },
        "keywords": {
            "IEEE Keywords": [
                "Green products",
                "Temperature sensors",
                "Predictive models",
                "Energy management",
                "Humidity",
                "Temperature distribution",
                "Job shop scheduling"
            ],
            "Author Keywords": [
                "Smart Greenhouse",
                "Machine Learning (ML)",
                "Demand side management (DSM)",
                "Sensor-less system"
            ]
        },
        "title": "Simplified Prediction Based AI-IoT Model for Energy Management Scheme in Standalone PV Powered Greenhouse"
    },
    {
        "authors": [
            "Ashu Taneja",
            "Shalli Rani",
            "Joel J. P. C. Rodrigues"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "25 September 2024",
        "doi": "10.1109/JIOT.2024.3464872",
        "publisher": "IEEE",
        "abstract": "There is an explosive growth of intelligent devices in the IoT ecosystem over the years. Owing to the massive multiple access at the network edge, there is increased latency and transmission overhead. Multi-access edge computing (MEC) is a key technology used to offload the wireless devices from the computational tasks. But the wireless signal propagation is subject to fading, attenuation, obstructions and other disturbances thereby affecting the performance of edge network. Reconfigurable intelligent surface (RIS) technology improves the quality of wireless propagation links through controlled reflection. This paper presents an RIS-aided framework for a heterogenous edge network to offload the computation tasks of the resource constraint user equipment to the small access points (APs). A resource control algorithm is proposed which enables selection of an RIS-AP pair for each node in the edge network. The proposed algorithm selects the RIS-AP pair using maximum channel gain criteria such that the system sum throughput is maximized. Also, enabling reflection through the multiple RISs, the shortest path is selected using graph theory to obtain the tradeoff between latency and reflection loss. It is observed that the proposed approach improves the achieved sum throughput of the system by 21.7% and the latency is reduced by 13.8%. The network performance is evaluated for varied RIS size and number of reflecting elements under different RIS phase shift design. It is shown that RIS with 1000 reflecting elements each of size λ/2 x λ/2 with equal phase shifts achieve sum throughput gain of 25.2% over randomly chosen phase shifts. Further, the comparison of IRS-aided MEC system with conventional MEC system and clustered MEC system is performed.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Wireless communication",
                "Internet of Things",
                "Reconfigurable intelligent surfaces",
                "Servers",
                "Fading channels",
                "Optimization",
                "Channel estimation"
            ],
            "Author Keywords": [
                "Graph computing",
                "edge network",
                "latency",
                "RIS",
                "phase shift design",
                "path selection",
                "resource control"
            ]
        },
        "title": "Leveraging Reconfigurable Intelligent Surfaces for Task Offloading in Edge IoT Networks"
    },
    {
        "authors": [
            "Junyang Zhang",
            "Jiahui Hou",
            "Ye Tian",
            "Xiang-Yang Li"
        ],
        "published_in": "Published in: IEEE Transactions on Mobile Computing ( Early Access )",
        "date_of_publication": "14 August 2024",
        "doi": "10.1109/TMC.2024.3443333",
        "publisher": "IEEE",
        "abstract": "Secure protocol-independent communication is increasingly demanding to support information exchange among neighbor Internet of Things (IoT) devices. For example, recent works utilize ultrasound at the resonant frequency range of a gyroscope to build communication between a speaker and the gyroscope. However, they are vulnerable to eavesdropping attacks and may have limitations in communication delays. In this work, we present WordWhisper, an efficient, word-level, and speaker-to-gyroscope communication system, with which only the target device can receive the correct information. We theoretically analyze Micro-Electro-Mechanical System (MEMS) gyroscope resonance and propose a hardware-dependent mechanism to defend against eavesdropping, making non-target gyroscopes receive ineffective information. Note that WordWhisper is free of costly data collection from gyroscopes, we train and update our decoding model based on the synthesized data (generated from theoretical MEMS resonance analysis) rather than the costly collected data from gyroscopes. Meanwhile, we address the challenge of eavesdropping when it comes to multiple attackers. We evaluate WordWhisper over 50 MEMS gyroscopes and 100 words. Extensive evaluations demonstrate that WordWhisper can achieve word-level communication with 99.33% accuracy while the recognition accuracy drops to a random guess for the non-target. Our decoding delay is lower than 0.63 seconds.",
        "issn": {
            "Print ISSN": "1536-1233",
            "Electronic ISSN": "1558-0660"
        },
        "keywords": {
            "IEEE Keywords": [
                "Gyroscopes",
                "Resonant frequency",
                "Internet of Things",
                "Micromechanical devices",
                "Eavesdropping",
                "Resonance",
                "Long short term memory"
            ],
            "Author Keywords": [
                "Side Channel",
                "Near Field Communication",
                "Internet of Things"
            ]
        },
        "title": "WordWhisper: Exploiting Real-time, Hardware-dependent IoT Communication against Eavesdropping"
    },
    {
        "authors": [
            "Abed Alanazi",
            "Abdullah Alqahtani",
            "Shtwai Alsubai",
            "Munish Bhatia"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "20 August 2024",
        "doi": "10.1109/JIOT.2024.3445884",
        "publisher": "IEEE",
        "abstract": "Smart logistics industry leverages advanced software and hardware systems to enable efficient transmission.The incorporation of smart technologies including Digital Twin (DT), and Blockchain assesses vulnerabilities in the logistics industry, making them effective for physical attacks by users for stealing and theft control. DT persists a transformative potential in optimizing industrial operations. By bridging the physical and digital worlds, they enable real-time monitoring, predictive analytics, and enhanced decision-making, driving innovations in efficiency, security, and sustainability.Conspicuously, the primary objective is to propose an effective logistic monitoring system for ensuring automated theft control. Specifically, the proposed model determines the logistic transmission patterns through secure surveillance using IoT-empowered blockchain technology. Moreover, the deep learning technique of a bi-directional convolutional neural network is used to assess theft and stealing vulnerability by users in real-time for optimal decision-making. The proposed approach has been demonstrated to enable accurate real-time analysis of vulnerable behavior. Based on the experimental simulations, the suggested solution effectively facilitates the development of superior logistic monitoring. The performance of the proposed system is evaluated using several statistical metrics, including latency rate (26.15s), data processing cost, prediction efficiency (accuracy (96.12%), specificity (97.53%), and F-Measure (97.25%), reliability (93.34%), and stability (0.74).",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Logistics",
                "Monitoring",
                "Global Positioning System",
                "Real-time systems",
                "Radiofrequency identification",
                "Servers",
                "Internet of Things"
            ],
            "Author Keywords": [
                "Smart Logistics",
                "Digital Twin",
                "Blockchain"
            ]
        },
        "title": "IoT-Inspired Smart Theft Control Framework for Logistic Industry"
    },
    {
        "authors": [
            "Zexuan Jing",
            "Yuanhao Cui",
            "Furong Chai",
            "Junsheng Mu",
            "Le Zheng",
            "Zhiqi Huang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "25 October 2024",
        "doi": "10.1109/JIOT.2024.3486573",
        "publisher": "IEEE",
        "abstract": "We propose a robust beamforming design methodology for integrated sensing and communications beamform, where the beamforming design is investigated under the sensing optimal beamforming designed to overcome the channel uncertainty that arises from the communication system. Under the assumption that the Channel State Information (CSI) error is elliptically bounded, we study the robust integrated sensing and communication (ISAC) beamforming design problem with the minimization of the Cramér-Rao bound (CRB) under the signal-to-noise ratio (SINR) threshold constraint. We consider the long-range and near-range cases separately and categorize them into point-target and extended-target for processing. In the point target scenario, we address the problem through distributed optimization using the S-procedure and solve it with the SDR method. Meanwhile, in the extended target scenario, we transform the infinite constraints of the robust ISAC design problem into a finite set, employing Linear Matrix Inequalities (LMI) for equivalent representation. Under specific conditions, we illustrate that the SDR problem in this scenario can yield a rank-1 solution. Simulation results verify the effectiveness of the proposed CRB optimizationmin method and prove its application value in the next generation of IoT devices.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Array signal processing",
                "Integrated sensing and communication",
                "Internet of Things",
                "Optimization",
                "Uncertainty",
                "Signal to noise ratio",
                "Radar",
                "Interference",
                "Base stations",
                "Wireless communication"
            ],
            "Author Keywords": [
                "Integrated sensing and communication (ISAC)",
                "Channel State Information (CSI) error",
                "Cramér-Rao bound (CRB)",
                "S-procedure"
            ]
        },
        "title": "A Robust Beamforming for Integrated Sensing and Communications in Edge IoT devices"
    },
    {
        "authors": [
            "Qianqian Wu",
            "Qiang Liu",
            "Wenliang Zhu",
            "Zefan Wu"
        ],
        "published_in": "Published in: IEEE Transactions on Network and Service Management ( Early Access )",
        "date_of_publication": "28 August 2024",
        "doi": "10.1109/TNSM.2024.3450964",
        "publisher": "IEEE",
        "abstract": "With the advancements in technologies such as 5G, Unmanned Aerial Vehicles (UAVs) have exhibited their potential in various application scenarios, including wireless coverage, search operations, and disaster response. In this paper, we consider the utilization of a group of UAVs as aerial base stations (BS) to collect data from IoT sensor devices. The objective is to maximize the volume of collected data while simultaneously enhancing the geographical fairness among these points of interest, all within the constraints of limited energy resources. Therefore, we propose a deep reinforcement learning (DRL) method based on Graph Attention Networks (GAT), referred to as “GADRL”. GADRL utilizes graph convolutional neural networks to extract spatial correlations among multiple UAVs and makes decisions in a distributed manner under the guidance of DRL. Furthermore, we employ Long Short-Term Memory to establish memory units for storing and utilizing historical information. Numerical results demonstrate that GADRL consistently outperforms four baseline methods, validating its computational efficiency.",
        "issn": {
            "Electronic ISSN": "1932-4537"
        },
        "keywords": {
            "IEEE Keywords": [
                "Autonomous aerial vehicles",
                "Data collection",
                "Task analysis",
                "Energy consumption",
                "Heuristic algorithms",
                "Energy efficiency",
                "Propulsion"
            ],
            "Author Keywords": [
                "Data collection",
                "energy efficiency",
                "unmanned aerial vehicle (UAV)",
                "graph attention network",
                "deep reinforcement learning"
            ]
        },
        "title": "Energy Efficient UAV-Aassisted IoT Data Collection: A Graph-Based Deep Reinforcement Learning Approach"
    },
    {
        "authors": [
            "Ali Siddiq",
            "Yahya Jaber Ghazwani"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "29 July 2024",
        "doi": "10.1109/TCE.2024.3430974",
        "publisher": "IEEE",
        "abstract": "As Internet of Things (IoT) technologies advance, applications such as smart cities, healthcare, and smart grids will become increasingly commonplace. A wireless sensor network (WSN) is one of the futuristic technologies used in IoT-enabled applications for sensing and data transmission. An IoT-enabled WSN (IWSN) is characterized by several sensors dispersed randomly in open and harsh environments. Given the resource constraints of sensor nodes (SNs) and the hostile deployment environments, designing routing protocols for WSNs necessitates a focus on energy efficiency and security. An optimized hybrid model, Hybrid Optimized Deep Neural Network (HODNN), is designed using Deep Neural Networks (DNNs) to maximize its detection accuracy. The source node determines the shortest path to the destination after detecting malicious nodes and performs secure routing without malicious nodes. A modified energy-efficient centralized clustering routing protocol determines the optimum path for routing data in the proposed model (MEECRP). The paper presents HMRP-IWSN, HODNN-based intrusion detection and MEECR protocol for securing IWSN data. Through comprehensive evaluation using various performance metrics, HMRP-IWSN demonstrates superior outcomes compared to existing methods, including a higher packet-delivery ratio (PDR), detection rate, lower delay and energy usage, and an extended network lifespan.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Wireless sensor networks",
                "Sensors",
                "Routing",
                "Security",
                "Wireless communication",
                "Reliability",
                "Routing protocols"
            ],
            "Author Keywords": [
                "Clustering",
                "Energy-Efficient",
                "Deep Neural Network",
                "Optimization",
                "Internet of Things (IoT)",
                "Intrusion Detection",
                "WSN"
            ]
        },
        "title": "Hybrid Optimized Deep Neural Network Based Intrusion Node Detection and Modified Energy Efficient Centralized Clustering Routing Protocol for Wireless Sensor Network"
    },
    {
        "authors": [
            "Javed Akhter",
            "Ranjay Hazra",
            "Albena Mihovska",
            "Ramjee Prasad"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "22 April 2024",
        "doi": "10.1109/TCE.2024.3392435",
        "publisher": "IEEE",
        "abstract": "Intelligent transportation systems (ITS) in smart cities offer extensive network coverage and simultaneous connections, even in high-mobility scenarios within the IoT network. This way, 5G technology will become a key enabler for IoT and ITS by allowing massive vehicular connections, high throughput, low latency, and wide coverage. Thus, resource sharing becomes very important in dealing with massive IoT connections in ITS and providing efficient communication among vehicles for smart cities. We propose a resource-sharing scheme for Cellular-vehicle-to-everything (C-V2X) communication in 5G cellular networks for smart cities, having two groups of users: Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I). Our primary goal is to maximise the throughput of V2V pairs and V2I users while preserving the QoS for both users. The maximisation problem formulated is non-convex and is solved using the Lagrange multiplier method by optimizing the transmit power of both V2V and V2I users. The overall system performance is evaluated in terms of throughput of V2V and V2I users for different SINR, number of vehicles, velocities, and transmit power of vehicular users. Furthermore, the EE of V2V communication is analyzed in terms of the transmit power of VUs. Also, the proposed resource-sharing scheme is compared with the existing schemes, demonstrated through simulations.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Resource management",
                "5G mobile communication",
                "Smart cities",
                "Internet of Things",
                "Cellular networks",
                "Interference",
                "Throughput"
            ],
            "Author Keywords": [
                "Smart cities",
                "IoT",
                "C-V2X",
                "V2X communication",
                "Resource allocation",
                "V2V communication",
                "V2I communication",
                "5G",
                "Resource sharing etc"
            ]
        },
        "title": "A Novel Resource Sharing Scheme for Vehicular Communication in 5G Cellular Networks for Smart Cities"
    },
    {
        "authors": [
            "Preeti Sharma",
            "Meenakshi Sharma",
            "Ramendra Singh",
            "Vinish Kumar",
            "Ritu Agarwal",
            "Praveen Kumar Malik"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "27 June 2024",
        "doi": "10.1109/TCE.2024.3418845",
        "publisher": "IEEE",
        "abstract": "Advances in Internet of Things (IoT) technologies will facilitate smart cities and industrial applications. IoT-based applications rely on Wireless Sensor Networks (WSNs) for data transmission and sensing. Battery power is limited in IoT-enabled WSNs (IWSN) due to their irreplaceable nature. Existing clustering schemes could not mitigate control packet overhead due to the extra energy consumption required for processing data, gathering data, and forwarding tasks. The proposed routing protocol uses a hybrid adaptive network-based fuzzy inference system with reptile search optimization (NHARSO) to determine the optimal routing path between cluster heads (CH) and base stations of Internet of Things-enabled Wireless Sensor Networks (IWSN). Afterward, the proposed NHARSO-IWSN protocol can detect faults in IWSN, such as sensing, residual energy, and communication. Simulation results indicate that NHARSO-IWSN achieves a lower energy consumption of 0.005 J and a higher detection accuracy of 99.11% compared to existing protocols.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Wireless sensor networks",
                "Internet of Things",
                "Optimization",
                "Routing",
                "Routing protocols",
                "Clustering algorithms",
                "Matlab"
            ],
            "Author Keywords": [
                "Clustering Protocol",
                "Internet of Things (IOT)",
                "Neuro-Fuzzy",
                "Optimization",
                "wireless sensor Network (WSN)"
            ]
        },
        "title": "NHARSO-IWSN: A Novel Hybridized Adaptive-Network-Based Fuzzy Inference System With Reptile Search Optimization Algorithm Based Routing Protocol for Internet of Things-Enabled Wireless Sensor Networks"
    },
    {
        "authors": [
            "Prajnyajit Mohanty",
            "Umesh C. Pati",
            "Kamalakanta Mahapatra",
            "Saraju P. Mohanty"
        ],
        "published_in": "Published in: IEEE Transactions on Sustainable Computing ( Early Access )",
        "date_of_publication": "03 June 2024",
        "doi": "10.1109/TSUSC.2024.3408630",
        "publisher": "IEEE",
        "abstract": "Street lighting is one of the prominent applications that demand a massive amount of power and substantially contributes to the energy budget of a country. Light Emitting Diode (LED) and the advancement of Internet of Things (IoT) have significantly improved conventional street light technology. Nevertheless, the rapid growth of IoT devices has presented a formidable challenge in powering the vast array of IoT devices. In this manuscript, a sustainable, battery-free, low-power street light management system has been proposed which is powered from hybrid solar and solar thermal energy harvesting scheme integrated with an efficient power management unit. As a specific case study, the prototype has been implemented with an existing LED street light in India. The characteristics and performance of the prototype have been evaluated to ensure its seamless operation under real-world scenarios. The average power consumption of the system is measured as 2.088 mW when operating in real-time with 50% duty cycle, exhibiting high Quality of Service (QoS). It features long-range communication up to 761 m through implementing LoRaWAN technology. Dimension of the prototype has been restricted to 10.5 cm x 6.5 cm x 2.3 cm to make it suitable for retrofitting with existing LED based street lights",
        "issn": {
            "Electronic ISSN": "2377-3782"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Energy efficiency",
                "Power demand",
                "Energy harvesting",
                "Batteries",
                "Light emitting diodes",
                "Smart cities"
            ],
            "Author Keywords": [
                "Smart Energy",
                "Sustainability",
                "Street Light Management",
                "Energy Harvesting",
                "Solar Energy",
                "IoT",
                "LoRaWAN",
                "Smart Cities"
            ]
        },
        "title": "bSlight 2.0: Battery-free Sustainable Smart Street Light Management System"
    },
    {
        "authors": [
            "Mayank Gulati",
            "Koen Zandberg",
            "Zhaolan Huang",
            "Gerhard Wunder",
            "Cedric Adjih",
            "Emmanuel Baccelli"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "06 November 2024",
        "doi": "10.1109/ACCESS.2024.3492921",
        "publisher": "IEEE",
        "abstract": "More and more, edge devices embark Artificial Neuron Networks. In this context, a trend is to simultaneously decentralize their training as much as possible while shrinking their resource requirements, both for inference and training—tasks that are typically intensive in terms of data, memory, and computation. At the edge’s extremity, a specific challenge arises concerning the inclusion of microcontroller-based devices typically deployed in the IoT. So far, no general framework has been provided for that. Such devices not only have extremely challenging resource constraints (weak CPUs, slow network connections, memory budgets measured in kilobytes) but also exhibit high polymorphism, leading to large variability in computational performance among these devices. In this paper, we design and implement TDMiL, a versatile framework for distributed training, and transfer learning. TDMiL interconnects and combines logical components including CoAPerator (a central aggregator) and various tiny embedded software runtimes that are specifically tailored for networks comprising heterogeneous, resource-constrained devices built on diverse types of microcontrollers. We report on experiments conducted with the TDMiL framework, which we use to comparatively evaluate several schemes devised to address computational variability among distributed learning microcontroller-based devices, i.e., stragglers. Additionally, we release the code of our implementation of TDMiL as an open-source project, which is compatible with common commercial off-the-shelf IoT hardware and a well-known open-access IoT testbed.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Protocols",
                "Training",
                "Internet of Things",
                "Microcontrollers",
                "Machine learning",
                "Data models",
                "Tiny machine learning",
                "Distance learning",
                "Computer aided instruction",
                "Transfer learning",
                "Distributed computing",
                "Federated learning",
                "Tiny machine learning"
            ],
            "Author Keywords": [
                "Distributed Learning",
                "Federated Learning (FL)",
                "Internet of Things (IoT)",
                "Machine Learning",
                "Microcontrollers",
                "TinyML-as-a-Service (TMLaaS)"
            ]
        },
        "title": "TDMiL: Tiny Distributed Machine Learning for Microcontroller-based Interconnected Devices"
    },
    {
        "authors": [
            "Ala Saleh Alluhaidan",
            "Rab Nawaz Bashir",
            "Rashid Jahangir",
            "Radwa Marzouk",
            "Oumaima Saidani",
            "Roobaea Alroobaea"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "12 September 2024",
        "doi": "10.1109/JSEN.2024.3451662",
        "publisher": "IEEE",
        "abstract": "Despite tremendous improvements in sensing mechanisms at the hardware level, sensor drift is still an issue for reliable IoT applications. Existing literature is limited in exploring the impacts of sensing contexts on sensor drift and handling sensor drift using soft computing approaches. The study intends to propose a fog-enabled IoT architecture to explore the impacts of different sensing contexts on sensor drift by sampling in remote areas without any connectivity issues. The proposed solution also incorporates machine learning capabilities for sensor drift level detection. The proposed solution is implemented for drift management of soil fertility sensors for sensing soil nitrogen (N) levels at different levels of soil Electric Conductivity (EC), soil pH, soil moisture, and soil temperature. The sensor drift is observed by comparing the nitrogen (N) sensed values against the standard method for observing soil nitrogen (N) levels. The evaluation of various machine learning models for sensor drift detection reveals that Light Gradient Boosting Machine Regression (LGBMR) outperforms other models, demonstrating superior predictive capabilities with a high mean coefficient of determination (R 2 ) of 0.87 and lower error metrics across five-fold cross-validation. The application of the proposed solution in the real world demonstrates that nitrogen (N) level observations by the proposed solution are more accurate with a mean difference of 1.43 mg Kg -1 against the standard method. The proposed solution has several implications for IoT applications in precision and smart agriculture.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Internet of Things",
                "Soil",
                "Intelligent sensors",
                "Sensor phenomena and characterization",
                "Temperature sensors",
                "Machine learning"
            ],
            "Author Keywords": [
                "Sensor Drift",
                "Sensor",
                "Internet of Things (IoT)",
                "Soil Fertility Sensor",
                "Machine Learning",
                "Fog Computing"
            ]
        },
        "title": "Machine Learning and Fog Computing Enabled Sensor Drift Management in Precision Agriculture"
    },
    {
        "authors": [
            "Srishti P. Chaturvedi",
            "Rahul Mukherjee",
            "Santosh Kumar",
            "Ajay Yadav"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "21 October 2024",
        "doi": "10.1109/JIOT.2024.3484394",
        "publisher": "IEEE",
        "abstract": "The future of Internet is envisioned as the Internet of Things (IoT), a concept where countless physical objects, many of which possess limited or minimal resources, interact with one another autonomously, eliminating the need for human involvement. In the development of IoT, one of the primary concerns is security and privacy protection of end-users. Ensuring the confidentiality of communication is crucial across a range of applications, particularly in situations where communication devices have various limitations. Traditional encryption algorithms like Rivest, Shamir, Adleman (RSA) and Advanced Encryption Standard (AES) are computationally demanding and necessitate significant memory resources, leading to potential performance drawbacks on IoT devices. Basic encryption methods are susceptible to being compromised easily. Hence, altered editions of the highly efficient lightweight cipher eXtended Tiny Encryption Algorithm (XTEA) has been proposed in this paper. The key scheduling of XTEA has been substituted with the key scheduling algorithm of an ultra-lightweight block cipher PRESENT and this modified version is named as PREXTEA. Additionally, the integration of key-bit generation mechanism of a lightweight stream cipher TRIVIUM, resulted in an enhanced version of XTEA named as TRIXTEA. Consequently, the strength of the original cipher has been enhanced with improved performance parameters.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Ciphers",
                "Internet of Things",
                "Encryption",
                "Schedules",
                "Scheduling",
                "Cryptography",
                "Performance evaluation",
                "Throughput",
                "Resistance",
                "Microcontrollers"
            ],
            "Author Keywords": [
                "Block cipher",
                "IoT security",
                "Lightweight Cryptography",
                "stream cipher",
                "XTEA"
            ]
        },
        "title": "Revolutionizing XTEA: Unveiling PREXTEA and TRIXTEA-Enhanced Efficiency and Security in Internet of Things"
    },
    {
        "authors": [
            "Vikas Kumar Malav",
            "Ashwani Sharma"
        ],
        "published_in": "Published in: IEEE Transactions on Microwave Theory and Techniques ( Early Access )",
        "date_of_publication": "04 October 2024",
        "doi": "10.1109/TMTT.2024.3468728",
        "publisher": "IEEE",
        "abstract": "In wireless power transfer (WPT), rectenna is utilized to wirelessly charge the Internet of Things (IoT) nodes. The harvesting performance of the rectenna depends on the wave polarization; thus, a polarization-insensitive (PoI) rectenna is desired to allow free rotation of the IoT nodes. Typically, a dual linearly polarized (DLP) antenna is employed for this, and to achieve full PoI, the prior arts emphasize the necessity to insert a hybrid coupler (HC) between the rectifiers and the DLP antenna. However, the use of complex circuits, i.e., HC, matching networks (MNs), and rectifiers, degrades the power conversion efficiency (PCE) and increases rectenna size. Therefore, in this article, a new rectenna system is analytically evolved having an HC-like feature to achieve PoI without employing an HC circuit. Hence, a fully integrated dual circularly polarized rectenna (DCPR) is proposed which simultaneously achieves the desired PoI performance together with enhanced PCE and miniaturization. The proposed design was experimentally validated. The results show a $126\\%$ enhancement in PCE by the proposed DCPR over the conventional HC-based rectenna where both achieve PoI and the former is miniaturized. Hence, the proposed DCPR is a good rectenna contender for wireless charging of randomly oriented miniature IoT nodes.",
        "issn": {
            "Print ISSN": "0018-9480",
            "Electronic ISSN": "1557-9670"
        },
        "keywords": {
            "IEEE Keywords": [
                "Rectennas",
                "Radio frequency",
                "Rectifiers",
                "Antennas",
                "Internet of Things",
                "Mathematical analysis",
                "Wireless power transfer",
                "Voltage",
                "Schottky diodes",
                "Patch antennas"
            ],
            "Author Keywords": [
                "Integrated circuit",
                "Internet of Things (IoT) nodes",
                "miniaturization",
                "polarization-insensitive (PoI)",
                "rectenna",
                "Schottky diode",
                "wireless power transfer (WPT)"
            ]
        },
        "title": "A Miniaturized Coupler-Integrated Rectenna Element to Eliminate Hybrid-Coupler Circuit for Polarization Insensitive Wireless Power Transmission"
    },
    {
        "authors": [
            "Jaideep Kaur Mudhar",
            "Jyoteesh Malhotra",
            "Shalli Rani"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "03 June 2024",
        "doi": "10.1109/TCE.2024.3408281",
        "publisher": "IEEE",
        "abstract": "The rapid growth of increasingly pervasive smart devices with smart applications has contributed to numerous wireless security vulnerabilities in consumer electronics, which compromises the integrity of the entire network. Attacks frequently involve unauthorized access to inadequate wireless Internet of Things(IoT) based consumer electronic devices, information security lapses, and hence leakage of private and sensitive data occurs. Due to the centralized architecture of IoT networks, data security depends on the owners’ trust in the entities. With the potential advantages of blockchain technology, a distributed ledger eliminates the limitations of centralized architecture while establishing trust, privacy, and security with assured reliability. This paper proposes a smart contract-based decentralized Selective Ring-Role-based access control framework for real-time health monitoring in consumer electronic networks. The proposed framework incorporates off-chain AI-driven anomaly detection for reduced network load and latency. The experimental results show that the proposed smart contract framework is robust in proving the security and privacy of IoT-based consumer electronic networks with improved anomaly detection over the state-of-the-art methods.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Blockchains",
                "Consumer electronics",
                "Security",
                "Access control",
                "Data privacy",
                "Medical services",
                "Internet of Things"
            ],
            "Author Keywords": [
                "Blockchain",
                "IoT",
                "Access control",
                "Machine Learning",
                "Consumer Electronics"
            ]
        },
        "title": "Blockchain-Based Decentralized Access Control Framework for Enhanced Security and Privacy for Consumer Electronic Devices"
    },
    {
        "authors": [
            "Wenjiang Shang",
            "Jian Fu",
            "Jun Ma",
            "Jieyu Lin",
            "Jingyu Tian",
            "Miao Yu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "11 September 2024",
        "doi": "10.1109/JIOT.2024.3454018",
        "publisher": "IEEE",
        "abstract": "This study addresses the optimization of cold chain logistics distribution routes by proposing a low-carbon distribution route optimization model based on the Internet of Things (IoT) technology. Using IoT technology to monitor key data such as temperature, humidity, and vehicle location along cold chain logistics distribution routes, the study dynamically optimizes the distribution process. Incorporating principles of low-carbon distribution, the model considers factors such as fuel consumption, vehicle emissions, and transport efficiency to minimize carbon emissions and ensure environmentally friendly logistics operations. A genetic algorithm is employed as a path optimization tool, iteratively evolving towards optimal or near-optimal distribution routes, effectively reducing carbon emissions. The introduction of IoT technology extends route optimization beyond traditional cost and time considerations to include environmental impact and sustainability, reflecting the logistics industrys commitment to green and low-carbon development. Actual distribution data from a cold chain logistics company in a specific city are collected and divided into training and test sets for model training and validation. The model is validated and analyzed through simulations and experiments under different scenarios. Experimental results demonstrate that the optimized distribution routes reduce total travel distance by an average of 10.2%, total energy consumption by 12.5%, and total carbon emissions by 12.8%. Additionally, the temperature fluctuation range is reduced by an average of 13.3%. The genetic algorithm employed in the study converges within 350 generations across all scenarios, with an average convergence time of 45 seconds. Hence, this study provides an effective solution for optimizing cold chain logistics distribution routes, offering significant practical value.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Logistics",
                "Optimization",
                "Temperature sensors",
                "Carbon dioxide",
                "Internet of Things",
                "Temperature measurement",
                "Temperature distribution"
            ],
            "Author Keywords": [
                "cold chain logistics",
                "low-carbon distribution route optimization",
                "IoT",
                "genetic algorithm",
                "environmental protection"
            ]
        },
        "title": "Internet of Things-Based Low-Carbon Distribution Route Optimization for Logistics"
    },
    {
        "authors": [
            "Lulwah M. Alkwai",
            "Kusum Yadav"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "23 September 2024",
        "doi": "10.1109/TCE.2024.3454299",
        "publisher": "IEEE",
        "abstract": "In recent years, blockchain technology has gained prominence beyond cryptocurrencies, finding applications in smart grids and the Internet of Things (IoT). With advancements in AI and next-generation networks, IoT device applications have expanded, requiring high computational resources and reliable data transmission. This study explores the use of 5G/6G communication networks and network slicing to enhance IoT-enabled environments. We propose a model integrating blockchain-based context-aware user authentication, handover, and secure network slicing to manage load and secure data forwarding in 6G networks. Performance evaluations show that our model outperforms existing blockchain-based techniques. The proposed model improves latency by approximately 17.33% compared to the existing model.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Blockchains",
                "6G mobile communication",
                "Internet of Things",
                "Security",
                "5G mobile communication",
                "Privacy",
                "Authentication"
            ],
            "Author Keywords": [
                "Blockchain",
                "Communication",
                "5G",
                "6G",
                "IoT",
                "Machine Learning",
                "Next-Generation",
                "LSTM",
                "Security"
            ]
        },
        "title": "Blockchain-Based Secure 5G/6G Communication for Internet of Things Devices in Consumer Electronic Systems"
    },
    {
        "authors": [
            "Xiuzhen Guo",
            "Yuan He",
            "Jiacheng Zhang",
            "Yunhao Liu",
            "Longfei Shangguan"
        ],
        "published_in": "Published in: IEEE/ACM Transactions on Networking ( Early Access )",
        "date_of_publication": "17 September 2024",
        "doi": "10.1109/TNET.2024.3454095",
        "publisher": "IEEE",
        "abstract": "This paper presents, a unified backscatter radio hardware abstraction that allows a low-power IoT device to directly communicate with heterogeneous wireless receivers. Unlike existing backscatter systems that are tailored to a specific wireless communication protocol, provides a programmable interface to the micro-controller, allowing IoT devices to synthesize different types of protocol-compliant backscatter signals in the PHY layer. By leveraging the nonlinear characteristics of the negative impedance, also achieves a cross-frequency backscatter design that enables IoT devices in harmonic frequency bands to communicate with each other. We implement a PCB prototype of on 2.4 GHz ISM band and conduct extensive experiments. We leverage the software defined platform USRP to transmit the carrier signal and receive the backscatter signal to verify the efficacy of our design. Our extensive field studies show that achieves 23.8 Mbps, 247.1 Kbps, 986.5 Kbps, and 27.3 Kbps throughput when generating standard Wi-Fi, ZigBee, Bluetooth, and LoRa signals.",
        "issn": {
            "Print ISSN": "1063-6692",
            "Electronic ISSN": "1558-2566"
        },
        "keywords": {
            "IEEE Keywords": [
                "Backscatter",
                "Wireless fidelity",
                "LoRa",
                "Internet of Things",
                "Zigbee",
                "Wireless networks",
                "Receivers"
            ],
            "Author Keywords": [
                "Wireless communication",
                "Internet of Things (IoT)",
                "backscatter technology",
                "PHY layer design"
            ]
        },
        "title": "Towards Programmable Backscatter Radio Design for Heterogeneous Wireless Networks"
    },
    {
        "authors": [
            "Balajee A",
            "Mahesh T R",
            "Vinoth Kumar V",
            "V Dhilip Kumar",
            "C Rohith Bhat"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "05 June 2024",
        "doi": "10.1109/TCE.2024.3410253",
        "publisher": "IEEE",
        "abstract": "Utilizing IoT devices for automated signal extraction and data processing is a cornerstone of Computer-Aided Diagnostics, addressing various clinical challenges. Among these, diagnosing osteoarthritis, a critical knee joint disorder early on is paramount to prevent severe joint damage. Vibroarthrography (VAG), a novel approach, leverages sound waves produced during knee joint movement to diagnose various stages of this disorder. This article presents a computational system based on VAG signals, seamlessly integrated with IoT devices for knee joint data extraction. Employing machine learning techniques facilitates the classification of osteoarthritis levels. By offering this system as consumer electronics, it reduces costs and radiation exposure compared to traditional clinical modalities. Our implementation gathered 187 clinical data points using the proposed computational system, integrating IoT devices to capture vibrations. Analyzing the recorded data involved computing various feature sets, enabling multiple classifications of osteoarthritis levels. Evaluation based on accuracy, precision, recall, and AUC demonstrated the efficacy of our proposed binary and multiclass classification models, indicating its potential as a mechanism for collecting and analyzing data for early-stage osteoarthritis detection.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Osteoarthritis",
                "Wearable devices",
                "Sensors",
                "Consumer electronics",
                "Vibrations",
                "Monitoring",
                "Mathematical models"
            ],
            "Author Keywords": [
                "Osteoarthritis",
                "Vibroarthrography",
                "IoT",
                "Signal analysis",
                "Machine Learning",
                "Consumer electronics"
            ]
        },
        "title": "VAG Signal Based Computational System for Consumer’s Utilization Devices in Osteoarthritis Data Extraction and Classification"
    },
    {
        "authors": [
            "Lameya Aldhaheri",
            "Noor Alshehhi",
            "Irfana Ilyas Jameela Manzil",
            "Ruhul Amin Khalil",
            "Shumaila Javaid",
            "Nasir Saeed",
            "Mohamed-Slim Alouini"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "25 October 2024",
        "doi": "10.1109/JIOT.2024.3486369",
        "publisher": "IEEE",
        "abstract": "The emerging field of smart agriculture leverages the power of the Internet of Things (IoT) to revolutionize farming practices. This paper investigates the transformative potential of Long Range (LoRa) technology as a key enabler of long-range wireless communication for agricultural IoT systems. By critically reviewing existing literature, we identify a lacuna in research specifically focused on LoRa’s prospects and challenges from a communication perspective in smart agriculture. We delve into the details of LoRa-based agricultural networks, encompassing network architecture design, Physical Layer (PHY) considerations tailored to the agricultural environment, and the development of channel modeling techniques that account for unique soil characteristics. The paper further explores relaying and routing mechanisms that address the challenges of extending network coverage and optimizing data transmission in vast agricultural landscapes. Transitioning to practical considerations, we discuss sensor deployment strategies and energy management techniques, providing valuable insights for real-world deployments. A comparative analysis of LoRa with other prevalent wireless communication technologies employed in agricultural IoT applications highlights its strengths and weaknesses within this specific context. Furthermore, the paper outlines several future research directions to leverage the potential of LoRa-based agriculture 4.0. These include advancements in channel modeling for heterogeneous farming environments, developing novel relay routing algorithms, integrating emerging sensor technologies like hyper-spectral imaging and drone-based sensing, on-device Artificial Intelligence (AI) models, and sustainable solutions. This survey can serve as a cornerstone for researchers, technologists, and practitioners seeking to understand, implement, and propel smart agriculture initiatives utilizing LoRa technology.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "LoRa",
                "Internet of Things",
                "Smart agriculture",
                "Agriculture",
                "Surveys",
                "Monitoring",
                "Wireless communication",
                "Farming",
                "Market research",
                "LoRaWAN"
            ],
            "Author Keywords": [
                "Agriculture IoT",
                "LoRa",
                "smart farming",
                "channel modeling",
                "relaying",
                "routing"
            ]
        },
        "title": "LoRa Communication for Agriculture 4.0: Opportunities, Challenges, and Future Directions"
    },
    {
        "authors": [
            "Lingling Zhang",
            "Zhenxiong Zhou",
            "Bo Yi",
            "Jing Wang",
            "Chien-Ming Chen",
            "Chunyang Shi"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "28 October 2024",
        "doi": "10.1109/JIOT.2024.3487538",
        "publisher": "IEEE",
        "abstract": "The rapid development of the Internet of Things (IoT) and wireless communication technologies has enabled the realization of vehicle-road cooperative systems. However, the vast amount of data generated by IoT devices in these systems poses challenges for traditional data processing methods. Augmented intelligence, such as deep reinforcement learning (DRL), has emerged as a powerful solution for processing large-scale real-time data and making accurate decisions. This paper proposes an edge-cloud framework for vehicle-road cooperative traffic signal control in the context of Augmented IoT (AIoT). The framework integrates an edge-cloud collaborative resource allocation algorithm based on DRL and a traffic signal timing method that combines DRL with an extended Kalman filter. Simulation results demonstrate the effectiveness of the proposed framework in improving traffic efficiency and reducing vehicle waiting times. The average queue length was reduced by 35.7%, and the average waiting time increased by 29.1%. The proposed edge-cloud framework for vehicle-road cooperative traffic signal control in AIoT provides a promising solution for enhancing traffic management in smart cities.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Cloud computing",
                "Servers",
                "Real-time systems",
                "Computer architecture",
                "Cooperative systems",
                "Resource management",
                "Process control",
                "Decision making",
                "Wireless communication"
            ],
            "Author Keywords": [
                "Augmented IoT",
                "deep reinforcement learning",
                "extended Kalman filter",
                "vehicle-road cooperation",
                "traffic signal control"
            ]
        },
        "title": "Edge-Cloud Framework for Vehicle-Road Cooperative Traffic Signal Control in Augmented Internet of Things"
    },
    {
        "authors": [
            "Jair A. Lima Silva",
            "Wesley Costa",
            "Khan Md Mazedul Islam",
            "Jan Kleine Deters",
            "Ewout Bergsma",
            "Helder R. Oliveira Rocha",
            "Patrick Noordhoek",
            "Heinrich Wörtche"
        ],
        "published_in": "Published in: IEEE Sensors Letters ( Early Access )",
        "date_of_publication": "30 October 2024",
        "doi": "10.1109/LSENS.2024.3488652",
        "publisher": "IEEE",
        "abstract": "Reliability is a constraint of low-power wireless connectivity, commonly addressed by the deployment of mesh topology. Accordingly, power consumption becomes a major concern during the design and implementation of such networks. Thus, a mono-objective optimization was implemented in this work to decrease the total amount of power consumed by a low-power wireless mesh network based on Thread protocol. Using a genetic algorithm, the optimization procedure takes into account a pre-defined connectivity matrix, in which the possible distances between all network devices are considered. The experimental proof-of-concept shows that a mean gain of 26.45 dB is achievable in a specific scenario. Through our experimental results, we conclude that the Thread mesh protocol has much leeway to meet the low-power consumption requirement of wireless sensor networks.",
        "issn": {
            "Electronic ISSN": "2475-1472"
        },
        "keywords": {
            "IEEE Keywords": [
                "Message systems",
                "Instruction sets",
                "Optimization",
                "Wireless sensor networks",
                "Routing protocols",
                "Mesh networks",
                "Topology",
                "Network topology",
                "Power demand",
                "Reliability"
            ],
            "Author Keywords": [
                "Wireless connectivity",
                "Thread network protocol",
                "optimization",
                "power consumption",
                "Internet-of-Things (IoT)"
            ]
        },
        "title": "Improving the Power Consumption of Thread Mesh Networks Through Genetic Algorithm Optimization"
    },
    {
        "authors": [
            "George Psaltakis",
            "Konstantinos Rogdakis",
            "Konstantinos Chatzimanolis",
            "Emmanuel Kymakis"
        ],
        "published_in": "Published in: IEEE Journal on Flexible Electronics ( Early Access )",
        "date_of_publication": "18 April 2024",
        "doi": "10.1109/JFLEX.2024.3390671",
        "publisher": "IEEE",
        "abstract": "The ever-increasing number of Internet of Thing devices requires the development of edge-computing platforms to address at low power consumption the associated demand for big data processing while minimizing cloud communication latency. Neuromorphic computation is a viable solution to avoid an unsustainable energy cost, however, achieving stable memristive switching is a complex process. Mixed-halide perovskite resistive memories is a promising technology that usually requires an extensive experimental characterization procedure till a stable operation mode is reached, resulting in an abundance of data that should be manually processed. In this study we build a dataset for pattern recognition based on thousands of images of experimental current-voltage (I-V) characteristics of solution-processed, and thus printable, mixed halide perovskite memristors. We have categorized our experimental data into seven distinct categories of memristive behaviour depending on the shape of the I-V curves. A machine learning (ML) approach is implemented using a convolutional neural network (CNN) trained using this image-based dataset. After the training phase, the CNN is able to categorize any new experimental I-V across the seven generic types, while a binary categorization process, by splitting the experimental data into those exhibiting good or bad switching characteristics, is demonstrated with validation accuracies of up to 91%. Overall, it is shown that this ML-based pattern recognition approach can assist in identifying how many of the tested memristive devices exhibit stable, optimum switching dynamics, and upon expanding the model, it could predict which characterization parameters are most influential toward achieving an efficient device operation.",
        "issn": {
            "Electronic ISSN": "2768-167X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Switches",
                "Perovskites",
                "Convolutional neural networks",
                "Voltage measurement",
                "Memristors",
                "Performance evaluation",
                "Data models"
            ],
            "Author Keywords": [
                "Memristive devices",
                "Current-voltage curves",
                "Dataset",
                "Neuromorphic computation",
                "Mixed halide perovskite",
                "IoT",
                "Big data processing"
            ]
        },
        "title": "Dataset of Perovskite Memristive Current-Voltage Characteristics for Pattern Recognition"
    },
    {
        "authors": [
            "Vidya Bhasker Shukla",
            "Ondrej Krejcar",
            "Kwonhue Choi",
            "Vimal Bhatia",
            "Ambuj Kumar Mishra"
        ],
        "published_in": "Published in: IEEE Transactions on Cognitive Communications and Networking ( Early Access )",
        "date_of_publication": "03 July 2024",
        "doi": "10.1109/TCCN.2024.3422510",
        "publisher": "IEEE",
        "abstract": "A viable technology for the future wireless communication system to obtain extremely high information rates with improved coverage is the collaborative incorporation of an intelligent reflecting surface (IRS) with millimeter-wave (mmWave) multiple-input multiple-output (MIMO) systems. An IRS provides a virtual line-of-sight (LoS) path to enhance the wireless system’s capacity. However, accurate channel state information is essential for the complete utilization of IRS and mmWave MIMO systems. Existing channel estimators based on orthogonal matching pursuit (OMP) and sparse Bayesian learning (SBL) entail large pilot overhead and matrix inversion. Therefore, these techniques offer low spectral efficiency and high computational complexity. To overcome the limitations of existing estimators, we propose an online variable step-size zero-attracting least mean square (VSS-ZALMS) based algorithm for IRS-assisted mmWave hybrid MIMO system channel estimation. Further, we derive analytical expressions for the range of step-size and regularization parameters to improve estimation accuracy and convergence rates. Moreover, we conduct an analysis of IRS location, spectral efficiency, complexity analysis, and pilot overhead requirements. Simulation results are then compared with OMP, SBL, and oracle least square for benchmarking. The results corroborate superiority of the proposed approach concerning accuracy, complexity, and robustness compared to the existing estimators.",
        "issn": {
            "Electronic ISSN": "2332-7731"
        },
        "keywords": {
            "IEEE Keywords": [],
            "Author Keywords": [
                "Millimeter wave",
                "MIMO",
                "IRS",
                "sparse recovery",
                "zero-attracting (ZA)",
                "variable step-size (VSS) adaptive algorithms"
            ]
        },
        "title": "Adaptive Sparse Channel Estimator for IRS-Assisted mmWave Hybrid MIMO System"
    },
    {
        "authors": [
            "Siva Sai",
            "Shubham Sharma",
            "Vinay Chamola"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "05 August 2024",
        "doi": "10.1109/TCE.2024.3438160",
        "publisher": "IEEE",
        "abstract": "Machine Learning has evolved significantly over the last decade, with models capable of robust and accurate predictions over data of various categories. The impact is rather big in the field of Healthcare. However, deployment of small, energy-efficient models remains a goal pursued by researchers across the field. Futhermore, the model inferences are hard to interpret and draw conclusions from, especially in healthcare, where its very important to know how the diagnosis decision was passed and what factors are causing the issues. In this paper, we address both of these shortcomings, where we develop a neuromorphic computing based machine learning model, namely an SNN, and using SHAP, LIME, and eli5 explainability techniques to explain the predictions of the Spiking Neural Network. The proposed SNN performs at an accuracy of 85.06%, better than the deep neural network by 5.73% for the Diabetes dataset and at an accuracy of 97.83% for the Mobile Health dataset, thereby mitigating the issue of both lack of performance-cum-efficiency, as well as uninterpretability of these machine learning models.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Medical services",
                "Neurons",
                "Computational modeling",
                "Spiking neural networks",
                "Predictive models",
                "Data models",
                "Accuracy"
            ],
            "Author Keywords": [
                "Neuromorphic Computing",
                "Explainable AI",
                "Spiking Neural Network",
                "Consumer Healthcare"
            ]
        },
        "title": "Explainable AI-empowered Neuromorphic Computing Framework for Consumer Healthcare"
    },
    {
        "authors": [
            "Jinyong Lei",
            "Changcheng Zhou",
            "Xiaolin Li",
            "Andi Huang",
            "Hao Bai",
            "Zhiyong Yuan",
            "Linjie Zhou"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "27 February 2020",
        "doi": "10.1109/ACCESS.2020.2976781",
        "publisher": "IEEE",
        "abstract": "This paper proposes an energy optimization management method of interaction between the smart energy hub (SEH) and users in the energy system. Internet of Things (IoT) and energy storage are also considered in the system. First, the models of the SEH and users are constructed. Then the profit models under the influence of energy storage are discussed. Next the Stackelberg game model is applied in this paper, where the SEH acts as the leader and the users are the followers. A double-layer algorithm is proposed to solve the problem. Finally, a case is studied to verify the correctness of the method. The results show that the method can be used for effective energy management in integrated energy system and IoT plays an important role in information exchange.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Energy storage",
                "Natural gas",
                "Companies",
                "Internet of Things",
                "Resistance heating",
                "Load management",
                "Games"
            ],
            "Author Keywords": [
                "Internet of Things (IoT)",
                "energy storage",
                "energy management",
                "Stackelberg game",
                "smart energy hub"
            ]
        },
        "title": "Energy Management Considering Energy Storage and Demand Response for Smart Energy Hub in Internet of Things"
    },
    {
        "authors": [
            "Jiahui Yu",
            "Xu Cheng",
            "Hang Chen",
            "Yingke Xu"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "04 April 2024",
        "doi": "10.1109/TCE.2024.3384974",
        "publisher": "IEEE",
        "abstract": "Skeleton-based human recognition is a key technology for visual feedback, which can help the Internet of Things (IoT) interact with humans in a non-contact manner outdoors. Graph Convolutional Networks (GCNs), achieving an intuitive understanding of the human skeleton, have received increasing attention. Although current GCN-based works explore how to model unlinked body parts, they still show weak robustness in noisy solid data, such as joint/frame loss, which often happens in outdoor IoTs. Towards robust visual feedback, in the paper, we propose robust skeleton-based action recognition neural networks (Robust-SAR), a new cost-efficient approach for recognizing activities in noisy outdoor scenarios. Instead of estimating 2D or 3D skeleton coordinates, we first extract the heatmap of human poses from videos. We propose S-pose to learn heatmap at multiple levels, i.e., joint, joint-scale, and part-scale learning, boosting higher-order motion pattern learning. Additionally, we propose T-pose to adaptively employ the features of previous frames to enhance the current frame, further enhancing the robustness of spatiotemporal human representation. Experimentally, Robust-SAR achieves state-of-the-art recognition results on four benchmarks, including NTU-60, NTU-120, NUCLA, and Kinetics-400 (outdoor datasets). Furthermore, in noise-filled outdoor conditions, the performance of Robust-SAR only drops by about 0.5%, while other state-of-the-art methods drop by about 2%.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Skeleton",
                "Heating systems",
                "Robustness",
                "Visualization",
                "Internet of Things",
                "Sensors",
                "Feature extraction"
            ],
            "Author Keywords": [
                "outdoor action recognition",
                "IoT-enabled sensing",
                "robust motion analysis",
                "human pose"
            ]
        },
        "title": "Pose-Guided Robust Action Recognition for Outdoor Internet of Things"
    },
    {
        "authors": [
            "Yin Xu",
            "Mingjun Xiao",
            "Jie Wu",
            "Guoju Gao",
            "Datian Li",
            "Haotian Xu",
            "Tongxiao Zhang"
        ],
        "published_in": "Published in: IEEE Transactions on Industrial Informatics ( Early Access )",
        "date_of_publication": "18 September 2024",
        "doi": "10.1109/TII.2024.3424497",
        "publisher": "IEEE",
        "abstract": "Federated learning (FL) is a distributed learning paradigm that enables large-scale IoT devices to collaboratively train a shared model while preserving the privacy of local data. To avoid the single-point-of-failure of the conventional parameter server architecture, the study concentrates on the decentralized FL (DFL) paradigm building on the device-to-device communication network. However, existing DFL frameworks encounter challenges related to resource limitations, privacy protection, and data heterogeneity. To overcome these challenges, the study proposes and implements DF $^{2}$ -MPC in industrial IoT, an efficient DFL framework with personalized model pruning and adaptive communication. Specifically, a personalized pruning ratio determination approach is designed by exploiting the model pruning technique. This approach enables all devices to flexibly determine pruning ratios by themselves, thereby achieving both communication savings and privacy protection. Then, this study designs an adaptive neighbor selection scheme, which can enhance model performance and foster model consensus under resource constraints. In addition, the study theoretically proves the convergence performance of DF $^{2}$ -MPC. Finally, extensive simulations on three real-world traces are conducted to corroborate the superiority of DF $^{2}$ -MPC, demonstrating that the method can improve communication efficiency with satisfactory model accuracy and convergence performance.",
        "issn": {
            "Print ISSN": "1551-3203",
            "Electronic ISSN": "1941-0050"
        },
        "keywords": {
            "IEEE Keywords": [
                "Adaptation models",
                "Performance evaluation",
                "Privacy",
                "Computational modeling",
                "Training",
                "Device-to-device communication",
                "Data models"
            ],
            "Author Keywords": [
                "Communication efficiency",
                "decentralized federated learning (DFL)",
                "industrial Internet of Things (IoT)",
                "model pruning",
                "neighbor selection"
            ]
        },
        "title": "Enhancing Decentralized Federated Learning With Model Pruning and Adaptive Communication"
    },
    {
        "authors": [
            "Vikas Pandey",
            "Amit Kumar",
            "Ahmed S Razeen",
            "Ankur Gupta",
            "Sudhiranjan Tripathy",
            "Mahesh Kumar"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/JSEN.2024.3487877",
        "publisher": "IEEE",
        "abstract": "There is a burgeoning need for miniaturized sensors to detect H 2 leaks throughout the entire value chain while envisioning a hydrogen economy. Developing a user-centric approach for manufacturing H 2 sensors exhibiting high performance, long-term stability, and ease in data communication still poses a significant challenge. With this objective in mind, we develop a Pd/AlGaN/GaN High Electron Mobility Transistor (HEMT)-based IoT-enabled H 2 sensing device capable of detecting extremely low concentrations (~0.5 ppm) at room temperature. The fabrication process of the device involves a photolithography technique for its fabrication and functionalization of the active area between the drain and source by Pd nanoparticles using the DC sputtering method. Afterward, Pd nanoparticles were functionalized onto the HEMT surface and sputtering times were also optimized. The sensor demonstrated shallow time parameters, with a recovery time of 52 s and a response time of 29 s for 10 ppm H 2 at room temperature (RT) respectively, with an exceptionally low detection limit of 0.5 ppm. Selectivity of the fabricated sensor was also investigated. Sensitivity towards NO 2 , CO 2 , H 2 S, NH 3 and SO 2 was approximately 1.5%, 4%, 2%, 3%, and 6.5%, respectively, compared to ~ 33% for H 2 . Furthermore, the sensor displayed marvelous replicability, working in a highly humid environment with operating in temperature range of 20 to 75°C. The sensor was incorporated into a prototype featuring a wireless capable Nano ESP32 IoT platform for real-time conditions. The reported proof of concept on room temperature H 2 sensor with enhanced characteristics can be envisioned for further technology demonstration.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Temperature sensors",
                "Hydrogen",
                "HEMTs",
                "Wide band gap semiconductors",
                "Sensor phenomena and characterization",
                "Aluminum gallium nitride",
                "Gas detectors",
                "Nanoparticles",
                "Sensitivity"
            ],
            "Author Keywords": [
                "HEMT sensor",
                "Hydrogen",
                "IoT",
                "Pd/AlGaN/GaN",
                "Room temperature sensor"
            ]
        },
        "title": "Pd/AlGaN/GaN HEMT-based room temperature hydrogen gas sensor"
    },
    {
        "authors": [
            "Sijun Lyu",
            "Zhi Jiao"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "28 October 2024",
        "doi": "10.1109/JIOT.2024.3486714",
        "publisher": "IEEE",
        "abstract": "This study proposes a new method for optimizing household financial asset allocation (FAA) and developing scientific risk control strategies by integrating Internet of Things (IoT) data and clustering algorithms. IoT technology is utilized to collect real-time data, including macroeconomic indicators, market dynamics, and investor behavior. The data comes from national statistical offices, financial market trading platforms, and social media. The optimized K-means++ clustering algorithm is used for asset classification. Moreover, its performance is evaluated through indexes such as the silhouette coefficient, Adjusted Rand Index (ARI), Davies-Bouldin Index (DBI), and Mutual Information (MI). The results show that the K-means++ algorithm outperforms these indexes, especially in terms of silhouette coefficient (0.62), ARI (0.75), and MI (0.80). In terms of asset allocation optimization, the proportion of stock products and wealth management products has increased to 4.94% and 12.34%, indicating an improvement in asset diversification and risk diversification. Risk assessment reveals that Internet financial behavior and economic development level have a marked impact on asset allocation. For example, the asset allocation of households with active Internet financial behavior is more diversified. The cash deposit ratio in the eastern region is 79.32%, lower than the 86.08% in the western region. This study provides effective methods for managing household financial assets, which can help improve household wealth management, enhance risk management capabilities, and promote the development of financial technology.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Resource management",
                "Clustering algorithms",
                "Internet of Things",
                "Risk management",
                "Real-time systems",
                "Classification algorithms",
                "Heuristic algorithms",
                "Investment",
                "Accuracy",
                "Optimization"
            ],
            "Author Keywords": [
                "IoT data",
                "clustering algorithms",
                "financial asset allocation",
                "risk management",
                "asset optimization"
            ]
        },
        "title": "Optimization of Financial Asset Allocation and Risk Management Strategies Combining Internet of Things and Clustering Algorithms"
    },
    {
        "authors": [
            "Seifu Birhanu Tadele",
            "Binayak Kar",
            "Frezer Guteta Wakgra",
            "Asif Uddin Khan"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "07 October 2024",
        "doi": "10.1109/JIOT.2024.3472026",
        "publisher": "IEEE",
        "abstract": "In real-time status update services for the Internet of Things (IoT), the timely dissemination of information requiring timely updates is crucial to maintaining its relevance. Failing to keep up with these updates results in outdated information. The age of information (AoI) serves as a metric to quantify the freshness of information. The Existing works to optimize AoI primarily focus on the transmission time from the information source to the monitor, neglecting the transmission time from the monitor to the destination. This oversight significantly impacts information freshness and subsequently affects decision-making accuracy. To address this gap, we designed an edge-enabled vehicular fog system to lighten the computational burden on IoT devices. We examined how information transmission and request-response times influence end-to-end AoI. As a solution, we proposed Dueling-Deep Queue Network (dueling-DQN), a deep reinforcement learning (DRL)-based algorithm, and compared its performance with DQN policy and analytical results. Our simulation results demonstrate that the proposed dueling-DQN algorithm outperforms both DQN and analytical methods, highlighting its effectiveness in improving real-time system information freshness. Considering the complete end-to-end transmission process, our optimization approach can improve decision-making performance and overall system efficiency.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Servers",
                "Internet of Things",
                "Monitoring",
                "Optimization",
                "Real-time systems",
                "Temperature measurement",
                "Computational modeling",
                "Temperature sensors",
                "Edge computing",
                "Analytical models"
            ],
            "Author Keywords": [
                "Information freshness",
                "AoI",
                "IoT",
                "vehicular fog",
                "edge computing",
                "DQN"
            ]
        },
        "title": "Optimization of End-to-End AoI in Edge-Enabled Vehicular Fog Systems: A Dueling-DQN Approach"
    },
    {
        "authors": [
            "Mahaboob Basha Shaik",
            "Kunam Subba Reddy",
            "K Chokkanathan",
            "Sardar Asad Ali Biabani",
            "P Shanmugaraja",
            "D R Denslin Brabin"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "01 November 2024",
        "doi": "10.1109/ACCESS.2024.3489960",
        "publisher": "IEEE",
        "abstract": "The rapid growth of Internet of Things (IoT) applications has led to the widespread adoption of fog-cloud computing environments, where efficient resource allocation is critical for ensuring optimal performance and cost-effectiveness. In this paper, we propose a novel hybrid algorithm that combines Particle Swarm Optimization (PSO) with Simulated Annealing (SA) and integrates a load balancing mechanism, termed PSOSA-LB, for resource allocation in fog-cloud environments. The algorithm aims to minimize the overall execution time, latency, and energy consumption while maintaining a balanced workload distribution across fog and cloud resources. The PSO component drives the exploration of the solution space by iteratively updating particle positions based on individual and collective experiences. To avoid premature convergence and escape local optima, SA is employed, allowing occasional acceptance of suboptimal solutions with a probability governed by a temperature parameter. To ensure load balancing, a load imbalance adjustment factor is incorporated into the PSO velocity update, guiding particles towards solutions that evenly distribute the computational load across available resources. Extensive simulations demonstrate that the PSOSA-LB algorithm outperforms traditional PSO, SA, and other hybrid approaches in terms of both resource utilization efficiency and load distribution. The proposed method recorded up to 33% faster execution, 35% lower latency, 20% reduced energy consumption, and 45% better load distribution, which provides a robust and scalable solution for dynamic resource management in fog-cloud environments, making it suitable for various IoT-driven applications that demand high performance and low latency.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Resource management",
                "Cloud computing",
                "Internet of Things",
                "Simulated annealing",
                "Load management",
                "Quality of service",
                "Particle swarm optimization",
                "Optimization",
                "Energy consumption",
                "Delays",
                "Edge computing"
            ],
            "Author Keywords": [
                "Fog Computing",
                "Cloud Computing",
                "Internet of Things (IoT)",
                "Load Balancing",
                "Resource Allocation",
                "PSO",
                "SA"
            ]
        },
        "title": "A Hybrid Particle Swarm Optimization and Simulated Annealing with Load Balancing Mechanism for Resource Allocation in Fog-Cloud Environments"
    },
    {
        "authors": [
            "Yoga Suhas Kuruba Manjunath",
            "Sihao Zhao",
            "Xiao-Ping Zhang",
            "Lian Zhao"
        ],
        "published_in": "Published in: IEEE Transactions on Network and Service Management ( Early Access )",
        "date_of_publication": "10 September 2024",
        "doi": "10.1109/TNSM.2024.3457579",
        "publisher": "IEEE",
        "abstract": "Deep learning-based network traffic classification (NTC) techniques, including conventional and class-of-service (CoS) classifiers, are a popular tool that aids in the quality of service (QoS) and radio resource management for the Internet of Things (IoT) network. Holistic temporal features consist of inter-, intra-, and pseudo-temporal features within packets, between packets, and among flows, providing the maximum information on network services without depending on defined classes in a problem. Conventional spatio-temporal features in the current solutions extract only space and time information between packets and flows, ignoring the information within packets and flow for IoT traffic. Therefore, we propose a new, efficient, holistic feature extraction method for deep-learning-based NTC using time-distributed feature learning to maximize the accuracy of the NTC. We apply a time-distributed wrapper on deep-learning layers to help extract pseudo-temporal features and spatio-temporal features. Pseudo-temporal features are mathematically complex to explain since, in deep learning, a black box extracts them. However, the features are temporal because of the time-distributed wrapper; therefore, we call them pseudo-temporal features. Since our method is efficient in learning holistic-temporal features, we can extend our method to both conventional and CoS NTC. Our solution proves that pseudo-temporal and spatial-temporal features can significantly improve the robustness and performance of any NTC. We analyze the solution theoretically and experimentally on different real-world datasets. The experimental results show that the holistic-temporal time-distributed feature learning method, on average, is 13.5% more accurate than the state-of-the-art conventional and CoS classifiers.",
        "issn": {
            "Electronic ISSN": "1932-4537"
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Telecommunication traffic",
                "Deep learning",
                "Internet of Things",
                "Convolutional neural networks",
                "Representation learning",
                "Long short term memory"
            ],
            "Author Keywords": [
                "network traffic classification (NTC)",
                "class-of-service (CoS) NTC",
                "Internet of Things (IoT)",
                "deep learning",
                "convolutional neural network (CNN)",
                "long short-term memory (LSTM)",
                "time-distributed feature learning"
            ]
        },
        "title": "Time-Distributed Feature Learning for Internet of Things Network Traffic Classification"
    },
    {
        "authors": [
            "Ru Zhou",
            "Biao Guo",
            "Wentao Wu",
            "Huan Wang",
            "Haihong Niu",
            "Lei Wan"
        ],
        "published_in": "Published in: IEEE Transactions on Electron Devices ( Early Access )",
        "date_of_publication": "25 October 2024",
        "doi": "10.1109/TED.2024.3481199",
        "publisher": "IEEE",
        "abstract": "The exponential growth of the Internet of Things (IoTs) electronics calls for the development of more energy-saving solutions to replace commonly used batteries. The indoor photovoltaics (IPVs), capable of harvesting energy from ambient lighting, emerge as an advanced energy-supply technology for sustainable IoTs. The binary semiconductor antimony sulfide (Sb $_{\\text{2}}$ S $_{\\text{3}}$ ) has a suitable bandgap of $\\sim$ 1.75 eV, close to the optimal value for IPVs. In this work, environment-friendly planar heterojunction Sb $_{\\text{2}}$ S $_{\\text{3}}$ solar cells using TiO $_{\\text{2}}$ buffer layers were constructed via close-spaced sublimation (CSS). Through careful characterization of morphological, structural, and optoelectronic properties, it is revealed that, compared to the conventional CdS-based Sb $_{\\text{2}}$ S $_{\\text{3}}$ films, the TiO $_{\\text{2}}$ -based Sb $_{\\text{2}}$ S $_{\\text{3}}$ films exhibit more compact morphology, improved hkl , l $\\ne $ 0-orientation, and more favorable band structure. Particularly, the significantly reduced parasitic absorption of TiO $_{\\text{2}}$ buffer layers over the short-wavelength range contributes to the remarkable photocurrent enhancement of devices. As a result, the TiO $_{\\text{2}}$ /Sb $_{\\text{2}}$ S $_{\\text{3}}$ heterojunction device yields a power conversion efficiency (PCE) of 4.55% under one-sun (AM1.5G, 100 mW cm $^{-\\text{2}}$ ), outperforming that of the CdS/Sb $_{\\text{2}}$ S $_{\\text{3}}$ device (4.11%). To the best of our knowledge, this is the highest efficiency value reported thus far for CSS-processed Sb $_{\\text{2}}$ S $_{\\text{3}}$ systems. Moreover, an appreciable indoor efficiency of 12.14% is achieved for the TiO $_{\\text{2}}$ /Sb $_{\\text{2}}$ S $_{\\text{3}}$ solar cell under 1000-lx white light-emitting diode (WLED) illumination. This work manifests the great potential of CSS-processed environment-friendly planar hetero...",
        "issn": {
            "Print ISSN": "0018-9383",
            "Electronic ISSN": "1557-9646"
        },
        "keywords": {
            "IEEE Keywords": [
                "Photovoltaic cells",
                "Films",
                "Buffer layers",
                "Substrates",
                "Lighting",
                "Performance evaluation",
                "Antimony",
                "Scanning electron microscopy",
                "Heterojunctions",
                "Absorption"
            ],
            "Author Keywords": [
                "Antimony sulfide (Sb $_{\\text{2}}$ S $_{\\text{3}}$ )",
                "close-spaced sublimation (CSS)",
                "indoor photovoltaics (IPVs)",
                "Internet of Things (IoT)",
                "solar cells"
            ]
        },
        "title": "Close-Spaced Sublimation Processed Environment-Friendly Planar Heterojunction Antimony Sulfide Solar Cells"
    },
    {
        "authors": [
            "Mianjie Li",
            "Haozheng Cui",
            "Chun Shan",
            "Xiaojiang Du",
            "Mohsen Guizani"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "30 September 2024",
        "doi": "10.1109/JIOT.2024.3470130",
        "publisher": "IEEE",
        "abstract": "The advent of Unmanned Aerial Vehicle (UAV) networks, renowned for their expansive coverage capabilities and heightened adaptability, presents a promising landscape for bolstering the efficacy of Internet of Things (IoT) data transmissions. Nevertheless, the integration of UAVs into IoT ecosystems introduces a spectrum of security challenges, notably data tampering, man-in-the-middle (MitM) attacks, and eavesdropping, which threaten the integrity and confidentiality of transmitted information. Since covert transmission has the characteristics of strong concealment and difficult detection, UAV networks based on information hiding become a new paradigm for solving these security problems. This paper proposes an Information Hiding Algorithm Based on Super-Resolution Enhancement and Significant Region Detection (IH-SESD). This algorithm incorporates the super-resolution enhanced SRCNN and the U2Net salient region detection technologies. Comprehensive performance analysis and comparisons with existing methods demonstrate the superiority of the proposed IH-SESD algorithm.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Superresolution",
                "Autonomous aerial vehicles",
                "Internet of Things",
                "Transforms",
                "Robustness",
                "Deep learning",
                "Saliency detection",
                "Histograms",
                "Feature extraction",
                "Interpolation"
            ],
            "Author Keywords": [
                "UAV Networks",
                "Internet of Things (IoT)",
                "Information Hiding",
                "Super-resolution Enhancement",
                "Significant Area Detection"
            ]
        },
        "title": "IH-SESD: Modeling Information Hiding With Super-Resolution Enhancement and Significant Region Detection for UAV Networks"
    },
    {
        "authors": [
            "Arghyadip Roy",
            "Nilanjan Biswas"
        ],
        "published_in": "Published in: IEEE Transactions on Green Communications and Networking ( Early Access )",
        "date_of_publication": "28 October 2024",
        "doi": "10.1109/TGCN.2024.3487293",
        "publisher": "IEEE",
        "abstract": "With the advent of Mobile Edge Computing (MEC), the arriving tasks in an Internet of Things (IoT) network can be executed locally or at an MEC server. A Constrained Markov Decision Process (CMDP) formulation can capture the trade-off between computation time and power consumption. However, the optimal policy obtained by solving the CMDP problem may be sensitive to the changes in the task arrival rate. Moreover, there may be constraint violations. To address these issues, in this paper, we provide a Robust Return Robust CMDP (R3CMDP) formulation that minimizes the worst-case total discounted power consumption subject to a constraint on the worst-case total discounted deadline violations. Based on robust Dynamic Programming (DP) methods, we propose a task allocation algorithm that provably provides the optimal R3C policy. We also establish that the proposed algorithm incorporates robustness into the MDP framework with almost no additional complexity. Furthermore, we propose a low-complexity robust heuristic that can be implemented online, unlike the former algorithm. The proposed algorithms are implemented in a Network Simulator-3 (ns-3) based IoT simulation package. Numerical and simulation results establish that the proposed algorithms are more robust compared to the state-of-the-art algorithms in the face of varying task arrival rates.",
        "issn": {
            "Electronic ISSN": "2473-2400"
        },
        "keywords": {
            "IEEE Keywords": [
                "Power demand",
                "Servers",
                "Internet of Things",
                "Robustness",
                "Wireless communication",
                "Scheduling algorithms",
                "Resource management",
                "Delays",
                "Heuristic algorithms",
                "5G mobile communication"
            ],
            "Author Keywords": [
                "Mobile Edge Computing",
                "IoT",
                "Robust MDP"
            ]
        },
        "title": "Robust & Low-Complexity Task Scheduling Algorithms for a Mobile Edge Computing System"
    },
    {
        "authors": [
            "Muhammad Khawar Nadeem",
            "Shaomeng Wang",
            "Bilawal Ali",
            "Jibran Latif",
            "Yubin Gong"
        ],
        "published_in": "Published in: IEEE Transactions on Plasma Science ( Early Access )",
        "date_of_publication": "22 February 2024",
        "doi": "10.1109/TPS.2024.3366295",
        "publisher": "IEEE",
        "abstract": "Gridless inductive output tube (IOT) amplifiers allow for multi-megawatt power generation, with compact size, moderate gain, and long operational lifetime. The beam collector is at risk of damage and surface erosion when subject to intense electron beams. Beam collectors with small internal surface areas are susceptible to surface plasma effects. Moreover, adequate water cooling is necessary at megawatt power. This article presents a comparative study of IOT collectors, subject to a high-energy 0.1-MeV annular electron beam. Three designs of copper collectors are presented and compared. Namely, a flat beam dump, a tapered conical collector, and a two-stage depressed collector. Surface plasma formation is characterized using the continuous slowing down approximation (CSDA) model for 5- $\\mu$ s device operation. Temperature distributions are obtained using coupled particle–thermal simulations in CST Studio. The heat transfer coefficient is calculated using the Nusselt number for water cooling. Results show that a cylindrical beam dump is generally inefficient under repetitive pulsed operation, with 181 J/g energy dose, and temperatures exceeding 1000 $^\\circ$ C at the incident surface. The conical collector shows temperatures below 700 $^\\circ$ C, and energy dose below the safe limit of 100 J/g if the surface area is higher than 49.5 cm $^2$ . Finally, the two-stage depressed collector with 43.3% collector efficiency is simulated with a 20-kV potential at the second stage. It shows the lowest average temperature, remaining below 550 $^\\circ$ C, with an electron energy dose of 19 J/g.",
        "issn": {
            "Print ISSN": "0093-3813",
            "Electronic ISSN": "1939-9375"
        },
        "keywords": {
            "IEEE Keywords": [
                "Electrons",
                "Plasmas",
                "Surface treatment",
                "Heat transfer",
                "Copper",
                "Voltage",
                "Plasma temperature"
            ],
            "Author Keywords": [
                "Beam collector",
                "continuous slowing down approximation (CSDA)",
                "inductive output tube (IOT)",
                "particle–thermal simulation",
                "surface plasma"
            ]
        },
        "title": "Comparative Study of Collectors for High-Power Gridless Inductive Output Tube"
    },
    {
        "authors": [
            "Adnan Hanif",
            "Miloš Doroslovački"
        ],
        "published_in": "Published in: IEEE Transactions on Wireless Communications ( Early Access )",
        "date_of_publication": "09 October 2024",
        "doi": "10.1109/TWC.2024.3472511",
        "publisher": "IEEE",
        "abstract": "The next-generation wireless networks operating in the Terahertz (THz) band have the potential to satisfy the diverse demands of their always-connected devices. Recent advances in device electronics and antenna design in the THz band herald the massive deployment of millimeter-scale Internet-of-Things (IoT) that often have limited battery capacity due to physical constraints. Consequently, the potential employment of simultaneous wireless information and power transfer (SWIPT) in the THz band emerges as a transformative paradigm shift, albeit complicated by challenges like molecular absorption loss and spectrum-efficient waveform design. In this paper, we demonstrate the applicability of low-complexity signal modulation schemes to achieve SWIPT to battery-less IoT devices in the THz band. Specifically, the comparative performances of two pulse-based THz signal modulation schemes, return-to-zero on-off keying (RZ-OOK) modulation and pulse position modulation (PPM), are analyzed for SWIPT from a transmitter to a rectenna-based receiver (RRx). To this end, tractable models are proposed for both wireless power transfer (WPT) and wireless information transfer (WIT) between the transmitter and RRx through a non-linear THz channel. The theoretical models are validated with circuit simulations comprising THz band GaAs Schottky diode to showcase the novel performance analysis of both pulse modulation schemes in terms of WPT and WIT metrics in the THz band.",
        "issn": {
            "Print ISSN": "1536-1276",
            "Electronic ISSN": "1558-2248"
        },
        "keywords": {
            "IEEE Keywords": [
                "Terahertz communications",
                "Modulation",
                "Terahertz radiation",
                "Simultaneous wireless information and power transfer",
                "Internet of Things",
                "Pulse modulation",
                "Wireless networks",
                "Integrated circuit modeling",
                "Schottky diodes",
                "Rectennas"
            ],
            "Author Keywords": [
                "6G and beyond",
                "THz band channel",
                "IoT",
                "SWIPT",
                "non-linear model",
                "battery-less RRx",
                "RZ-OOK",
                "PPM",
                "STIPT"
            ]
        },
        "title": "Performance Analysis of Pulse Modulation Schemes for SWIPT in Non-Linear Terahertz Channel"
    },
    {
        "authors": [
            "Jinhang Wei",
            "Longyue Wang",
            "Zhecheng Zhou",
            "Linlin Zhuo",
            "Xiangxiang Zeng",
            "Xiangzheng Fu",
            "Quan Zou",
            "Keqin Li",
            "Zhongjun Zhou"
        ],
        "published_in": "Published in: IEEE Journal of Biomedical and Health Informatics ( Early Access )",
        "date_of_publication": "11 November 2024",
        "doi": "10.1109/JBHI.2024.3496294",
        "publisher": "IEEE",
        "abstract": "Cloud computing and Internet of Things (IoT) technologies are gradually becoming the technological changemakers in cancer diagnosis. Blood cancer is an aggressive disease affecting the blood, bone marrow, and lymphatic system, and its early detection is crucial for subsequent treatment. Flow cytometry has been widely studied as a commonly used method for detecting blood cancer. However, the high computation and resource consumption severely limit its practical application, especifically in regions with limited medical and computational resources. In this study, with the help of cloud computing and IoT technologies, we develop a novel blood cancer dynamic monitoring diagnostic model named BloodPatrol based on an intelligent feature weight fusion mechanism. The proposed model is capable of capturing the dual-view importance relationship between cell samples and features, greatly improving prediction accuracy and significantly surpassing previous models. Besides, benefiting from the powerful processing ability of cloud computing, BloodPatrol can run on a distributed network to efficiently process large-scale cell data, which provides immediate and scalable blood cancer diagnostic services. We have also created a cloud diagnostic platform to facilitate access to our work, the latest access link and updates are available at: https://github.com/kkkayle/BloodPatrol .",
        "issn": {
            "Print ISSN": "2168-2194",
            "Electronic ISSN": "2168-2208"
        },
        "keywords": {
            "IEEE Keywords": [
                "Cancer",
                "Blood",
                "Computer architecture",
                "Microprocessors",
                "Predictive models",
                "Data models",
                "Feature extraction",
                "Computational modeling",
                "Cloud computing",
                "Analytical models"
            ],
            "Author Keywords": [
                "cloud computing in healthcare",
                "IoT in medical diagnostics",
                "blood cancer diagnosis",
                "intelligent feature weight fusion mechanism",
                "flow cytometry data analysis"
            ]
        },
        "title": "BloodPatrol: Revolutionizing Blood Cancer Diagnosis - Advanced Real-Time Detection Leveraging Deep Learning & Cloud Technologies"
    },
    {
        "authors": [
            "Lennart Almstedt",
            "Francesco Betti Sorbelli",
            "Bas Boom",
            "Rosalba Calvini",
            "Elena Costi",
            "Alexandru Dinca",
            "Veronica Ferrari",
            "Daniele Giannetti",
            "Loretta Ichim",
            "Amin Kargar",
            "Catalin Lazar",
            "Lara Maistrello",
            "Alfredo Navarra",
            "David Niederprüm",
            "Peter Offermans",
            "Brendan O'Flynn",
            "Lorenzo Palazzetti",
            "Niccolò Patelli",
            "Cristina M. Pinotti",
            "Dan Popescu",
            "Aravind K. Rangarajan",
            "Liviu Serghei",
            "Alessandro Ulrici",
            "Lars Wolf",
            "Dimitrios Zorbas",
            "Leonard Zurek"
        ],
        "published_in": "Published in: IEEE Transactions on AgriFood Electronics ( Early Access )",
        "date_of_publication": "14 October 2024",
        "doi": "10.1109/TAFE.2024.3469538",
        "publisher": "IEEE",
        "abstract": "The invasive insect brown marmorated stink bug (BMSB) is an emerging pest of global importance, as it is destroying fruits and seeds, having caused estimated damages of € 588 million to crops in 2019 in Northern Italy alone. An open challenge is to improve monitoring of BMSB in order to be able to deploy countermeasures more efficiently and to increase consumer confidence in the end product. The Horizon 2020 Haly.ID project seeks to reduce or eliminate dependence on conventional monitoring tools and practices, such as traps, baits, visual inspections, sweep netting, and tree beating. In their place, the project proposes the use of unmanned aerial vehicle (UAV) and Internet of Things (IoT) solutions for monitoring the insect population and investigates novel methods for enhancing the quality of fruit in the market. In this work, we focus on the novel autonomous IoT insect monitoring system consisting of multiple innovative solutions for BMSB monitoring and trusted data management developed in Haly.ID . In particular, this article describes the challenges faced when integrating and deploying this monitoring system consisting of those different parts and aims at presenting valuable “lessons learned” for the realization of future deployments. We show that massive over-provisioning of power supply and network speed allows to adapt the system at run-time reflecting changing project requirements, and to conduct experiments remotely. At the same time, over-provisioning introduces new weak points impacting the system reliability, such as cables that can be unplugged or damaged.",
        "issn": {
            "Electronic ISSN": "2771-9529"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Cameras",
                "Monitoring",
                "Insects",
                "Blockchains",
                "Sensor systems",
                "Temperature sensors",
                "Temperature measurement",
                "Printed circuits",
                "Humidity"
            ],
            "Author Keywords": [
                "Blockchain",
                "camera trap",
                "halyomorpha halys",
                "Internet of Things (IoT)",
                "pest monitoring",
                "real-world deployment",
                "sensor networks",
                "smart farming"
            ]
        },
        "title": "A Comprehensive Pest Monitoring System for Brown Marmorated Stink Bug"
    },
    {
        "authors": [
            "Vignon Fidele Adanvo",
            "Samuel Mafra",
            "Samuel Montejo-Sánchez",
            "Felipe Augusto Tondo",
            "Richard Demo Souza"
        ],
        "published_in": "Published in: IEEE Transactions on Aerospace and Electronic Systems ( Early Access )",
        "date_of_publication": "06 September 2024",
        "doi": "10.1109/TAES.2024.3455312",
        "publisher": "IEEE",
        "abstract": "The task of offloading packets in LEO satellite networks in an IoT cluster context is often affected by excessive energy consumption, frequent loss of data packets in the medium access, limited visibility time with the ground station, and limited resources available to the satellites. Inter-satellite communication (ISC) techniques have been introduced to address these challenges. However, these techniques require high technical and strategic development to meet the requirements of the end users. In this regard, four strategies are proposed: Brief (which eliminates all redundancy); Balanced (which eliminates the redundancy but keeps the amount of information for each satellite balanced); Redundancy (which includes all the information collected on each satellite); and Coded (which provides redundancy through information coding). Our techniques are to exchange signaling or data packets between satellites via ISC to download the collected data. At the same time, one of these goals is prioritized: the lowest energy consumption, the highest reliability of information delivery, or the balance between the previous two. Consequently, the system performance is evaluated from each evolved stage and the overall system. The results show that Redundancy and Coded techniques increase the offloading rate, while Brief and Balanced techniques increase the system's energy efficiency. Since Brief and Balanced techniques avoid redundant information transmission, decreasing the slot time. The Coded technique presents a better tradeoff between energy efficiency and reliability than the Brief , Balanced , and Redundancy techniques. Finally, the results show the positive impact of ISC on satellite networks that collect information from ground IoT devices to download it to an Earth station.",
        "issn": {
            "Print ISSN": "0018-9251",
            "Electronic ISSN": "1557-9603"
        },
        "keywords": {
            "IEEE Keywords": [
                "Satellites",
                "Satellite broadcasting",
                "Low earth orbit satellites",
                "Internet of Things",
                "Redundancy",
                "Throughput",
                "Energy efficiency"
            ],
            "Author Keywords": [
                "Inter-satellite communications",
                "internet of things (IoT)",
                "low earth orbit (LEO)",
                "satellite"
            ]
        },
        "title": "Improving Efficiency and Reliability in Information Offloading for LEO Satellite Networks by Inter-Satellite Communication Techniques"
    },
    {
        "authors": [
            "Ying Qiao",
            "Shuyang Teng",
            "Juan Luo",
            "Peng Sun",
            "Fan Li",
            "Fengxiao Tang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "30 October 2024",
        "doi": "10.1109/JIOT.2024.3488076",
        "publisher": "IEEE",
        "abstract": "In satellite Internet of Things (IoT), the remote sensing satellites capture images and then transmit them to a ground station through Low Earth Orbit (LEO) communication satellites for model inference. However, this process results in significant transmission latency and communication overhead. In response, researchers have proposed various satellite on-orbit model inference methods. Nonetheless, the limited computation capacity and memory space of a single remote sensing satellite impose processing delays when dealing with large quantities of high-resolution images, thereby making it difficult to ensure real-time service. To tackle this issue, we propose an on-orbit deep neural network (DNN) distributed inference framework for remote sensing images in satellite IoT, leveraging the availability of numerous LEO computing satellites. Designing such a framework involves two crucial questions: first, determining which LEO satellites should participate in distributed DNN inference, and second, how to partition the images among the selected LEO satellites. To address these questions, we formulate the distributed inference process as a mixed integer non-linear optimization problem, which is known to be NP-hard. The objective is to minimize overall energy consumption while ensuring that the distributed inference is accomplished when the satellite dynamic network remains unchanged. We initially propose a dynamic optimization algorithm that derives the optimal solution with rigorous theoretical guarantees. Subsequently, to reduce computational complexity, we introduce an approximate solution based on an improved simulated annealing algorithm. We demonstrate that the approximate algorithm performs within a limited range of the optimal algorithm. Finally, we build a heterogeneous testbed based on Kubernetes and conduct extensive experiments to validate that our proposed algorithms reduce energy consumption by an average of 24.63% and 25.98% on Faster-RCNN inference model, 47.0...",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Satellites",
                "Low earth orbit satellites",
                "Remote sensing",
                "Heuristic algorithms",
                "Computational modeling",
                "Collaboration",
                "Inference algorithms",
                "Delays",
                "Data models",
                "Energy consumption"
            ],
            "Author Keywords": [
                "LEO Satellites",
                "DNN Inference",
                "Dynamic Programming",
                "Simulated Annealing Algorithm",
                "Satellite IoT",
                "Kubernetes"
            ]
        },
        "title": "On-Orbit DNN Distributed Inference for Remote Sensing Images in Satellite Internet of Things"
    },
    {
        "authors": [
            "Minsoo Kim",
            "Jalel Ben-Othman",
            "Bang Chul Jung",
            "Hyunbum Kim"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/JIOT.2024.3490560",
        "publisher": "IEEE",
        "abstract": "Addressing the limitations of traditional sensor-based systems vulnerable to environmental constraints and trust issues, this paper introduces a blockchain-assisted maximum evacuation framework in zero trust hiking trail and mountainous terrain using IoT devices that leverages blockchain technology for enhanced security and reliability. We devise three innovative algorithms that are designed to maximize the activation of evacuation nodes, ensuring rapid and efficient disaster response. By updating safe areas and evacuation routes dynamically in real-time IoT environment, the developed algorithms aim to significantly improve emergency response capabilities in challenging terrains. Also, the extensive simulations are achieved to demonstrate the performances of the proposed schemes with discussions for obtained outcomes.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Disasters",
                "Internet of Things",
                "Sensors",
                "Surveillance",
                "Reliability",
                "Blockchains",
                "Zero Trust",
                "Sensor systems",
                "Vectors",
                "Sensor phenomena and characterization"
            ],
            "Author Keywords": [
                "Blockchain",
                "evacuation",
                "zero trust",
                "IoT",
                "mountainous terrain"
            ]
        },
        "title": "Blockchain-Enabled Maximum Evacuation System Using Hybrid Voting in Zero Trust Hiking Trail and Mountainous Terrain"
    },
    {
        "authors": [
            "Archana K. Rajan",
            "Masaki Bandai"
        ],
        "published_in": "Published in: IEICE Transactions on Communications ( Early Access )",
        "date_of_publication": "28 June 2024",
        "doi": "10.23919/transcom.2024EBP3023",
        "publisher": "IEICE",
        "abstract": "Constrained Application Protocol (CoAP) is a popular UDP based data transfer protocol designed for constrained nodes in lossy networks. Congestion control method is essentially required in such environment to ensure proper data transfer. CoAP offers a default congestion control technique based on binary exponential backoff (BEB) where the default method calculates retransmission time out (RTO) irrespective of Round Trip Time (RTT). CoAP simple congestion control/advanced (COCOA) is a standard alternative algorithm. COCOA computes RTO using exponential weighted moving average (EWMA) based on type of RTT; strong RTT or weak RTT. The constant weight values used in COCOA delays the variability of RTO. This delay in converging the RTO can cause QoS problems in many IoT applications. In this paper, we propose a new method called Flexi COCOA to accomplish a flexible weight based RTO computation for strong RTT samples. We show that Flexi COCOA is more network sensitive than COCOA. Specifically, the variable weight focuses on deriving better RTO and utilizing the resources more. Flexi COCOA is implemented and validated against COCOA using the Cooja simulator in Contiki OS. We carried out extensive simulations using different topologies, packet sending rates and packet error rates. Our results show that Flexi COCOA outshines COCOA and can improve QoS of IoT monitoring applications.",
        "issn": {
            "Electronic ISSN": "1745-1345",
            "Print ISSN": "0916-8516"
        },
        "keywords": {
            "IEEE Keywords": [
                "Protocols",
                "Internet of Things",
                "Delays",
                "Servers",
                "Receivers",
                "Packet loss",
                "Estimation"
            ],
            "Author Keywords": [
                "IoT",
                "Application Protocol",
                "CoAP",
                "COCOA",
                "Congestion Control"
            ]
        },
        "title": "Flexi COCOA: Flexible weight based RTO computation approach for COCOA"
    },
    {
        "authors": [
            "Erfan Delfani",
            "George J. Stamatakis",
            "Nikolaos Pappas"
        ],
        "published_in": "Published in: IEEE Transactions on Green Communications and Networking ( Early Access )",
        "date_of_publication": "14 October 2024",
        "doi": "10.1109/TGCN.2024.3479460",
        "publisher": "IEEE",
        "abstract": "In this study, we investigate the optimal transmission policies within an energy harvesting status update system, where the demand for status updates depends on the state of the source. The system monitors a source with two contextual states characterized by a Markovian stochastic process, which can be in either a normal state or an alarm state, with a higher demand for fresh updates when the source is in the alarm state. We propose a metric to capture the freshness of status updates for each state of the stochastic process by introducing two Age of Information (AoI) variables, extending the definition of AoI to account for the state changes of the stochastic process. We formulate the problem as a Markov Decision Process (MDP), utilizing a transition cost function that applies linear and non-linear penalties based on AoI and the state of the stochastic process. Through analytical investigation, we delve into the structure of the optimal transmission policy for the resulting MDP problem. Furthermore, we evaluate the derived policies via numerical results and demonstrate their effectiveness in reserving energy in anticipation of forthcoming alarm states.",
        "issn": {
            "Electronic ISSN": "2473-2400"
        },
        "keywords": {
            "IEEE Keywords": [
                "Stochastic processes",
                "Sensors",
                "Monitoring",
                "Energy harvesting",
                "Sensor systems",
                "Measurement",
                "Internet of Things",
                "Optimal scheduling",
                "Costs",
                "Receivers"
            ],
            "Author Keywords": []
        },
        "title": "State-Aware Timeliness in Energy Harvesting IoT Systems Monitoring a Markovian Source"
    },
    {
        "authors": [
            "Hao Wang",
            "Zhichao Chen",
            "Zhaoran Liu",
            "Licheng Pan",
            "Hu Xu",
            "Yilin Liao",
            "Haozhe Li",
            "Xinggao Liu"
        ],
        "published_in": "Published in: IEEE Transactions on Industrial Informatics ( Early Access )",
        "date_of_publication": "13 September 2024",
        "doi": "10.1109/TII.2024.3452241",
        "publisher": "IEEE",
        "abstract": "Missing data imputation is a critical aspect of the Industrial Internet-of-Things (IIoT), which is uniquely challenged by local relationships within data due to different operational contexts and phases. Current imputation methods struggle to accommodate local relationships due to their black-box nature or limited capacity. To bridge this gap, we approach data imputation as a distribution alignment problem and leverage optimal transport to instantiate it for enhanced capacity. Specifically, we first introduce the similarity preserved optimal transport (SPOT) problem, with a conditional gradient solution to compute the transport cost. Subsequently, we propose the SPOT for imputation (SPOT-I) framework. It minimizes the transport cost of SPOT for distribution alignment and uses the gradient to update imputations, which maintains local similarity and refines imputation due to the characteristics of SPOT. Experiments on IIoT datasets showcase the superiority of SPOT-I over state-of-the-art imputation methods.",
        "issn": {
            "Print ISSN": "1551-3203",
            "Electronic ISSN": "1941-0050"
        },
        "keywords": {
            "IEEE Keywords": [
                "Imputation",
                "Industrial Internet of Things",
                "Costs",
                "Standards",
                "Instruments",
                "Informatics",
                "Computational modeling"
            ],
            "Author Keywords": [
                "Industrial time series",
                "Internet of Things",
                "missing data imputation (MDI)",
                "process monitoring"
            ]
        },
        "title": "SPOT-I: Similarity Preserved Optimal Transport for Industrial IoT Data Imputation"
    },
    {
        "authors": [
            "Subhranshu Sekhar Tripathy",
            "Sujit Bebortta",
            "Chinmay Chakraborty",
            "Dilip Senapati",
            "Subhendu Kumar Pani",
            "Manisha Guduri"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "10 June 2024",
        "doi": "10.1109/TCE.2024.3411869",
        "publisher": "IEEE",
        "abstract": "The integration of Fog computing and 5G communication may enhance Cyber Physical Systems (CPSs) for effective time identification of cyber attacks among consumer electronics. An unsupervised Intrusion Detection System (IDS) based on Generative Adversarial Networks (GANs) and Recurrent Neural Networks (RNNs) is presented in this study. This system is tailored to the resource limitations of consumer electronics within CPSs. By leveraging the processing power of fog nodes and the low-latency capabilities of 5G networks, cyber attacks can be swiftly identified. The use of a trained Long Short-Term Memory (LSTM) network encoder improves detection rates by enhancing reconstruction loss computation. Experimental results demonstrate that this approach, implemented through distributed fog computing infrastructure, offers better detection rates with a 15.2% reduction in detection latency and a 24.2% decrease in overall energy consumption compared to baseline methods. This innovative system could serve as an effective alternative for securing consumer electronic devices integrated into CPSs.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Consumer electronics",
                "Edge computing",
                "Internet of Things",
                "Training",
                "Intrusion detection",
                "Data models",
                "Long short term memory"
            ],
            "Author Keywords": [
                "Internet of Things",
                "Fog Computing",
                "Deep Reinforcement Learning",
                "Consumer Electronic Devices",
                "Offloading",
                "Task Prioritization",
                "Energy Efficiency",
                "Latency"
            ]
        },
        "title": "Leveraging Resource-Aware Deep Collaborative Learning Towards Secure B5G-Driven IoT-Fog-Based Consumer Electronic Systems"
    },
    {
        "authors": [
            "Duc Thang Ngo",
            "Yen-Ju Tseng",
            "Duc Huy Nguyen",
            "Paul C.-P. Chao"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "09 September 2024",
        "doi": "10.1109/JIOT.2024.3454691",
        "publisher": "IEEE",
        "abstract": "A high-accuracy biometric identification system based on photoplethysmography (PPG) is proposed in this study. Equipped with continuous quality assessment on PPG in real-time by calculated power spectral density (PSD) and large-area organic photodetectors (OPDs) in the PPG sensor offering low-noise PPG, the deep learning model built herein is able to acquire delicate PPG features varying clearly from subject to subject, and then achieves high accuracy for biometric applications. It is known that PPG is a technology capable of measuring blood volume changes by emitting optical power into skin, reaching blood vessels and collects the reflected optical power back and out of skin, suitable for ensuring live-body bio-metrics while many other bio-metrics are unable to. The raw PPG measured by the PPG device is first pre-processed by a bandpass filter, and then those with low power spectral density (PSD) of PPG versus noise or large DC drifts are screened out in real time to ensure the signal quality of PPG prior to bio-metrics. This pre-processing step is crucial to disregard all the unqualified PPG that may lead to wrongful result of biometrics later. The bio-metrics is next conducted by a built deep-learning (DL) model of a convolutional neural network (CNN) and long short-term memory (LSTM) layers. The DL model is trained by the PPG data collected from 42 subjects. Experimental results show an accuracy of 99.64% for binary while 98.8% for multi-class classification, outperforming other related works using PPG.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Biometrics",
                "Biological system modeling",
                "Real-time systems",
                "Deep learning",
                "Photodiodes",
                "Band-pass filters",
                "Accuracy"
            ],
            "Author Keywords": [
                "Biometric",
                "deep learning",
                "quality check (QC)",
                "photoplethysmography (PPG)",
                "non-invasive"
            ]
        },
        "title": "Precision Biometrics Based on the PPG Measured From an IoT Device With OPDs, Real-Time Quality Check Through PSD, DC-Drift and Deep-Learning"
    },
    {
        "authors": [
            "Dignde Jiang",
            "Bowen Zhu",
            "Xinhui Liu",
            "Shahid Mumtaz"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "15 July 2024",
        "doi": "10.1109/JIOT.2024.3427642",
        "publisher": "IEEE",
        "abstract": "Container technologies promise efficient deployment of distributed services, but their potential is hampered by suboptimal resource utilization and network congestion stemming from initial placement decisions made without knowledge of future demands. Existing container cluster management strategies lack robust adaptive capabilities to efficiently balance load as work-loads evolve unpredictably over time. This paper puts forth the Adaptive Load-aware Container Deployment (ALCoD), a novel container cluster management approach integrating worst fit decreasing heuristic placement with deep reinforcement learning-based migration optimization. ALCoD adapts to fluctuating resource availability and service demands by leveraging the complementary strengths of each technique. The worst fit decreasing approach allows rapid initial cluster deployment when resources are abundantly available, while the deep reinforcement learning policy orchestrates intelligent container migrations to optimize load balancing during times of resource scarcity, maintaining service availability throughout. Comprehensive evaluations verified that compared to state-of-the-art strategies, ALCoD reduces system response times by 29.19%, improves load balancing by 51.31%, and decreases bandwidth usage by 27.4% under real-world conditions. Beyond these raw performance improvements, ALCoD demonstrates the potential of hybrid algorithms that blend complementary techniques to match the intrinsic dynamics of container clusters. This pioneering approach establishes a solid foundation for realizing the full promise of containerized services through reliable, responsive delivery even as operating conditions continuously evolve.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Containers",
                "Load management",
                "Internet of Things",
                "Heuristic algorithms",
                "Cloud computing",
                "Resource management",
                "Optimization"
            ],
            "Author Keywords": [
                "Cloud container cluster",
                "adaptive load balancing",
                "container deployment",
                "container migration",
                "deep reinforcement learning"
            ]
        },
        "title": "ALCoD: An Adaptive Load-Aware Approach to Load Balancing for Containers in IoT Edge Computing"
    },
    {
        "authors": [
            "Hai Zhu",
            "Xingsi Xue",
            "Mengmeng Xu",
            "Byung-Gyu Kim"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "01 October 2024",
        "doi": "10.1109/TCE.2024.3471921",
        "publisher": "IEEE",
        "abstract": "The rapid proliferation of smart consumer devices has given rise to the consumer Internet of Things (CIoT), enabling immense data collection and valuable insights for enhancing con-sumer experiences. However, the distributed nature of CIoT sys-tems and the sensitivity of consumer data pose significant chal-lenges in ensuring security, privacy, and zero trust. This paper proposes a novel framework integrating robust federated learning with a main-side blockchain architecture and zero trust principles to enable secure and privacy-preserving data sharing and collabo-rative learning in CIoT environments. The proposed system model consists of CIoT devices as side nodes, edge servers as main nodes, and a cloud server for global aggregation. A lightweight privacy-preserving aggregation protocol is designed based on secret shar-ing to protect raw data during local model updates. To enhance the robustness against Byzantine attacks and data heterogeneity, a resampling-based robust aggregation method is developed, which evaluates the cosine similarity of local updates against a ref-erence gradient securely selected via the main-side blockchain. Ex-periments demonstrate that the proposed framework performs well regarding model accuracy, convergence speed, and resiliency compared with state-of-the-art methods.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Servers",
                "Computational modeling",
                "Blockchains",
                "Data models",
                "Zero Trust",
                "Training",
                "Security",
                "Cryptography",
                "Protocols",
                "Performance evaluation"
            ],
            "Author Keywords": [
                "Consumer Internet of Things",
                "zero trust",
                "feder-ated learning",
                "blockchain",
                "privacy",
                "Byzantine attacks"
            ]
        },
        "title": "Zero Trust Consumer IoT With Robust Federated Learning Over Main-Side Blockchain"
    },
    {
        "authors": [
            "Mahsa Raeiszadeh",
            "Amin Ebrahimzadeh",
            "Roch H. Glitho",
            "Johan Eker",
            "Raquel A. F. Mini"
        ],
        "published_in": "Published in: IEEE Transactions on Network and Service Management ( Early Access )",
        "date_of_publication": "21 August 2024",
        "doi": "10.1109/TNSM.2024.3447532",
        "publisher": "IEEE",
        "abstract": "To ensure reliability and service availability, nextgeneration networks are expected to rely on automated anomaly detection systems powered by advanced machine learning methods with the capability of handling multi-dimensional data. Such multi-dimensional, heterogeneous data occurs mostly in todays Industrial Internet of Things (IIoT), where real-time detection of anomalies is critical to prevent impending failures and resolve them in a timely manner. However, existing anomaly detection methods often fall short of effectively coping with the complexity and dynamism of multi-dimensional data streams in IIoT. In this paper, we propose an adaptive method for detecting anomalies in IIoT streaming data utilizing a multi-source prediction model and concept drift adaptation. The proposed anomaly detection algorithm merges a prediction model into a novel drift adaptation method resulting in accurate and efficient anomaly detection that exhibits improved scalability. Our trace-driven evaluations indicate that the proposed method outperforms the state-of-theart anomaly detection methods by achieving up to an 89.71 accuracy (in terms of Area under the Curve (AUC)) while meeting the given efficiency and scalability requirements.",
        "issn": {
            "Electronic ISSN": "1932-4537"
        },
        "keywords": {
            "IEEE Keywords": [
                "Anomaly detection",
                "Industrial Internet of Things",
                "Real-time systems",
                "Concept drift",
                "Predictive models",
                "Autoregressive processes",
                "Accuracy"
            ],
            "Author Keywords": [
                "Anomaly Detection",
                "Real-time Analytics",
                "Concept Drift",
                "Streaming Data",
                "Industrial Internet of Things (IIoT)"
            ]
        },
        "title": "Real-Time Adaptive Anomaly Detection in Industrial IoT Environments"
    },
    {
        "authors": [
            "Xiaohuan Li",
            "Bitao Chen",
            "Junchuan Fan",
            "Jiawen Kang",
            "Jin Ye",
            "Xun Wang",
            "Dusit Niyato"
        ],
        "published_in": "Published in: IEEE Transactions on Network and Service Management ( Early Access )",
        "date_of_publication": "19 August 2024",
        "doi": "10.1109/TNSM.2024.3441231",
        "publisher": "IEEE",
        "abstract": "By using the intelligent edge computing technologies, a large number of computing tasks of end devices in Industrial Internet of Things (IIoT) can be offloaded to edge servers, which can effectively alleviate the burden and enhance the performance of IIoT. However, in large-scale multi-service-oriented IIoT scenarios, offloading service resources are heterogeneous and offloading requirements are mutually exclusive and time-varying, which reduce the offloading efficiency. In this paper, we propose a cloud-edge-end collaboration intelligent service computation offloading scheme based on Digital Twin (DT) driven Edge Coalition Formation (DECF) approach to improve the offloading efficiency and the total utility of edge servers, respectively. Firstly, we establish a DT model to obtain accurate digital representations of heterogeneous end devices and network state parameters in dynamic and complex IIoT scenarios. The DT model can capture time-varying requirements in a low latency manner. Secondly, we formulate two optimization problems to maximize the offloading throughput and total system utility. Finally, we convert the multiobjective optimization problems to a Stackelberg coalition game model and develop a distributed coalition formation approach to balance the two optimizing objectives. Simulation results indicate that, compared with the nearest coalition scheme and non-coalition scheme, the proposed approach achieves offloading throughput improvements of 11.5% and 148%, and enhances the overall utility by 12% and 170%, respectively.",
        "issn": {
            "Electronic ISSN": "1932-4537"
        },
        "keywords": {
            "IEEE Keywords": [
                "Cloud computing",
                "Industrial Internet of Things",
                "Task analysis",
                "Optimization",
                "Games",
                "Computational modeling",
                "Heuristic algorithms"
            ],
            "Author Keywords": [
                "Cloud-edge-end",
                "Digital Twin (DT)",
                "Coalition game",
                "Computation offloading"
            ]
        },
        "title": "Cloud-Edge-End Collaborative Intelligent Service Computation Offloading: A Digital Twin Driven Edge Coalition Approach for Industrial IoT"
    },
    {
        "authors": [
            "Amar Kumar Mishra",
            "Kamal Agrawal",
            "Shankar Prakriya"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/JIOT.2024.3490794",
        "publisher": "IEEE",
        "abstract": "This work investigates a multi-hop multi-relay network in which all nodes are of the battery-assisted (BA) energy harvesting (EH) type, and harvest energy from a power beacon. In each hop, the node with the most harvested energy is selected for relaying and augments the harvested energy with some battery energy. Note that such BA EH nodes are immediately realizable to prolong battery lifetimes. Considering nonlinear EH, analytical expressions are derived for outage probability, throughput, average battery energy consumption, and battery energy efficiency (BEE). It is first shown how the parameters can be chosen to maximize the network throughput. Since unsuccessful relaying that results in outage causes battery energy to be consumed, optimizing the network to minimize the average battery energy consumption or to maximize the node-level BEE is well motivated. We show that the judicious selection of the EH duration can minimize average battery energy consumption while achieving the target throughput requirement. Moreover, a joint optimal choice of EH duration, hop count, and target information rate is shown to maximize overall BEE. Monte Carlo simulations validate the accuracy of the derived expressions.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Batteries",
                "Relays",
                "Spread spectrum communication",
                "Throughput",
                "Protocols",
                "Probability",
                "Power system reliability",
                "Energy efficiency",
                "Energy consumption",
                "Wireless sensor networks"
            ],
            "Author Keywords": [
                "Average Battery energy consumption",
                "battery energy efficiency",
                "Energy harvesting",
                "multi-hop",
                "outage probability",
                "relay-selection",
                "time-splitting",
                "throughput"
            ]
        },
        "title": "Performance of a Cluster-Based Multi-hop IoT Network with Battery-Assisted Energy Harvesting"
    },
    {
        "authors": [
            "Jianhua Tang",
            "Fangfang Chen",
            "Jiaping Li",
            "Zilong Liu"
        ],
        "published_in": "Published in: IEEE Transactions on Cognitive Communications and Networking ( Early Access )",
        "date_of_publication": "19 August 2024",
        "doi": "10.1109/TCCN.2024.3445342",
        "publisher": "IEEE",
        "abstract": "In the context of the Industrial Internet of Things (IIoT), developing an accurate and timely scheduling policy is essential. Recently, the Age of Incorrect Information (AoII) is proposed for measuring the timeliness and accuracy of certain status information for monitoring/controlling purposes. In this work, we investigate a multi-sensor state updating system in which AoII is used for quantifying information freshness. We aim to find an optimal scheduling policy to minimize the system-wide cost under bandwidth constraint. We first model the source status updates monitored by sensors as Markov chains and the scheduling problem as a constrained Markov decision process (CMDP). It is challenging to solve the formulated CMDP problem by conventional methods, due to the heterogeneity of source status updates in IIoT and the bandwidth constraint. As such, a framework with the aid of deep reinforcement learning, i.e., Order-Preserving Quantization-Based Constrained Reinforcement Learning Algorithm with Historical Adjustment (OPQ-RL_HA) is developed. Furthermore, by integrating it with the Asynchronous Advantage Actor-Critic (A3C) and the Deep Deterministic Policy Gradient (DDPG), two different algorithms are proposed, i.e., OPQ-A3C_HA and OPQ-DDPG_HA. With extensive numerical validation, it is demonstrated that the proposed algorithm has a lower average system-wide cost compared to the benchmark algorithms.",
        "issn": {
            "Electronic ISSN": "2332-7731"
        },
        "keywords": {
            "IEEE Keywords": [
                "Servers",
                "Optimal scheduling",
                "Sensors",
                "Industrial Internet of Things",
                "Monitoring",
                "Reinforcement learning",
                "Measurement"
            ],
            "Author Keywords": [
                "Age of Incorrect Information",
                "Data Freshness",
                "Deep Reinforcement Learning",
                "Constrained Markov Decision Process",
                "Industrial Internet of Things"
            ]
        },
        "title": "Learn to Schedule: Data Freshness-Oriented Intelligent Scheduling in Industrial IoT"
    },
    {
        "authors": [
            "Lijuan Xu",
            "Baolong An",
            "Xin Li",
            "Dawei Zhao",
            "Haipeng Peng",
            "Weizhao Song",
            "Fenghua Tong",
            "Xiaohui Han"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "13 September 2024",
        "doi": "10.1109/JIOT.2024.3459921",
        "publisher": "IEEE",
        "abstract": "Current vulnerability detection methods encounter challenges such as inadequate feature representation, constrained feature extraction capabilities, and coarse-grained detection. To address these issues, we propose a fine-grained hybrid semantic vulnerability detection framework based on Transformer, named TFHSVul. Initially, the source code is transformed into sequential and graph-based representations to capture multi-level features, thereby solving the problem of insufficient information caused by a single intermediate representation. To enhance feature extraction capabilities, TFHSVul integrates multi-scale fusion convolutional neural network, residual graph convolutional network, and pre-trained language model into the core architecture, significantly boosting performance. We design a fine-grained detection method based on a self-attention mechanism, achieving statement-level detection to address the issue of coarse detection granularity. In comparison to existing baseline methods on public datasets, TFHSVul achieves a 0.58 improvement in F1 score at the function level compared to the best performing model. Moreover, it demonstrates a 10% enhancement in Top-10 accuracy at the statement level detection compared to the best performing method.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Codes",
                "Internet of Things",
                "Feature extraction",
                "Source coding",
                "Security",
                "Accuracy",
                "Semantics"
            ],
            "Author Keywords": [
                "Vulnerability detection",
                "Network security",
                "Deep learning",
                "Software vulnerabilities"
            ]
        },
        "title": "TFHSVul:A Fine-Grained Hybrid Semantic Vulnerability Detection Method Based on Self-Attention Mechanism in IOT"
    },
    {
        "authors": [
            "Liangyin Chen",
            "Yihan Wang",
            "Xuanyi Xiang",
            "Dian Jin",
            "Yi Ren",
            "Yunhai Zhang",
            "Zhiwen Pan",
            "Yanru Chen"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "16 August 2024",
        "doi": "10.1109/JIOT.2024.3444893",
        "publisher": "IEEE",
        "abstract": "In recent years, Industrial Control Systems (ICS) security incidents have revealed vulnerabilities in system hardware, user programs, and communication protocols. The various components of the ICS are connected by the Industrial Internet of Things (IIoT) protocol. Nevertheless, malicious attackers can exploit vulnerabilities in the IIoT protocol to manipulate the ICS, potentially causing damage to the associated ICS equipment. This work focuses on the challenge of identifying vulnerabilities in IIoT protocols, aiming to enhance system security through advanced fuzz testing techniques. To address the limitations of current fuzz testing in IIoT protocols, such as short prediction sequence lengths and low recognition rates, this work proposes a novel fuzz testing model based on the long attention mechanism, named TXL-Fuzz. This model is capable of handling longer protocol sequences and improving the diversity of generated test cases. Experimental results demonstrate that the model outperforms existing fuzz testers in test case recognition rate (TCRR) for protocols of different lengths. Notably, TXL-Fuzz achieves a bits-per-character (BPC) of approximately 0.5, significantly lower by nearly 0.3 compared to the Anti-Sample Fuzzer, LSTM-based model, and GRU-based model. Furthermore, it exhibits a TCRR that is 5% to 15% higher than Peach Fuzzer, Anti-Sample Fuzzer, and BLSTM-DCNNFuzz under similar conditions.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Protocols",
                "Fuzzing",
                "Industrial Internet of Things",
                "Transformers",
                "Security",
                "Task analysis",
                "Data models"
            ],
            "Author Keywords": [
                "Industrial Internet of Things Protocols",
                "Industrial Control Systems",
                "Fuzz Testing",
                "Transformer-XL"
            ]
        },
        "title": "TXL-Fuzz: A Long Attention Mechanism-Based Fuzz Testing Model for Industrial IoT Protocols"
    },
    {
        "authors": [
            "Yang Li",
            "Shuyi Chen",
            "Weixiao Meng",
            "Jiangzhou Wang"
        ],
        "published_in": "Published in: IEEE Transactions on Wireless Communications ( Early Access )",
        "date_of_publication": "23 October 2024",
        "doi": "10.1109/TWC.2024.3482187",
        "publisher": "IEEE",
        "abstract": "Low earth orbit (LEO) satellites are expected to play an important role in enhancing terrestrial Internet of Things. This paper focuses on the challenge of joint terminal activity detection (TAD) and channel estimation (CE) in grant-free non-orthogonal random access (GF-NORA)-enabled LEO communication networks. To leverage the dominant line-of-sight (LoS) path and the limited angular spread characteristic of terrestrial-satellite links, we propose a two-stage joint TAD and CE (TS-JDE) detection scheme aimed to achieve lower computational overhead. This scheme utilizes a high-accuracy algorithm for LoS path estimation and leverages the channel correlations for low-complexity NLoS paths estimation. Specifically, in LoS estimation stage, to mitigate the inaccuracies caused by Taylor expansion errors, we propose an enhanced message-passing algorithm, termed TaMP-LoS. This algorithm exploits the structured sparsity in the delay-Doppler domain and incorporates the Lagrange remainder to evaluate the expansion error, thereby enhancing estimation accuracy. In the NLoS estimation stage, we develop a sparse Bayesian learning-based algorithm, named SBL-NLoS, designed to estimate NLoS channels characterized by the limited residuals of delay and Doppler. Simulation results show that our proposed algorithm can achieve better detection performance than the existing approaches for GF-NORA-enabled LEO networks.",
        "issn": {
            "Print ISSN": "1536-1276",
            "Electronic ISSN": "1558-2248"
        },
        "keywords": {
            "IEEE Keywords": [
                "Low earth orbit satellites",
                "Estimation",
                "Satellites",
                "Delays",
                "Approximation algorithms",
                "Doppler shift",
                "Channel estimation",
                "Bayes methods",
                "Accuracy",
                "Wireless communication"
            ],
            "Author Keywords": [
                "GF-NORA",
                "LEO satellite communications",
                "Taylor expansion error",
                "message-passing algorithm",
                "sparse Bayesian learning"
            ]
        },
        "title": "Low-Complexity Grant-Free Detection with Enhanced Message-Passing in LEO Satellite-IoT"
    },
    {
        "authors": [
            "Bo-Wei Chen"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/JIOT.2024.3490252",
        "publisher": "IEEE",
        "abstract": "Visual Internet of Things (VIoT) empowers intelligent sensing by equipping terminal devices with the capability to preliminarily screen and tag sensing data for further processing. However, sensing environments are often imperfect, and partially observed data may be collected at the terminals. This causes several challenges. Firstly, the model fitting process may become oversensitive owing to the presence of varying corrupted data, e.g., continuous occlusion, therefore compromising robustness because of fitting biases. Secondly, model fitting relies on label information, which consists of fixed and equally spaced categorical variables. Such information cannot adequately reflect the underlying distribution of collected data, as it assumes that each categorical variable is uniformly distributed. This deepens the difficulty of data fitting. To solve the aforementioned problems, this study proposes robust data sensing based on ℓ2,p norms, where data collected by VIoT terminals can be converted into resilient perceptual data signatures while label information is embedded inside. To deal with the problem stemming from rigid label marginal space, this study introduces an adaptive slack variable to the proposed model. Such a slack variable can automatically adapt itself to sensing data during model fitting while adjusting label marginal space by providing flexible labels. Moreover, a new adaptive regulating mechanism is developed to control the slack variables, such that they can consider multiple coeffects from different loss terms and penalties during optimization, creating error-tolerant soft margins. This is conducive to model fitting, especially for partially observed data. In addition to label slack variables, this study also derives slack variables for ℓ2,p-norm loss that is used to capture the nuances of the data, thereby providing flexible Hamming marginal space for resilient signature generation. Experiments on open datasets show that the proposed method yields be...",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Fitting",
                "Aerospace electronics",
                "Synchronous digital hierarchy",
                "Data models",
                "Internet of Things",
                "Adaptation models",
                "Kernel",
                "Covariance matrices",
                "Visualization"
            ],
            "Author Keywords": [
                "Robust data sensing",
                "partially observed data",
                "adaptive label marginal space",
                "adaptive Hamming marginal space",
                "discriminative least squares regression",
                "Visual Internet of Things"
            ]
        },
        "title": "Robust Partially-Observed Data Sensing via ℓ2,p Norms with Flexible Adaptive Label Marginal Space for Visual IoT"
    },
    {
        "authors": [
            "Shuying Xu",
            "Ji-Hwei Horng",
            "Ching-Chun Chang",
            "Chin-Chen Chang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "08 October 2024",
        "doi": "10.1109/JIOT.2024.3476312",
        "publisher": "IEEE",
        "abstract": "Crypto-space reversible data hiding has emerged as an effective technique for transmitting secret information over the internet. However, most existing schemes are designed for uncompressed images, while almost all images are processed and transmitted in compressed formats. There is an urgent need to develop methods for compressed images, such as JPEG (Joint Photographic Experts Group). In this paper, we propose a reversible data hiding in encrypted JPEG images, where the bitstreams of AC (alternating current) coefficients and the secret data are mapped to numbers over Galois field. The obtained numbers are then utilized to conduct a polynomial for secret sharing. By reproduction into secret shares, the AC coefficients and the secret data are secured. In addition, a block sorting strategy is used to reduce image distortion under low data payload. Experimental results demonstrate that the proposed scheme outperforms state-of-the-art methods in embedding capacity while preserving the file size and conforming to the JPEG format.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Cryptography",
                "Transform coding",
                "Polynomials",
                "Image coding",
                "Encryption",
                "Internet of Things",
                "Galois fields",
                "Media",
                "Receivers",
                "Payloads"
            ],
            "Author Keywords": [
                "Crypto-space steganography",
                "Galois field",
                "JPEG",
                "reversible data hiding",
                "secret sharing"
            ]
        },
        "title": "Reversible Data Hiding in Encrypted JPEG Images With Polynomial Secret Sharing for IoT Security"
    },
    {
        "authors": [
            "Chandan Kumar",
            "Salil Kashyap"
        ],
        "published_in": "Published in: IEEE Transactions on Green Communications and Networking ( Early Access )",
        "date_of_publication": "15 May 2024",
        "doi": "10.1109/TGCN.2024.3401486",
        "publisher": "IEEE",
        "abstract": "We derive new upper bounds on outage probability (OP) and spectral efficiency (SE) for a simultaneous wireless information and energy transfer system under spatial correlation and optimal phase configuration at intelligent reflecting surface (IRS) when users are served based on round-robin (RR) scheduling, share common source to IRS links and adopt nonlinear energy harvesting. Diversity order for this system is characterized. We then extend our study to a multi-antenna source and analyze OP and SE under random and equal phase shift configurations at IRS. We design beamformers at the source and at IRS under different strategies, namely RR scheduling and simultaneous service with and without signal-to-interference-plus-noise ratio (SINR) constraint. Numerical results are presented to validate the accuracy of our statistical modeling and mathematical analysis and quantify the gain in performance relative to random and equal phase shifts. We illustrate that higher number of users can be served by increasing number of IRS elements while keeping OP fixed. We identify the operational regime where RR scheduling yields better performance than serving users simultaneously without SINR constraint. We show that increasing IRS elements can help maintain target harvested power even under stricter SINR constraint. Impact of estimation error on performance is illustrated.",
        "issn": {
            "Electronic ISSN": "2473-2400"
        },
        "keywords": {
            "IEEE Keywords": [
                "Correlation",
                "Wireless communication",
                "Wireless sensor networks",
                "Signal to noise ratio",
                "Internet of Things",
                "Array signal processing",
                "Analytical models"
            ],
            "Author Keywords": [
                "Simultaneous wireless information and energy transfer (SWIET)",
                "intelligent reflecting surface (IRS)",
                "outage probability (OP)",
                "spatial correlation",
                "round-robin (RR)"
            ]
        },
        "title": "Intelligent Reflecting Surface Aided Simultaneous Wireless Information and Energy Transfer to IoT Users Under Spatial Correlation"
    },
    {
        "authors": [
            "Xinjing Liu",
            "Taifeng Liu",
            "Hao Yang",
            "Jiakang Dong",
            "Zuobin Ying",
            "Zhuo Ma"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "29 April 2024",
        "doi": "10.1109/JIOT.2024.3386670",
        "publisher": "IEEE",
        "abstract": "Model stealing (MS) attacks pose a significant security concern for machine learning models on cloud platforms, as they can reconstruct a substitute model with limited effort to evade ownership. While detection-based methods show promise in preventing MS attacks, they often face practical challenges. Specifically, setting an appropriate threshold to distinguish malicious features from benign ones is a difficult task, often leading to a trade-off between false alarm rates and detection accuracy. To address this challenge, we design a multi-dimensional feature extraction-and-distinction scheme called MED. It is achieved through a two-layer optimization: 1) the inner layer of extraction to maximize the difference of extracted multi-dimensional features between attack and benign samples; 2) the outer layer of distinction to maximize the accuracy of distinguishing malicious features automatically. Recognizing that different MS attacks result in varied features, we design a group of feature extraction functions in the inner layer optimization, which addresses the limitations of single-feature based detection methods. Further, we employ three differently characterized models for distinction, enabling MED to distinguish different types of malicious features. Comprehensive experiments are conducted to evaluate the effectiveness of the proposed scheme: MED can detect all types of MS attacks with no more than 100 samples, with an average detection rate greater than 0.99.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Internet of Things",
                "Optimization",
                "Training",
                "Data models",
                "Computational modeling",
                "Task analysis"
            ],
            "Author Keywords": [
                "Model Stealing/Extraction Attack",
                "Model Stealing/Extraction Detection",
                "Security and Privacy"
            ]
        },
        "title": "Model Stealing Detection for IoT Services Based on Multi-Dimensional Features"
    },
    {
        "authors": [
            "Jiancheng Chi",
            "Xiaobo Zhou",
            "Fu Xiao",
            "Tie Qiu",
            "C. L. Philip Chen"
        ],
        "published_in": "Published in: IEEE Transactions on Network Science and Engineering ( Early Access )",
        "date_of_publication": "07 November 2024",
        "doi": "10.1109/TNSE.2024.3493053",
        "publisher": "IEEE",
        "abstract": "In the Multi-access Edge Computing (MEC)-based Industrial Internet of Things (IIoT), a key challenge is to make an efficient task-offloading decision. Machine learning methods have emerged as popular solutions to address this issue. However, in IIoT, it is common for the feature distribution of data to change significantly over time, i.e., data drift, and existing machine learning-based schemes struggle to frequent data drift, failing to maintain consistent high accuracy of task-offloading decisions. This struggle arises because they require extended retraining or extensive model adjustments, which involve significant delays and increased computational overhead due to the complex network structure. In this paper, we propose a B road learning-based task OFF loading scheme (BOFF). In BOFF, a data drift detection method based on statistical features and a sliding window is established to determine the occurrence of data drift in the system, while utilizing the Gini coefficient to enhance feature extraction and improve accuracy of task-offloading decision model under data drift. When data drift is detected, BOFF leverages its fast training and redeployment capabilities based on feature-enhanced broad learning to update the task offloading model and maintain accuracy. In the absence of significant data drift, minor changes in data distribution are addressed through incremental updates to slow the decline in model accuracy. Numerical results demonstrate that BOFF significantly improves the adaptability of data drift, ensuring high accuracy and efficiency of task offloading in dynamic IIoT environments.",
        "issn": {
            "Electronic ISSN": "2327-4697"
        },
        "keywords": {
            "IEEE Keywords": [
                "Industrial Internet of Things",
                "Accuracy",
                "Data models",
                "Adaptation models",
                "Computational modeling",
                "Optimization",
                "Feature extraction",
                "Servers",
                "Costs",
                "Wireless communication"
            ],
            "Author Keywords": [
                "industrial Internet of Things",
                "multi-access edge computing",
                "broad learning",
                "task offloading"
            ]
        },
        "title": "Enhancing Adaptability and Efficiency of Task Offloading by Broad Learning in Industrial IoT"
    },
    {
        "authors": [
            "Jie Wang",
            "Junhui Jiang",
            "Xinlong Chen",
            "Defu Cai",
            "Yue Wu",
            "Renzhi Lu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "14 October 2024",
        "doi": "10.1109/JIOT.2024.3479772",
        "publisher": "IEEE",
        "abstract": "Energy costs associated with the consumption of non-renewable energy sources have become an important issue in improving the international competitiveness of agriculture. Wind power, as a renewable energy source, can replace non-renewable energy sources to reduce energy costs and improve the sustainability of agricultural. However, the inherent intermittency, randomness, and volatility within weather conditions and wind speed presents a substantial challenge in accurately predicting wind power generation. This work proposes a novel YJQR-LSTM algorithm that leverages Yeo-Johnson quantile regression (YJQR) with a long short-term memory (LSTM) network for nonparametric probabilistic forecasting of wind power generation via the Intelligent Internet of Things. First, an improved YJQR model based on the YJ transformation is designed to obtain a more precise characterization of wind uncertainty, providing a more flexible probability density function for wind power generation. Then, utilizing the unique structure of the LSTM network to learn the parameters of the YJQR model, temporal features can be extracted from time series data. To mitigate the impact of outliers in the raw data on accuracy and improve computational efficiency, a novel logarithmic likelihood function is developed as the loss function utilized in the training phase. The effectiveness of the proposed algorithm is validated using a real-world dataset from five wind farms from the Global Energy Forecasting Competition. Numerical results demonstrate that the algorithm provides more accurate wind power prediction results in complex wind power data environments, which is important for making full use of wind energy and thus reducing the consumption of non-renewable energy in agriculture.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Wind power generation",
                "Predictive models",
                "Forecasting",
                "Long short term memory",
                "Probabilistic logic",
                "Accuracy",
                "Wind forecasting",
                "Prediction algorithms",
                "Internet of Things",
                "Uncertainty"
            ],
            "Author Keywords": [
                "Agriculture wind",
                "Intelligent Internet of Things",
                "Probabilistic forecasting",
                "Yeo-Johnson quantile regression",
                "Long short-term memory"
            ]
        },
        "title": "A Novel YJQR-LSTM Model for Nonparametric Probabilistic Sustainable Agriculture Wind Power Forecasting Based on Intelligent IoT"
    },
    {
        "authors": [
            "Muyan Yao",
            "Dan Tao",
            "Ruipeng Gao",
            "Peng Qi"
        ],
        "published_in": "Published in: IEEE Transactions on Industrial Informatics ( Early Access )",
        "date_of_publication": "24 September 2024",
        "doi": "10.1109/TII.2024.3421600",
        "publisher": "IEEE",
        "abstract": "Most existing works in Industrial Internet of Things (IIoT) anomaly detection either depend on computationally intensive models that exceed the capabilities of multiaccess edge computing (MEC) servers, or lightweight models that lack robustness, making them unadaptable in IIoT infrastructures. To address these challenges, we propose THREADS , a hierarchical anomaly detection framework tailored for IIoT applications. The Instance thread utilizes an efficient variational auto encoder to produce instant feedback and offloads most of the workload to MECs. On the other hand, the Shadow thread employs an attention-enhanced transformer discriminator to examine low-confidence results in the cloud. Experimental results on five large-scale datasets show THREADS achieves an average F1-Score of 0.8537 in the hierarchical mode where most of the workloads are handled by MECs, and the random access memory and CPU usage is reduced by up to 29% and 88%, respectively. Meanwhile, THREADS achieves an F1-Score of 0.8563 in a cloud-based mode, consistently outperforming state-of-the-art approaches.",
        "issn": {
            "Print ISSN": "1551-3203",
            "Electronic ISSN": "1941-0050"
        },
        "keywords": {
            "IEEE Keywords": [
                "Anomaly detection",
                "Instruction sets",
                "Industrial Internet of Things",
                "Training",
                "Transformers",
                "Computational modeling",
                "Data models"
            ],
            "Author Keywords": [
                "Anomaly detection",
                "edge computing",
                "hierarchical",
                "Industrial Internet-of-Things (IIoT)",
                "transformer",
                "variational auto encoder (VAE)"
            ]
        },
        "title": "Anomaly Detection for MEC Enabled Hierarchical Industrial IoT With Transformer Enhanced Variational Auto Encoder"
    },
    {
        "authors": [
            "Jingwei Liu",
            "Yufeng Wu",
            "Wei Du",
            "Rong Sun",
            "Guangxia Xu",
            "Lei Liu",
            "Celimuge Wu"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "27 August 2024",
        "doi": "10.1109/TCE.2024.3450649",
        "publisher": "IEEE",
        "abstract": "Nowadays, Federated Learning (FL) has emerged as a prominent technique of model training in Consumer Internet of Things (CIoT) without sharing sensitive local data. Targeting privacy leakage of cross-device FL in CIoT, various privacy-preserving FL schemes have been proposed. Regrettably, existing schemes still face three significant challenges: 1) Current privacy-preserving strategies struggle to fully defend against Byzantine attacks in FL without compromising data privacy; 2) Most privacy-preserving techniques (e.g. secret sharing) in FL result in substantial computation and communication overhead; 3) The non-colluding dual-server setting limits the applicability of FL. To overcome these challenges, we propose a Byzantine-robust hierarchical federated learning scheme, named BHFL. This scheme not only effectively defends against Byzantine attacks while safeguarding user privacy but also avoids the need for a dual-server architecture. Simultaneously, the hierarchical aggregation structure can effectively train non-IID cross-device data while maintaining high communication efficiency. We evaluate BHFL on several benchmark datasets, and the experimental results demonstrate that BHFL achieves high accuracy and Byzantine robustness compared to the popular FedAvg scheme. Therefore, BHFL is well-suited for CIoT scenarios.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Federated learning",
                "Privacy",
                "Servers",
                "Data models",
                "Computational modeling",
                "Cryptography",
                "Training"
            ],
            "Author Keywords": [
                "Federated learning",
                "Byzantine-robustness",
                "Privacy-preserving",
                "Homomorphic encryption"
            ]
        },
        "title": "Byzantine-Robust Hierarchical Aggregation for Cross-Device Federated Learning in Consumer IoT"
    },
    {
        "authors": [
            "Joonho Seon",
            "Seongwoo Lee",
            "Young Ghyu Sun",
            "Soo Hyun Kim",
            "Dong In Kim",
            "Jin Young Kim"
        ],
        "published_in": "Published in: IEEE Transactions on Emerging Topics in Computational Intelligence ( Early Access )",
        "date_of_publication": "04 June 2024",
        "doi": "10.1109/TETCI.2024.3406719",
        "publisher": "IEEE",
        "abstract": "In industrial Internet of Things (IIoT) systems, imbalanced datasets are prevalent because of the relative ease of acquiring normal operational data compared to abnormal or faulty data. An unbalanced distribution of data may lead to a biased learning problem, resulting in performance degradation of deep learning models. Data augmentation approaches based on generative adversarial networks (GAN) have been proposed to mitigate biased learning problems. However, GAN-based approaches constructed solely with convolutional neural networks may be incapable of extracting temporal properties from data. To utilize the temporal properties of data, a novel GAN structure consisting of an embedding network and recurrent neural networks is proposed in this paper. Additionally, in the novel GAN model based on mean-squared error, modified loss and mutual information terms are employed to improve training stability. From simulation results, it is confirmed that classification accuracy can be significantly improved by up to 54% based on the proposed method when compared with conventional fault diagnosis methods.",
        "issn": {
            "Electronic ISSN": "2471-285X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Generative adversarial networks",
                "Data models",
                "Generators",
                "Training",
                "Industrial Internet of Things",
                "Computational modeling",
                "Feature extraction"
            ],
            "Author Keywords": [
                "Data augmentation",
                "fault diagnosis",
                "generative adversarial networks",
                "time-series data"
            ]
        },
        "title": "Least Information Spectral GAN With Time-Series Data Augmentation for Industrial IoT"
    },
    {
        "authors": [
            "Zihao Pan",
            "Bangning Zhang",
            "Heng Wang",
            "Wenfeng Ma",
            "Daoxing Guo"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "11 July 2024",
        "doi": "10.1109/TCE.2024.3420450",
        "publisher": "IEEE",
        "abstract": "In this paper, a MVDR-beamformer-based anti-jamming scheme by using covariance matrix reconstruction (CMR) is proposed only with little prior knowledge. First, the lacking prior knowledge is divided into two parts, including steering vector and covariance matrix, where the former is acquired by adopting sliding window in Capon spatial spectrum and the latter is extracted by integrating over angular sectors complementary to sliding window. Then, we develop two efficient approximate methods to reduce the complexity. In proposed-1 methods, the Gauss-Legendre quadrature (GLQ) is used to simply the integral operation. Furthermore, the proposed-2 method employs the iterative conjugate gradient to avoid the explicit inversion of the matrix by updating the beam vectors with iterative techniques. Finally, simulation results show that the proposed beamformer can achieve good performance only with little prior konwledge, which can further attain a flexible tradeoff between complexity and performance by using approximate methods..",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Vectors",
                "Interference",
                "Covariance matrices",
                "Uncertainty",
                "Jamming",
                "Array signal processing",
                "Internet of Things"
            ],
            "Author Keywords": [
                "Beamforming",
                "little prior knowledges",
                "MVDR",
                "covariance matrix reconstruction"
            ]
        },
        "title": "Beamforming Design for Anti-Jamming IoT Communication Based on Covariance Matrix Reconstruction With Little Prior Knowledge"
    },
    {
        "authors": [
            "Yulan Gao",
            "Ziqiang Ye",
            "Yue Xiao",
            "Ming Xiao",
            "Wei Xiang"
        ],
        "published_in": "Published in: IEEE Transactions on Cognitive Communications and Networking ( Early Access )",
        "date_of_publication": "14 October 2024",
        "doi": "10.1109/TCCN.2024.3480053",
        "publisher": "IEEE",
        "abstract": "Addressing data privacy concerns, Federated Learning (FL) has been recognized for its ability to train parameters locally on resource-constrained clients in a distributed manner. However, the problem of optimization of FL client selection and resource allocation in hierarchical Internet of Things () networks, where clients move in and out of each others’ D2D communication coverage and no FL server knows all the data owners, remains open. To bridge this gap, we propose a learner referral aided federated client selection (LRef-FedCS) approach, complemented by communications and computing resource scheduling, along with local model accuracy optimization (LMAO). LRef-FedCS enhances cost efficiency and FL model quality by enabling data owners to share FL task details within their trusted local networks, increasing the opportunity of the FL server choosing the optimal clients. Using Lyapunov optimization, the problem is transformed into a joint optimization problem (JOP). To address the JOP’s complexities, we combine a centralized method for LRef-FedCS and the self-adaptive global best harmony search algorithm for LMAO. For enhance scalability, a distributed LRef-FedCS based on a matching game is proposed. Numerical experiments on the Fashion-MNIST dataset show LRef-FedCS outperforms existing state-of-the-art approaches, delivering enhanced model accuracy with notable cost savings.",
        "issn": {
            "Electronic ISSN": "2332-7731"
        },
        "keywords": {
            "IEEE Keywords": [
                "Servers",
                "Wireless communication",
                "Optimization",
                "Training",
                "Games",
                "Costs",
                "Computational modeling",
                "Accuracy",
                "Data privacy",
                "Social networking (online)"
            ],
            "Author Keywords": [
                "Federated learning",
                "learner referral",
                "client selection",
                "Lyapunov optimization",
                "matching game"
            ]
        },
        "title": "Learner Referral for Cost-Effective Federated Learning Over Hierarchical IoT Networks"
    },
    {
        "authors": [
            "Siwei Chen",
            "Zejun Xiang",
            "Xiangyong Zeng",
            "Guangxue Qin"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "28 October 2024",
        "doi": "10.1109/JIOT.2024.3486965",
        "publisher": "IEEE",
        "abstract": "Simon and Simeck are two famous lightweight block cipher families, both of which have good implementation performance benefiting from their extremely simple round functions. So, they are suitable and friendly in use for the Internet of Things devices that require high security but low-latency and low-energy. In this paper, we aim to improve the Mixed-Integer Linear Programming/Mixed-Integer Quadratic Constraint Programming (MILP/MIQCP) based method, to find better differential-linear (DL) distinguishers for the above ciphers, which can be exploited to mount distinguishing or key-recovery attacks. In particular, firstly, we give the completely precise MILP model to describe the linear part, and utilize the general expressions of Gurobi optimizer to model middle part in a quite easy way. Secondly, to explore DL trails in a reasonable time, we propose two heuristic strategies to speed up the searching process. Lastly, we introduce the transforming technique, which exploits the clustering effect on DL trails, to improve the estimated correlation of the DL approximation. By applying our enhanced method, we improve the DL distinguisher correlation from 2-59.75 to 2-59.62 for 32-round Simon128, and extend the number of longest rounds of valid DL distinguishers for Simon32/48/64/96 from 11/16/16/25 to 14/17/21/26. For Simeck, we do not outperform the currently best work, but refresh Zhou et al.’s results (the first work to automate finding DL distinguishers for Simon/Simeck using MILP/MIQCP). Our work not only provides a new insight on the automatic DL cryptanalysis, but also further confirms that Simon and Simeck are sufficiently strong to resist the DL attacks.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Correlation",
                "Ciphers",
                "Internet of Things",
                "Security",
                "Search problems",
                "Merging",
                "Linear approximation",
                "Genetic expression",
                "Computational modeling",
                "Wireless communication"
            ],
            "Author Keywords": [
                "Simon",
                "Simeck",
                "Lightweight Cryptography",
                "Differential-Linear Cryptanalysis",
                "Automatic Tool",
                "MILP/MIQCP"
            ]
        },
        "title": "Enhancing the MILP/MIQCP-Based Automatic Search for Differential-Linear Distinguishers of IoT-Friendly Block Ciphers Simon and Simeck"
    },
    {
        "authors": [
            "Hyeong-Gun Joo",
            "Seunghwan Lee",
            "Dong-Joon Shin"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "07 November 2024",
        "doi": "10.1109/JIOT.2024.3494047",
        "publisher": "IEEE",
        "abstract": "The primary computational complexity of the lattice-based post-quantum cryptosystems (PQCs), aside from hashing, comes from the polynomial multiplication. Especially, the multiplication efficiency of large-degree polynomials becomes increasingly important for current applications. Therefore, in this paper, we propose efficient polynomial multiplication methods, termed as the extended number theoretic transform (NTT), eventually aimed at developing efficient light-weight PQCs suitable for the Internet of Things. The proposed methods utilize the extension of finite fields and an early termination technique to enable efficient NTT implementation using better parameter values. The adoption of arithmetic in the extension field Fqm provides more efficient NTT computation, even when a primitive 2n-th root of unity, required for NTT, does not exist in the base field Zq. More importantly, the proposed methods allow for more flexible selection of PQC parameters. Moreover, by using early termination of NTT over the extension field, we further enhance the efficiency of polynomial multiplication. As a result, the proposed methods flexibly select suitable (or smaller) modulus q by optimizing the extension degree m and the incomplete degree l. Finally, we validate the effectiveness of our methods by demonstrating their capability to offer a wide range of parameter values and significantly reduce the size of modulus q. It is confirmed through simulation that the communication efficiency and security are improved by up to 21.43% and 9.09%, respectively. Finally, it is shown that light-weight Crystals-Kyber can be easily constructed by using the proposed methods.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Polynomials",
                "Internet of Things",
                "Cryptography",
                "Galois fields",
                "Transforms",
                "Arithmetic",
                "Computational efficiency",
                "Standards",
                "Quantum mechanics",
                "Quantum computing"
            ],
            "Author Keywords": [
                "Number theoretic transform (NTT)",
                "polynomial multiplication",
                "incomplete NTT",
                "extended NTT",
                "extension field",
                "light-weight post-quantum cryptosystem"
            ]
        },
        "title": "Extended Number Theoretic Transform for Light-Weight Post-Quantum Cryptosystems in IoT"
    },
    {
        "authors": [
            "Lun Tang",
            "Zhoulin Pu",
            "Zhixuan Li",
            "Dongxu Fang",
            "Li Li",
            "Qianbin Chen"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "23 August 2024",
        "doi": "10.1109/JIOT.2024.3448466",
        "publisher": "IEEE",
        "abstract": "Digital twins (DTs), as an effective technology for remote monitoring and management of devices, enhances the intelligence of the Industrial Internet of Things (IIoT). Nonetheless, the unreliable and delayed transmission of sensory data in wireless access networks hinders the accurate reflection of DTs on the physical world. In this paper, we present an intelligent dual time-scale network slicing strategy utilizing the long-term and short-term trends of network, aiming to make fuller use of network resources and improve the synchronization information accuracy of DTs. Specifically, within the dual time scale slicing framework, this strategy collaboratively optimize slice scaling and sensory information synchronization for DTs, aiming to maximize sensory information satisfaction and minimize the cost of slice reconfiguration and synchronization. Firstly, at large time scales, we utilize slices to provide isolation and address deployment issues for DTs with different Quality of Service (QoS) requirements. At small time scales, we aim to enhance the adaptability of estimation tasks to dynamic environments through more flexible wireless resource allocation, further improving communication performance, and establishing DTs that closely resemble physical entities. Furthermore, to solve optimization problems at different time scales, we propose a two-layer Deep Reinforcement Learning (DRL) framework to achieve efficient network resource interactions, in which the lower-layer control algorithms utilize the Prioritized Experience Replay (PER) mechanism to accelerate the convergence speed. Finally, simulation results validate the effectiveness of the proposed strategy.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Resource management",
                "Synchronization",
                "Network slicing",
                "Accuracy",
                "Industrial Internet of Things",
                "Task analysis",
                "Wireless sensor networks"
            ],
            "Author Keywords": [
                "Digital Twin",
                "Network Slice",
                "Deep Reinforcement Learning",
                "Industrial Internet of Things(IIoT)",
                "State Estimation"
            ]
        },
        "title": "Intelligent Dual Time Scale Network Slicing for Sensory Information Synchronization in Industrial IoT Networks"
    },
    {
        "authors": [
            "Qianru Wang",
            "Li Ping Qian",
            "Mingqing Li",
            "Wei Jiang",
            "Yuan Wu"
        ],
        "published_in": "Published in: IEEE Transactions on Green Communications and Networking ( Early Access )",
        "date_of_publication": "06 November 2024",
        "doi": "10.1109/TGCN.2024.3492258",
        "publisher": "IEEE",
        "abstract": "The Industrial Internet of Things (IIoT) is a key application of 5th Generation Mobile Communication Technology (5G), with latency-sensitive data transmission forming the foundation of IIoT. In this paper, we study the latency-sensitive data transmission in IIoT with the assistance of relay nodes based on the Non-Orthogonal Multiple Access-Wireless Power Transfer (NOMA-WPT) technology. Specifically, we aim at minimizing the transmission delay by jointly optimizing the uplink transmit power of IIoT devices and relays, the downlink charge power and the uplink bandwidth allocated of relays, the charging time fraction, the IIoT device-to-relay transmission time fraction, and the IIoT device-relay grouping factor, while guaranteeing the amount of data transmitted. To solve the formulated non-convex problem, we equivalently transform it into the problem of maximizing the minimum data transmission across IIoT devices in the framework of bisection searching. That is, the minimum transmission delay can be obtained by alternatively maximizing the minimum data transmission across IIoT devices during the time durations derived by the bisection searching. For the maximization of minimum data transmission, the Karush-Kuhn-Tucker (KKT) condition and Lagrangian function are first used to obtain the optimal solutions to uplink transmit power of IIoT devices and relays, downlink charge power and uplink bandwidth allocated of relays, charging time fraction, and IIoT device-to-relay transmission time fraction. The cross-entropy (CE) algorithm is then used to get the optimal solution of the IIoT device-relay grouping. Simulation results show that the proposed solution reduces the transmission delay by 30.77% compared to using dual-layer Frequency Division Multiple Access (FDMA) in the system.",
        "issn": {
            "Electronic ISSN": "2473-2400"
        },
        "keywords": {
            "IEEE Keywords": [
                "Industrial Internet of Things",
                "Relays",
                "NOMA",
                "Servers",
                "Delays",
                "Uplink",
                "Resource management",
                "Bandwidth",
                "Cloud computing",
                "Downlink"
            ],
            "Author Keywords": [
                "Wireless Power Transfer",
                "Non-Orthogonal Multiple Access",
                "Bisection Searching",
                "Karush-Kuhn-Tucker",
                "Cross-Entropy"
            ]
        },
        "title": "Delay-Minimized Resource Allocation in Relay-Assisted NOMA-WPT Industrial IoT Networks"
    },
    {
        "authors": [
            "Avinash Mohan",
            "Arpan Chattopadhyay",
            "Shivam Vinayak Vatsa",
            "Anurag Kumar"
        ],
        "published_in": "Published in: IEEE Transactions on Control of Network Systems ( Early Access )",
        "date_of_publication": "03 July 2024",
        "doi": "10.1109/TCNS.2024.3419822",
        "publisher": "IEEE",
        "abstract": "For a system of collocated nodes sharing a time-slotted wireless channel, we seek a medium access control that provides low mean delay, has distributed control, and does not require explicit exchange of state information or control signals. We consider a practical information structure where each node has local information and some common information obtained from overhearing. We approach the problem via two steps: 1) we show that it is sufficient for the policy to be “greedy” and “exhaustive”; limiting the policy to this class reduces the problem to obtaining a queue switching policy at queue emptiness instants; and 2) by formulating the delay optimal scheduling as a partially observed Markov decision process, we show that the optimal switching rule is stochastic largest queue. Using this theory as the basis, we develop a practical, tunable, distributed scheduler, QZMAC, which is an extension to the existing ZMAC protocol. We implement QZMAC on standard off-the-shelf TelosB motes and also use simulations to compare QZMAC with the full-knowledge centralized scheduler and with ZMAC. We use our implementation to study the impact of false detection, while overhearing the common information, and the efficiency of QZMAC. Simulation results show that the mean delay with QZMAC is close to that of the full-knowledge centralized scheduler.",
        "issn": {
            "Electronic ISSN": "2325-5870"
        },
        "keywords": {
            "IEEE Keywords": [
                "Delays",
                "Optimal scheduling",
                "Media Access Protocol",
                "Switches",
                "Wireless communication",
                "Servers",
                "Protocols"
            ],
            "Author Keywords": []
        },
        "title": "A Low-Delay MAC for IoT Applications: Decentralized Optimal Scheduling of Queues Without Explicit State Information Sharing"
    },
    {
        "authors": [
            "Xuankai Zhang",
            "Jianhua Li",
            "Jun Wu",
            "Guoxing Chen",
            "Yan Meng",
            "Haojin Zhu",
            "Xiaosong Zhang"
        ],
        "published_in": "Published in: IEEE Transactions on Dependable and Secure Computing ( Early Access )",
        "date_of_publication": "16 October 2024",
        "doi": "10.1109/TDSC.2024.3481433",
        "publisher": "IEEE",
        "abstract": "Currently, the security of the control logic of Programmable Logic Controllers (PLCs) is facing a serious threat, significantly impacting industrial production. Consequently, ensuring the security of PLC control logic becomes imperative. Formal verification emerges as a promising methodology for verifing PLC security through behavioral modeling and security testing. However, existing formal verification approaches primarily focus on modeling the PLC source code, overlooking the identification of compile-time errors and real-time runtime logic checks. Therefore, it is essential to apply formal verification to PLC control logic at the binary level. In this study, we introduce VoICS, a system designed to facilitate binary-level formal verification. Using reverse engineering, VoICS automatically parses PLC programs written by various programming languages at the binary level and constructs control flow graphs (CFGs). Furthermore, we use an algorithm combining two model optimization methods (i.e., trim invalid states and unnecessary states compression) to convert the reversed PLC assembly program into nuXmv format model. Lastly, VoICS establishes the corresponding constraints and performs formal verification on the model using nuXmv. The evaluation results demonstrate the capability of VoICS in identifying instances of unreliable control logic within PLC control programs, thus reinforcing the dependability of the industrial automation system.",
        "issn": {
            "Print ISSN": "1545-5971",
            "Electronic ISSN": "1941-0018"
        },
        "keywords": {
            "IEEE Keywords": [
                "Logic",
                "Formal verification",
                "Source coding",
                "Security",
                "Runtime",
                "Loading",
                "Binary codes",
                "Reverse engineering",
                "Industrial Internet of Things",
                "Workstations"
            ],
            "Author Keywords": [
                "Formal verification",
                "model checking",
                "programmable logic controller",
                "threats detection"
            ]
        },
        "title": "Binary-Level Formal Verification Based Automatic Security Ensurement for PLC in Industrial IoT"
    },
    {
        "authors": [
            "Lingxiao Li",
            "Kai Hu",
            "Xiaobo Zhu",
            "Shanshan Jiang",
            "Liguo Weng",
            "Min Xia"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "26 August 2024",
        "doi": "10.1109/JIOT.2024.3449910",
        "publisher": "IEEE",
        "abstract": "In the context of Vertical Federated Learning (VFL), agents utilize multimodal data on their edge devices to corporately train and inference with the deep learning models. However, in classical VFL, there exists three problems from the perspective of embeddings. Firstly, the utilization of oversimplified embedding fusion mechanism may result in suboptimal performance of the models. Secondly, the exchange of embeddings and their gradients poses a potential risk of private information leakage, as they inherently contain sensitive information. Lastly, the withdrawal of some agents from cooperation disrupts the collaborative inference capabilities of the remaining agents. To mitigate these problems, this paper introduces a novel VFL algorithm grounded in embedding alignment. It includes two distinct schemes: Performance-Oriented Scheme (POS) and Privacy-Respecting Scheme (PRS). Within POS, this paper employs Contrastive Loss and joint fine-tuning to augment the expressiveness and the overall performance of models. While the PRS incorporates Homomorphic-Encryption-based Contrastive Loss and individual fine-tuning to safeguard the data security. In addition, the PRS eliminates the necessity of collaborative inference. In this paper, comprehensive security analysis and proofs are conducted for PRS. Moreover, experiments demonstrate the superior performance of the proposed POS over classical VFL, showcasing a substantial performance improvement. Simultaneously, the PRS surpasses the performance of training alone, even under stringent security constraints.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Security",
                "Data models",
                "Merging",
                "Training",
                "Federated learning",
                "Collaboration",
                "Information leakage"
            ],
            "Author Keywords": [
                "Vertical Federated Learning",
                "Contrastive Loss",
                "Homomorphic Encryption"
            ]
        },
        "title": "Approaching Expressive and Secure Vertical Federated Learning With Embedding Alignment in Intelligent IoT Systems"
    },
    {
        "authors": [
            "Xue Tan",
            "Di Xiao",
            "Hui Huang",
            "Mengdi Wang",
            "Min Li"
        ],
        "published_in": "Published in: IEEE Transactions on Industrial Informatics ( Early Access )",
        "date_of_publication": "04 September 2024",
        "doi": "10.1109/TII.2024.3431100",
        "publisher": "IEEE",
        "abstract": "Federated learning (FL) enables decentralized industrial-Internet-of-Things devices (also called clients) to share model parameters to build a joint model. Fair rewards, security of shared data, and transmission cost are the important factors that influence clients to participate in FL. Few existing works can solve these problems at the same time. Therefore, we propose a hierarchically fair and differentially private federated learning (HFDPFL), which regards the model itself as a reward to promote fairness. Reputation is used to measure the client's contribution to FL, and clients with high reputation will be rewarded with high accuracy models. In order to ensure the security of the shared data and reduce communication overhead, we implement differentially private gradient compression based on compressed sensing, which achieves differential privacy protection of gradients and improves communication efficiency. Extensive experiments are conducted to demonstrate the superiority of HFDPFL in terms of fairness, privacy preserving, and communication efficiency.",
        "issn": {
            "Print ISSN": "1551-3203",
            "Electronic ISSN": "1941-0050"
        },
        "keywords": {
            "IEEE Keywords": [
                "Servers",
                "Industrial Internet of Things",
                "Data models",
                "Costs",
                "Noise",
                "Accuracy",
                "Protection"
            ],
            "Author Keywords": [
                "Communication efficiency",
                "compressed sensing (CS)",
                "differential privacy (DP)",
                "federated learning (FL)",
                "hierarchically fair"
            ]
        },
        "title": "Hierarchically Fair and Differentially Private Federated Learning in Industrial IoT Based on Compressed Sensing With Adaptive-Thresholding Sparsification"
    },
    {
        "authors": [
            "Baosheng Li",
            "Weifeng Gao",
            "Jin Xie",
            "Maoguo Gong",
            "Ling Wang",
            "Hong Li"
        ],
        "published_in": "Published in: IEEE Transactions on Industrial Informatics ( Early Access )",
        "date_of_publication": "16 October 2024",
        "doi": "10.1109/TII.2024.3468446",
        "publisher": "IEEE",
        "abstract": "Federated learning (FL) is a distributed learning paradigm that leverages local updates and parameter sharing to address privacy concerns in Industrial Internet of Things (IIoT) environments. The presence of statistical heterogeneity among IIoT devices poses significant challenges for FL, impacting convergence and model performance. Although previous studies have attempted to mitigate this issue, a fundamental solution remains elusive. To address this, we propose a novel approach called federated multidiscriminators multigenerators generative adversarial network (FedMDMG-GAN), which employs a distributed generative adversarial network (GAN). IIoT devices concurrently train local generators and discriminators to generate data with global information. The server then aggregates parameters and redistributes them to the devices to refine local GANs. In addition, we introduce a proximal term for global aggregation to enhance convergence. Theoretical analysis suggests that the FedMDMG-GAN algorithm can asymptotically converge to a stable point. Qualitative assessments demonstrate that our method can generate images closely resembling real data with comprehensive global information. Quantitative results indicate that FedMDMG-GAN outperforms vanilla FL and state-of-the-art methods.",
        "issn": {
            "Print ISSN": "1551-3203",
            "Electronic ISSN": "1941-0050"
        },
        "keywords": {
            "IEEE Keywords": [
                "Industrial Internet of Things",
                "Generative adversarial networks",
                "Servers",
                "Convergence",
                "Training",
                "Optimization",
                "Data models",
                "Performance evaluation",
                "Mathematical models",
                "Generators"
            ],
            "Author Keywords": [
                "Edge computing",
                "federated optimization",
                "generative adversarial networks (GANs)",
                "heterogeneous networks",
                "Industrial Internet of Things (IIoT)"
            ]
        },
        "title": "Federated Multidiscriminators Multigenerators for Heterogeneous Industrial IoT"
    },
    {
        "authors": [
            "Guisheng Zhang",
            "Mingliang Gao",
            "Qilei Li",
            "Siyou Guo",
            "Gwanggil Jeon"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "13 June 2024",
        "doi": "10.1109/TCE.2024.3414319",
        "publisher": "IEEE",
        "abstract": "Recently, the Consumer Internet of Things (CIoT) has brought great convenience to people. In CIoT, face image information is indispensable for payment and checking the identity of the user in the transaction. However, the misuse of deepfake face information in CIoT transactions is a growing problem. It has seriously violated the property and privacy of individuals. Moreover, with the proliferation of easily accessible facial editing applications, individuals can effortlessly manipulate facial components through sequential multi-step manipulations. To solve this issue, we propose a Spectral Transformer with a Pyramid Attention (STPA) model to detect sequence permutations in manipulated facial images. Specifically, we introduce a pyramid attention module that integrates both spatial and channel attention mechanisms to prioritize the face region over the background region. Additionally, a spectral Transformer is employed concurrently to extract global and local features to facilitate the fine-grained extraction of the face forgery region. Comprehensive experiments prove that the proposed method can enhance the detection accuracy of the sequential deepfake manipulation task through the fine-grained extraction of features in the face forgery region.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Deepfakes",
                "Feature extraction",
                "Transformers",
                "Privacy",
                "Face recognition",
                "Forgery",
                "Internet of Things"
            ],
            "Author Keywords": [
                "Consumer Security",
                "Privacy Preservation",
                "Sequential Deepfake Detection",
                "Spectral Transformer",
                "Pyramid Attention"
            ]
        },
        "title": "Detecting Sequential Deepfake Manipulation via Spectral Transformer With Pyramid Attention in Consumer IoT"
    },
    {
        "authors": [
            "Harshpreet Kaur",
            "Munish Bhatia"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "16 October 2024",
        "doi": "10.1109/JIOT.2024.3481501",
        "publisher": "IEEE",
        "abstract": "Digital Twin technology is emerging as a key innovation to enhance efficiency and performance in the automobile industry. Despite its potential, several challenges persist, notably the need for accurate performance evaluation and robust predictive capabilities. This paper addresses these challenges by presenting a comprehensive model that delineates the vehicle metrics essential for performance evaluation. The novelty of this research lies in the integration of multiple advanced techniques, including a Naive Bayes Model for categorizing data segments for quantitative analysis, spatial-temporal mining and regression analysis to abstract temporal data for deeper evaluation, and Recurrent Neural Network (RNN) technology to ensure robust predictive capabilities. Experimental validation in a simulated environment comprising 50,250 data segments demonstrates the efficacy of the proposed model, showing significant improvements in performance metrics. Prediction analysis yields promising results with high Specificity (92.44%), Sensitivity (95.81%), Precision (94.33%), and F1-score (88.33%). Notably, the proposed model achieves temporal efficiency with a minimum time delay of 99.83 seconds, underscoring its effectiveness in real-time assessment of driverless automobile performance.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Real-time systems",
                "Sensors",
                "Measurement",
                "Data models",
                "Temperature sensors",
                "Data mining",
                "Computational modeling",
                "Bayes methods",
                "Autonomous automobiles",
                "Automotive engineering"
            ],
            "Author Keywords": [
                "Digital Twin",
                "Vehicular Edge Computing",
                "Recurrent Neural Network"
            ]
        },
        "title": "Digital Twin-Driven Performance Assessment for IoT-Integrated Autonomous Driving Systems"
    },
    {
        "authors": [
            "Jamil Farhat",
            "Glauber Brante",
            "João Luiz Rebelatto",
            "Richard Demo Souza"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "11 October 2024",
        "doi": "10.1109/JSEN.2024.3474978",
        "publisher": "IEEE",
        "abstract": "In this paper, we investigate the age of information (AoI) of a wireless network employing an iterative decoding (ID) strategy with non-orthogonal multiple access (NOMA). The ID-NOMA protocol is based on the idea that packets successfully decoded at time slots inside the same frame may be employed to solve previous successive interference cancellation (SIC) collisions, contributing to enhancing the AoI of the system. To demonstrate the benefits of the proposed strategy, we compare ID-NOMA with orthogonal multiple access (OMA) and conventional NOMA. Numerical results show that the joint optimization of the number of slots per frame, transmit power, and transmission probability of each device per time slot are crucial to minimize the AoI. Furthermore, our results demonstrate that ID-NOMA outperforms OMA and conventional NOMA in terms of AoI in all scenarios considered, which demonstrates the benefits of the proposed protocol.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "NOMA",
                "Internet of Things",
                "Protocols",
                "Interference cancellation",
                "Measurement",
                "Iterative decoding",
                "Information age",
                "Sensors",
                "Uplink",
                "Signal to noise ratio"
            ],
            "Author Keywords": [
                "Age of Information",
                "Grant-Free Access",
                "Internet of Things",
                "Non-Orthogonal Multiple Access"
            ]
        },
        "title": "Age-of-Information of NOMA-aided Grant-Free IoT Networks with Iterative Decoding"
    },
    {
        "authors": [
            "Li Zhang",
            "Cheng-Xiang Wang",
            "Zihao Zhou",
            "Yuxiao Li",
            "Jie Huang",
            "Lijian Xin",
            "Chun Pan",
            "Dabo Zheng",
            "Xiping Wu"
        ],
        "published_in": "Published in: IEEE Transactions on Vehicular Technology ( Early Access )",
        "date_of_publication": "15 October 2024",
        "doi": "10.1109/TVT.2024.3480528",
        "publisher": "IEEE",
        "abstract": "Wireless Fidelity (Wi-Fi) communication technologies hold significant potential for realizing the Industrial Internet of Things (IIoT). In this paper, both Single-Input Single-Output (SISO) and polarized Multiple-Input Multiple-Output (MIMO) channel measurements are conducted in an IIoT scenario at the less congested Wi-Fi band, i.e., 5.5 GHz. The purpose is to investigate wireless characteristics of communications between access points and terminals mounted on automated guided vehicles as well as those surrounding manufacturing areas. For SISO channel measurements, statistical properties including the delay Power Spectral Density (PSD), path loss, shadowing fading, delay spread, excess delay, K-factor, and amplitude distribution of small-scale fading are analyzed and compared with those observed in an office scenario. For MIMO channel measurements, results show that there are multiple Dense Multipath Component (DMC) processes in the delay PSD. An estimation algorithm based on the algorithm for a single DMC process is proposed to effectively process the multi-processes data. Moreover, delay, angular, power, and polarization properties of DMCs are investigated and compared with those of specular multipath components. Furthermore, effects of DMCs on Singular Values (SVs) and channel capacities are explored. Ignoring DMCs can overestimate SVs and underestimate channel capacities.",
        "issn": {
            "Print ISSN": "0018-9545",
            "Electronic ISSN": "1939-9359"
        },
        "keywords": {
            "IEEE Keywords": [
                "Delays",
                "Industrial Internet of Things",
                "Antenna measurements",
                "Wireless fidelity",
                "Frequency measurement",
                "Power measurement",
                "Channel capacity",
                "Production facilities",
                "Fading channels",
                "Conferences"
            ],
            "Author Keywords": [
                "IIoT scenarios",
                "wireless channel measurements",
                "channel characterization",
                "specular multipath components",
                "dense multipath components"
            ]
        },
        "title": "Wireless Channel Measurements and Characterization in Industrial IoT Scenarios"
    },
    {
        "authors": [
            "Yujie Peng",
            "Tiecheng Song",
            "Xiaoqin Song",
            "Yang Yang",
            "Wangdong Lu"
        ],
        "published_in": "Published in: IEEE Transactions on Wireless Communications ( Early Access )",
        "date_of_publication": "07 October 2024",
        "doi": "10.1109/TWC.2024.3470525",
        "publisher": "IEEE",
        "abstract": "The collaboration between unmanned aerial vehicles (UAVs) and intelligent reflecting surfaces (IRSs) presents an innovative approach for delay-tolerant data harvesting in distributed Internet of Things (IoT) networks. However, existing research mostly overlooks the dynamic changes in communication links caused by the real-time UAV movement and the realistic geographical features. In this paper, we address these challenges by considering a practical three-dimensional (3D) urban scenario with a centralized IRS. Our aim is to minimize the completion time of data harvesting missions by jointly optimizing the 3D trajectory of the UAV and the phase shift of the IRS. Specifically, the formulated problem is decoupled into two subproblems. First, for the 3D continuous trajectory design, we propose a robust memory-based softmax deep double deterministic policy gradients (MSD3) approach, which enables the UAV to adaptively collect delay-tolerant data from randomly distributed ground devices starting from any arbitrary point. Second, we present a comprehensive theoretical analysis for the continuous IRS phase control, which provides a practical and intuitive numerical solution. Simulation results demonstrate that the proposed MSD3-IRS algorithm outperforms other mainstream baselines based on deep reinforcement learning.",
        "issn": {
            "Print ISSN": "1536-1276",
            "Electronic ISSN": "1558-2248"
        },
        "keywords": {
            "IEEE Keywords": [
                "Autonomous aerial vehicles",
                "Trajectory",
                "Three-dimensional displays",
                "Minimization",
                "Optimization",
                "Heuristic algorithms",
                "Data collection",
                "Distributed databases",
                "Resource management",
                "Real-time systems"
            ],
            "Author Keywords": [
                "Data harvesting",
                "unmanned aerial vehicle",
                "intelligent reflecting surface",
                "deep reinforcement learning"
            ]
        },
        "title": "Time-Effective UAV-IRS-Collaborative Data Harvesting: A Robust Deep Reinforcement Learning Approach"
    },
    {
        "authors": [
            "Yomna Gamal",
            "Ahmed Soltan",
            "Lobna A. Said",
            "Ahmed H. Madian",
            "Ahmed G. Radwan"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "02 March 2023",
        "doi": "10.1109/ACCESS.2023.3251655",
        "publisher": "IEEE",
        "abstract": "Countries are collaborating to make agriculture more efficient by combining new technologies to improve its procedure. Improving irrigation efficiency in agriculture is thus critical for the survival of sustainable agricultural production. Smart irrigation methods can enhance irrigation efficiency, specially with the introduction of wireless communication systems, monitoring devices, and enhanced control techniques for efficient irrigation scheduling. The study compared on a wide range of study subjects to investigate scientific approaches for smart irrigation. As a result, this project included a wide range of topics related to irrigation methods, decision-making, and technology used. Information was gathered from a variety of scientific papers. So, our research relied on several published documents, the majority of which were published during the last four years, and authors from all over the world. In the meantime, various irrigation initiatives were given special attention. Following that, the evaluation focuses on the key components of smart irrigation, such as real-time irrigation scheduling, IoT, the importance of an internet connection, smart sensing, and energy harvesting.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Irrigation",
                "Crops",
                "Water resources",
                "Soil moisture",
                "Agriculture",
                "Intelligent sensors",
                "Artificial neural networks"
            ],
            "Author Keywords": [
                "Smart irrigation",
                "Soil monitoring",
                "Smart agriculture",
                "IoT",
                "Energy harvesting"
            ]
        },
        "title": "Smart Irrigation Systems: Overview"
    },
    {
        "authors": [
            "Maldon Patrice Goodridge",
            "Subhash Lakshminarayana",
            "Alessandro Zocca"
        ],
        "published_in": "Published in: IEEE Transactions on Power Systems ( Early Access )",
        "date_of_publication": "11 July 2024",
        "doi": "10.1109/TPWRS.2024.3419725",
        "publisher": "IEEE",
        "abstract": "Load-alteringattacks (LAAs) targeting a large number of IoT-based high-wattage devices (e.g., smart electric vehicle charging stations) can lead to serious disruptions of power grid operations. In this work, we aim to uncover spatiotemporal characteristics of LAAs that can lead to serious impact. The problem is challenging since existing protection measures, such as $N-1$ security designed to make the power system resilient to single component failures, also provide resilience to load changes to a large extent. Thus, strategically injected load perturbations that lead to network failure can be regarded as rare events . To this end, we adopt a rare-event sampling approach to uncover LAAs distributed temporally and spatially across the power network. The key advantage of this sampling method is the ability to sample efficiently from multi-modal conditional distributions with disconnected support. Furthermore, we systematically compare the impacts of static (one-time manipulation of demand) and dynamic (attack over multiple time periods) LAAs. We perform extensive simulations using benchmark IEEE test bus systems. The results show (i) the superiority and the need for rare-event sampling in the context of uncovering LAAs as compared to other sampling methodologies, (ii) statistical analysis of attack characteristics and impacts of static and dynamic LAAs, and (iii) cascade sizes (due to LAA) for different network sizes and load conditions.",
        "issn": {
            "Print ISSN": "0885-8950",
            "Electronic ISSN": "1558-0679"
        },
        "keywords": {
            "IEEE Keywords": [
                "Power grids",
                "Load modeling",
                "Generators",
                "Power system dynamics",
                "Mathematical models",
                "Trajectory",
                "Resilience"
            ],
            "Author Keywords": [
                "Cascading failures",
                "IoT-controlled loads",
                "load-altering attacks",
                "rare-event sampling"
            ]
        },
        "title": "Uncovering Load-Altering Attacks Against\nN−1\nSecure Power Grids: A Rare-Event Sampling Approach"
    },
    {
        "authors": [
            "Xiazhi Lai",
            "Tuo Wu",
            "Cunhua Pan",
            "Lifeng Mai",
            "Arumugam Nallanathan"
        ],
        "published_in": "Published in: IEEE Transactions on Green Communications and Networking ( Early Access )",
        "date_of_publication": "06 March 2024",
        "doi": "10.1109/TGCN.2024.3373911",
        "publisher": "IEEE",
        "abstract": "Low-latency computational tasks in Internet-of-Things (IoT) networks require short-packet communications. In this paper, we consider a mobile edge computing (MEC) network under time division multiple access (TDMA)-based short-packet communications. Within the considering network, a mobile user partitions an urgent task into multiple sub-tasks and delegates portions of these sub-tasks to edge computing nodes (ECNs). However, the required computing resource varies randomly along with execution failure. Thus, we explore the execution uncertainty of the proposed MEC network, which holds broader implications across the MEC network. In order to minimize the probability of execution failure in computational tasks, we present an optimal solution that determines the sub-task lengths and the blocklengths for offloading. However, the complexity of the optimal solution increases due to the involvement of the Q function and incomplete Gamma function. Consequently, we develop a low-complexity algorithm that leverages alternating optimization and majorization-maximization (MM) methods, enabling efficient computation of semi-closed-form solutions. Furthermore, to reduce the computational complexity associated with sorting the offloading order of sub-tasks, we propose two sorting criteria based on the computing speeds of the ECNs and the channel gains of the transmission links, respectively. Numerical results have validated the effectiveness of the proposed algorithm and criteria. The results also suggest that the proposed network achieves significant performance gains over the non-orthogonal multiple access (NOMA) and full offloading networks.",
        "issn": {
            "Electronic ISSN": "2473-2400"
        },
        "keywords": {
            "IEEE Keywords": [
                "Task analysis",
                "Uncertainty",
                "Internet of Things",
                "Resource management",
                "NOMA",
                "Computational modeling",
                "Time division multiple access"
            ],
            "Author Keywords": [
                "Internet-of-Things (IoT)",
                "short-packet",
                "execution uncertainty",
                "mobile edge computing (MEC)"
            ]
        },
        "title": "Short-Packet Edge Computing Networks With Execution Uncertainty"
    },
    {
        "authors": [
            "Naiyu Zheng",
            "Yuanchun Li",
            "Shiqi Jiang",
            "Yuanzhe Li",
            "Rongchun Yao",
            "Chuchu Dong",
            "Ting Chen",
            "Yubo Yang",
            "Zhimeng Yin",
            "Yunxin Liu"
        ],
        "published_in": "Published in: IEEE Transactions on Mobile Computing ( Early Access )",
        "date_of_publication": "14 October 2024",
        "doi": "10.1109/TMC.2024.3474853",
        "publisher": "IEEE",
        "abstract": "Deep learning (DL) based Wi-Fi sensing has witnessed great development in recent years. Although decent results have been achieved in certain scenarios, Wi-Fi based activity recognition is still difficult to deploy in real smart homes due to the limited cross-environment adaptability, i.e. a well-trained Wi-Fi sensing neural network in one environment is hard to adapt to other environments. To address this challenge, we propose AdaWiFi , a DL-based Wi-Fi sensing framework that allows multiple Internet-of-Things (IoT) devices to collaborate and adapt to various environments effectively. The key innovation of AdaWiFi includes a collective sensing model architecture that utilizes complementary information between distinct devices and avoids the biased perception of individual sensors and an accompanying model adaptation technique that can transfer the sensing model to new environments with limited data. We evaluate our system on a public dataset and a custom dataset collected from three complex sensing environments. The results demonstrate that AdaWiFi is able to achieve significantly better sensing adaptation effectiveness (e.g. 30% higher accuracy with one-shot adaptation) as compared with state-of-the-art baselines.",
        "issn": {
            "Print ISSN": "1536-1233",
            "Electronic ISSN": "1558-0660"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Wireless fidelity",
                "Adaptation models",
                "Data models",
                "Feature extraction",
                "Solid modeling",
                "Accuracy",
                "Wireless sensor networks",
                "Wireless communication",
                "Smart homes"
            ],
            "Author Keywords": [
                "Deep learning",
                "domain adaptation",
                "IoT devices",
                "smart home",
                "wi-fi sensing"
            ]
        },
        "title": "AdaWiFi, Collaborative WiFi Sensing for Cross-Environment Adaptation"
    },
    {
        "authors": [
            "Tengfei Yang",
            "Yuanyuan Li",
            "Jiawei He",
            "Zhiquan Liu",
            "Fang Ren",
            "Teng Wang",
            "Gaopan Hou"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "10 September 2024",
        "doi": "10.1109/JIOT.2024.3457017",
        "publisher": "IEEE",
        "abstract": "The privacy-preserving image retrieval technology permits users to retrieve outsourced images in a secure manner in cloud-assisted Internet of Things (IoT) environment. However, most of the existing schemes still have some blemishes, such as low performance, shared key and untraceable malicious users. To this end, we present a secure and traceable multikey image retrieval, named as STMIR. First, we design a novel privacy-preserving Mahalanobis distance comparison method (PPMDC) based on the learning with errors technology and Mahalanobis distance. And STMIR extracts image features utilizing convolutional neural network model to improve retrieval accuracy. Then, STMIR employs extracted image features, PPMDC and key conversion technology to achieve secure image retrieval that supports the multikey setting. Meanwhile, STMIR uses encrypted image watermarking technology to protect the content of images and track malicious users who redistribute images. Formal security analysis shows that STMIR can resist both ciphertext only attack and known background attack, and extensive experiments in real-world image data sets demonstrate effectiveness of STMIR in terms of retrieval accuracy, retrieval efficiency and traceability to malicious query users.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Accuracy",
                "Image retrieval",
                "Cloud computing",
                "Vectors",
                "Cryptography",
                "Internet of Things",
                "Indexes"
            ],
            "Author Keywords": [
                "Privacy-preserving image retrieval",
                "Internet of Things (IoT)",
                "Mahalanobis distance",
                "multikey settings",
                "malicious user traceability"
            ]
        },
        "title": "Secure and Traceable Multikey Image Retrieval in Cloud-Assisted Internet of Things"
    },
    {
        "authors": [
            "Aoto Kaburaki",
            "Koichi Adachi",
            "Osamu Takyu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "05 November 2024",
        "doi": "10.1109/JIOT.2024.3491182",
        "publisher": "IEEE",
        "abstract": "Low-power wide-area networks (LPWANs), which achieve low-power consumption, enabling long-term battery operation and long-range communication capabilities, have emerged as a new standard for realizing massive wireless sensor networks (WSN). LPWANs are becoming increasingly popular due to low introduction costs, which stem from features such as using unlicensed bands and low-cost nodes. LPWANs are particularly useful for Internet-of-Things (IoT) applications that periodically collect information about specific observation targets. However, LPWAN generally adopts a simple medium access control (MAC), which significantly degrades communication quality due to packet collisions when the traffic load increases. Thus, MAC design is critical for realizing large-scale LPWANs. Carrier sense multiple access (CSMA) can autonomously avoid packet collisions. However, its performance is drastically deteriorated due to the hidden node problem in large-scale LPWANs. This paper proposes an autonomous distributed MAC strategy that can suppress the hidden node problem by utilizing traffic periodicity. The proposed method is designed carefully considering LPWAN-specific constraints, such as duty cycle limitations in unlicensed bands, low clock accuracy of nodes, and limited downlink communication opportunities. From numerical results, the proposed method improves the packet delivery rate (PDR) performance by up to approximately by 29%, 9% and 8% compared to ALOHA, CSMA-x, and the state-of-the-art LoRa MAC respectively.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Clocks",
                "Signal to noise ratio",
                "Low-power wide area networks",
                "Media Access Control",
                "LoRaWAN",
                "Wireless sensor networks",
                "Wireless communication",
                "Topology",
                "Symbols"
            ],
            "Author Keywords": [
                "Internet of Things (IoT)",
                "LoRaWAN",
                "low-power wide area networks (LPWAN)",
                "resource allocation"
            ]
        },
        "title": "Tackling Hidden Node Problem Utilizing Traffic Periodicity and Downlink Carrier Sense in LPWAN"
    },
    {
        "authors": [
            "Bin Yang",
            "Jiawei Zhou",
            "Shihao Zhang",
            "Ying Xing",
            "Weiwei Jiang",
            "Lexi Xu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "18 October 2024",
        "doi": "10.1109/JIOT.2024.3446640",
        "publisher": "IEEE",
        "abstract": "Along with the development of IoT systems, numerous edge intelligent devices can obtain a large amount of user data, and the analysis of this user data can be applied to business scenarios such as user Click-Through Rate (CTR) prediction. In the recommendation, advertising and other scenarios, the users at the edge have high response requirements for CTR prediction model training and inference. In the current edge scenario of CTR prediction, there are problems of overly complex model structure and highly sparse original features, which makes it difficult to deploy CTR prediction models at the edge. Therefore, we propose KD-GAFIM, a lightweight recommendation algorithm that combines graph neural networks with knowledge distillation. The approach uses Graph Attention Networks to flexibly capture feature dependencies in a way that maintains a small model size while augmenting the feature vector with feature dependencies. And by sharing the embedding layer of the teacher model, KD-GAFIM improves the efficiency of user CTR prediction. On top of that, we also propose a feature compression strategy guided by model interpretability, which identifies high-contributing features for inference and model refinement based on their performance in model interpretability. This strategy improves efficiency, making KD-GAFIM suitable for training and inference on edge devices. We conducted extensive experiments on multiple datasets. The experimental results show that KD-GAFIM outperforms various state-of-the-art CTR prediction models, demonstrating that graph neural network based knowledge distillation models can improve model performance while reducing model size and feature dimensionality, and have significant potential for application at the edge.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Predictive models",
                "Computational modeling",
                "Data models",
                "Vectors",
                "Analytical models",
                "Accuracy",
                "Performance evaluation",
                "Training",
                "Recommender systems",
                "Frequency modulation"
            ],
            "Author Keywords": [
                "IoT Systems",
                "Edge Learning",
                "CTR Prediction",
                "Knowledge Distillation",
                "Feature Compression"
            ]
        },
        "title": "Lightweight Knowledge Distillation and Feature Compression Model for User Click-Through Rates Prediction in Edge Computing Scenarios"
    },
    {
        "authors": [
            "Kasem Khalil",
            "Bappaditya Dey",
            "Magdy Bayoumi"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/JIOT.2024.3490893",
        "publisher": "IEEE",
        "abstract": "Hardware implementation of neural networks is challenging due to varying application requirements. This often necessitates creating specific Field Programmable Gate Arrays (FPGA) configurations from scratch for each application. This paper proposes a flexible, self-supervised reconfigurable method to fit several application requirements by providing only the maximum available computational nodes a priori. The proposed method dynamically reconfigures the required number of hidden layers and nodes based on the application. The goal is to automatically determine the optimal Neural Network (NN) configuration through reconfigurability to achieve maximum accuracy. Optimality is demonstrated through minimum average power, average delay, and area overhead, as well as maximum throughput and accuracy. Experimental results show that the proposed approach significantly reduces the optimized architecture search cost (the number of online training iterations) and associated average power consumption for successive datasets/applications. The method’s effectiveness is shown both quantitatively and qualitatively, verified against the MNIST and CIFAR-10 classification problems. Our reconfigurable method demonstrates stable accuracy of 98.97% and 98.95% compared to state-of-the-art neural networks with fixed configurations (98.85% and 73.0% for MNIST and 93.47% and 70.21% for CIFAR-10, respectively). Additionally, the proposed method shows a 20.9% reduction in average power dissipation compared to state-of-the-art methods. Implemented and tested using VHDL and Altera FPGA, the results indicate resource utilization comparable with the state-of-the-art method. This reconfigurability is especially advantageous for IoT applications where power efficiency and adaptability to different tasks are critical.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Hardware",
                "Computer architecture",
                "Neural networks",
                "Biological neural networks",
                "Artificial neural networks",
                "Internet of Things",
                "Neurons",
                "Field programmable gate arrays",
                "Accuracy",
                "Resource management"
            ],
            "Author Keywords": [
                "Artificial Neural Network",
                "Internet of Things (IoT)",
                "Reconfiguration",
                "FPGA",
                "Hardware Accelerator"
            ]
        },
        "title": "S2RNN: Self-Supervised Reconfigurable Neural Network Hardware Accelerator for Machine Learning Applications"
    },
    {
        "authors": [
            "Ratha Yeu",
            "Yosuke Tanigawa",
            "Akio Hasegawa",
            "Hideki Tode"
        ],
        "published_in": "Published in: IEICE Transactions on Communications ( Early Access )",
        "date_of_publication": "07 November 2024",
        "doi": "10.23919/transcom.2024EBP3057",
        "publisher": "IEICE",
        "abstract": "In smart IoT environments such as smart factory ones, periodically generated data packets as well as aperiodically generated ones with various sizes and generation time intervals must be transferred at low costs from the viewpoints of network installation and operation. Using Wireless Sensor Networks (WSNs) based on IEEE 802.11 is desirable for the low cost nature, but the channel access mechanism with Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA) suffers from packet collision as packet transfer load increases. In this paper, we establish a new transmission time scheduling method for avoiding packet collision among wireless stations (STAs) that generate and transmit packets to their access point at various time periods, give the detailed system design, and verify its operation through prototype implementation and experimental studies. In the proposed method, transmission durations during which each STA is permitted to transmit packets are assigned in a discrete-time manner on the time axis, so that the overlap of transmission durations from different STAs is minimized based on the periodicity of packet generation. The transmission permission control is realized with a software buffer provided just before packets are passed from the upper IP layer to the MAC layer. This has the advantage that no modifications and replacements of existing IEEE 802.11 devices are required. The discrete-time assignment of the transmission durations ensures the low cost nature and feasibility of the transmission timing control for STAs whose computation resource is assumed to be limited as wireless sensor nodes. We implement the proposed method as a prototype system and conduct several experiments to verify its operation and evaluate its performance.",
        "issn": {
            "Electronic ISSN": "1745-1345",
            "Print ISSN": "0916-8516"
        },
        "keywords": {
            "IEEE Keywords": [
                "Wireless sensor networks",
                "Timing",
                "Costs",
                "IEEE 802.11 Standard",
                "Time division multiple access",
                "Software",
                "Smart manufacturing",
                "Resource management",
                "Prototypes",
                "Production"
            ],
            "Author Keywords": [
                "Transmission timing scheduling",
                "Collision avoidance",
                "IoT",
                "Smart factory",
                "IR 4.0"
            ]
        },
        "title": "Transmission Time Scheduling for Stations with Different Packet Generation Periods in Smart Factory Environment: System Implementation and Experimental Evaluation"
    },
    {
        "authors": [
            "Yang Hu",
            "Shaobo Li",
            "Dawen Xia",
            "Wenyong Zhang",
            "Panliang Yuan",
            "Fengbin Wu",
            "Huaqing Li"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "12 November 2024",
        "doi": "10.1109/JIOT.2024.3496795",
        "publisher": "IEEE",
        "abstract": "Accurate traffic flow prediction is a key aspect of building data-driven intelligent transportation systems (ITS) which relies on the Internet of Things (IoT) sensors deployed along roads, and dynamic spatial-temporal dependencies mining is a major area of interest in traffic flow prediction. Existing methods, however, overlook the diversities of traffic flow patterns from the perspectives of temporal and spatial dimensions. To this end, this paper presents a Multi-view Spatial-temporal Adaptive Transformer-GRU (MST-ATG) framework based on the encoder-decoder architecture to capture complex spatial-temporal dependencies from various perspectives. Specifically, a multi-view embedding layer (MEL) containing original traffic data and spatial-temporal correlated features is designed to enrich the feature encoding. Then, based on the inherent characteristics of traffic flow, we introduce a periodicity-trend decomposition (PTD) method to fully consider the periodic and trend-oriented features of time series. Finally, we propose a spatial-temporal adaptive transformer-GRU (ST-ATG) to dynamically extract spatial-temporal dependencies and adaptively choose computation steps in which a Temporal Adaptive stacked-GRU Module (T-AGM) is proposed to extract correlations in temporal dimension and spatial dependencies captured by a Spatial Adaptive Transformer Module (S-ATM). Experimental results on six large-scale real-world datasets demonstrate that our MST-ATG framework outperforms the benchmarks in prediction accuracy. For instance, the average RMSE of MST-ATG on PeMS08 is reduced by 48.3%, 41.09%, 12.95%, 17.67%, 18.64%, 2.4%, 14.67%, 9.15%, 1.1%, 2.4%, 2.51%, and 1.2% compared to that of ARIMA, LSTM, DCRNN, STGCN, ASTGCN, GWNet, STSGCN, AGCRN, Bi-STAT, STAEformer, PDFormer, and STPGNN, respectively.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Transformers",
                "Sensor phenomena and characterization",
                "Accuracy",
                "Roads",
                "Predictive models",
                "Market research",
                "Internet of Things",
                "Feature extraction",
                "Time series analysis"
            ],
            "Author Keywords": [
                "IoT sensors",
                "spatial-temporal dependencies",
                "multi-view embedding layer",
                "periodicity-trend decomposition",
                "adaptive transformer-GRU"
            ]
        },
        "title": "A Multi-view Spatial-temporal Adaptive Transformer-GRU Framework for Traffic Flow Prediction"
    },
    {
        "authors": [
            "Zhuming Bi",
            "Aki Mikkola",
            "Dilipkummar Devpalli",
            "Chaomin Luo"
        ],
        "published_in": "Published in: IEEE/ASME Transactions on Mechatronics ( Early Access )",
        "date_of_publication": "05 June 2024",
        "doi": "10.1109/TMECH.2024.3402358",
        "publisher": "IEEE",
        "abstract": "With the rapid development of information technologies, networking, and artificial intelligence, mechatronics has continuously evolved for several generations from traditional mechatronics to digital twins (DT-I), cyber-physical systems (CPSs), Internet of Things (IoT), human CPSs , and to metaverse nowadays. This article examines such a development trend from a new perspective in enhancing the sustainability of products or systems. To fill the technical gaps of existing concepts in supporting sustainable developments of next-generation mechatronic systems, we propose the concepts of digital triads (DT-II) and Internet of DT-II things ; these concepts aim to expand the capabilities of mechatronic systems in dealing with changes and uncertainties in dynamic business environments and prolonging systems’ lifespans through technological integration. For example, DT-II is innovative in the sense that it extends the capabilities of CPS or DT-I by introducing a life model so that relevant data, models, and methods can be preserved and transferred when a product or system needs reconfiguring to satisfy new requirements. DT-II promotes concurrent engineering in decision-making supports in the digital and physical worlds so that a product or system can be reconfigured to satisfy new requirements optimally. This smoothens the transitions in sustainable manufacturing and prolongs the enterprise's lifespan. To showcase the use and significance of DT-II, a case study of DT-II for sustainable product development is developed to predict the performance of new products for a client company. The developed DT-II is fully verified and validated for use, and the proposed procedure is generalized and can be used to develop DT-II for other products and systems for enhanced sustainability.",
        "issn": {
            "Print ISSN": "1083-4435",
            "Electronic ISSN": "1941-014X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Mechatronics",
                "Sustainable development",
                "Manufacturing",
                "Business",
                "Computational modeling",
                "Internet of Things",
                "Uncertainty"
            ],
            "Author Keywords": [
                "Artificial intelligence (AI)",
                "cyber-physical systems (CPSs)",
                "digital triads (DT-II)",
                "digital twins (DT-I)",
                "Internet of Digital Triad Things (IoDDT)",
                "Internet of Things (IoT)",
                "sustainability",
                "verification and validation (V&V)"
            ]
        },
        "title": "Digital Triads as Next-Generation Mechatronic Systems for Sustainability—A Case Study"
    },
    {
        "authors": [
            "Alejandro Fernández",
            "Aurora Andújar",
            "Jussi Rahola",
            "Joan L. Pijoan",
            "Jaume Anguera"
        ],
        "published_in": "Published in: IEEE Open Journal of Antennas and Propagation ( Early Access )",
        "date_of_publication": "28 February 2024",
        "doi": "10.1109/OJAP.2024.3365703",
        "publisher": "IEEE",
        "abstract": "The proliferation of the Internet of Things (IoT) has resulted in a demand for wireless connectivity to a multitude of devices. To address this need, a rapid and straightforward approach for designing multiband antenna systems utilizing antenna boosters is proposed. In this method, the antenna booster is presented as an impedance box, with a multiband matching network comprising lumped components that determine the number of frequency bands. The multiband matching network is created using a synthesizer, taking into account the measured antenna booster input impedance and simulated radiation efficiency. To validate the process, a multiband antenna system operating at 824-960 MHz and 1710-2690 MHz is implemented. The matching network has been obtained through a fully automated process, without any human intervention or adjustments, enabling the design of multiband antenna systems in a quick, easy, and systematic manner.",
        "issn": {
            "Electronic ISSN": "2637-6431"
        },
        "keywords": {
            "IEEE Keywords": [
                "Antennas",
                "Antenna measurements",
                "Impedance",
                "Transmission line measurements",
                "Bandwidth",
                "Wireless communication",
                "Frequency measurement"
            ],
            "Author Keywords": [
                "small and multiband antenna systems",
                "wireless devices",
                "matching network synthesis",
                "matching networks",
                "antenna boosters",
                "IoT"
            ]
        },
        "title": "Multiband Operation With Antenna Boosters Using Matching Network Synthesis"
    },
    {
        "authors": [
            "Yingjie Zhao",
            "Zhengyi Chai",
            "Yalun Li",
            "Hao Huang",
            "Hongshen Kang"
        ],
        "published_in": "Published in: IEEE Transactions on Cognitive Communications and Networking ( Early Access )",
        "date_of_publication": "24 September 2024",
        "doi": "10.1109/TCCN.2024.3466889",
        "publisher": "IEEE",
        "abstract": "With the increasing scale of industrial equipments, delay and energy consumption have emerged as critical concerns within the Industrial Internet of Things (Industrial IoT). Mobile edge computing (MEC) offloads tasks to nearby edge servers to meet the demands of delay-sensitive applications. However, the limitations of edge computing resources can lead to significant processing delays or even task failures when offloading numerous tasks. Furthermore, it is difficult for existing centralized algorithms to acquire global information within large-scale industrial environment. To tackle these challenges, the computation offloading problem is transformed into a decentralized partially observable Markov decision process (Dec-POMDP) with rewards for delay and energy consumption, and a decentralized multi-objective computation offloading method is proposed to achieve the long-term reward maximization. Specifically, two deep neural networks, namely the delay and energy network, are designed to estimate the expected rewards for each offloading decision in terms of delay and energy consumption. Meanwhile, to achieve dynamic offloading, gated recurrent unit (GRU) is introduced to predict the occupancy of edge computing resources, and an adaptive weight network is devised to dynamically adjust the weights of optimization objectives based on historical information. Comprehensive experiments demonstrate that the proposed method effectively meets the requirements of delay-sensitive tasks, as well as minimizing long-term delay and energy consumption.",
        "issn": {
            "Electronic ISSN": "2332-7731"
        },
        "keywords": {
            "IEEE Keywords": [
                "Delays",
                "Industrial Internet of Things",
                "Optimization",
                "Heuristic algorithms",
                "Energy consumption",
                "Dynamic scheduling",
                "Resource management"
            ],
            "Author Keywords": [
                "Industrial IoT",
                "Mobile edge computing",
                "computation offloading",
                "multi-objective optimization",
                "deep reinforcement learning"
            ]
        },
        "title": "Multi-Objective Computation Offloading based on Decentralized Deep Reinforcement Learning in Industrial Internet of Things"
    },
    {
        "authors": [
            "Fei Wang",
            "Baochun Li"
        ],
        "published_in": "Published in: IEEE Transactions on Big Data ( Early Access )",
        "date_of_publication": "20 May 2024",
        "doi": "10.1109/TBDATA.2024.3403383",
        "publisher": "IEEE",
        "abstract": "Federated learning is widely accepted as a privacy-preserving paradigm for training a shared global model across multiple client devices in a collaborative fashion. However, in practice, the significantly limited computational power on client devices has been a major barrier when we wish to train large models with potentially hundreds of millions of parameters. In this paper, we propose a new architecture, referred to as Infocomm , that incorporates locally supervised learning in federated learning. With locally supervised learning, the disadvantages of split learning can be avoided by using a more flexible way to offload training from resource constrained clients to a more capable server. Infocomm enables parallel training of different modules of the neural network in both the server and clients in a gradient-isolated fashion. The efficacy in reducing both training time and communication time is supported by our theoretical analysis and empirical results. In the scenario involving larger models and fewer available local data, Infocomm has been observed to reduce the elapsed time per round by over 37% without sacrificing accuracy compared to both conventional federated learning or directly combining federated learning and split learning, which showcases the advantages of Infocomm under power-constrained IoT scenarios.",
        "issn": {
            "Electronic ISSN": "2332-7790"
        },
        "keywords": {
            "IEEE Keywords": [
                "Servers",
                "Training",
                "Federated learning",
                "Computational modeling",
                "Data models",
                "Internet of Things",
                "Neural networks"
            ],
            "Author Keywords": [
                "Federated learning",
                "locally supervised learning",
                "split learning",
                "large models",
                "resource-constrained IoT devices"
            ]
        },
        "title": "Harnessing the Power of Local Supervision in Federated Learning"
    },
    {
        "authors": [
            "Wen-Liang Liu",
            "Jing-Ya Deng",
            "Chu-Peng Yi",
            "Zi-Yue Zhao",
            "Ting Feng",
            "Xin Liu",
            "Yang Lu",
            "Xiao-Hua Ma",
            "Yue Hao"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "30 September 2024",
        "doi": "10.1109/JIOT.2024.3470109",
        "publisher": "IEEE",
        "abstract": "This paper introduces a novel power amplifier integrated active antenna (PAIAA) with increased efficiency that integrates a Class-F-1 gallium nitride (GaN) power amplifier (PA) and a stub-loaded wide slot antenna (SLSA) for wireless power transmission (WPT) applications. Unlike conventional PA integrated active antennas in which antennas and PAs are separately matched to 50 ohms, the input impedance of the SLSA is meticulously engineered to match the fundamental impedance and modulate harmonic impedances of the GaN transistor by optimizing the dimensions of the SLSA’s stubs. This approach eliminates the output matching network (OMN), the PA’s harmonic modulation network (HMN), and the antenna’s input matching network (IMN), which typically incur unavoidable insertion loss but are essential for conventional Class-F-1 PAs and antennas. Consequently, it enhances the power-added efficiency (PAE) within the 3.3-3.8 GHz range and reduces the overall size of the PA integrated active antenna. Thanks to the extended design freedom offered by SLSA, precise impedance values can be achieved across a broader frequency range, resulting in increased efficiency across a larger spectrum. In addition, an innovative insertion loss measurement method for impedance matching network with a non-50 Ω-port is introduced to accurately measure the Class-F-1 PA’s PAE incorporated in the proposed PAIAA. The measured PAIAA demonstrates a peak PAE of 70.1%, surpassing the performances of reported integrated active antennas. The measured EIRP of the proposed PAIAA is 44.52 dBm. A conventional PA-antenna design, where an antenna and a Class-F-1 PA are separately matched to 50Ω and simply cascaded, is also designed and measured as a contrast, whose measured peak PAE and effective isotropic radiated power (EIRP) of the conventional design are only 61.5% and 43.9 dBm, respectively. The proposed PAIAA, with its increased efficiency, can be practically utilized for WPT applications in the Internet of Th...",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Impedance",
                "Transistors",
                "Harmonic analysis",
                "Antennas",
                "Antenna measurements",
                "Impedance matching",
                "Slot antennas",
                "Power amplifiers",
                "Loss measurement",
                "Internet of Things"
            ],
            "Author Keywords": [
                "power amplifier integrated active antenna (PAIAA)",
                "Class-F-1",
                "stub-loaded slot antenna (SLSA)",
                "harmonic modulation",
                "Internet of Things (IoT)"
            ]
        },
        "title": "Class-F-1 GaN Power Amplifier Integrated Active Antenna With Increased Efficiency for Wireless Power Transmission Applications"
    },
    {
        "authors": [
            "Marcel Koch",
            "Thomas Pfitzinger",
            "Fabian Schlenke",
            "Fabian Kohlmorgen",
            "Roland Groll",
            "Hendrik Wöhrle"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "11 September 2024",
        "doi": "10.1109/ACCESS.2024.3457912",
        "publisher": "IEEE",
        "abstract": "Most human activity recognition (HAR) systems are based on computer vision or wearable sensors. However, these methods have limitations, such as privacy concerns, the need for high computational power, and the requirement for frequent battery recharging. In this paper, we present a distributed multisensor system for the recognition of human activities based on ambient audio and vibration data. The system comprises several ambient multisensor nodes (AMSNs) located within a smart home environment, along with a data transfer and analysis system. The data transfer and analysis system includes an IoT gateway and an MQTT broker. The data obtained by the AMSNs is classified using various machine learning algorithms. We utilized a ResNet model to analyze ambient acoustic signals and an encoder network for the classification of vibrational data. An empirical evaluation of the proposed approachwas conducted using a dataset recorded in a real-world environment and different combinations of modalities and sensor setups. The results indicate that audio and vibration-based data can be leveraged for accurate activity detection, eliminating the need for specialized sensor equipment. These findings have several implications for smart home environments, such as improving user experience and comfort or enhancing safety and security for inhabitants.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Human activity recognition",
                "Accuracy",
                "Vibrations",
                "Wearable sensors",
                "Temperature sensors",
                "Sensor phenomena and characterization",
                "Internet of Things",
                "Ambient intelligence"
            ],
            "Author Keywords": [
                "IoT",
                "Machine Learning",
                "Human Activity Recognition",
                "Ambient Intelligence"
            ]
        },
        "title": "Recognition of human activities based on ambient audio and vibration data"
    },
    {
        "authors": [
            "Pratik Goswami",
            "Ranjay Hazra",
            "Milos Prokysek",
            "Kwonhue Choi",
            "Markus Eider"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "13 June 2024",
        "doi": "10.1109/TCE.2024.3414124",
        "publisher": "IEEE",
        "abstract": "A network of smart consumer electronics devices is related with two key terms: data security and resource allocation. Because multiobjective sensor data is being transferred at the same time, the data transfer across communication channels within the IoT framework is subject to multiple channel interference. It results in the depletion of network resources and makes data security susceptible. Dedicated channel states are considered for fixed resources in the majority of earlier research, which continues to be a significant problem for the flexibility and security of consumer electronics device networks. Both of the issues are included in this work by determining the channel security and extracting the ideal channel state for various consumer electronics device applications using principal component analysis. This produces a quick system that uses resources efficiently and is verified through simulations and mathematical analysis.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Resource management",
                "Consumer electronics",
                "Principal component analysis",
                "Internet of Things",
                "Protocols",
                "Monitoring",
                "Wireless sensor networks"
            ],
            "Author Keywords": [
                "Channel security",
                "Smart Consumer Electronics Devices",
                "Resource allocation",
                "IoT"
            ]
        },
        "title": "A Novel Approach of Efficient Resource Allocation in Smart Consumer Electronics Device Network"
    },
    {
        "authors": [
            "Julian Roqui",
            "Alain Pegatoquet",
            "Luca Santamaria",
            "Leonardo Lizzi"
        ],
        "published_in": "Published in: IEEE Open Journal of Antennas and Propagation ( Early Access )",
        "date_of_publication": "28 October 2024",
        "doi": "10.1109/OJAP.2024.3487498",
        "publisher": "IEEE",
        "abstract": "In this paper, an approach based on Machine Learning (ML) to predict the maximum achievable performance (fractional bandwidth and total efficiency) of a printed Inverted F-antennas (IFAs) integrated into compact IoT terminals is proposed. This original approach relies on the use of a Multi-Layer Perceptron (MLP) artificial neural network (ANN), which is trained using data from numerical simulations that take into account the constraints of practical implementations. The effectiveness of the approach is demonstrated through comparisons with numerical and experimental results, as well as with theoretical results available in the literature. The obtained results show that the proposed supervised regression ML model is capable of predicting the maximum achievable fractional bandwidth and total efficiency, with an accuracy of 95.9% and 98.6%, respectively.",
        "issn": {
            "Electronic ISSN": "2637-6431"
        },
        "keywords": {
            "IEEE Keywords": [
                "Antennas",
                "Bandwidth",
                "Design methodology",
                "Antenna theory",
                "Prediction algorithms",
                "Optimization",
                "Machine learning",
                "Geometry",
                "Electromagnetics",
                "Radar antennas"
            ],
            "Author Keywords": [
                "Integrated antennas",
                "terminal antennas",
                "machine learning",
                "inverted-F antennas (IFA)",
                "artificial neural networks",
                "internet-of-things (IoT)"
            ]
        },
        "title": "Predicting the Maximum Achievable Antenna Bandwidth and Efficiency Using Machine Learning: a Terminal-Integrated Meander IFA Case Study"
    },
    {
        "authors": [
            "Liam Boyle",
            "Julian Moosmann",
            "Nicolas Baumann",
            "Seonyeong Heo",
            "Michele Magno"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "16 July 2024",
        "doi": "10.1109/JSEN.2024.3425904",
        "publisher": "IEEE",
        "abstract": "Advances in lightweight neural networks have revolutionized computer vision in a broad range of Internet of Things (IoT) applications, encompassing remote monitoring and process automation. However, the detection of small objects, which is crucial for many of these applications, remains an underexplored area in current computer vision research, particularly for low-power embedded devices that host resource-constrained processors. To address said gap, this paper proposes an adaptive tiling method for lightweight and energy-efficient object detection networks, including YOLO-based models and the popular Faster Objects More Objects (FOMO) network. The proposed tiling enables object detection on low-power Microcontroller Units (MCUs) with no compromise on accuracy compared to large-scale detection models. The benefit of the proposed method is demonstrated by applying it to FOMO and TinyissimoYOLO networks on a novel RISC-V -based MCU with built-in Machine Learning (ML) accelerators. Extensive experimental results show that the proposed tiling method boosts the F1-score by up to 225% for both FOMO and TinyissimoYOLO networks while reducing the average object count error by up to 76% with FOMO and up to 89% for TinyissimoYOLO. Furthermore, the findings of this work indicate that using a soft F1 loss over the popular binary cross-entropy loss can serve as an implicit non-maximum suppression for the FOMO network. To evaluate the real-world performance, the networks are deployed on the RISC-V based GAP9 microcontroller from GreenWaves Technologies , showcasing the proposed method’s ability to strike a balance between detection performance (58% − 95% F1 score), low latency (0.6ms/Inference - 16.2ms/Inference), and energy efficiency (31 μJ/Inference - 1.27mJ/Inference) while performing multiple predictions using high-resolution images on a MCU.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Accuracy",
                "Feature extraction",
                "Neural networks",
                "Random access memory",
                "Detectors",
                "Computer architecture",
                "Real-time systems"
            ],
            "Author Keywords": [
                "Object Detection",
                "TinyML",
                "IoT",
                "Microcontrollers"
            ]
        },
        "title": "DSORT-MCU: Detecting Small Objects in Real-Time on Microcontroller Units"
    },
    {
        "authors": [
            "Le Tung Giang",
            "Nguyen Xuan Tung",
            "Vu Hoang Viet",
            "Trinh Van Chien",
            "Nguyen Tien Hoa",
            "Won Joo Hwang"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "29 October 2024",
        "doi": "10.1109/JSEN.2024.3485058",
        "publisher": "IEEE",
        "abstract": "During the flourishing era of the Internet of Things (IoTs), wireless sensor networks (WSNs) have emerged as a critical backbone for sensing, connectivity, and automation in 6G communications. Due to limited energy sources, minimizing power consumption is the primary focus in extending the lifespan of WSNs. Unfortunately, conventional approaches often face difficulties with scalability and computation complexity, thereby making them insufficient for large-scale WSNs. To address these challenges, graph neural networks (GNNs), have gained significant research attention thanks to their scalability and generalization capabilities. Nonetheless, existing GNN architectures may struggle to effectively capture the hierarchical topology of WSN systems, where interactions between different levels significantly influence overall network performance. To overcome this challenge, this paper proposes a novel hierarchical GNN (HGNN) architecture to learn power allocation and sensor-AP selection policies that minimizes power consumption in hierarchical WSNs. In this architecture, node and edge update mechanisms are designed to reflect the internal structure of WSNs. Besides, the proposed HGNN is guaranteed representational power, ensuring its ability to capture the graph’s information. Numerical results demonstrate the superior performance of the solution produced by the proposed HGNN in reducing power consumption under various network settings. The HGNN can reduce total power consumption by approximately 30% compared with the model-based approaches.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Wireless sensor networks",
                "Sensors",
                "Resource management",
                "Throughput",
                "Servers",
                "Power demand",
                "Graph neural networks",
                "Wireless communication",
                "Quality of service",
                "Internet of Things"
            ],
            "Author Keywords": [
                "Access Point Selection",
                "Hierarchical Graph Neural Networks",
                "Hierarchical Wireless Sensor Network (HWSNs)",
                "IoT Sensor Networks",
                "Power Allocation"
            ]
        },
        "title": "HGNN: A Hierarchical Graph Neural Network Architecture for Joint Resource Management in Dynamic Wireless Sensor Networks"
    },
    {
        "authors": [
            "Yishan Chen",
            "Jie Wu",
            "Shumei Ye",
            "Wei Li",
            "Zhonghui Xu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "11 November 2024",
        "doi": "10.1109/JIOT.2024.3486378",
        "publisher": "IEEE",
        "abstract": "As a rapid increase in Internet of Things (IoT) devices, Vehicle Fog/Edge Computing (VFC/VEC) has seen swift development. Consequently, these devices often opt to purchase computing resources from Edge-Cloud Service Providers (ECSP) to expand their capabilities. However, due to limited edge server resources, they may face risks of overload or breakdowns. Meanwhile, there are often large amounts of underutilized idle resources near roads (such as parked vehicles), which can provide additional computing and communication capabilities to the system. Inspired by this, we propose a scheme that utilizes idle vehicles to assist in computation. To coordinate the interests of various participating entities and incentivize resource sharing, we construct a Multi-Stage Multi-Leader Multi-Follower (MSMLMF) Stackelberg Game model that encompasses the collaboration and competition among users, ECSPs, Vehicle Operators (VOP), and idle vehicles. Participants aim to maximize their utility while considering the potential actions of others. Additionally, considering the information asymmetry between VOP and Vehicles, we introduce Individual Rationality (IR) and Incentive Compatibility (IC) constraints from contract theory to analyze and ensure the effectiveness of contracts. Next, we employ backward induction to gradually simplify the game model into convex optimization problems and theoretically prove the existence and uniqueness of Nash equilibrium (NE) points. Finally, through simulation experiments verify that our proposed model and scheme outperform other baselines in overall social welfare.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Resource management",
                "Games",
                "Contracts",
                "Servers",
                "Internet of Things",
                "Computational modeling",
                "Security",
                "Scalability",
                "Pricing",
                "Computational efficiency"
            ],
            "Author Keywords": [
                "IoT",
                "Contract Theory",
                "Incentive Mechanism",
                "Stackelberg Game",
                "Pricing",
                "Lagrangian Dual",
                "Idle Resources"
            ]
        },
        "title": "Budget-Constrained Resource Allocation and Pricing in VEC: A MSMLMF Stackelberg Game With Contract Incentive Mechanism"
    },
    {
        "authors": [
            "Tiago Troccoli",
            "Hans Jakob Damsgaard",
            "Juho Pirskanen",
            "Elena Simona Lohan",
            "Aleksandr Ometov",
            "Jorge Morte",
            "Jari Nurmi",
            "Ville Kaseva"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "07 October 2024",
        "doi": "10.1109/JIOT.2024.3474918",
        "publisher": "IEEE",
        "abstract": "This research investigates an affordable, energy-efficient Direction-of-Arrival (DOA)-based localization solution for Digital Enhanced Cordless Telecommunications (DECT) 2020 New Radio (NR), a new standard lacking a native positioning feature. This standard enables Massive Internet of Things (IoT) networks, a vast 5G network interconnecting an unparalleled number of low-cost and battery-operated smart sensors. However, integrating DOA localization into such networks is challenging due to cost constraints and power limitations. We propose a potentially cost-effective solution using a single Radio-Frequency (RF) chain for uniform L-shaped antenna arrays. Each antenna takes turns sampling the Orthogonal Frequency Division Multiplexing (OFDM) signal via an RF switch, enabled by time-dividing the OFDM signal into sample and switch slots. Further, we introduce a novel DOA method optimized for single Line-of-Sight (LOS) OFDM signals and array sequential sampling. This method leverages the dual shift-invariant properties of L-shaped antenna arrays and the array frequency response to estimate azimuth and elevation angles. Experiments in an indoor environment reveal that at a Signal-to-Noise Ratio (SNR) of 15 dB, over 50% of data achieve sub-degree angular accuracy, increasing to 75% at 20 dB. Thus, over 50% of position estimations fall below the sub-meter error level at 15 dB SNR, rising to nearly 75% at 25 dB SNR. Our findings also indicate that halving the slot rate by proportionately reducing active subcarriers does not compromise accuracy. Experiments on the nRF52480 system-on-chip show the new DOA method is both fast and energy-efficient, taking only 0.76 – 2.26 ms and consuming 5.08 – 15.1 nWh.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Antenna arrays",
                "OFDM",
                "Direction-of-arrival estimation",
                "Accuracy",
                "Estimation",
                "Location awareness",
                "Arrays",
                "Mobile nodes",
                "Bluetooth"
            ],
            "Author Keywords": [
                "Direction of Arrival (DOA)",
                "5G indoor localization",
                "massive IoT",
                "OFDM signals"
            ]
        },
        "title": "Novel Direction-of-Arrival-Based Localization in Massive DECT-2020 5G NR Networks"
    },
    {
        "authors": [
            "D Kavitha",
            "S Thejas"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "08 November 2024",
        "doi": "10.1109/ACCESS.2024.3493957",
        "publisher": "IEEE",
        "abstract": "This comprehensive review examines the role of artificial intelligence (AI) in enhancing threat detection and cybersecurity, focusing on recent advancements and ongoing challenges in this dynamic field. The ability to identify and counteract cybersecurity threats including network breaches, adversarial assaults, and zero-day vulnerabilities has significantly increased with the inclusion of AI, especially machine learning and deep learning techniques. The review underscores the critical role of explainability and resilience in AI models to ensure trustworthiness and reliability in AI-driven security solutions. The studies analyzed span a wide range of sectors, including Industry 5.0, the Internet of Things (IoT), 5G networks, and autonomous vehicles, illustrating AI’s adaptability in tackling unique security issues across these domains. Cutting-edge approaches, such as transformer-based models, federated learning, and blockchain integration, are advancing the development of more robust and real-time threat detection systems. However, challenges persist, particularly in managing large-scale data, enabling real-time processing, and ensuring privacy and security. The review concludes that although substantial progress has been achieved, ongoing research and collaboration are vital to fully harness AI’s potential in securing digital landscapes.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Artificial intelligence",
                "Computer security",
                "Data models",
                "Threat assessment",
                "Adaptation models",
                "Internet of Things",
                "Deep learning",
                "Accuracy",
                "Complexity theory",
                "Analytical models"
            ],
            "Author Keywords": [
                "Zero-Day Vulnerabilities",
                "Network Intrusion Detection",
                "Federated Learning",
                "Blockchain",
                "Internet of Things (IoT)",
                "Adversarial Attacks"
            ]
        },
        "title": "AI Enabled Threat Detection: Leveraging Artificial Intelligence for Advanced Security and Cyber Threat Mitigation"
    },
    {
        "authors": [
            "Yongtai Yin",
            "Yuexian Wang",
            "Yanyun Gong",
            "Neeraj Kumar",
            "Ling Wang",
            "Joel J. P. C. Rodrigues"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "17 June 2024",
        "doi": "10.1109/JIOT.2024.3377437",
        "publisher": "IEEE",
        "abstract": "Efficient communication in massive multiple-input multiple-output (MIMO) systems relies on accurate channel estimation to optimize signal transmission efficiency, reliability, and minimize interference and power consumption. However, the presence of non-uniform array gain-phase perturbations among antenna elements poses practical challenges, degrading the precision of estimation. In response, this paper introduces a parameterized joint angle and delay estimation (JADE) method tailored for multipath channel estimation in fully uncalibrated arrays within massive MIMO systems. Our innovative spatial and frequency-based co-smoothing method is proposed to construct a rank-recovered data covariance matrix, enhancing the system’s ability to distinguish coherent multipath signals. The JADE method employs a one-dimensional angular spectrum and delay spectrum search under the principle of rank reduction, providing a closed-form solution for array gain-phase perturbation estimates. The deterministic Cramér-Rao lower bound for the proposed model is derived. Numerical simulations affirm the method’s superior performance. In conclusion, our approach addresses the demand for precise channel estimation in low signal-to-noise ratio scenarios, particularly benefiting Internet of Things (IoT) applications.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Perturbation methods",
                "Channel estimation",
                "Massive MIMO",
                "Estimation",
                "Antenna arrays",
                "Internet of Things",
                "Symbols"
            ],
            "Author Keywords": [
                "Channel estimation",
                "massive multiple-input multiple-output (MIMO)",
                "gain-phase perturbation",
                "Cramér-Rao lower bound",
                "Internet of Things (IoT)"
            ]
        },
        "title": "Joint Multipath Channel Estimation and Array Channel Inconsistency Calibration for Massive MIMO Systems"
    },
    {
        "authors": [
            "Hamid Reza Massrur",
            "Mahmud Fotuhi-Firuzabad",
            "Payman Dehghanian",
            "Frede Blaabjerg"
        ],
        "published_in": "Published in: IEEE Transactions on Power Systems ( Early Access )",
        "date_of_publication": "22 April 2024",
        "doi": "10.1109/TPWRS.2024.3392018",
        "publisher": "IEEE",
        "abstract": "Part I of this two-part paper series designed a solution for hierarchical coordination among Household Demand Response (HDR), Residential Aggregators (RAs) and DSO. This companion paper presents a novel three-layer fog-based architecture to establish an efficient data transmission network for the proposed coordination framework. The envisioned communication architecture includes the end-device, edge fog nodes, and cloud-server layers, providing a reliable solution for data collection of large-scale IoT-based Demand Responsive (DR) customers. The time delay and required bandwidth for the proposed data transmission architecture are modeled. To trace the effectiveness and economic impacts of the proposed fog-based hierarchical HDR-RAs-DSO distributed coordination framework, we perform extensive numerical studies on an enhanced IEEE 33-Bus test system. Several simulations are performed to investigate the time delay and required bandwidth of the proposed data transmission architecture in various scenarios, where it is found that the proposed coordination framework increases the RAs' benefits and flexibility, while in compliance with the network operating constraints. Additionally, the proposed HDR-RAs-DSO coordination framework provides a less costly solution for customers with HDR capability and results in greater profits for residential prosumers.",
        "issn": {
            "Print ISSN": "0885-8950",
            "Electronic ISSN": "1558-0679"
        },
        "keywords": {
            "IEEE Keywords": [
                "Data communication",
                "Computer architecture",
                "Cloud computing",
                "Bandwidth",
                "Wireless communication",
                "Servers",
                "Logic gates"
            ],
            "Author Keywords": [
                "AMI",
                "bandwidth",
                "cloud-computing",
                "demand response",
                "data transmission",
                "fog-computing",
                "IoT",
                "latency",
                "time delay",
                "transactive energy"
            ]
        },
        "title": "Fog-Based Hierarchical Coordination of Residential Aggregators and Household Demand Response with Power Distribution Grids—Part II: Data Transmission Architecture and Case Studies"
    },
    {
        "authors": [
            "Rong Zhou",
            "Jianhang Yang",
            "Xiaoteng Zhao",
            "Depeng Sun",
            "Shubin Liu",
            "Zhangming Zhu"
        ],
        "published_in": "Published in: IEEE Transactions on Microwave Theory and Techniques ( Early Access )",
        "date_of_publication": "12 August 2024",
        "doi": "10.1109/TMTT.2024.3434342",
        "publisher": "IEEE",
        "abstract": "This article introduces a variable-gain low-noise amplifier (VG-LNA) designed for IoT applications. The VG-LNA utilizes a similar architecture to the gm-boost input topology to address input impedance matching degradation during gain adjustment. In addition, it incorporates a dual noise cancellation (NC) path to reduce thermal noise. Due to the differential output characteristics of this topology, this LNA can simultaneously realize the balun function. Fabricated using a 65-nm CMOS process and without on-chip inductors, the VG-LNA occupies only 0.002 mm\n2\nof the core area. The VG-LNA achieves a turntable voltage gain range of 4.1–20 dB, an IIP3 range of\n−\n5.3–6.2 dBm, and a noise figure (NF) range of 1.8–6.8 dB over a 3-dB bandwidth across 0.1–4.3 GHz.",
        "issn": {
            "Print ISSN": "0018-9480",
            "Electronic ISSN": "1557-9670"
        },
        "keywords": {
            "IEEE Keywords": [
                "Gain",
                "Topology",
                "Impedance",
                "Voltage",
                "Noise",
                "Internet of Things",
                "Baluns"
            ],
            "Author Keywords": [
                "Gm-boost",
                "Internet of Things (IoT)",
                "noise-canceling (NC)",
                "variable-gain low-noise amplifier (VG-LNA)",
                "wideband"
            ]
        },
        "title": "A 0.1-to-4.3-GHz Variable-Gain Balun LNA With Dual-Path Noise-Canceling Technique"
    },
    {
        "authors": [
            "Yao Wang",
            "Guangyang Zhang",
            "Xue Mei",
            "Chongyan Gu"
        ],
        "published_in": "Published in: IEEE Transactions on Circuits and Systems I: Regular Papers ( Early Access )",
        "date_of_publication": "08 October 2024",
        "doi": "10.1109/TCSI.2024.3466972",
        "publisher": "IEEE",
        "abstract": "As a lightweight hardware security primitive, physical unclonable functions (PUFs) can provide reliable identity authentication for the Internet of Things (IoT) devices with limited resources. Arbiter PUF (APUF) is one of the most well-known PUF circuits. However, its hardware implementation has poor reliability on field programmable gate arrays (FPGAs). This paper proposed a highly reliable APUF that uses a delay difference quantization strategy (DDQ-APUF). By adding multiple configurable delay units to the two symmetrical paths of the conventional APUF, the delay difference between the two symmetrical paths of APUF can be obtained by collecting the output of APUF under different delay configurations. Compared to conventional APUFs, DDQ-APUF does not use the arbitration result of signal transmission in two symmetric paths as its response, but rather uses the quantified delay difference between the two paths as its response. A tolerance threshold is adopted in the authentication to accommodate the variations in delay differences due to environmental changes. Moreover, the modeling attack resistance of DDQ-APUF is evaluated, and a strategy for improving this resistance by incorporating pseudo-XOR technique is proposed. The circuit was implemented on Xilinx Artix-7 FPGAs and the experimental results show that the reliability achieves 99.95 $\\%$ with non-CRP-discard.",
        "issn": {
            "Print ISSN": "1549-8328",
            "Electronic ISSN": "1558-0806"
        },
        "keywords": {
            "IEEE Keywords": [
                "Delays",
                "Circuits",
                "Authentication",
                "Integrated circuit reliability",
                "Field programmable gate arrays",
                "Error correction codes",
                "Physical unclonable function",
                "Nonvolatile memory",
                "Quantization (signal)",
                "Internet of Things"
            ],
            "Author Keywords": [
                "DDQ-APUF",
                "FPGA",
                "IoTs",
                "security",
                "authentication"
            ]
        },
        "title": "A High-Reliability, Non-CRP-Discard Arbiter PUF Based on Delay Difference Quantization"
    },
    {
        "authors": [
            "Pedro Andrade",
            "Marianne Silva",
            "Morsinaldo Medeiros",
            "Daniel G. Costa",
            "Ivanovitch Silva"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "17 September 2024",
        "doi": "10.1109/JSEN.2024.3458917",
        "publisher": "IEEE",
        "abstract": "The Internet of Things (IoT) paradigm encompasses computing and networking capabilities within electronic objects, acting as a fundamental development framework with vast potential for improving lives, enhancing industrial processes, and enabling real-time decision-making. However, as the number of connected objects increases, the infrastructure for processing and handling large volumes of data is highly impacted. In response, edge computing has been exploited as a way to bring the processing burden closer to the data sources, shifting the conventional data processing flow. As a result, a series of innovative machine learning applications has been developed for resource-constrained devices, such as microcontrollers, enabling efficient data processing on the edge and inaugurating the era of Tiny Machine Learning (TinyML). Nevertheless, although the benefits have proven promising in different scenarios, particularly when the flexibility of embedded models meets the requisites of unsupervised learning approaches, TinyML-based applications may need to perform real-time identification of data outliers, since they could waste resources and impair the expected model accuracy. In this context, this article proposes an innovative TinyML outlier detection and correction algorithm based on incremental learning. This algorithm was implemented in an OBD-II scanner as a proof of concept, where a microcontroller acquires real-time vehicle data to identify data outliers and perform necessary corrections, benefiting practical applications in multiple scenarios.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Tiny machine learning",
                "Anomaly detection",
                "Streams",
                "Sensors",
                "Data models",
                "Internet of Things",
                "Real-time systems"
            ],
            "Author Keywords": [
                "IoT",
                "Machine learning",
                "Embedded systems",
                "Smart vehicles",
                "OBD-II"
            ]
        },
        "title": "TEDA-RLS: A TinyML Incremental Learning Approach for Outlier Detection and Correction"
    },
    {
        "authors": [
            "Syed Thouheed Ahmed",
            "Kiran Kumari Patil",
            "Sreedhar Kumar S",
            "Rajesh Kumar Shanraj",
            "Surbhi Bhatia Khan",
            "Saeed Alzahrani",
            "Shalli Rani"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "07 October 2024",
        "doi": "10.1109/TCE.2024.3473291",
        "publisher": "IEEE",
        "abstract": "Telemedicine infrastructure is enhanced in recent times and applications developed have adopted base-line networking standards according to 4G/5G and LTE. The major challenge in exiting infrastructural setups is higher-latency and exposed privacy of resources and sensitive information. In this manuscript, we have proposed a 6G enabled resource recommendation framework for telemedicine. The framework is developed on the Edge-AI computational principles to cater the needs and demands of medical devices associated in telemedicine. The approach is to customize the network via Distributed Telemedicine Network (DTN) protocol for edge-devices such IoT/IoMT and medical consumers’ calibration on an existing TelMED protocol of dynamic resource allocation. The DTN aims to generate a resource recommendation stack for incoming user demand via 6G spectrum. The edge-AI framework supports resources allocation with minimal latency and delay and improved privacy of data under the operations. The framework further interfaces the Industry 5.0 applications and consumer demands for effective resources allocation, scheduling and monitoring.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Telemedicine",
                "6G mobile communication",
                "Resource management",
                "Protocols",
                "Medical services",
                "Servers",
                "Dynamic scheduling",
                "5G mobile communication",
                "Internet of Things",
                "Artificial intelligence"
            ],
            "Author Keywords": [
                "6G",
                "Telemedicine",
                "Industry 5.0",
                "resources recommendation",
                "IoT/IoMT",
                "Edge-AI"
            ]
        },
        "title": "6GTelMED: Resources Recommendation Framework on 6G Enabled Distributed Telemedicine Using Edge-AI"
    },
    {
        "authors": [
            "Yanbin Li",
            "Yuxin Huang",
            "Yikang Guo",
            "Chunpeng Ge",
            "Fanyu Kong",
            "Yongjun Ren"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "16 September 2024",
        "doi": "10.1109/JIOT.2024.3460802",
        "publisher": "IEEE",
        "abstract": "Profiled side-channel analysis presents a significant risk to embedded devices in Internet of Things (IoT). Typically, a single trace is insufficient to successfully key recovery in practical scenarios. It still requires several traces based on Bayes’ posterior probability. In this paper, we introduce a chosen-plaintext strategy into the deep learning-based profiled attacks to improve the attack efficiency. Firstly, we present a general strategy to profile the leakage model by exploiting the sensitivity analysis and clustering analysis. The leakage model derived from Deep Neural Network is to characterize the leakage of the target algorithm. Secondly, we propose an adaptive chosen-plaintext method in the deep learning-based attack, transforming the conditional probability distribution of the leakage into the entropy of the key candidates under the profiled leakage model. Finally, we evaluate the efficiency of the attack by practical measurements. The results demonstrate that the proposed method requires fewer traces to retrieve the key of AES on devices of different types, e.g., Smartcard, FPGA, and ARM. Moreover, our attack improves the attack efficiency on masked implementations.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Cryptography",
                "Analytical models",
                "Sensitivity analysis",
                "Entropy",
                "Deep learning",
                "Adaptation models"
            ],
            "Author Keywords": [
                "Side-channel analysis",
                "chosen-plaintext",
                "deep learning",
                "IoT"
            ]
        },
        "title": "Adaptive Chosen-Plaintext Deep Learning-Based Side-Channel Analysis"
    },
    {
        "authors": [
            "Payman Pahlavan",
            "Alexander Wilcher",
            "Hanna Jang",
            "Yong-Kyu Yoon"
        ],
        "published_in": "Published in: IEEE Transactions on Antennas and Propagation ( Early Access )",
        "date_of_publication": "15 October 2024",
        "doi": "10.1109/TAP.2024.3477004",
        "publisher": "IEEE",
        "abstract": "A dual-band compact patch antenna array is designed on an ultra-low-loss substrate with a smooth surface, made of two layers of fused silica. The antenna array operates simultaneously in the 5G New Radio (5G-NR) band at 28 GHz and the Internet of Things (IoT) band at 24 GHz. The proposed array design consists of 2 ☓ 2 subarrays, each incorporating diagonal complementary split ring resonators (CSRRs) in a meanderline fashion on the ground plane. These meanderline CSRRs leverage the coupled resonance theory and a novel CSRR feeding technique to enhance decoupling among closely placed array elements at 28.5 GHz and enable a second band of operation at 24.5 GHz. Antenna excitation is achieved through the CSRRs slot instead of traditional via feeding or aperture coupling methods. The meanderline CSRRs serve as both a bandgap structure between antenna elements and a coupling structure between the antenna elements and feedlines. We fabricated and measured both 2 ☓ 2 and 4 ☓ 4 arrays on a double-layer fused silica substrate, with each layer having a thickness of 0.35 mm and 0.18 mm, respectively. The measurement results are in good agreement with the simulation results. This innovative approach reduces the array size by 70% compared to combining two separate single-band arrays.",
        "issn": {
            "Print ISSN": "0018-926X",
            "Electronic ISSN": "1558-2221"
        },
        "keywords": {
            "IEEE Keywords": [
                "Antenna arrays",
                "Substrates",
                "Antenna radiation patterns",
                "Resonant frequency",
                "Millimeter wave communication",
                "Bandwidth",
                "Antenna feeds",
                "Couplings",
                "Mutual coupling",
                "Surface roughness"
            ],
            "Author Keywords": [
                "Compact",
                "dualband",
                "antenna-in-package (AiP)",
                "heterogeneous integration",
                "5G",
                "IoT",
                "mmWave",
                "metamaterial",
                "split ring resonator",
                "coupled resonance",
                "fused silica"
            ]
        },
        "title": "Dual Band Ultra Compact mmWave Patch Antenna Array on Fused Silica Packaging"
    },
    {
        "authors": [
            "Yu Song",
            "Jia Yu",
            "Xinrui Ge",
            "Rong Hao"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "16 August 2024",
        "doi": "10.1109/JIOT.2024.3445170",
        "publisher": "IEEE",
        "abstract": "With the rapid development of IoT technology, a vast quantity of spatial data with text information is generated. Because of the explosive growth of spatial data, users usually encrypt these data and outsource them to the cloud for enjoying the storage and computing capability. Privacy-preserving boolean k Nearest Neighbor (kNN) query is a typical query technique over spatial data. It finds k objects that exactly match the query keyword and are nearest to the query point upon encrypted spatial data. We propose a scheme which supports privacy-preserving boolean kNN query over cloud-based spatial data in this paper. In order to efficiently obtain the spatial objects containing the query keywords, we ask the cloud to pick the objects containing the query keyword with the lowest frequency. Then the cloud filters out the objects that do not contain other query keywords. Since the number of objects containing the query keyword with the lowest frequency is minimal, the number of objects to filter is also minimal. In this way, query efficiency is improved. In order to realize convenient and safe distance comparison over encrypted spatial data, we convert the coordinates to vectors. The distance between two points can be expressed as the inner product of two vectors. Furthermore, we use the enhanced asymmetric scalar-product-preserving encryption algorithm to protect the data privacy. We prove that the proposed scheme satisfies CQA2-security. Meanwhile, we conduct experiments using real datasets to show the performance of the proposed scheme.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Cryptography",
                "Indexes",
                "Spatial databases",
                "Encryption",
                "Cloud computing",
                "Vectors",
                "Data privacy"
            ],
            "Author Keywords": [
                "boolean kNN query",
                "privacy preserving",
                "spatial data",
                "Iot",
                "cloud security"
            ]
        },
        "title": "Enabling Privacy-Preserving Boolean kNN Query over Cloud-Based Spatial Data"
    },
    {
        "authors": [
            "Ramya Priya",
            "Satish Kumar Dubey",
            "Sanket Goel"
        ],
        "published_in": "Published in: IEEE Open Journal of Engineering in Medicine and Biology ( Early Access )",
        "date_of_publication": "09 October 2024",
        "doi": "10.1109/OJEMB.2024.3477315",
        "publisher": "IEEE",
        "abstract": "Goal: To introduce a proof-of-concept prototype for isothermal recombinase polymerase amplification (RPA) incorporating miniaturized photometric detection. The study aimed to demonstrate the feasibility of utilizing this platform for the rapid detection of P. aeruginosa using RPA assay. Methods: The amplification process has been conducted within a microchamber with a diameter of 10 mm, utilizing a standalone Thermostat driven thermal management setup. RPA, requiring a lower operating temperature of 37℃ - 40℃, was employed to complete the reaction. The amplified amplicon was labeled with a fluorophore reporter, stimulated by an LED light source, and detected in real-time using a photodiode. Results: The developed prototype demonstrated the rapid detection of P. aeruginosa using the RPA assay. The process only required the utilization of 0.04 ng of working concentration of DNA. The entire process, from amplification to detection, was completed in approximately 15 minutes. Conclusions: The integration of isothermal RPA with the miniaturized photometric detection platform proved successful in achieving the goal of rapid and specific pathogen detection. The study showcased the advantages of using Isothermal Nucleic Acid Amplification Technology (INAAT) and highlighted its potential as a user-friendly point-of-care (PoC) device in hospitals with limited resources. This proof-of-concept prototype demonstrated the feasibility of implementing RPA-based assays without the need for extensive laboratory equipment. The developed platform, when combined with Internet of Things (IoT) enabled cloud platform, also allowed remote monitoring of data. Overall, the methodology presented in this study offered a cost-effective, accurate, and convenient solution for on-site testing in resource-limited settings.",
        "issn": {
            "Electronic ISSN": "2644-1276"
        },
        "keywords": {
            "IEEE Keywords": [
                "Isothermal processes",
                "Temperature sensors",
                "Heating systems",
                "Polymers",
                "Microfluidics",
                "Microcontrollers",
                "DNA",
                "Thermostats",
                "Photodiodes",
                "Pathogens"
            ],
            "Author Keywords": [
                "Internet-of-things (IoT)",
                "Isothermal Nucleic Acid Amplification",
                "LED-Photodiode",
                "Microcontroller",
                "Pseudomonas Aeruginosa",
                "Recombinase Polymerase Amplification"
            ]
        },
        "title": "Miniaturized Devices for Isothermal Amplification and photometric Quantification of Pseudomonas Aeruginosa"
    },
    {
        "authors": [
            "Weizheng Wang",
            "Dusit Niyato",
            "Zehui Xiong",
            "Zhimeng Yin"
        ],
        "published_in": "Published in: IEEE Transactions on Vehicular Technology ( Early Access )",
        "date_of_publication": "24 September 2024",
        "doi": "10.1109/TVT.2024.3467109",
        "publisher": "IEEE",
        "abstract": "Cross-Technology Communication (CTC) introduces novel security challenges, demanding urgent mitigation strategies. Although recent literature offers the possibility of detecting malicious CTC, they commonly require access to In-phase and Quadrature (IQ) signals, thus not compatible with existing billions of IoT devices. In this paper, we propose a lightweight model to detect unauthorized CTC signals at ZigBee devices by analyzing CTC's inherent chip error patterns that are available on commodity ZigBee devices. To further enhance the detection accuracy, we propose an augmentation framework that integrates both theoretical analysis and channel fading models, and adopts an LSTM-based deep learning method. As for authentication of authorized CTC, we introduce a dynamic method that flips the error-prone chips. This enables authentication without introducing additional chip errors, further ensuring the transparency of existing CTC. Evaluation with testbeds demonstrates transparent detection and reliable authentication.",
        "issn": {
            "Print ISSN": "0018-9545",
            "Electronic ISSN": "1939-9359"
        },
        "keywords": {
            "IEEE Keywords": [
                "Zigbee",
                "Wireless fidelity",
                "Authentication",
                "Security",
                "Wireless communication",
                "Reliability",
                "Computational modeling"
            ],
            "Author Keywords": [
                "Cross-Technology Communication (CTC)",
                "IoT Security",
                "unauthorized signal detection",
                "device authentication"
            ]
        },
        "title": "Detection and Authentication for Cross-Technology Communication"
    },
    {
        "authors": [
            "Fatemeh Najafi",
            "Masoud Kaveh",
            "Mohammad Reza Mosavi",
            "Alessandro Brighente",
            "Mauro Conti"
        ],
        "published_in": "Published in: IEEE Transactions on Mobile Computing ( Early Access )",
        "date_of_publication": "07 November 2024",
        "doi": "10.1109/TMC.2024.3494612",
        "publisher": "IEEE",
        "abstract": "Physical Unclonable Functions (PUFs) are hardware-based mechanisms that exploit inherent manufacturing variations to generate unique identifiers for devices. Dynamic Random Access Memory (DRAM) has emerged as a promising medium for implementing PUFs, providing a cost-effective solution without the need for additional circuitry. This makes DRAM PUFs ideal for use in resource-constrained environments such as Internet of Things (IoT) networks. However, current DRAM PUF implementations often either disrupt host system functions or produce unreliable responses due to environmental sensitivity. In this paper, we present EPUF, a novel approach to extracting random and unique features from DRAM cells to generate reliable PUF responses. We leverage bitmap images of binary DRAM values and their entropy features to enhance the robustness of our PUF. Through extensive real-world experiments, we demonstrate that EPUF is approximately 1.7 times faster than existing solutions, achieves 100% reliability, produces features with 47.79% uniqueness, and supports a substantial set of Challenge-Response Pairs (CRPs). These capabilities make EPUF a powerful tool for DRAM PUF-based authentication. Based on EPUF, we then propose a lightweight authentication protocol that not only offers superior security features but also surpasses state-of-the-art authentication schemes in terms of communication overhead and computational efficiency.",
        "issn": {
            "Print ISSN": "1536-1233",
            "Electronic ISSN": "1558-0660"
        },
        "keywords": {
            "IEEE Keywords": [
                "Random access memory",
                "Reliability",
                "Entropy",
                "Security",
                "Authentication",
                "Timing",
                "Feature extraction",
                "Physical unclonable function",
                "Internet of Things",
                "Hardware"
            ],
            "Author Keywords": [
                "Latency-based DRAM PUF",
                "entropy features",
                "IoT security",
                "authentication protocol"
            ]
        },
        "title": "EPUF: An Entropy-derived Latency-Based DRAM Physical Unclonable Function for Lightweight Authentication in Internet of Things"
    },
    {
        "authors": [
            "Yongsoo Kim",
            "Jaehyeon So",
            "Chanwook Hwang",
            "Wencan Cheng",
            "Jaehyuk Choi",
            "Jong Hwan Ko"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "26 September 2024",
        "doi": "10.1109/JIOT.2024.3468344",
        "publisher": "IEEE",
        "abstract": "As artificial intelligence (AI) technology advances, Internet of Things (IoT) devices such as mobile phones and augmented reality devices are increasingly becoming crucial enablers of user-device interactions. Among the various methods of interaction, hand pose recognition and analysis is a crucial method to understand the intentions of users and perform precise functions. However, to perform such functions, a substantial amount of computation and resources are required, making it challenging to implement them on small form-factor devices with low power consumption. For this reason, improving energy efficiency is a crucial objective in real-time hand pose estimation applied to low-power platforms with limited resources. In this paper, we introduce an FPGA-based energy-efficient real-time hand pose estimation (HPE) system with an integrated image signal processor (ISP). The proposed system uses several low-power design techniques, including a systolic array with dynamic on/off control per processing element (PE), to minimize power consumption and save energy when not in use. In addition, we improve area efficiency by reducing the buffer size in the systolic array using a half-size shift buffer stack. Furthermore, the use of parallel and pipelined structures improved operational efficiency, resulting in a reduction in both operational time and power consumption. The evaluation results on a KU115 FPGA board show that the system achieves an error of 7.78mm and can process 52 fps, demonstrating its capability for real-time hand pose estimation. Moreover, this system achieves high energy efficiency, up to 61.74 GOPs/W, making it suitable for energy-efficient and accurate hand pose estimation in low-power environments.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Pose estimation",
                "Sensors",
                "Real-time systems",
                "Internet of Things",
                "Energy efficiency",
                "Three-dimensional displays",
                "Accuracy"
            ],
            "Author Keywords": [
                "Hand pose estimation",
                "Image signal processor (ISP)",
                "Convolutional neural network(CNN)",
                "FPGA",
                "Internet of Things (IoT)",
                "Low power architecture"
            ]
        },
        "title": "An FPGA-Based Energy-Efficient Real-Time Hand Pose Estimation System With an Integrated Image Signal Processor for Indirect 3D Time-of-Flight Sensors"
    },
    {
        "authors": [
            "Mahima Karim",
            "Md Arafatur Rahman",
            "Mohammed Atiquzzaman"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "24 October 2024",
        "doi": "10.1109/JIOT.2024.3483228",
        "publisher": "IEEE",
        "abstract": "The increasing importance of intra-vehicular health monitoring systems (IVHMS) necessitates robust communication protocols within vehicles, especially with the integration of the Internet of Things (IoT). This integration presents challenges in enhancing vehicle health monitoring efficiency due to the constrained space for numerous sensing devices, leading to scalability and performance issues. Existing MAC (Medium Access Control) schemes often result in congestion, decreased throughput, and increased delays. This study proposes a scalable hybrid MAC strategy to improve throughput and reduce congestion in IVHMS. The approach features a two-phase communication model—Non-emergency and Emergency phases—for scalability. Our hybrid MAC strategy combines a history-based approach for Emergency Communication and a priority-based approach for Non-emergency Communication. A Markov chain model evaluates the expected throughput and delay of the proposed MAC strategy, with numerical analysis validating the approach. Results show the hybrid MAC achieves a 66.7% throughput improvement over the history-based strategy and a 16.7% improvement over the priority-based approach while effectively reducing data collisions and delays. Furthermore, the hybrid MAC demonstrates an 8.3% increase in throughput compared to a previously proposed distributed hybrid MAC. Implementing this Hybrid MAC in industrial-scale vehicular health monitoring can enhance vehicle safety, benefiting manufacturers and passengers alike.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Media Access Control",
                "Throughput",
                "Delays",
                "Monitoring",
                "Internet of Things",
                "Scalability",
                "Analytical models",
                "Protocols",
                "Sensors",
                "IEEE 802.15 Standard"
            ],
            "Author Keywords": [
                "History-based MAC (Medium Access Control)",
                "Hybrid MAC",
                "IoT (Internet of Things)",
                "Markov chain",
                "Priority-based MAC",
                "Vehicular Health Monitoring"
            ]
        },
        "title": "Hybrid Medium Access Control Strategy for Internet of Things-Enabled Intra-Vehicular Health Monitoring System"
    },
    {
        "authors": [
            "Qun Yan Zhou",
            "Jun Yan Dai",
            "Zuqi Fang",
            "Lijie Wu",
            "Zhen Jie Qi",
            "Si Ran Wang",
            "Rui Zhe Jiang",
            "Qiang Cheng",
            "Tie Jun Cui"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "19 August 2024",
        "doi": "10.1109/JIOT.2024.3445451",
        "publisher": "IEEE",
        "abstract": "Direction of arrival (DOA) estimation is essential in building the wireless electromagnetic (EM) environment. However, the conventional DOA estimation methods have been criticized for their hardware complexity, high cost, and low energy efficiency. Recently, metasurface-based methods have emerged as promising alternatives that offer cost-effective solutions. Nevertheless, the existing studies have neglected angle sensitivity, an intrinsic physical property of metasurface, leading to errors in DOA estimations. To address this issue, we propose a generalized high-precision and wide-angle DOA estimation method based on space-time-coding digital metasurfaces (STCDM). By carefully designing two elementary STC sequences, we realize two angle-independent harmonic states to construct a specified orthogonal matrix. Then, the harmonics, modulated by each meta-column in accordance with the matrix, are sampled by a single channel receiver at different time intervals. Upon the reception of the signal, the information on the metasurface can be comprehensively retrieved through matrix operations, facilitating the high-accuracy DOA estimations using the Array Signal Processing (ASP) algorithms. This work focuses on eliminating the influence of angle sensitivity on metasurface-based DOA estimations, which guarantees high accuracy and substantially reduces hardware requirements. The proposed method holds potential for various applications, including the Internet of Things (IoT) and integrated sensing and communication (ISAC).",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Direction-of-arrival estimation",
                "Harmonic analysis",
                "Estimation",
                "Metasurfaces",
                "Internet of Things",
                "Sensitivity",
                "Hardware"
            ],
            "Author Keywords": [
                "direction of arrival",
                "Internet of Things (IOT)",
                "metasurface",
                "space-time-coding"
            ]
        },
        "title": "Generalized High-Precision and Wide-Angle DOA Estimation Method Based on Space-Time-Coding Digital Metasurfaces"
    },
    {
        "authors": [
            "Shubham Vaishnav",
            "Sarit Khirirat",
            "Sindri Magnússon"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/JIOT.2024.3490855",
        "publisher": "IEEE",
        "abstract": "Federated learning has emerged as a popular distributed machine-learning paradigm. It involves many rounds of iterative communication between nodes to exchange model parameters. With the increasing complexity of ML tasks, the models can be large, having millions of parameters. Moreover, edge and IoT nodes often have limited energy resources and channel bandwidths. Thus, reducing the communication cost in Federated Learning is a bottleneck problem. This cost could be in terms of energy consumed, delay involved, or amount of data communicated. We propose a communication cost-adaptive model sparsification for Federated Learning with Error Compensation. The central idea is to adapt the sparsification level in run-time by optimizing the ratio between the impact of the communicated model parameters and communication cost. We carry out a detailed convergence analysis to establish the theoretical foundations of the proposed algorithm. We conduct extensive experiments to train both convex and non-convex machine learning models on a standard dataset. We illustrate the efficiency of the proposed algorithm by comparing its performance with three baseline schemes. The performance of the proposed algorithm is validated for two communication models and three cost functions. Simulation results show that the proposed algorithm needs a substantially less amount of communication than the three baseline schemes while achieving the best accuracy and fastest convergence. The results are consistent for all the considered cost models, cost functions, and ML models. Thus, the proposed FL-CATE algorithm can substantially improve the communication efficiency of federated learning, irrespective of the ML tasks, costs, and communication models.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Costs",
                "Training",
                "Servers",
                "Convergence",
                "Data models",
                "Stochastic processes",
                "Error compensation",
                "Adaptation models",
                "Quantization (signal)",
                "Adaptive systems"
            ],
            "Author Keywords": [
                "Federated learning",
                "Communication efficiency",
                "IoT",
                "Gradient sparsification",
                "Distributed learning"
            ]
        },
        "title": "Communication-Adaptive Gradient Sparsification for Federated Learning with Error Compensation"
    },
    {
        "authors": [
            "Quantao Yu",
            "Hua Wang",
            "Dongxuan He",
            "Zhiping Lu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "24 October 2024",
        "doi": "10.1109/JIOT.2024.3486126",
        "publisher": "IEEE",
        "abstract": "LoRa is one of the most prominent low power wide area network (LPWAN) technologies for Internet of Things (IoT) applications. As the core technique of LoRa physical (PHY) layer, chirp spread spectrum (CSS) modulation is employed to support low power and long range communications. Although it provides a compelling trade-off between coverage and data rate, the relatively low spectral efficiency (SE) is still a limiting factor for its extensive applications. In this paper, we propose two enhanced group-based CSS modulation schemes, named IQ-GCSS (in-phase and quadrature group-based CSS) and TDM-GCSS (time domain multiplexed group-based CSS), which can achieve much higher SE than the conventional LoRa modulation. The transmitter architectures of our proposed modulation schemes are presented along with both coherent and non-coherent detection methods. Moreover, an overall performance analysis of our proposed schemes is provided in terms of bit error rate (BER) and computational complexity. Numerical results not only validate the accuracy of our theoretical analysis but also demonstrate substantial performance improvements of our proposed schemes in terms of effective throughput compared to the classical counterparts.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Modulation",
                "LoRa",
                "Chirp",
                "Multiplexing",
                "Phase modulation",
                "Internet of Things",
                "Indexes",
                "Symbols",
                "Performance analysis",
                "Frequency modulation"
            ],
            "Author Keywords": [
                "IoT",
                "LPWAN",
                "LoRa",
                "chirp spread spectrum",
                "performance analysis"
            ]
        },
        "title": "Enhanced Group-Based Chirp Spread Spectrum Modulation: Design and Performance Analysis"
    },
    {
        "authors": [
            "Erfan Delfani",
            "Nikolaos Pappas"
        ],
        "published_in": "Published in: IEEE Transactions on Communications ( Early Access )",
        "date_of_publication": "02 October 2024",
        "doi": "10.1109/TCOMM.2024.3471969",
        "publisher": "IEEE",
        "abstract": "In this work, we consider a real-time IoT monitoring system in which an energy harvesting sensor with a finite-size battery measures a physical process and transmits the status updates to an aggregator. The aggregator, equipped with caching capabilities, can serve the external requests of a destination network with either a stored update or a fresh update from the sensor. We assume the destination network acts as a gossiping network in which the update packets are forwarded among the nodes in a randomized setting. We utilize the Markov Decision Process framework to model and optimize the network’s average Version Age of Information (AoI) and obtain the optimal policy at the aggregator. We demonstrate analytically and verify numerically that the optimal policy structure conforms to a threshold policy regarding the Version AoI at the aggregator. Furthermore, we establish that the optimal policy is independent of the Version AoI value at the destination nodes. Through numerical results, we elucidate the impact of system parameters on the average Version AoI of the network and the rationale behind this impact. Additionally, the simulations unveil scenarios wherein the performance of the optimal policy significantly surpasses that of a set of baseline policies.",
        "issn": {
            "Print ISSN": "0090-6778",
            "Electronic ISSN": "1558-0857"
        },
        "keywords": {
            "IEEE Keywords": [
                "Monitoring",
                "Energy harvesting",
                "Batteries",
                "Measurement",
                "Internet of Things",
                "Markov decision processes",
                "Information age",
                "Battery charge measurement",
                "Real-time systems",
                "Energy consumption"
            ],
            "Author Keywords": [
                "Age of Information",
                "Version Age",
                "Energy Harvesting",
                "Markov Decision Process",
                "Gossiping network",
                "IoT"
            ]
        },
        "title": "Version Age-Optimal Cached Status Updates in a Gossiping Network with Energy Harvesting Sensor"
    },
    {
        "authors": [
            "Yuxian Jiang",
            "Zhan Li",
            "Zhihan Zhang",
            "Hao Wang",
            "Sheng Ch"
        ],
        "published_in": "Published in: IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems ( Early Access )",
        "date_of_publication": "23 October 2024",
        "doi": "10.1109/TCAD.2024.3485571",
        "publisher": "IEEE",
        "abstract": "In the era of artificial intelligence (AI), rapidly growing data and computing demands stimulate a shift toward more intelligent processing at the edge in the Internet of Things (IoT). AI application scenarios, such as image segmentation, pose new challenges to the computing capability of edge hardware, which cannot be solved by traditional AI accelerators with the traditional Processing Element (PE) architecture. Recently, the streaming architecture has received more attention due to its higher performance. To improve the throughput of edge platforms for segmentation tasks, customizing streaming accelerators for segmentation models is now necessary. Based on these motivations, we proposed PEDSA, a fully pipelined streaming accelerator for convolutional encoder-decoder segmentation networks. PEDSA maps all the layers in the network into a pixel-level pipeline. All operations, especially upsampling (unpooling, deconvolution, etc.) which involves complex data rearrangement, are integrated into a regular, concise, and fully on-chip data flow. On Xilinx field-programmable gate arrays (FPGAs), our acceleration of SegNet-Basic and U-Net reached performances of 2676.47GOPS and 7646.29GOPS respectively, outperforming previous accelerators for this kind of network. This work provides new ideas for the deployment of segmentation algorithms at the edge.",
        "issn": {
            "Print ISSN": "0278-0070",
            "Electronic ISSN": "1937-4151"
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer architecture",
                "Pipelines",
                "Streaming media",
                "Image segmentation",
                "System-on-chip",
                "Convolution",
                "Hardware",
                "Computational modeling",
                "Deconvolution",
                "Convolutional neural networks"
            ],
            "Author Keywords": [
                "hardware accelerator",
                "semantic segmentation",
                "convolutional neural network",
                "edge computing",
                "pipeline",
                "IoT"
            ]
        },
        "title": "PEDSA: High-Throughput Pipeline-Based FPGA Accelerator for Convolutional Encoder-Decoder Segmentation Networks"
    },
    {
        "authors": [
            "Xuan Liu",
            "Jinglong Chen",
            "Jingsong Xie",
            "Yuanhong Chang"
        ],
        "published_in": "Published in: IEEE Transactions on Intelligent Transportation Systems ( Early Access )",
        "date_of_publication": "31 October 2024",
        "doi": "10.1109/TITS.2024.3482106",
        "publisher": "IEEE",
        "abstract": "Generative Adversarial Networks (GANs) for generating realistic data, have substantially improved fault diagnosis algorithms in various Internet of Things (IoT) systems. However, challenges such as training instability and dynamical inaccuracy limit their utility in high-speed rail (HSR) bogie fault diagnosis. To address these challenges, we introduce the Pulse Voltage-Guided Conditional Diffusion Model (VGCDM). Unlike traditional implicit GANs, VGCDM adopts a sequential U-Net architecture, facilitating multi-steps denoising diffusion for generation, which bolsters training stability and mitigate convergence issues. VGCDM also incorporates control pulse voltage by cross-attention mechanism to ensure the alignment of vibration with voltage signals, enhancing the Conditional Diffusion Model’s progressive controlablity. Consequently, solely straightforward sampling of control voltages, ensuring the efficient transformation from Gaussian Noise to vibration signals. This adaptability remains robust even in scenarios with time-varying speeds. To validate the effectiveness, we conducted two case studies using SQ dataset and high-simulation HSR bogie dataset. The results of our experiments unequivocally confirm that VGCDM outperforms other generative models, achieving the best RSME, PSNR, and FSCS, showing its superiority in conditional HSR bogie vibration signal generation. For access, our code is available at https://github.com/xuanliu2000/VGCDM.",
        "issn": {
            "Print ISSN": "1524-9050",
            "Electronic ISSN": "1558-0016"
        },
        "keywords": {
            "IEEE Keywords": [
                "Vibrations",
                "Diffusion models",
                "Feature extraction",
                "Training",
                "Attention mechanisms",
                "Internet of Things",
                "Fault diagnosis",
                "Gaussian noise",
                "Voltage control",
                "Generative adversarial networks"
            ],
            "Author Keywords": [
                "Fault diagnosis",
                "signal generation",
                "diffusion model",
                "Internet of Things (IoT)",
                "high-speed railway (HSR) bogie"
            ]
        },
        "title": "Generating HSR Bogie Vibration Signals via Pulse Voltage-Guided Conditional Diffusion Model"
    },
    {
        "authors": [
            "Shashank Mishra",
            "Jia-Ming Liang"
        ],
        "published_in": "Published in: IEEE Transactions on Intelligent Vehicles ( Early Access )",
        "date_of_publication": "28 May 2024",
        "doi": "10.1109/TIV.2024.3406552",
        "publisher": "IEEE",
        "abstract": "The growing prevalence of Internet of Things (IoT) technologies has led to a rise in the popularity of intelligent vehicles that incorporate a range of sensors to monitor various aspects, such as driving speed, fuel usage, distance proximity and tire anomalies. Nowadays, real-time tire sensing systems play important roles for intelligent vehicles in increasing mileage, reducing fuel consumption, improving driving safety, and reducing the potential for traffic accidents. However, the current tire sensing system drains a significant vehicle' energy and lacks effective collection of sensing data, which may not guarantee the immediacy of driving safety. Thus, this paper designs an energy-efficient wireless tire sensing system (WTSS), which leverages energy-saving techniques to significantly reduce power consumption while ensuring data retrieval delays during real-time monitoring. Additionally, we mathematically analyze the worst-case transmission delay of the system to ensure the immediacy based on the collision probabilities of sensor transmissions. This system has been implemented and verified by the simulation and field trial experiments. These results show that the proposed scheme provides enhanced performance in energy efficiency up to 60.7% on average and accurately identifies the worst transmission delay.",
        "issn": {
            "Electronic ISSN": "2379-8904",
            "Print ISSN": "2379-8858"
        },
        "keywords": {
            "IEEE Keywords": [
                "Monitoring",
                "Delays",
                "Wireless communication",
                "Wireless sensor networks",
                "Sensors",
                "Temperature sensors",
                "Real-time systems"
            ],
            "Author Keywords": [
                "Delay analysis",
                "Internet-of-Things (IoT)",
                "energy saving",
                "intelligent vehicles",
                "wireless tire sensing system (WTSS)"
            ]
        },
        "title": "Design and Implementation of Energy-Efficient Wireless Tire Sensing System with Delay Analysis for Intelligent Vehicles"
    },
    {
        "authors": [
            "Thai-Hoc Vu",
            "Senthil Kumar Jagatheesaperumal",
            "Minh-Duong Nguyen",
            "Nguyen Van Huynh",
            "Sunghwan Kim",
            "Quoc-Viet Pham"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "29 October 2024",
        "doi": "10.1109/JIOT.2024.3487627",
        "publisher": "IEEE",
        "abstract": "The success of Artificial Intelligence (AI) in multiple disciplines and vertical domains in recent years has promoted the evolution of mobile networking and the future Internet toward an AI-integrated Internet-of-Things (IoT) era. Nevertheless, most AI techniques rely on data generated by physical devices (e.g., mobile devices and network nodes) or specific applications (e.g., fitness trackers and mobile gaming). Therefore, Generative AI (GAI), a.k.a. AI-generated content (AIGC), has emerged as a powerful AI paradigm; thanks to its ability to efficiently learn complex data distributions and generate synthetic data to represent the original data in various forms. This impressive feature is projected to transform the management of mobile networking and diversify the current services and applications provided. On this basis, this work presents a concise tutorial on the role of GAIs in mobile and wireless networking. In particular, this survey first provides the fundamentals of GAI and representative GAI models, serving as an essential preliminary to the understanding of GAI’s applications in mobile and wireless networking. Then, this work provides a comprehensive review of state-of-the-art studies and GAI applications in network management, wireless security, semantic communication, and lessons learned from the open literature. Finally, this work summarizes the current research on GAI for mobile and wireless networking by outlining important challenges that need to be resolved to facilitate the development and applicability of GAI in this edge-cutting area.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Artificial intelligence",
                "Security",
                "Communication system security",
                "Data models",
                "Wireless sensor networks",
                "Wireless networks",
                "Surveys",
                "Generative adversarial networks",
                "Internet of Things",
                "Training"
            ],
            "Author Keywords": [
                "Artificial Intelligence",
                "Generative AI",
                "Internet of Things (IoT)",
                "Mobile Networking",
                "Machine Learning",
                "Wireless Networks"
            ]
        },
        "title": "Applications of Generative AI (GAI) for Mobile and Wireless Networking: A Survey"
    },
    {
        "authors": [
            "Dries Van Leemput",
            "Jeroen Hoebeke",
            "Eli De Poorter"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "21 August 2024",
        "doi": "10.1109/JIOT.2024.3447234",
        "publisher": "IEEE",
        "abstract": "Industrial wireless sensor networks offer a viable alternative to wired solutions where there is a lack of suitable communication infrastructure. Among these networks, TSCH emerges as a noteworthy choice due to their capacity for deterministic latency, heightened reliability, and low power consumption. Nonetheless, challenges arise from the energy-intensive joining procedure and inherent idle listening associated with TSCH, impeding the integration of battery-powered end devices. Therefore, this paper introduces a novel 6LPN that supports ultra-low-power operations. The proposed solution optimizes the energy-intensive joining procedure through reduced advertisement channels, optimal scanning time, and a delayed join, achieving a 90% reduction in energy consumption. Furthermore, idle listening is eliminated by queueing downlink traffic in a 6FN, ensuring an 87-94% reduction in power consumption during operational mode while maintaining an average latency of queued frames of 7.62 seconds. By comparing the impact of the optimizations on three IoT hardware platforms, we demonstrate that optimal results are obtained for devices with low RX and idle transceiver current. Finally, our solution maintains full backward compatibility with 6TiSCH and does not introduce additional control traffic.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Synchronization",
                "Standards",
                "Optimization",
                "Batteries",
                "Wireless sensor networks",
                "IEEE 802.15 Standard",
                "Wireless communication"
            ],
            "Author Keywords": [
                "Internet of Things (IoT)",
                "Industrial Wireless Sensor Networks (IWNSs)",
                "Energy Efficiency",
                "6TiSCH",
                "Time Slotted Channel Hopping (TSCH)"
            ]
        },
        "title": "Supporting Ultra-Low-Power Nodes in 6TiSCH Industrial Wireless Sensor Networks"
    },
    {
        "authors": [
            "Azeem Irshad",
            "Amer Aljaedi",
            "Zaid Bassfar",
            "Sajjad Shaukat Jamal",
            "Muhammad Usman",
            "Shehzad Ashraf Chaudhry"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "30 October 2024",
        "doi": "10.1109/JIOT.2024.3484945",
        "publisher": "IEEE",
        "abstract": "The fog-enabled healthcare IoT (F-HIoT) paradigm is impending for the prospective patient-healthcare applications. The fog nodes permit the smart medical wearables (SMW) to upload the information with low latency using 5G services. The security risks associated with the distributed attributes of fog nodes may hamper the monitoring of real-time medical reports with privacy. Many authenticated key exchange techniques can be witnessed to strengthen the interactions among SMW devices, fog nodes, and cloud server on insecure channels; however these are either based on computation-intensive crypto-primitives, or fail to provide anonymity to the F-HIoT end users. In this scheme, we propose an ultra-lightweight anonymous authentication protocol (FA-SMW) leveraging low cost crypto-primitive operations, i.e. Chebyshev chaotic map for setting up a mutually agreed session key among three entities. The security properties of FA-SMW are evaluated and analyzed under random oracle model and Canetti-Krawczyk (eCK) threat model. These findings suggest that the proposed protocol considerably meets the desired requirements in the fog-enabled F-HIoT system.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Protocols",
                "Medical services",
                "Servers",
                "Cloud computing",
                "Medical diagnostic imaging",
                "Authentication",
                "Chebyshev approximation",
                "Wearable devices",
                "Computational modeling",
                "Real-time systems"
            ],
            "Author Keywords": [
                "Fog computing",
                "IoT",
                "Healthcare",
                "Authentication",
                "Security",
                "5G"
            ]
        },
        "title": "FA-SMW: Fog-Driven Anonymous Lightweight Access Control for Smart Medical Wearables"
    },
    {
        "authors": [
            "Wei Wei",
            "Chen Pan",
            "Sahidul Islam",
            "Jishnu Banerjee",
            "Shyamala Palanisamy",
            "Mimi Xie"
        ],
        "published_in": "Published in: IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems ( Early Access )",
        "date_of_publication": "24 June 2024",
        "doi": "10.1109/TCAD.2024.3418396",
        "publisher": "IEEE",
        "abstract": "The widespread deployment of various tiny energy harvesting devices has facilitated the expansion of Internet of Things (IoT) applications, notably in remote and hard-to-reach areas. Once deployed, a critical limitation of these devices is their inability to adapt code to evolving environmental conditions or user requirements. This challenge primarily arises from frequent power interruptions during code updates in energy harvesting devices, unlike their battery-powered counterparts, which can lead to significant errors or system failures. In response, we have designed an innovative framework for facilitating intermittent over-the-air (OTA) code updates in tiny energy harvesting devices. Our approach incorporates Intermittent-aware Update Operations, including insert, modify, delete, and copy, tailored for a variety of update scenarios while accommodating intermittent power and resource constraints. Furthermore, We have designed a Fault-tolerant Bootloader that enables the intermittent update capability. This advanced bootloader enables code updates without system reboots and ensures correct task resumption of both routine and update tasks. This not only conserves energy by reducing the need for repetitive reboots but also ensures consistent code updates despite frequent power failures. Additionally, our framework integrates an Update-aware Checkpointing mechanism to provide reliable backups for both routine tasks and update tasks. This proposed framework presents a general solution for enabling intermittent code updates in tiny energy harvesting devices. Our experimental results demonstrate that the proposed approach outperforms existing approaches under conditions of insufficient harvested energy.",
        "issn": {
            "Print ISSN": "0278-0070",
            "Electronic ISSN": "1937-4151"
        },
        "keywords": {
            "IEEE Keywords": [
                "Codes",
                "Task analysis",
                "Energy harvesting",
                "Checkpointing",
                "Internet of Things",
                "Hardware",
                "Security"
            ],
            "Author Keywords": [
                "Embedded System",
                "OTA Code Update",
                "Energy Harvesting",
                "IoT",
                "Tiny Devices"
            ]
        },
        "title": "Intermittent OTA Code Update Framework for Tiny Energy Harvesting Devices"
    },
    {
        "authors": [
            "Sourav Bagchi",
            "Mamata Jenamani",
            "Aurobinda Routray"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "30 September 2024",
        "doi": "10.1109/JSEN.2024.3466966",
        "publisher": "IEEE",
        "abstract": "Internal ambience monitoring of reefer containers using IoT is important for cold chain tracking and traceability. However, disruptions due to power fluctuations, communication failures, and sensor malfunctions can lead to missing values in the data streams, impacting the effectiveness of the monitoring system. Existing algorithms addressing this issue assume the stationarity of the training dataset, ignoring the fact that the real-world datasets are mostly non-stationary. This paper presents a two-stage framework to address this issue in the context of reefer container monitoring. The first stage detects the missing values and segments the training set, while the second stage employs a rotational transformation, followed by an imputation algorithm. The framework is demonstrated using datasets collected during the remote monitoring of a reefer container, which is found to be non-stationary using the Augmented Dickey-Fuller test. The accuracy is evaluated using metrics like RMSE, MAE, MAPE, and Mean (bias) and shows better performance of the imputation algorithms with the proposed transformation when compared without it or with few other existing ones. Besides the imputation accuracy, the information preserved within the transformed dataset is evaluated using mutual information. The results suggest that the amount of retained information in case of the proposed approach surpasses that of existing techniques. Additionally, a metric to assess the predictive capability of an algorithm for a specific dataset is used to evaluate the framework and the results indicate improved predictability as compared to the existing ones.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Time series analysis",
                "Sensors",
                "Containers",
                "Monitoring",
                "Imputation",
                "Measurement",
                "Accuracy",
                "Temperature sensors",
                "Internet of Things",
                "Training"
            ],
            "Author Keywords": [
                "Internet of Things (IoT)",
                "time series",
                "missing data imputation",
                "non-stationarity",
                "data transformation"
            ]
        },
        "title": "A novel data transformation for improving predictive accuracy of online missing value imputation during reefer container monitoring"
    },
    {
        "authors": [
            "Riccardo Della Sala",
            "Davide Bellizia",
            "Francesco Centurelli",
            "Giuseppe Scotti",
            "Alessandro Trifiletti"
        ],
        "published_in": "Published in: IEEE Transactions on Circuits and Systems I: Regular Papers ( Early Access )",
        "date_of_publication": "16 May 2024",
        "doi": "10.1109/TCSI.2024.3396916",
        "publisher": "IEEE",
        "abstract": "This paper introduces an innovative approach to designing a mismatched current mirror with a fully unbalanced output, significantly reducing the minimum supply voltage requirements for Regulated Cascode Current Mirror (RCCM) Physical Unclonable Functions (PUFs). Leveraging body-driven feedback mechanisms, the proposed circuit reliably operates with supply voltages as low as 0.3V, maintaining stable power consumption through a reference bias current. The resulting PUF achieves remarkable energy efficiency, consuming only 0.3 fJ per bit, without compromising statistical performance. It exhibits a response bias of 49.42%, a reliability of 99.483%, and a uniqueness of 50.176%. Validation of this novel approach is conducted through simulations and measurements on a 130nm CMOS test-chip, considering a nominal supply voltage of 0.3V,\n±\n10% supply voltage variations, and a temperature range from 0\n∘\nC to 75\n∘\nC. Rigorous experimental verification on 20 chip samples, along with detailed explanations of design methodologies, underscores the robustness and practicality of the proposed Body-PUF design. Comparative analyses against state-of-the-art literature reveal that the Body-PUF outperforms previous PUF designs in Figures of Merit (FOM), making it promising for real-world authentication scenarios. Its outstanding trade-off between performance and practicality positions it as a compelling solution for secure applications, including Internet of Things (IoT) devices and other security-critical systems.",
        "issn": {
            "Print ISSN": "1549-8328",
            "Electronic ISSN": "1558-0806"
        },
        "keywords": {
            "IEEE Keywords": [
                "Current mirrors",
                "Threshold voltage",
                "Voltage control",
                "Computer architecture",
                "Circuits",
                "Voltage measurement",
                "Reliability"
            ],
            "Author Keywords": [
                "IoT",
                "ultra-low voltage (ULV)",
                "physical unclonable functions (PUFs)",
                "body-driven",
                "hardware security",
                "key generation"
            ]
        },
        "title": "Exploiting Body-Driven Feedbacks in Physical Unclonable Functions for Ultra Low Voltage, Ultra Low Power Applications: A 0.3 V Weak-PUF"
    },
    {
        "authors": [
            "Mohammadreza Rouhi",
            "Roshan Nepal",
            "Simran Chathanat",
            "Nimesh Kotak",
            "Nathan Johnston",
            "Ahmad Ansariyan",
            "Kamalpreet Kaur",
            "Kushant Patel",
            "Norman Zhou",
            "George Shaker"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "03 October 2024",
        "doi": "10.1109/JSEN.2024.3469632",
        "publisher": "IEEE",
        "abstract": "Water leaks pose remarkable challenges to infrastructure, leading to costly damage and substantial resource waste. Traditional battery-powered leak detection systems present significant environmental challenges due to their non-sustainable nature, frequent replacements, recycling complexities, and associated operational costs. This work introduces a novel approach to water leak detection that circumvents these limitations using a self-powered water leak detection sensor system that harnesses hydroelectric energy. The self-powered system comprises a highly responsive sensor unit and a low-power wireless communication circuit, all interconnected through an IoT hub. Our research includes the design of the self-powered system, electrical assessments of the sensor unit under various load conditions, and the development of a custom energy management circuit utilizing an ultra-low power Bluetooth Low Energy (BLE) chipset. Performance evaluation tests demonstrated the system’s capabilities, with sensitivity to water leaks as low as 1 mm in depth, activation times of around 1 minute, reliable operation across a temperature range of -20°C to 60°C, consistent performance over multiple cycles, efficient indoor signal transmission over distances up to 15 meters, and minimal voltage degradation after 18 months shelf life ensuring sufficient power for BLE activation. These quantitative results highlight the system’s edge over traditional methods, showcasing its novelty and potential for widespread application in sustainable infrastructure management.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Monitoring",
                "Leak detection",
                "Batteries",
                "Temperature sensors",
                "Water resources",
                "Vibrations",
                "Sensor systems",
                "Reliability",
                "Radio frequency"
            ],
            "Author Keywords": [
                "Bluetooth Low Energy (BLE)",
                "Energy Harvesting",
                "Internet of Things (IoT)",
                "MQTT protocol",
                "Nanomaterials",
                "Self-powered leak detection"
            ]
        },
        "title": "Wireless Battery-Free Self-Powered Water Leak Detection Through Hydroelectric Energy Harvesting"
    },
    {
        "authors": [
            "Amar Banerjee",
            "Venkatesh Choppella"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "29 October 2024",
        "doi": "10.1109/JIOT.2024.3487578",
        "publisher": "IEEE",
        "abstract": "Dynamic control software reconfiguration for the Internet of Things (IoT) and cyber-physical systems (CPS) is crucial for adaptable and efficient automation. This paper presents a knowledge-driven architecture enabling dynamic device reconfiguration using the Web Ontology Language (OWL) and Terse Triple Language (TTL) formats. Key components include a capability ontology, session-type information for sequencing and concurrent operations, and an Integrated Development Environment (IDE) for automated control design. The capability ontology standardizes machine capabilities, facilitating device integration based on their capabilities, while session-type information ensures correct sequencing and synchronization of machine functions. The IDE platform supports dynamic reconfiguration by automating device selection, control strategy formulation, and system adjustments across diverse use cases. The architecture has been validated in real-world scenarios, including smart meeting rooms, warehouse automation, and energy management, showing a reduction in manual configuration time (up to 50%), development time (86% in some cases), and error rates (30%). Benchmarking results indicate faster code generation (40% improvement) and efficient component integration across different CPS environments. Challenges like computational complexity, scalability, and integration with existing systems highlight limitations. Future research will explore further optimizations and broader applicability to ensure low-latency, high-accuracy, and seamless integration in complex CPS. This work advances dynamic control software reconfiguration by providing a flexible solution that enhances CPS reliability and efficiency through a knowledge-driven approach.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Software",
                "Internet of Things",
                "Real-time systems",
                "Ontologies",
                "Power system dynamics",
                "Dynamic scheduling",
                "Computer architecture",
                "Robots",
                "OWL",
                "Manufacturing"
            ],
            "Author Keywords": [
                "industrial automation systems",
                "cyber-physical systems",
                "IoT",
                "control design",
                "knowledge-driven approach",
                "control software",
                "dynamic reconfiguration",
                "capability ontology"
            ]
        },
        "title": "A Knowledge-Driven Approach for Dynamic Reconfiguration of Control Design in Internet of Things and Cyber-Physical Systems"
    },
    {
        "authors": [
            "Lingzhong Zhang",
            "Jianquan Lu",
            "Fengyi Liu",
            "Jungang Lou"
        ],
        "published_in": "Published in: IEEE Transactions on Neural Networks and Learning Systems ( Early Access )",
        "date_of_publication": "11 October 2023",
        "doi": "10.1109/TNNLS.2023.3320651",
        "publisher": "IEEE",
        "abstract": "This brief studies the distributed synchronization of time-delay coupled neural networks (NNs) with impulsive pinning control involving stabilizing delays. A novel differential inequality is proposed, where the state’s past information at impulsive time is effectively extracted and used to handle the synchronization of coupled NNs. Based on this inequality, the restriction that the size of impulsive delay is always limited by the system delay is removed, and the upper bound on the impulsive delay is relaxed, which is improved the existing related results. By using the methods of average impulsive interval (AII) and impulsive delay, some relaxed criteria for distributed synchronization of time-delay coupled NNs are obtained. The proposed synchronization conditions do not impose on the upper bound of two consecutive impulsive signals, and the lower bound is more flexible. Moreover, our results reveal that the impulsive delays may contribute to the synchronization of time-delay systems. Finally, typical networks are presented to illustrate the advantage of our delayed impulsive control method.",
        "issn": {
            "Print ISSN": "2162-237X",
            "Electronic ISSN": "2162-2388"
        },
        "keywords": {
            "IEEE Keywords": [
                "Delays",
                "Synchronization",
                "Artificial neural networks",
                "Vehicle dynamics",
                "Delay effects",
                "Nonlinear dynamical systems",
                "Upper bound"
            ],
            "Author Keywords": [
                "Delayed dynamical networks",
                "distributed impulsive control",
                "impulsive delays",
                "synchronization"
            ]
        },
        "title": "Synchronization of Time-Delay Coupled Neural Networks With Stabilizing Delayed Impulsive Control"
    },
    {
        "authors": [
            "Haomiao Yang",
            "Dongyun Xue",
            "Mengyu Ge",
            "Jingwei Li",
            "Guowen Xu",
            "Hongwei Li",
            "Rongxing Lu"
        ],
        "published_in": "Published in: IEEE Transactions on Dependable and Secure Computing ( Early Access )",
        "date_of_publication": "18 April 2024",
        "doi": "10.1109/TDSC.2024.3387570",
        "publisher": "IEEE",
        "abstract": "Federated learning (FL) is a distributed machine learning technique that guarantees the privacy of user data. However, FL has been shown to be vulnerable to gradient leakage attacks (GLA), which have the ability to reconstruct private training data from public gradients with high probability. These attacks are either analytic-based, requiring modification of the FL model, or optimization-based, requiring long convergence times and failing to effectively address the challenge of dealing with highly compressed gradients in practical FL systems. This paper presents a pioneering generation-based GLA method called FGLA that can reconstruct batches of user data without the need for the optimization process. We specifically design a feature separation technique that first extracts the features of each sample in a batch and then directly generates the user data. Our extensive experiments on multiple image datasets show that FGLA can reconstruct user images in seconds with a batch size of 256 from highly compressed gradients (0.8% compression ratio or higher), thereby significantly outperforming state-of-the-art methods.",
        "issn": {
            "Print ISSN": "1545-5971",
            "Electronic ISSN": "1941-0018"
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Image reconstruction",
                "Generators",
                "Servers",
                "Data models",
                "Security",
                "Optimization"
            ],
            "Author Keywords": [
                "Data privacy",
                "federated learning",
                "gradient leakage attack",
                "image generation"
            ]
        },
        "title": "Fast Generation-Based Gradient Leakage Attacks: An Approach to Generate Training Data Directly From the Gradient"
    },
    {
        "authors": [
            "Hongyang Du",
            "Dusit Niyato",
            "Jiawen Kang",
            "Zehui Xiong",
            "Ping Zhang",
            "Shuguang Cui",
            "Xuemin Shen",
            "Shiwen Mao",
            "Zhu Han",
            "Abbas Jamalipour",
            "H. Vincent Poor",
            "Dong In Kim"
        ],
        "published_in": "Published in: IEEE Network ( Early Access )",
        "date_of_publication": "02 July 2024",
        "doi": "10.1109/MNET.2024.3422241",
        "publisher": "IEEE",
        "abstract": "Generative AI (GAI) has emerged as a significant advancement in artificial intelligence, renowned for its language and image generation capabilities. This paper presents \"AI-Generated Everything\" (AIGX), a concept that extends GAI beyond mere content creation to real-time adaptation and control across diverse technological domains. In networking, AIGX collaborates closely with physical, data link, network, and application layers to enhance real-time network management that responds to various system and service settings as well as application and user requirements. Networks, in return, serve as crucial components in further AIGX capability optimization through the AIGX lifecycle, i.e., data collection, distributed pre-training, and rapid decision-making, thereby establishing a mutually enhancing interplay. Moreover, we offer an in-depth case study focused on power allocation to illustrate the inter-dependence between AIGX and networking systems. Through this exploration, the article analyzes the significant role of GAI for networking, clarifies the ways networks augment AIGX functionalities, and underscores the virtuous interactive cycle they form. This article paves the way for subsequent future research aimed at fully unlocking the potential of GAI and networks.",
        "issn": {
            "Print ISSN": "0890-8044",
            "Electronic ISSN": "1558-156X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Generative AI",
                "Resource management",
                "Optimization",
                "Laboratories",
                "Communication system security",
                "Adaptation models"
            ],
            "Author Keywords": [
                "Generative AI (GAI)",
                "networks",
                "AI-generated everything (AIGC)",
                "generative diffusion model"
            ]
        },
        "title": "The Age of Generative AI and AI-Generated Everything"
    },
    {
        "authors": [
            "Zhidan Liu",
            "Guofeng Ouyang",
            "Bolin Zhang",
            "Bo Du",
            "Chao Chen",
            "Kaishun Wu"
        ],
        "published_in": "Published in: IEEE Transactions on Mobile Computing ( Early Access )",
        "date_of_publication": "07 November 2024",
        "doi": "10.1109/TMC.2024.3493974",
        "publisher": "IEEE",
        "abstract": "Dynamic ridesharing has gained significant attention in recent years. However, existing ridesharing studies often focus on optimizing order dispatching and vehicle repositioning separately, leading to short-sighted decisions and underutilization of the ridesharing potential. In this paper, we propose a novel joint optimization framework called JODR. By coordinating order dispatching and vehicle repositioning, JODR enhances ridesharing efficiency while ensuring high-quality service. The core idea of JODR is to dispatch ride orders with high demand in specific mobility directions to vehicles with sufficient available capacity, effectively balancing future supply and demand in those directions. To achieve this, we introduce a novel mobility value function that can predict the long-term mobility value of matching an order with its travel direction. By considering orders' directional mobility values, service quality assessments, and available vehicle capacities, JODR formulates the order dispatching as a minimum-cost maximum-flow problem to derive the optimal order-vehicle assignments. Furthermore, the value function helps the intelligent repositioning of idle vehicles. Extensive experiments conducted on a large real-world dataset demonstrate the superiority of JODR over state-of-the-art methods across various performance metrics. These experimental results validate the effectiveness of JODR in improving the ridesharing efficiency and experience.",
        "issn": {
            "Print ISSN": "1536-1233",
            "Electronic ISSN": "1558-0660"
        },
        "keywords": {
            "IEEE Keywords": [
                "Shared transport",
                "Dispatching",
                "Vehicle dynamics",
                "Optimization",
                "Supply and demand",
                "Real-time systems",
                "Costs",
                "Mobile computing",
                "Indexes",
                "Vectors"
            ],
            "Author Keywords": [
                "Dynamic ridesharing",
                "order dispatching",
                "vehicle repositioning",
                "mobility value"
            ]
        },
        "title": "Joint Order Dispatching and Vehicle Repositioning for Dynamic Ridesharing"
    },
    {
        "authors": [
            "Himani Sikarwar",
            "Harsha Vasudev",
            "Debasis Das",
            "Mauro Conti",
            "Koustav Kumar Mondal"
        ],
        "published_in": "Published in: IEEE Transactions on Network and Service Management ( Early Access )",
        "date_of_publication": "17 September 2024",
        "doi": "10.1109/TNSM.2024.3462746",
        "publisher": "IEEE",
        "abstract": "The Internet of Vehicles (IoV) faces significant challenges related to secure authentication, efficient communication, and privacy preservation due to the high mobility of vehicles, the need for real-time data processing, varying quality of communication links, and the diverse range of devices and protocols requiring interoperability. These challenges are further complicated by the large-scale, dynamic, and heterogeneous nature of IoV systems. Traditional approaches using Road Side Connecting Nodes (RSCNs) face challenges like limited range, high costs, and single points of failure. Drone-assisted IoV (DIoV) networks address these issues by using Unmanned Aerial Vehicles (UAVs) as mobile edge nodes, enhancing connectivity, extending coverage, and improving adaptability and resilience. To address these challenges, we propose SECURE, a drone-assisted, Physically Unclonable Function (PUF)-based authentication and privacy-preserving protocol integrated with edge computing. This architecture replaces RSCNs with edge nodes and incorporates UAVs as mobile edge nodes, providing extended coverage, reduced latency, and enhanced adaptability. The PUFs in SECURE generate unique hardware-based cryptographic keys, adding an additional layer of security, while edge computing offloads computational tasks, improves network efficiency, and further reduces latency. The formal security analysis, conducted using the Random Oracle Model (ROM), proves the robustness of the session key against active and passive adversaries. Furthermore, informal security analysis demonstrates that SECURE effectively resists various security attacks, while achieving confidentiality, integrity, and authenticity in DIoV. In SECURE, we have considered two types of devices for experiments: NVIDIA Jetson Xavier NX and Raspberry Pi 4. The performance analysis, considering the results from Jetson Xavier NX, demonstrates that SECURE achieves maximum upto approximately 82.1% less communication cost and 78% faster c...",
        "issn": {
            "Electronic ISSN": "1932-4537"
        },
        "keywords": {
            "IEEE Keywords": [
                "Protocols",
                "Authentication",
                "Drones",
                "Physical unclonable function",
                "Costs",
                "Cryptography",
                "Computational efficiency"
            ],
            "Author Keywords": [
                "Internet of Vehicles",
                "Security",
                "Edge-Computing",
                "Physical Unclonable Function",
                "Drone-assisted IoV"
            ]
        },
        "title": "SECURE: Secure and Efficient ProtoCol Using Randomness and Edge-Computing for Drone-Assisted Internet of Vehicles"
    },
    {
        "authors": [
            "Fenghua Liu",
            "Xiaoming Li"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "01 October 2024",
        "doi": "10.1109/ACCESS.2024.3471076",
        "publisher": "IEEE",
        "abstract": "The integration of Deep Reinforcement Learning (DRL) with Evolutionary Algorithms (EAs) represents a significant advancement in optimizing smart city energy operations, addressing the inherent uncertainties and dynamic conditions of urban environments. This study explores how the synergy between DRL and EAs, including Genetic Algorithms (GAs) and Differential Evolution (DE), can enhance the efficiency and sustainability of smart city energy systems. DRL, known for its adaptive learning capabilities in complex environments, is combined with EAs, which excel in exploring diverse solution spaces and managing multi-objective optimization problems. The proposed methodology leverages DRL’s ability to learn optimal policies through interaction with the environment and EAs’ robust search mechanisms to address stochastic elements in energy consumption and generation. This integration is applied to various components of smart city energy operations, such as demand response, energy storage management, and renewable energy integration. The results from simulated smart city environments demonstrate significant improvements in energy efficiency, cost reduction, and emission control. This study highlights the potential of combining DRL with EAs to provide a comprehensive approach to tackling the challenges of stochastic optimization, offering a promising solution for achieving adaptive and resilient urban energy management in the face of uncertainty. Application of this integrated approach to demand response, energy storage management, and renewable energy integration in simulated smart city environments resulted in a 15% improvement in energy efficiency, a 12% reduction in operational costs, and a 20% decrease in emissions. These numeric results underscore the effectiveness of combining DRL with EAs in achieving significant gains in energy management. The study highlights the potential of this integrated approach for addressing the challenges of stochastic optimization, offering ...",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Smart cities",
                "Optimization",
                "Stochastic processes",
                "Uncertainty",
                "Renewable energy sources",
                "Costs",
                "Urban areas",
                "Genetic algorithms",
                "Evolutionary computation",
                "Sustainable development",
                "Energy efficiency"
            ],
            "Author Keywords": [
                "Stochastic Optimization",
                "Smart City Energy",
                "Operations",
                "Evolutionary Algorithms",
                "Energy Efficiency",
                "Renewable Energy Integration"
            ]
        },
        "title": "Integrating Deep Reinforcement Learning with Evolutionary Algorithms for Advanced Optimization in Smart City Energy Management"
    },
    {
        "authors": [
            "Jialiuyuan Li",
            "Changyan Yi",
            "Jiayuan Chen",
            "You Shi",
            "Tong Zhang",
            "Xiaolong Li",
            "Ran Wang",
            "Kun Zhu"
        ],
        "published_in": "Published in: IEEE Transactions on Green Communications and Networking ( Early Access )",
        "date_of_publication": "05 July 2024",
        "doi": "10.1109/TGCN.2024.3424449",
        "publisher": "IEEE",
        "abstract": "In this paper, we study the energy-efficient unmanned aerial vehicle (UAV) swarm assisted mobile edge computing (MEC) with dynamic clustering and scheduling. In the considered system model, UAVs are divided into multiple swarms, with each swarm consisting of a leader UAV and several follower UAVs. These UAVs serve as mobile edge servers, providing computing services to their covered ground end-users. Unlike existing works, we allow UAVs to dynamically cluster into different swarms, in other words, each follower UAV can change its leader based on the time-varying spatial positions, updated application placement, etc. in a dynamic manner. With the objective of maximizing the long-term energy efficiency of the UAV swarm assisted MEC system, a joint optimization problem of UAV swarm dynamic clustering and scheduling is formulated. Considering the inherent cooperation and competition among intelligent UAVs, we further reformulate this problem as a combination of a series of strongly interconnected multi-agent stochastic games, and theoretically prove the existence of the corresponding Nash Equilibrium (NE). Then, we propose a novel reinforcement learning based UAV swarm dynamic coordination (RLDC) algorithm for obtaining such an equilibrium. Furthermore, the convergence and complexity of the RLDC algorithm are analyzed. Simulations are performed to evaluate the performance of RLDC and illustrate its superiority compared to existing approaches.",
        "issn": {
            "Electronic ISSN": "2473-2400"
        },
        "keywords": {
            "IEEE Keywords": [
                "Autonomous aerial vehicles",
                "Task analysis",
                "Internet of Things",
                "Dynamic scheduling",
                "Games",
                "Heuristic algorithms",
                "Optimization"
            ],
            "Author Keywords": [
                "UAV swarm",
                "MEC",
                "long-term energy efficiency",
                "stochastic game",
                "reinforcement learning"
            ]
        },
        "title": "A Reinforcement Learning Based Stochastic Game for Energy-Efficient UAV Swarm Assisted MEC With Dynamic Clustering and Scheduling"
    },
    {
        "authors": [
            "Hui Zhao",
            "Zhicheng Li",
            "Dongsheng She"
        ],
        "published_in": "Published in: IEEE Transactions on Vehicular Technology ( Early Access )",
        "date_of_publication": "08 October 2024",
        "doi": "10.1109/TVT.2023.3293229",
        "publisher": "IEEE",
        "abstract": "The safety platoon performance could be significantly enhanced by exchanging vehicle's information over the inter-vehicle communication (IVC). However, a preceding vehicle's jerk could seriously compromise platoon's stability under the limited bandwidth. This paper focuses on the safety platoon control to overcome the above problem. Firstly, a distributed adaptive event-triggered control (ETC) is proposed to adjust the transmission frequency by adaptively tuning the event-triggered threshold according to the preceding vehicle's jerk, which could be a tradeoff between improving transmission efficiency and reducing bandwidth occupation. Under such communication mechanism, a control scheme is proposed to obtain the safety platoon performance by adaptively tuning the cost function. Then, some sufficient conditions of the strictly $\\mathcal {L}_{2}$ string stability and controller design method are provided under the distributed adaptive ETC and the safety control scheme. Further, an optimization algorithm for the controller gains is proposed to reduce on-line computation and improve the control performance by introducing an additional degree of design freedom into control input. Finally, a numerical example is used to verify the effectiveness and advantages of the proposed methods.",
        "issn": {
            "Print ISSN": "0018-9545",
            "Electronic ISSN": "1939-9359"
        },
        "keywords": {
            "IEEE Keywords": [
                "Safety",
                "Bandwidth",
                "Stability criteria",
                "Vehicle dynamics",
                "Topology",
                "Numerical stability",
                "Interference",
                "Tuning",
                "Symmetric matrices",
                "Predictive control"
            ],
            "Author Keywords": [
                "Vehicular networked systems",
                "controller synthesis",
                "event-triggered control",
                "vehicle platoon",
                "strictly $\\mathcal {L}_{2}$ string stability"
            ]
        },
        "title": "Adaptive Event-triggered Safety Control for String Stability of Vehicular Networked Systems"
    },
    {
        "authors": [
            "Yuning Qiu",
            "Guoxu Zhou",
            "Andong Wang",
            "Qibin Zhao",
            "Shengli Xie"
        ],
        "published_in": "Published in: IEEE Transactions on Neural Networks and Learning Systems ( Early Access )",
        "date_of_publication": "24 April 2024",
        "doi": "10.1109/TNNLS.2024.3373384",
        "publisher": "IEEE",
        "abstract": "The recently proposed tensor tubal rank has been witnessed to obtain extraordinary success in real-world tensor data completion. However, existing works usually fix the transform orientation along the third mode and may fail to turn multidimensional low-tubal-rank structure into account. To alleviate these bottlenecks, we introduce two unfolding induced tensor nuclear norms (TNNs) for the tensor completion (TC) problem, which naturally extends tensor tubal rank to high-order data. Specifically, we show how multidimensional low-tubal-rank structure can be captured by utilizing a novel balanced unfolding strategy, upon which two TNNs, namely, overlapped TNN (OTNN) and latent TNN (LTNN), are developed. We also show the immediate relationship between the tubal rank of unfolding tensor and the existing tensor network (TN) rank, e.g., CANDECOMP/PARAFAC (CP) rank, Tucker rank, and tensor ring (TR) rank, to demonstrate its efficiency and practicality. Two efficient TC models are then proposed with theoretical guarantees by analyzing a unified nonasymptotic upper bound. To solve optimization problems, we develop two alternating direction methods of multipliers (ADMM) based algorithms. The proposed models have been demonstrated to exhibit superior performance based on experimental findings involving synthetic and real-world tensors, including facial images, light field images, and video sequences.",
        "issn": {
            "Print ISSN": "2162-237X",
            "Electronic ISSN": "2162-2388"
        },
        "keywords": {
            "IEEE Keywords": [
                "Tensors",
                "Video sequences",
                "Automation",
                "Vectors",
                "Upper bound",
                "Transforms",
                "Light fields"
            ],
            "Author Keywords": [
                "High-order tensor recovery",
                "low-rank tensor completion (LRTC)",
                "tensor decomposition",
                "tensor nuclear norm (TNN)",
                "tensor singular value decomposition (t-SVD)"
            ]
        },
        "title": "Balanced Unfolding Induced Tensor Nuclear Norms for High-Order Tensor Completion"
    },
    {
        "authors": [
            "Peng Hou",
            "Yi Huang",
            "Hongbin Zhu",
            "Zhihui Lu",
            "Shin-Chia Huang",
            "Yang Yang",
            "Hongfeng Chai"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/JIOT.2024.3489655",
        "publisher": "IEEE",
        "abstract": "The integration of sensing, communication, and computation (ISCC) is a critical technology that will support various emerging wireless services in future 6G networks. The unmanned aerial vehicles (UAVs) equipped with edge servers can be used as an aerial service platform in intelligent transportation systems (ITS) to offer ISCC services to vehicles. This paper studies an aerial UAV network comprising a central UAV and secondary UAVs to realize sensing of the global ITS environment and data fusion computation through collaborative UAVs. To enhance the service performance of ISCC, we maximize the success rate of ISCC services and the energy efficiency of UAVs by jointly optimizing bandwidth allocation, power allocation, and computing capacity control while ensuring the sensing and data processing latency requirements. Leveraging the network architecture and collaboration requirements of UAVs, we propose the multi-UAV collaborative Air-ISCC (MCAI) algorithm based on the asynchronous advantage actor-critic algorithm, which obtains the optimal ISCC service policy by co-training a deep reinforcement learning model with multiple UAVs. Sufficient experimental results show that MCAI enhances energy efficiency by 10.51% to 80.12% compared with the baselines. Moreover, MCAI exhibits good scalability, strengthening its feasibility in real scenarios.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Autonomous aerial vehicles",
                "Internet of Things",
                "Optimization",
                "Integrated sensing and communication",
                "Energy consumption",
                "Collaboration",
                "Iterative methods",
                "Servers",
                "Computational modeling"
            ],
            "Author Keywords": [
                "Unmanned aerial vehicles",
                "integrated sensing and communication",
                "reinforcement learning",
                "intelligent transportation systems",
                "multi-access edge computing"
            ]
        },
        "title": "Distributed DRL-Based Integrated Sensing, Communication and Computation in Cooperative UAV-Enabled Intelligent Transportation Systems"
    },
    {
        "authors": [
            "Congcong Liu",
            "Minghua Xia",
            "Junhui Zhao",
            "Huaicheng Li",
            "Yi Gong"
        ],
        "published_in": "Published in: IEEE Transactions on Vehicular Technology ( Early Access )",
        "date_of_publication": "14 October 2024",
        "doi": "10.1109/TVT.2024.3480277",
        "publisher": "IEEE",
        "abstract": "Integrated sensing and communications (ISAC) technology is identified as a breakthrough in optimizing resource allocation and pursuing mutual benefits between radar sensing and wireless communications. In the complex Internet of Vehicles, however, the rapid increase in vehicles leads to a severe scarcity of spectrum resources that is fundamental for wireless communications. To fully exploit ISAC technology potential under strict resource constraints, this paper develops a joint channel, power, and bandwidth allocation scheme based on deep reinforcement learning to improve resource allocation efficiency. Firstly, a deep deterministic policy gradient algorithm is designed to ensure communication reliability by dynamically allocating wireless channels and transmitting power to vehicles based on their geographic positions. Then, we exploit a deep Q-network algorithm to maximize the communication rates by flexibly allocating frequency bandwidth. Simulation results demonstrate that the proposed algorithm exhibits higher spectral efficiency and communication rates than benchmark schemes.",
        "issn": {
            "Print ISSN": "0018-9545",
            "Electronic ISSN": "1939-9359"
        },
        "keywords": {
            "IEEE Keywords": [
                "Resource management",
                "Bandwidth",
                "Indexes",
                "Wireless communication",
                "Vehicle dynamics",
                "Radio spectrum management",
                "Integrated sensing and communication",
                "Heuristic algorithms",
                "Dynamic scheduling",
                "Spectral efficiency"
            ],
            "Author Keywords": [
                "Deep reinforcement learning",
                "Internet of Vehicles",
                "integration sensing and communications",
                "resource allocation"
            ]
        },
        "title": "Optimal Resource Allocation for Integrated Sensing and Communications in Internet of Vehicles: A Deep Reinforcement Learning Approach"
    },
    {
        "authors": [
            "Fangyi Mou",
            "Jiong Lou",
            "Zhiqing Tang",
            "Yuan Wu",
            "Weijia Jia",
            "Yan Zhang",
            "Wei Zhao"
        ],
        "published_in": "Published in: IEEE Transactions on Vehicular Technology ( Early Access )",
        "date_of_publication": "07 November 2024",
        "doi": "10.1109/TVT.2024.3492349",
        "publisher": "IEEE",
        "abstract": "The surge in mobile vehicles and data traffic in Vehicular Edge Computing and Networks (VECONs) requires innovative approaches for low latency, stable connectivity, and efficient resource usage in fast-moving vehicles. Existing studies have identified that utilizing digital twins (DTs) can effectively improve service quality in VECONs. However, it still faces substantial challenges posed by large-scale complex DT communications in sustaining real-time collaborative endeavors. In particular, within the dynamic VECONs, the decision regarding DT migration plays a pivotal role in sustaining the quality of services. In this paper, we propose an adaptive DT migration (ADM) algorithm to minimize the overall migration costs when DTs deliver services. Specifically, 1) We formulate ADM as a combinatorial optimization problem in VECONs, comprehensively considering communication latency and migration latency under complex DT communications, vehicular mobilities, and dynamic states of edges; 2) An ADM algorithm based on off-policy actor-critic reinforcement learning is proposed to make migration decisions. Moreover, the ADM agent employs warm-up policies to address exploration challenges in sparse state spaces; 3) Simulations based on real-world, large-scale urban vehicular mobility datasets demonstrate that our method outperforms existing algorithms by approximately 39% on average, and it can achieve results close to the optimal.",
        "issn": {
            "Print ISSN": "0018-9545",
            "Electronic ISSN": "1939-9359"
        },
        "keywords": {
            "IEEE Keywords": [
                "Vehicle dynamics",
                "Real-time systems",
                "Heuristic algorithms",
                "Costs",
                "Feature extraction",
                "Edge computing",
                "Approximation algorithms",
                "Adaptive systems",
                "Servers",
                "Training"
            ],
            "Author Keywords": [
                "Digital twin",
                "migration",
                "vehicular edge computing",
                "deep reinforcement learning"
            ]
        },
        "title": "Adaptive Digital Twin Migration in Vehicular Edge Computing and Networks"
    },
    {
        "authors": [
            "Zeyang Zhang",
            "Shuchen Wang",
            "Qiwei Zhang",
            "Tian Hong Loh",
            "Yang Yang",
            "Fei Qin"
        ],
        "published_in": "Published in: IEEE Antennas and Wireless Propagation Letters ( Early Access )",
        "date_of_publication": "22 October 2024",
        "doi": "10.1109/LAWP.2024.3484998",
        "publisher": "IEEE",
        "abstract": "With the development and application of Fifth-Generation (5G) mobile communication technologies, cutting-edge techniques including Multiple-Input Multiple-Output (MIMO) and beamforming have come to the forefront. However, the performance gain of these techniques is in the trade of deep understanding of regional multipath channel. To obtain the Channel Impulse Response (CIR) of a given area will typically require pointwise scanning in both channel measurements or Ray Traceing (RT) based modelling method. In this paper, we propose the Ray Inversion (RI) method. The RI method aims to obtain the Region Channel Response (RCR) with only a limited number of observed CIR and avoid the involvement of surrounding environment information. The method classifies of observed propagation paths first and then inverts the ray transmission process to identify the locations of the secondary wave sources and obtain the local regional response. The method significantly enhances the efficiency of obtaining RCR. An iterative fission method has been further designed to guarantee both the efficiency and accuracy. The utilization of this technology can assist the RT method in achieving rapid acquisition of RCR. The RI method is based on geometric inversion, making it applicable to different frequencies. Compared to the traditional point-by-point scanning RT method, the computation speed in an open scene is increased by 15 times, and in a closed scene by 4 times when the receiving antenna spacing of RCR is 1m. The RI method provides an innovative perspective for the research and application of next-generation wireless communication.",
        "issn": {
            "Print ISSN": "1536-1225",
            "Electronic ISSN": "1548-5757"
        },
        "keywords": {
            "IEEE Keywords": [
                "Reflection",
                "Channel impulse response",
                "Attenuation",
                "Transmitters",
                "Fading channels",
                "Delays",
                "Ray tracing",
                "Multipath channels",
                "MIMO communication",
                "Channel models"
            ],
            "Author Keywords": [
                "Ray inversion",
                "ray tracing",
                "multipath channel",
                "regional channel impulse response",
                "acceleration"
            ]
        },
        "title": "The Ray Inversion Method to Obtain Regional Multipath Channels from Finite Impulse Responses"
    },
    {
        "authors": [
            "Zhidan Liu",
            "Yingqian Zhou",
            "Xiaosi Liu",
            "Haodi Zhang",
            "Yabo Dong",
            "Dongming Lu",
            "Kaishun Wu"
        ],
        "published_in": "Published in: IEEE Transactions on Knowledge and Data Engineering ( Early Access )",
        "date_of_publication": "23 October 2024",
        "doi": "10.1109/TKDE.2024.3485195",
        "publisher": "IEEE",
        "abstract": "Map matching aims to align GPS trajectories to their actual travel routes on a road network, which is an essential pre-processing task for most of trajectory-based applications. Many map matching approaches utilize Hidden Markov Model (HMM) as their backbones. Typically, HMM treats GPS samples of a trajectory as observations and nearby road segments as hidden states. During map matching, HMM determines candidate states for each observation with a fixed searching range, and computes the most likely travel route using the Viterbi algorithm. Although HMM-based approaches can derive high matching accuracy, they still suffer from high computation overheads. By inspecting the HMM process, we find that the computation bottleneck mainly comes from improper candidate sets, which contain many irrelevant candidates and incur unnecessary computations. In this paper, we present \\mathtt {LiMM}\n – a learned road network index structure for efficient map matching. \\mathtt {LiMM}\nimproves existing HMM-based approaches from two aspects. Firstly, we propose a novel learned index for road networks, which considers the characteristics of road data. Secondly, we devise an adaptive searching range mechanism to dynamically adjust the searching range for GPS samples based on their locations. As a result, \\mathtt {LiMM}\ncan provide refined candidate sets for GPS samples and thus accelerate the map matching process. Extensive experiments are conducted with three large real-world GPS trajectory datasets. The results demonstrate that \\mathtt {LiMM}\nsignificantly reduces computation overheads by achieving an average speedup of 11.7\\times\nthan baseline methods, merely with a subtle accuracy loss of 1.8%.",
        "issn": {
            "Print ISSN": "1041-4347",
            "Electronic ISSN": "1558-2191"
        },
        "keywords": {
            "IEEE Keywords": [
                "Hidden Markov models",
                "Roads",
                "Global Positioning System",
                "Trajectory",
                "Indexes",
                "Accuracy",
                "Viterbi algorithm",
                "Computational modeling",
                "Predictive models",
                "Data models"
            ],
            "Author Keywords": [
                "GPS Trajectory",
                "hidden markov model",
                "learned index",
                "map matching",
                "road network"
            ]
        },
        "title": "Learning Road Network Index Structure for Efficient Map Matching"
    },
    {
        "authors": [
            "Zhicong Chen",
            "Haoxin Zheng",
            "Lijun Wu",
            "Jingcang Huang",
            "Yang Yang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/JIOT.2024.3489963",
        "publisher": "IEEE",
        "abstract": "Reliable identification of gunshot events is crucial for reducing gun violence and enhancing public safety. However, current gunshot detection and recognition methods are still affected by complex shooting scenarios, various non-gunshot events, diverse firearm types and scarce gunshot datasets. To address these issues, based on tri-axial acceleration of guns, a novel general deep transfer learning approach is proposed for gunshot detection and recognition, which combines a temporal deep learning model with transfer learning and automated machine learning to improve the accuracy, reliability and generalization performance. Firstly, a new gunshot recognition model named as MobileNetTime is proposed for the 2-class gunshot event detection, 3-class coarse firearm recognition and 15-class fine firearm recognition, which utilizes one-dimensional convolution and inverted residual modules to autonomously extract higher-level features from the time series acceleration data. Secondly, considering the impact of non-gunshot events, the automated machine learning is employed for model fine-tuning, to transfer the pre-trained MobileNetTime from the handgun to various firearm types. In addition, we propose a low-power versatile gunshot recognition system framework employing a tri-axial accelerometer for both of wrist-worn and gun-embedded scenarios, which adopts a two-stage wake-up mechanism that selectively monitors gunshot events using temporal and spectral energy features. The experimental results on the two gunshot datasets DGUWA and GRD show that the proposed model can achieve up to 100% accuracy on DGUWA dataset and 98.98% accuracy on GRD dataset for the 2-class gunshot detection. Moreover, the proposed deep transfer learning approach achieves a 98.98% accuracy for 16-class firearm classification, which is 6.21% higher than the model without transfer learning.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Accuracy",
                "Transfer learning",
                "Monitoring",
                "Accelerometers",
                "Training",
                "Adaptation models",
                "Real-time systems",
                "Internet of Things",
                "Data models"
            ],
            "Author Keywords": [
                "gunshot detection and recognition",
                "deep transfer learning",
                "tri-axial acceleration",
                "automated machine learning"
            ]
        },
        "title": "Deep Transfer Learning Based Intelligent Gunshot Detection and Firearm Recognition Using Tri-Axial Acceleration"
    },
    {
        "authors": [
            "Xiaoling Wang",
            "Weichen Xu",
            "Housheng Su",
            "Zhiwei Gao",
            "Guanrong Chen"
        ],
        "published_in": "Published in: IEEE Transactions on Automatic Control ( Early Access )",
        "date_of_publication": "30 September 2024",
        "doi": "10.1109/TAC.2024.3471332",
        "publisher": "IEEE",
        "abstract": "In this paper, the distributed interval observer design problem for a continuous-time linear time-invariant system with unknown external disturbance and measurement noise is revisited. By performing detectability decomposition for partial outputs of each sensor from the system matrix, a classical interval observer and an internally positive representation-based interval observer are designed for the detectable and the un-detectable subsystems, respectively, which reduces the order of the resulting distributed interval observer. Furthermore, with a dynamic coupling gain on the communications among the interval observers, a completely distributed interval observer is designed without any prerequisite, except for the joint detectability of the partial measurements and the strong connectivity of the properly defined directed sensor network. Finally, after detailed analyses, the theoretical results were validated through numerical simulations.",
        "issn": {
            "Print ISSN": "0018-9286",
            "Electronic ISSN": "1558-2523"
        },
        "keywords": {
            "IEEE Keywords": [
                "Observers",
                "Measurement uncertainty",
                "Uncertainty",
                "Noise measurement",
                "Linear systems",
                "Observability",
                "Noise",
                "Intellectual property",
                "Time measurement",
                "Performance evaluation"
            ],
            "Author Keywords": [
                "Distributed interval observer",
                "LTI system",
                "bounded information",
                "internally positive representation",
                "order-reduction"
            ]
        },
        "title": "Designing a Completely Distributed Interval Observer for the LTI System"
    },
    {
        "authors": [
            "Ting-Wei Lu",
            "Dong-Kai Yang",
            "Yu-Rong Dai",
            "Shou-Qiang Lai",
            "Yi-Jun Lu",
            "Yu-Fu Yang",
            "Hao-Chung Kuo",
            "Zhong Chen",
            "Ting-Zhu Wu"
        ],
        "published_in": "Published in: IEEE Transactions on Electron Devices ( Early Access )",
        "date_of_publication": "22 October 2024",
        "doi": "10.1109/TED.2024.3478197",
        "publisher": "IEEE",
        "abstract": "Polarization effects introduced by the electron-blocking layer (EBL) in InGaN-based micro light-emitting diodes (\nμ\nLEDs) have a significant impact on their display and detection applications. Therefore, the impact and necessity of EBL structure for\nμ\nLED devices intended for such applications need to be reassessed. This article investigates the effect of removing p-AlGaN EBL structure on the performance of blue\nμ\nLEDs when used as light-emitting devices under low-current injection and photodetectors (PDs). Luminescence efficiency and color stability of the\nμ\nLED without EBL improve under low-current injection due to weakened polarization-induced band-bending and enhanced hole injection and electron confinement, as numerical simulations show. For photodetection applications, removing EBL reduces the photogenerated carrier escape barrier, improving the responsivity and bandwidth of\nμ\nLED PDs. Additionally, anomalous responsivities, dependent on bias and excitation energy, were observed in\nμ\nLED PDs with EBL. These observations can be elucidated by the trap-assisted tunneling (TAT) mechanism.",
        "issn": {
            "Print ISSN": "0018-9383",
            "Electronic ISSN": "1557-9646"
        },
        "keywords": {
            "IEEE Keywords": [
                "Light emitting diodes",
                "Quantum well devices",
                "Lattices",
                "Performance evaluation",
                "Wide band gap semiconductors",
                "Tunneling",
                "Luminescence",
                "Aluminum gallium nitride",
                "Energy barrier",
                "Electrons"
            ],
            "Author Keywords": [
                "Electron blocking layer",
                "low current density",
                "micro light-emitting diodes ( $\\mu$ LEDs)",
                "photodetectors (PDs)"
            ]
        },
        "title": "Investigations on Electro-Optical and Photoelectric Detection Performance of GaN-Based micro-LEDs by Removing p-AlGaN Electron-Blocking Layer"
    },
    {
        "authors": [
            "Changxin Shi",
            "Ying Cui",
            "Feng Yang",
            "Lianghui Ding"
        ],
        "published_in": "Published in: IEEE Transactions on Communications ( Early Access )",
        "date_of_publication": "19 July 2024",
        "doi": "10.1109/TCOMM.2024.3430969",
        "publisher": "IEEE",
        "abstract": "The costs of channel estimation, reflection adjustment, and computation have severe impacts on intelligent reflection surface (IRS)-aided physical layer security (PLS) wireless communication systems in practice but are usually overlooked for simplicity in most existing works. This paper considers a multi-antenna base station serving a single-antenna legitimate user with the assistance of a multi-element IRS under the surveillance of a single-antenna eavesdropper. Firstly, we introduce a partial instantaneous CSI and statistical CSI (PICSI-SCSI)-adaptive beamforming and SCSI-adaptive reflection design. Secondly, we maximize the achievable ergodic secrecy rate (ESR) with respect to the PICSI-SCSI-adaptive beamforming and SCSI-adaptive reflection design, resulting in a two-timescale stochastic non-convex problem. Thirdly, we present two stochastic iterative algorithms to reach stationary and approximate stationary points. Moreover, we show that the two proposed designs achieve lower computational complexities and adjustment costs for reflection than the existing PICSI-SCSI-adaptive beamforming and reflection design. Lastly, we numerically demonstrate the two proposed designs’ notable gains over baselines. To our knowledge, this is the first work providing an optimization-based PICSI-SCSI-adaptive beamforming and SCSI-adaptive reflection design in an IRS-aided PLS wireless communication system, achieving promising secure performance at the minimum adjustment cost for reflection.",
        "issn": {
            "Print ISSN": "0090-6778",
            "Electronic ISSN": "1558-0857"
        },
        "keywords": {
            "IEEE Keywords": [
                "Reflection",
                "Array signal processing",
                "Wireless communication",
                "Costs",
                "Fading channels",
                "Computational complexity",
                "Reflection coefficient"
            ],
            "Author Keywords": [
                "Intelligent reflection surface",
                "physical layer security",
                "beamforming",
                "reflection",
                "optimization"
            ]
        },
        "title": "Optimization of PICSI & SCSI-adaptive Beamforming and SCSI-adaptive Reflection in an IRS-aided PLS Wireless Communication System"
    },
    {
        "authors": [
            "Yikun Li",
            "Ying Liu",
            "Yu Xia",
            "Weiting Zhang",
            "Wei Quan",
            "Jiawen Kang",
            "Hongke Zhang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "01 October 2024",
        "doi": "10.1109/JIOT.2024.3471615",
        "publisher": "IEEE",
        "abstract": "In the current era of rapid advancements in Artificial Intelligence of Things (AIoT), with the increase in cloud data center operations and the limited security computing capabilities of AIoT terminal devices, Link Flooding Attack (LFA) has emerged as a complex and stealthy new threat. However, the existing defense methods based on programmable networks usually have issues of slow offline inference and delayed defense activation. To address these issues, we propose a collaborative programmable defense framework (CPDTG) to predict, detect, and mitigate LFA. First, an early attack intention prediction model based on temporal graph learning (TGL) is proposed to accurately locate attacks and promptly activate defenses to save resource consumption during idle time. Second, a switch-native clustering algorithm independent of the global perspective is introduced for line-speed detection of LFA. The unsupervised algorithm does not rely on labeled datasets for training, which enhances its robustness against differentiated attack scenarios. Third, we propose a distributed defense mechanism that achieves the pushback deployment of adaptive rate-limiting strategies. Compressing the potential attack vector space effectively increases the difficulty of launching rolling attacks. Extensive experimental validation demonstrates the effectiveness of the proposed CPDTG in predicting and defending against LFA.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Switches",
                "Servers",
                "Cloud computing",
                "Internet of Things",
                "Prevention and mitigation",
                "Predictive models",
                "Denial-of-service attack",
                "Computer crime",
                "Collaboration",
                "Clustering algorithms"
            ],
            "Author Keywords": [
                "Link flooding attack",
                "attack intention prediction",
                "temporal graph learning",
                "programmable data plane"
            ]
        },
        "title": "A Collaborative Programmable LFA Defense Using Temporal Graph Learning in AIoT"
    },
    {
        "authors": [
            "Amritesh Kumar",
            "Lokendra Vishwakarma",
            "Debasis Das"
        ],
        "published_in": "Published in: IEEE Transactions on Reliability ( Early Access )",
        "date_of_publication": "11 April 2024",
        "doi": "10.1109/TR.2024.3382490",
        "publisher": "IEEE",
        "abstract": "The process of transferring land records (LRs) and ownership between users is facilitated by a LR transfer system. This system encompasses a series of procedures, including conducting title search, establishing agreements, executing legal documentation, verifying and transferring ownership, and updating LRs. Despite its importance, the system encounters notable challenges, such as insufficient tamper-proof record-keeping, lack of system compatibility, time-consuming processes, and the presence of intermediaries and brokers leading to potentially fraudulent claims. To address these challenges, a novel solution called LandChain is proposed in this article. The LandChain utilizes MultiChain, consisting of MainChain and SideChain, to securely transfer LRs among users, such as buyers, sellers, land donors, and owners. The LandChain incorporates innovative algorithms like record forwarder selection (RFS), trust establishment (TE), and record transfer and confirmation (RTC). Furthermore, LandChain verifies the legitimacy of users before transferring LRs through the verify user legitimacy algorithm. Security analysis shows LandChain is secure from double-spending, liveness, Sybil, replay, and man-in-the-middle attacks. The implementation of LandChain is developed and tested on the docker engine platform. According to performance analysis, the LandChain reduces record confirmation latency by 18% (MultiChain) and 50% (Blockchain). LandChain also increases throughput by 34% (MultiChain) and 45% (Blockchain) when compared to state-of-the-art approaches.",
        "issn": {
            "Print ISSN": "0018-9529",
            "Electronic ISSN": "1558-1721"
        },
        "keywords": {
            "IEEE Keywords": [
                "Blockchains",
                "Protocols",
                "Security",
                "Radio frequency",
                "Bitcoin",
                "Throughput",
                "Scalability"
            ],
            "Author Keywords": [
                "Blockchain",
                "MultiChain",
                "record confirmation",
                "record forwarder selection (RFS)",
                "record transfer"
            ]
        },
        "title": "LandChain: A MultiChain Based Novel Secure Land Record Transfer System"
    },
    {
        "authors": [
            "Peng Hou",
            "Hongbin Zhu",
            "Zhihui Lu",
            "Shin-Chia Huang",
            "Yang Yang",
            "Hongfeng Chai"
        ],
        "published_in": "Published in: IEEE Transactions on Green Communications and Networking ( Early Access )",
        "date_of_publication": "05 November 2024",
        "doi": "10.1109/TGCN.2024.3492028",
        "publisher": "IEEE",
        "abstract": "The Over-the-air Integrated Sensing, Communication, and Computation (Air-ISCC), supported by Unmanned Aerial Vehicles (UAVs), is a key technology for future 6G wireless networks. Air-ISCC can facilitate the mutual gain of communication, sensing, and computation functions. Equipping UAVs with sensing and communication units and computation resources empowers them to sense network environments and incorporate sensing information to provide computation offloading and mobile computing services. To optimize sensing, communication, and computation performance jointly, we present a multi-objective optimization framework in this paper. This framework jointly optimizes time slot scheduling, power control, resource allocation, and service association to maximize the service success of Air-ISCC while minimizing the energy consumption of UAVs. We transform the Air-ISCC problem into a sequential decision-making problem and propose a Proximity policy optimization-Based Intelligent Air-ISCC algorithm (PBIA) based on deep reinforcement learning. Leveraging the parallelization capability of the PBIA algorithm, we further propose training intelligent agents based on parallel deep reinforcement learning to realize autonomous decision-making of UAV swarm. Experimental results show that PBIA can learn effective policies with high learning efficiency and stability. Compared to baselines, PBIA significantly enhances the service success rate from 16.32% to 61.44%.",
        "issn": {
            "Electronic ISSN": "2473-2400"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Autonomous aerial vehicles",
                "Resource management",
                "Energy consumption",
                "Radar",
                "Decision making",
                "Atmospheric modeling",
                "Power control",
                "Integrated sensing and communication",
                "Heuristic algorithms"
            ],
            "Author Keywords": [
                "Integrated sensing and communications",
                "unmanned aerial vehicles",
                "reinforcement learning",
                "intelligent transportation systems",
                "multi-access edge computing"
            ]
        },
        "title": "Learning-Based Over-the-Air Integrated Sensing, Communication and Computation in UAV Swarm-Enabled Intelligent Transportation Systems"
    },
    {
        "authors": [
            "Denghui Liu",
            "Ruoyang Chen",
            "Changyan Yi",
            "Tong Zhang",
            "Xiaolong Li",
            "Juan Li",
            "Ran Wang",
            "Kun Zhu"
        ],
        "published_in": "Published in: IEEE Transactions on Vehicular Technology ( Early Access )",
        "date_of_publication": "11 September 2024",
        "doi": "10.1109/TVT.2024.3457777",
        "publisher": "IEEE",
        "abstract": "This paper investigates a dynamic game framework for physical layer security (PLS)-aware wireless network with third-party collaborative users (TCUs). In the considered system, legitimate users (LUs) aim to transmit secret signals under the threat of eavesdroppers (EVs). There exists TCUs rationally selecting to form coalitions with LUs or EVs, strategically enhancing the secrecy transmission rates for LUs or increasing eavesdropping rates for EVs to obtain rewards. Due to the unpredictability of wireless systems (e.g., time-varying channel conditions, user mobility), coalitions among the three parties may be unfixed and dynamically changing. A hierarchical game integrating a matching subgame with a coalition formation subgame is formulated to model the interactions among LUs, EVs, and TCUs. In order to derive the optimal long-term strategies while solving the equilibrium of the hierarchical game, we first introduce a constrained Gale-Shapley algorithm which determines the optimal power, mode, and channel selections of TCUs with the known coalition partition, and then propose a coalition selection algorithm for reaching the stability of coalition partitions. Moreover, we develop a deep reinforcement learning (DRL)-based solution to concatenate the aforementioned algorithms, producing the long-term optimal strategies for LUs, EVs and TCUs over multiple time slots. Simulations evaluate the performance of the proposed scheme, and demonstrate its superiority over counterparts.",
        "issn": {
            "Print ISSN": "0018-9545",
            "Electronic ISSN": "1939-9359"
        },
        "keywords": {
            "IEEE Keywords": [
                "Games",
                "Wireless communication",
                "Communication system security",
                "Jamming",
                "Collaboration",
                "Relays",
                "Security"
            ],
            "Author Keywords": [
                "Physical layer security",
                "cooperative communication",
                "hierarchical game",
                "dynamic tripartite coalitions",
                "matching",
                "deep reinforcement learning"
            ]
        },
        "title": "A Hierarchical Game for Physical Layer Security Aware Cooperative Communications with Dynamic Interchangeable Relaying and Jamming"
    },
    {
        "authors": [
            "Yunzhi Zhao",
            "Yanhua Pei",
            "Yong Liu",
            "Fen Hou",
            "Weihua Zhuang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "25 July 2024",
        "doi": "10.1109/JIOT.2024.3433558",
        "publisher": "IEEE",
        "abstract": "We consider a mobile edge computing (MEC) assisted Industrial Internet of Things (IIoT) network, where multiple assembly processing lines in a smart factory are equipped with sensing devices. They sense raw products, generate and offload computing tasks, and finally process the raw products based on the computing results. In this scenario, different positions of the processing machines lead to different priorities and diverse Quality-of-Service (QoS) requirements of tasks. Therefore, how to schedule tasks and allocate the network resources becomes a critical and challenging issue. In this study, we introduce a novel batch-based hybrid non-orthogonal multiple access (NOMA)/orthogonal multiple access (OMA) transmission scheme. The selection between NOMA and OMA schemes is optimized based on the QoS requirements of tasks. Then, we formulate a production efficiency maximization problem with the objective of maximizing the speed of the assembly lines subject to the deadline constraints of offloading and computing procedures. To this end, a two-layer decomposition method is used to decompose the formulated problem into two sub-problems. Furthermore, we utilize a bisection searching method to approximate the optimal solution, and propose an efficient method to determine the feasibility of the top-layer sub-problem. Simulation results demonstrate the significant performance improvement of our proposed method. In specific, the production efficiency is enhanced by 525% in comparison with pure NOMA scheme.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Task analysis",
                "NOMA",
                "Production",
                "Resource management",
                "Industrial Internet of Things",
                "Quality of service",
                "Delays"
            ],
            "Author Keywords": [
                "Industrial Internet of Things",
                "mobile edge computing",
                "delay requirement",
                "hybrid transmission",
                "task priority"
            ]
        },
        "title": "Hybrid NOMA-OMA Transmission Scheduling for Production Efficiency Maximization in Industrial Edge Computing Networks"
    },
    {
        "authors": [
            "Asif Siddiqui",
            "Bhaskar P. Rimal",
            "Martin Reisslein",
            "Yong Wang"
        ],
        "published_in": "Published in: IEEE Communications Surveys & Tutorials ( Early Access )",
        "date_of_publication": "28 March 2024",
        "doi": "10.1109/COMST.2024.3382470",
        "publisher": "IEEE",
        "abstract": "Home networks increasingly support important networked applications with limited professional network administration support, while sophisticated attacks pose enormous security risks for networked applications. A Unified Threat Management (UTM) system strives to comprehensively protect a home network by providing firewall, intrusion detection and prevention, as well as antibot protection in an integrated, easy-to-configure manner. Previous surveys have extensively covered the individual components of a UTM system, i.e., there is extensive literature on firewall surveys, intrusion detection and prevention surveys, and antibot protection surveys. Importantly, the previous surveys covered these protection services separately, without considering their integration (however, this integration is critical for comprehensive home network protection). In contrast, the present survey covers for the first time UTM systems, i.e., the integrated network security services provided by a UTM system. This UTM survey is organized according to the UTM components, i.e., we comprehensively survey the firewall methods, the intrusion detection and prevention methods, as well as the antibot protection methods that are suitable for a UTM system for a home network. Throughout, we view these methods from the perspective of integration into a UTM system with limited computational resources and limited network administration support. Our survey includes the protection capabilities, as well as the design and deployment aspects and software/hardware limitations of available off-the-shelf and open-source UTM systems. We find that effective integrated home network protection where the UTM system components synergistically support each other while operating with limited computational resources and network administration support still requires extensive future research and development.",
        "issn": {
            "Electronic ISSN": "1553-877X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Home automation",
                "Surveys",
                "Firewalls (computing)",
                "Security",
                "Internet of Things",
                "Inspection",
                "Payloads"
            ],
            "Author Keywords": [
                "Authentication",
                "firewall",
                "home network",
                "Internet of Things (IoT)",
                "intrusion detection",
                "privacy",
                "proxy",
                "security",
                "Unified Threat Management (UTM)",
                "vulnerabilities"
            ]
        },
        "title": "Survey on Unified Threat Management (UTM) Systems for Home Networks"
    },
    {
        "authors": [
            "Lei Wang",
            "Rongfei Fan",
            "Han Hu",
            "Gongpu Wang",
            "Julian Cheng"
        ],
        "published_in": "Published in: IEEE Transactions on Communications ( Early Access )",
        "date_of_publication": "17 June 2024",
        "doi": "10.1109/TCOMM.2024.3415608",
        "publisher": "IEEE",
        "abstract": "This paper investigates how to suppress the Age of Information (AoI) in an opportunistic channel access system, which allows multiple mobile devices to access a base station without central coordination while being aware of instant channel quality. An optimization problem is formulated to minimize the average AoI by optimizing each mobile device’s probability of contending for channel access opportunity and the threshold of offload rate. We derive the exact expression of the average AoI and generate a reformulated optimization problem. Although being non-convex, the reformulated problem is tackled by the following operations. First, we leverage the Dinkelbach method and the block coordinate descent method to convert the reformulated problem into an iterative solving procedure of two non-convex sub-problems, which optimize the contending probability and the threshold of offload rate respectively. Second, for each non-convex sub-problem, we explore the piecewise differential monotonicity for the cost function, and achieve the associated optimal solution by transforming them into standard monotonic optimization problems. Numerical results can verify the effectiveness of the proposed method through the comparison with benchmark methods.",
        "issn": {
            "Print ISSN": "0090-6778",
            "Electronic ISSN": "1558-0857"
        },
        "keywords": {
            "IEEE Keywords": [
                "Mobile handsets",
                "Internet of Things",
                "Wireless communication",
                "Wireless sensor networks",
                "Media Access Protocol",
                "Minimization",
                "Cost function"
            ],
            "Author Keywords": [
                "Age of Information (AoI)",
                "Internet of Things (IoT)",
                "opportunistic channel access"
            ]
        },
        "title": "Age of Information Minimization for Opportunistic Channel Access"
    },
    {
        "authors": [
            "Biswajit Paul",
            "Chadi Assi",
            "Georges Kaddoum"
        ],
        "published_in": "Published in: IEEE Transactions on Green Communications and Networking ( Early Access )",
        "date_of_publication": "08 April 2024",
        "doi": "10.1109/TGCN.2024.3385707",
        "publisher": "IEEE",
        "abstract": "The goal of this research is to offer recommendations on how to deploy a network more effectively by making better use of LoRaWAN features. The goals of the existing research are as follows: (i) analyzing or improving the network lifespan, delay, capacity, and interference issues, as well as (ii) the creation of network models and performance comparisons for single-hop and multi-hop routings. The authors used either analytical, simulation, or experimental framework for the evaluation of the network performance. The dilemma of how to pick/build a network model that best fits those requirements arises since each application in reality has a different set of requirements, such as lifetime, delay, coverage, connection, etc. In this research, we want to steer network designers away from selecting an available alternative that might only satisfy a portion of application needs and toward analyzing, selecting, or building a suitable network model by utilizing our proposed framework that best satisfies the application requirements. We also provide simulation data to show how the options can alter network performance. Although we limit our discussion to network lifetime, the framework provided in this paper can be easily extended following a similar methodology to incorporate any other performance concerns.",
        "issn": {
            "Electronic ISSN": "2473-2400"
        },
        "keywords": {
            "IEEE Keywords": [
                "Planning",
                "Low-power wide area networks",
                "Internet of Things",
                "Energy efficiency",
                "Costs",
                "Wireless sensor networks",
                "Spread spectrum communication"
            ],
            "Author Keywords": [
                "IoT",
                "LoRaWAN",
                "network performance",
                "routing",
                "transmission configuration"
            ]
        },
        "title": "LoRaWAN Network Planning"
    },
    {
        "authors": [
            "Mohamed Mohamadi",
            "Quentin Lampin",
            "Marion Dumay"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "29 July 2024",
        "doi": "10.1109/JIOT.2024.3435395",
        "publisher": "IEEE",
        "abstract": "The Industrial Internet of Things (IIoT), integral to Industry 4.0, leverages Wireless Low-power and Lossy Networks (LLNs) for critical applications. These networks fulfill demanding industrial requirements, ensuring limited end-to-end latency and high reliability. The Internet Engineering Task Force (IETF) has developed the 6TiSCH protocol stack, a network architecture comprising cutting-edge standards to support this IIoT paradigm. Particularly, the 6TiSCH architecture introduces id=R1“tracks”’tracks’ to deliver deterministic Quality of Service (QoS) across diverse networks, aligning with stringent Service Level Agreements (SLAs). Furthermore, the Reliable and Available Wireless (RAW) work group, now part of the Deterministic Networking (DetNet) group, enhances this concept for wireless-specific challenges. The RAW framework effectively balances redundancy against constraints like spectrum and battery usage. It incorporates reliability mechanisms, including tracks and specialized control routines. This paper focuses on applying id=R1“tracksid=R1” within deterministic 6TiSCH networks, in line with IETF standardsid=R1, to demonstrate the ability of tracks to tend towards deterministic id=R1performanceperformace in wireless mesh networks. id=R1We provide a new remote network management interface, evolved from current 6TiSCH protocols, particularly 6top and CoAP, designed to install and maintain tracks in 6TiSCH networks. id=R1We aim to extend current 6TiSCH protocols, particularly 6top and CoAP, to establish a remote monitoring system for track processing and lifecycle management. Our approach, implemented in the OpenWSN open-source protocol stack, is id=R1evaluatedtested on id=R1real OpenMote-B hardware for two real-life use cases. id=R1The performance evaluation shows drastic improvement in latency (up to 4.5 times).",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Reliability",
                "Schedules",
                "Protocols",
                "Industrial Internet of Things",
                "Wireless communication",
                "Quality of service",
                "IEEE 802.15 Standard"
            ],
            "Author Keywords": [
                "6TiSCH",
                "IoT",
                "IIoT",
                "IEEE 802.15.4",
                "TSCH",
                "CoAP",
                "Tracks",
                "DetNet",
                "RAW"
            ]
        },
        "title": "CoAP-Based Remote Network Management Model for Deterministic 6TiSCH Networks"
    },
    {
        "authors": [
            "Hongxin Wu",
            "Qinghou Zeng",
            "Chen Guo",
            "Tiesong Zhao",
            "Chang Wen Chen"
        ],
        "published_in": "Published in: IEEE Transactions on Circuits and Systems for Video Technology ( Early Access )",
        "date_of_publication": "16 August 2024",
        "doi": "10.1109/TCSVT.2024.3445151",
        "publisher": "IEEE",
        "abstract": "In large-scale surveillance of urban or rural areas, an effective placement of cameras is critical in maximizing surveillance coverage or minimizing economic cost of cameras. Existing Surveillance Camera Placement (SCP) methods generally focus on physical coverage of surveillance by implicitly assuming uniform distribution of interested targets or objects across all blocks, which is, however, uncommon in real-world scenarios. In this paper, we are the first to propose a target-aware SCP (tSCP) model, which prioritizes optimizing the task based on uneven target densities, allowing cameras to preferentially cover blocks with more interested targets. First, we define target density as the likelihood of interested targets occurring in a block, which is positively correlated with the importance of the block. Second, we combine aerial imagery with a lightweight object detection network to identify target density. Third, we formulate tSCP as an optimization problem to maximize target coverage in surveillance area, and solve this problem with a target-guided genetic algorithm. Our method optimizes the rational and economical utilization of cameras in large-scale video survillance. Compared with the state-of-the-art methods, our tSCP achieves the highest target coverage with a fixed number of cameras (8.31%-14.81% more than its peers), or utilizes the minimum number of cameras to achieve a preset target coverage. Codes are available at https://github.com/wu-hongxin/tSCP_main.",
        "issn": {
            "Print ISSN": "1051-8215",
            "Electronic ISSN": "1558-2205"
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Optimization",
                "Video surveillance",
                "Task analysis",
                "Robot vision systems",
                "Visualization",
                "Three-dimensional displays"
            ],
            "Author Keywords": [
                "Surveillance Camera Placement (SCP)",
                "large-scale video surveillance",
                "Internet of Things (IoT)",
                "smart city"
            ]
        },
        "title": "Target-Aware Camera Placement for Large-Scale Video Surveillance"
    },
    {
        "authors": [
            "Javier Saez-Perez",
            "Pablo Benlloch-Caballero",
            "David Tena-Gago",
            "Jose Garcia-Rodriguez",
            "Jose Maria Alcaraz Calero",
            "Qi Wang"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/ACCESS.2024.3491306",
        "publisher": "IEEE",
        "abstract": "With the increasing prominence of self-driving vehicles, there has been a pressing need to accurately estimate their carbon dioxide (CO 2 ) emissions and evaluate their environmental sustainability. This paper has introduced a novel approach that leverages Artificial Intelligence (AI) transformer architectures to predict CO 2 emissions in Society of Automotive Engineers (SAE) Level 2 self-driving cars, surpassing the performance of previous algorithms. After examining and comparing the use of previously proposed LSTM-based and the proposed transformer architecture (CO 2 ViT), and identifying their strengths and limitations, we have explored the vehicular networking paradigm with the Mobile/Multi-Access Edge Computing (MEC) capabilities of 5G infrastructure to provide the prediction service of the proposed transformer model under different networking topologies. Through extensive experimentation and evaluation on a dataset specifically designed for CO 2 emissions prediction in self-driving vehicles, we have demonstrated the superior predictive capabilities of our proposed CO 2 ViT model based on the Visual Transformer architecture, achieving a model that predicts the CO 2 emissions 71.14% faster than the previous state-of-the-art model (LSTM) applied to the same problem and that achieves an R 2 higher score of 0.9898 against the one achieved by the LSTM (0.9712). Furthermore, we have deployed a 5G emulation testbed with MEC capabilities to demonstrate the proposed Deep Learning (DL) resilience of the model to changes and concurrent connections. While delays for 2 to 16 connected vehicles have grown linearly with a maximum delay value of 41.01 ms, resource limitations have arisen with 32 or more cars due to varied delays, necessitating additional physical resources for the emulated 5G network to achieve better performance under high stress. The deployed models’ inference time over the 5G infrastructure for 64 concurrent connected vehicles in scenarios A and B has ...",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Predictive models",
                "Transportation",
                "Transformers",
                "Computer architecture",
                "5G mobile communication",
                "Biological system modeling",
                "Long short term memory",
                "Automobiles",
                "Deep learning",
                "Data models"
            ],
            "Author Keywords": [
                "Environmental sustainability",
                "IoT",
                "Deep Learning",
                "Transformers"
            ]
        },
        "title": "Optimizing AI Transformer Models for CO2 Emission Prediction in Self-Driving Vehicles with Mobile/Multi-Access Edge Computing Support"
    },
    {
        "authors": [
            "Carlos Iván del Valle Morales",
            "Juan Carlos Torres Zafra",
            "Máximo Morales Céspedes",
            "Iñaki Martinez-Sarriegui",
            "José Manuel Sánchez-Pena"
        ],
        "published_in": "Published in: IEEE Transactions on Industrial Informatics ( Early Access )",
        "date_of_publication": "08 October 2024",
        "doi": "10.1109/TII.2024.3468449",
        "publisher": "IEEE",
        "abstract": "The growing number of connected wireless devices is causing interference in traditional radio frequency bands, which restricts communications capacity and efficiency. This issue motivates the pursuit of alternative technologies, such as optical wireless communication, which could potentially resolve electromagnetic congestion. This technology leverages the visible and infrared spectrum, along with other wavelengths, to transmit data. Recently, photovoltaic (PV) modules have emerged as an innovative solution for data reception and enhancing self-sufficiency. However, within a range of industrial settings, including factories, warehouses, and offices, data transmission may be affected by fluctuations in lighting conditions. This work focuses on exploring the impact of lighting conditions on the frequency response of visible light communication (VLC) systems that utilize silicon PV cells as photodetectors, a crucial aspect to optimize the efficiency of data transmission. This analysis opens new possibilities for the effective deployment of PV cells as optical receivers in indoor settings, thereby significantly enhancing the usage of VLC technology in industrial applications.",
        "issn": {
            "Print ISSN": "1551-3203",
            "Electronic ISSN": "1941-0050"
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Resistance",
                "Bandwidth",
                "Impedance",
                "Frequency response",
                "Photovoltaic cells",
                "Cutoff frequency",
                "Visible light communication",
                "Silicon",
                "Wireless communication"
            ],
            "Author Keywords": [
                "Energy harvesting",
                "frequency response",
                "indoor applications",
                "Internet-of-Things (IoT)",
                "optical wireless communications (OWC)",
                "photovoltaic (PV) technology",
                "solar cells",
                "visible light communication (VLC)"
            ]
        },
        "title": "Exploring Bandwidth Capabilities of Solar Cells for VLC Applications"
    },
    {
        "authors": [
            "Savio Sciancalepore"
        ],
        "published_in": "Published in: IEEE Network ( Early Access )",
        "date_of_publication": "23 July 2024",
        "doi": "10.1109/MNET.2024.3432730",
        "publisher": "IEEE",
        "abstract": "While drone-based civilian services and applications are appearing on the market at a high pace, recent efforts in the security and privacy community mainly focused on drone detection and neutralization when unauthorized invasions occur. Conversely, more attention must be paid to unveiling potential privacy and confidentiality threats to drone users and operators arising from using such a technology. Such threats, emerging from drones’ adoption for entertainment and business operations, are increasingly concerning due to the recently-introduced regulation on the Remote Identification of Unmanned Aircraft (Remote ID (RID)), mandating persistent disclosure of identity and location of the drone at run-time. This paper sheds some light on the aforementioned context, identifying several privacy and confidentiality threats connected to regular drone operations. Such threats originate from the nature of the drones’ ecosystem and actors and are magnified by the adoption of the RID regulation. For all the identified threats, we pinpoint similarities with issues faced in other research domains, potential solutions, and constraints owing to the drone technology, making the solutions conceived therein hardly applicable for drone-based services. The final result is a set of appealing research challenges, calling for joint efforts from Academia and industry.",
        "issn": {
            "Print ISSN": "0890-8044",
            "Electronic ISSN": "1558-156X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Drones",
                "Privacy",
                "Regulation",
                "Security",
                "Internet",
                "Wireless fidelity",
                "Aircraft"
            ],
            "Author Keywords": [
                "Unmanned Aircraft",
                "IoT",
                "Privacy Enhancing Technologies",
                "Security"
            ]
        },
        "title": "Privacy and Confidentiality Issues in Drone Operations: Challenges and Road Ahead"
    },
    {
        "authors": [
            "Dmitriy Rivkin",
            "Francois Hogan",
            "Amal Feriani",
            "Abhisek Konar",
            "Adam Sigal",
            "Xue Liu",
            "Gregory Dudek"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "23 October 2024",
        "doi": "10.1109/JIOT.2024.3471904",
        "publisher": "IEEE",
        "abstract": "The common-sense reasoning abilities and vast general knowledge of Large Language Models (LLMs) make them a natural fit for interpreting user requests in a smart home assistant context. LLMs, however, lack specific knowledge about the user and their home, which limits their potential impact. SAGE (Smart Home Agent with Grounded Execution), overcomes these and other limitations by using a scheme in which a user request triggers an LLM-controlled sequence of discrete actions. These actions can be used to retrieve information, interact with the user, or manipulate device states. SAGE controls this process through a dynamically constructed tree of LLM prompts, which help it decide which action to take next, whether an action was successful, and when to terminate the process. The SAGE action set augments an LLM’s capabilities to support some of the most critical requirements for a smart home assistant. These include: flexible and scalable user preference management (“Is my team playing tonight?”), access to any smart device’s full functionality without device-specific code via API reading (“Turn down the screen brightness on my dryer”), persistent device state monitoring (“Remind me to throw out the milk when I open the fridge”), natural device references using only a photo of the room (“Turn on the lamp on the dresser”), and more. We introduce a benchmark of 50 new and challenging smart home tasks where SAGE achieves a 76% success rate, significantly outperforming existing LLM-enabled baselines (30% success rate).",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Smart homes",
                "Codes",
                "Performance evaluation",
                "Internet of Things",
                "Benchmark testing",
                "Smart devices",
                "Logic",
                "Process control",
                "Monitoring",
                "Manuals"
            ],
            "Author Keywords": [
                "Autonomous LLM Agents",
                "Smart Home",
                "IoT",
                "Generative AI",
                "Embodied AI",
                "Personalized AI",
                "AI Assistant"
            ]
        },
        "title": "AIoT Smart Home via Autonomous LLM Agents"
    },
    {
        "authors": [
            "Kharol Chicaiza",
            "Ricardo Paredes",
            "Isaac Mateo Sarzosa",
            "Sang Guun Yoo",
            "Naeun Zang"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "28 October 2024",
        "doi": "10.1109/ACCESS.2024.3487497",
        "publisher": "IEEE",
        "abstract": "This paper presents a comprehensive examination of smart farming solutions through a systematic review of literature available in various digital repositories. Methodologically, we categorize the devices and technologies utilized in these solutions into sensors, actuators, gateways, power supplies, networking, data storage, data processing, and information delivery. Through this analysis, we identify the most commonly employed devices and technologies in smart farming solutions and discuss their utilization within the proposed categories. By synthesizing the gathered information, we offer insights into the current landscape of smart farming, accompanied by recommendations for the selection of devices and technologies tailored to each category. This research contributes to the understanding of smart farming technology and aids stakeholders in making informed decisions regarding the implementation of such solutions.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Smart agriculture",
                "Internet of Things",
                "Sensors",
                "Intelligent sensors",
                "Wireless sensor networks",
                "Protocols",
                "Metadata",
                "Real-time systems",
                "Libraries",
                "Farming"
            ],
            "Author Keywords": [
                "Smart Farming",
                "Internet of Things",
                "IoT",
                "LoRaWAN",
                "Network",
                "Sensors",
                "WiFi",
                "Wireless communication"
            ]
        },
        "title": "Smart Farming Technologies: A Methodological Overview and Analysis"
    },
    {
        "authors": [
            "Sergio Trilles",
            "Aida Monfort-Muriach",
            "Enrique Cueto-Rubio",
            "Carmen López-Girona",
            "Carlos Granell"
        ],
        "published_in": "Published in: IEEE Transactions on Education ( Early Access )",
        "date_of_publication": "29 July 2024",
        "doi": "10.1109/TE.2024.3422666",
        "publisher": "IEEE",
        "abstract": "This article discusses the latest developments of the Sucre4Stem tool, as part of the Sucre initiative, which aims to promote interest in computational thinking and programming skills in K-12 students. The tool follows the Internet of Things approach and consists of two prominent components: 1) SucreCore and 2) SucreCode . SucreCore incorporates an advanced microcontroller packaged in a more compact design and enables wireless connectivity. SucreCode , the block-based visual programming tool, supports two different sets of blocks depending on the education grade, and facilitates wireless communication with SucreCore . At the educational level, Sucre4Stem fosters new group dynamics and encourages students to experiment real-world projects by promoting the “programming to learn” approach to concepts from other disciplines as opposed to the strategy widely applied in schools of “learning to program” in isolation.",
        "issn": {
            "Print ISSN": "0018-9359",
            "Electronic ISSN": "1557-9638"
        },
        "keywords": {
            "IEEE Keywords": [
                "Programming profession",
                "Education",
                "Sensors",
                "Visualization",
                "Internet of Things",
                "Actuators",
                "STEM"
            ],
            "Author Keywords": [
                "Computational thinking",
                "Internet of Things (IoT)",
                "K-12",
                "multidisciplinary disciplines",
                "programming promotion"
            ]
        },
        "title": "Sucre4Stem: A K-12 Educational Tool for Integrating Computational Thinking and Programming Across Multidisciplinary Disciplines"
    },
    {
        "authors": [
            "Marco Arazzi",
            "Antonino Nocera",
            "Emanuele Storti"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "02 September 2024",
        "doi": "10.1109/JIOT.2024.3452945",
        "publisher": "IEEE",
        "abstract": "Recently, the Industry 5.0 is gaining attention as a novel paradigm, defining the next concrete steps toward more and more intelligent, green-aware and user-centric digital systems. In an era in which smart devices typically adopted in the industry domain are more and more sophisticated and autonomous, the Internet of Things and its evolution, known as the Internet of Everything (IoE, for short), involving also people, robots, processes and data in the network, represent the main driver to allow industries to put the experiences and needs of human beings at the center of their ecosystems. However, due to the extreme heterogeneity of the involved entities, their intrinsic need and capability to cooperate, and the aim to adapt to a dynamic user-centric context, special attention is required for the integration and processing of the data produced by such an IoE. This is the objective of the present paper, in which we propose a novel semantic model that formalizes the fundamental actors, elements and information of an IoE, along with their relationships. In our design, we focus on state-of-the-art design principles, in particular reuse, and abstraction, to build “SemIoE”, a lightweight ontology inheriting and extending concepts from well-known and consolidated reference ontologies. The defined semantic layer represents a core data model that can be extended to embrace any modern industrial scenario. It represents the base of an IoE Knowledge Graph, on top of which, as an additional contribution, we analyze and define some essential services for an IoE-based industry.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Ontologies",
                "Internet of Things",
                "Semantics",
                "Industries",
                "Fifth Industrial Revolution",
                "Smart devices",
                "Organizations"
            ],
            "Author Keywords": [
                "Industry 5.0",
                "Internet-of-Everything",
                "IoT",
                "ontology",
                "Knowledge Graph"
            ]
        },
        "title": "The SemIoE Ontology: A Semantic Model Solution for an IoE-Based Industry"
    },
    {
        "authors": [
            "Massimo Vatalaro",
            "Raffaele De Rose",
            "Vincenzo Maccaronio",
            "Marco Lanuzza",
            "Felice Crupi"
        ],
        "published_in": "Published in: IEEE Transactions on Circuits and Systems I: Regular Papers ( Early Access )",
        "date_of_publication": "29 July 2024",
        "doi": "10.1109/TCSI.2024.3432173",
        "publisher": "IEEE",
        "abstract": "This paper explores a class of highly stable static monostable physically unclonable functions (PUFs) based on stacked sub-threshold voltage dividers between two nominally identical sub-circuits as bitcell core block. More specifically, compared to our previous works where two-transistor (2T) and four-transistor (4T) voltage divider based PUFs were presented and analyzed, here we propose two novel topological variants based on six-transistor (6T) and eight-transistor (8T) solutions which arise from adopting a proper reverse gate-biasing strategy within the stack with the aim of improving the resilience to on-chip noise and voltage variations, while keeping the area overhead low. These novel solutions, along with those already proposed, were tested in 180-nm CMOS technology. Raw measurements show a nominal (at 1.8 V and 25 $^{\\circ}$ C) bit error rate (BER) of 0.15% and 0.08% for the 6T-and 8T-based solutions, respectively, along with a BER variation of 0.016% and 0.002% per 0.1 V. With the implementation of a simple masking technique based on measurements at low supply voltage ( $V_{DD}$ $=$ 0.3 V at 25 $^{\\circ}$ C) along with a temporal majority voting (TMV) scheme, a BER of 0.006% and lower than 9.77 $\\times$ $10^{-5}$ %, which is the minimum observable BER for the adopted statistical set, was observed for the 6T-, and 8T-core based implementations, respectively, with a corresponding masking ratio of 8.71% and 7.59%. This is achieved with an area per bit of 5,174 $F^2$ (6T solution) and 6,994 $F^2$ (8T solution).",
        "issn": {
            "Print ISSN": "1549-8328",
            "Electronic ISSN": "1558-0806"
        },
        "keywords": {
            "IEEE Keywords": [
                "Voltage measurement",
                "Transistors",
                "Circuits",
                "Logic gates",
                "Circuit stability",
                "Thermal stability",
                "Bit error rate"
            ],
            "Author Keywords": [
                "Hardware security",
                "IoT",
                "physically unclonable function",
                "voltage divider",
                "CMOS design",
                "masking"
            ]
        },
        "title": "Highly Stable PUFs Based on Stacked Voltage Divider for Near-Zero BER Native Sensitivity to Voltage Variations"
    },
    {
        "authors": [
            "Zhizhan Yang",
            "Haochen Zhang",
            "Jun Yin",
            "Rui P. Martins",
            "Pui-In Mak"
        ],
        "published_in": "Published in: IEEE Transactions on Circuits and Systems I: Regular Papers ( Early Access )",
        "date_of_publication": "26 September 2024",
        "doi": "10.1109/TCSI.2024.3460032",
        "publisher": "IEEE",
        "abstract": "The article introduces an antenna-envelope detector (ED) co-designed wake-up receiver (WuRX) that can automatically detect the frequency-hopping sequence from 840 to 970 MHz. Prototyped in 65nm CMOS, the WuRX supports three modes: 1) low-power mode that achieves\n−\n68 dBm sensitivity with 9.9nW power consumption, 2)\nQ\n-enhanced mode that provides 22 dB rejection to an on-off-keying (OOK) modulated pseudo-random-bit-sequence blocker at 10 MHz offset and 41 dB rejection to a continuous-wave one at 10 MHz offset, with a power consumption of 39.6\nμ\nW and 3) 2-stage wake-up mode that combines the advantages of both the low-power mode and\nQ\n-enhanced mode, with a power consumption of 33.7 nW. These characteristics are achieved by exploiting 1) an antenna-ED interface where the\nQ\n-factor of the antenna is equal to the\nQ\n-factor of the capacitor to ensure a conjugate matching condition for the maximum available power transfer from the antenna to the ED, 2) a frequency tuner connected to the antenna-ED interface calibrated by a frequency-locked loop, and 3) a\nQ\n-booster reconfigured from the frequency tuner for boosting the passive gain and narrowing the bandwidth of the interface.",
        "issn": {
            "Print ISSN": "1549-8328",
            "Electronic ISSN": "1558-0806"
        },
        "keywords": {
            "IEEE Keywords": [
                "Antennas",
                "Resonant frequency",
                "Sensitivity",
                "Tuners",
                "Power demand",
                "Receiving antennas",
                "Radiofrequency identification"
            ],
            "Author Keywords": [
                "Internet of Things (IoT)",
                "frequency-hopping",
                "loop antenna",
                "radio-frequency identification (RFID)",
                "envelope detector",
                "wake-up receiver",
                "ultra-low power",
                "Q-enhanced"
            ]
        },
        "title": "An 840-to-970 MHz Multimodal Wake-Up Receiver With a $Q$ -Equalized Antenna-ED Interface and 2-Dimensional Wake-Up Identification"
    },
    {
        "authors": [
            "Jia-Ming Liang",
            "Shashank Mishra",
            "Chun-Che Wu"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "29 July 2024",
        "doi": "10.1109/JSEN.2024.3422844",
        "publisher": "IEEE",
        "abstract": "For smart cities, video surveillance has been widely used for security and management purposes. In video surveillance, a fundamental challenge is person identification (PID), which involves promptly tagging individuals in videos with their IDs. Using RFID and fingerprint/iris/face recognition is a possible solution. However, the identification results are highly related to environmental factors, such as line of sight, lighting conditions, and distance. Fingerprint/face recognition also has privacy concerns. In this work, we show how to achieve immediate PID through two sensor data sources: (i) human objects and their pixel locations retrieved from videos and (ii) user trajectory data retrieved from wearable devices through indoor localization. By fusing these pixel trajectories and indoor trajectories, we demonstrate an enhancing-vision capability in the sense that PID can be achieved on surveillance videos even when no clear human biological features are seen. Two types of fusion are proposed: (i) similarity-based and (ii) machine learning-based. We have developed lightweight prototyping with off-the-shelf equipment and validated our results through extensive experiments. The performance evaluation showed that our system has an accuracy of up to 92% for person identification.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Wearable devices",
                "Cameras",
                "Accuracy",
                "Trajectory",
                "Radiofrequency identification",
                "Sensors",
                "Location awareness"
            ],
            "Author Keywords": [
                "Internet of Things (IoT)",
                "localization",
                "machine learning",
                "sensor fusion",
                "video surveillance"
            ]
        },
        "title": "Enhancing Person Identification for Smart Cities: Fusion of Video Surveillance and Wearable Device Data based on Machine Learning"
    },
    {
        "authors": [
            "Hamza Kheddar",
            "Diana W. Dawoud",
            "Ali Ismail Awad",
            "Yassine Himeur",
            "Muhammad Khurram Khan"
        ],
        "published_in": "Published in: IEEE Communications Surveys & Tutorials ( Early Access )",
        "date_of_publication": "22 October 2024",
        "doi": "10.1109/COMST.2024.3484491",
        "publisher": "IEEE",
        "abstract": "Modern communication networks have to meet the performance requirements of contemporary industrial control systems (ICSs), which are increasingly being connected to the external Internet. This connectivity exposes them to vulnerabilities that necessitate timely and effective protection measures. The integration of intrusion-detection systems (IDSs) into communication networks serves as a preventive mechanism to defend against malicious threats and hostile activities, ensuring secure operations within the broader industrial infrastructure. This review explores the cutting-edge artificial-intelligence techniques that are employed in the development of IDSs for diverse industrial control networks, emphasizing the application of deep reinforcement learning (DRL) within IDS-based systems across various communication networks. DRL has been successful in solving complex sequential decision-making problems in various domains, including robotics, game playing, and natural-language processing. The review examines a broad scope of publications, and these are categorized into three groups: DRL-only and IDS-only in the introduction and background, and DRL-based IDS papers in the core section of the review. This seeks to provide researchers with an overview of the current state of DRL approaches in IDSs for various network types. Through a meticulous comparative analysis with existing surveys, our review stands out, emphasizing its uniqueness and comprehensiveness. This inclusivity extends beyond traditional boundaries, encompassing a wide array of IDS techniques and environments, ranging from the Internet of Things to ICSs, smart grids, and other domains. Additionally, this review provides useful information such as the datasets used, types of DRL employed, pretrained networks, IDS techniques, evaluation metrics, and improvements gained. Furthermore, the algorithms and methods used in several studies are presented to illustrate the principles of each DRL-based IDS subcategory cl...",
        "issn": {
            "Electronic ISSN": "1553-877X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Protocols",
                "Communication networks",
                "Security",
                "Q-learning",
                "Internet of Things",
                "Reviews",
                "Monitoring",
                "Wireless sensor networks",
                "Surveys",
                "Ultra reliable low latency communication"
            ],
            "Author Keywords": [
                "Intrusion-detection systems",
                "machine learning",
                "deep learning",
                "reinforcement learning",
                "Internet of Things (IoT) security",
                "industrial control systems security"
            ]
        },
        "title": "Reinforcement-Learning-Based Intrusion Detection in Communication Networks: A Review"
    },
    {
        "authors": [
            "Muhammad Faizan Khan",
            "Guojun Wang",
            "Zhigao Zheng",
            "Xiangyong Liu"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "18 June 2024",
        "doi": "10.1109/TCE.2024.3416166",
        "publisher": "IEEE",
        "abstract": "Wi-Fi sensing has recently gained popularity as it leverages channel state information (CSI) for various applications. Many researchers have studied different sensing applications, from activity recognition to Wi-Fi password cracking. However, there are limited studies on the potential use of Wi-Fi signals in collapsed structures, which could boost rescue operations. Wi-Fi signals at higher frequencies have poor interaction with rubble. However, Wi-Fi HaLow at 902MHz has shown promising results. To achieve reliable sensing, it is crucial to have improved coverage that can effectively measure CSI. Thus, in this study, we propose a state-of-the-art Hybrid Wi-Fi HaLow radar mechanism using Frequency Modulated Continous Wave (FMCW), Pulse-Dopplar (PDR), and Ultrawide Band (UWB) radars. We aim to achieve three objectives: earthquake debris assessment, FMCW, PDR, UWB radar techniques adaption at 902 MHz, and Hybrid Wi-Fi HaLow radar fusion. Our primary goal is achieved through meticulous site inspections of earthquake-related areas. Subsequently, we convert conventional FMCW, PDR, and UWB signals to Wi-Fi HaLow for optimal debris identification based on the characteristics of the structural engineering involved. Furthermore, we fuse individual signal components to form a comprehensive hybrid Wi-Fi HaLow radar. Based on simulation outcomes, the proposed methodology surpasses previous research and yields promising results.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Wireless fidelity",
                "Radar",
                "Sensors",
                "Earthquakes",
                "Ultra wideband radar",
                "Doppler radar",
                "Wireless communication"
            ],
            "Author Keywords": [
                "Wi-Fi HaLow",
                "IoT",
                "hybrid radar",
                "coverage",
                "collapsed structure",
                "rescue"
            ]
        },
        "title": "Towards Hybrid Wi-Fi HaLow Radar CSI Coverage Estimation in Collapsed Structures"
    },
    {
        "authors": [
            "Anders E. Kalor",
            "Giuseppe Durisi",
            "Sinem Coleri",
            "Stefan Parkvall",
            "Wei Yu",
            "Andreas Mueller",
            "Petar Popovski"
        ],
        "published_in": "Published in: Proceedings of the IEEE ( Early Access )",
        "date_of_publication": "05 November 2024",
        "doi": "10.1109/JPROC.2024.3484529",
        "publisher": "IEEE",
        "abstract": "Compared to the generations up to 4G, whose main focus was on broadband and coverage aspects, 5G has expanded the scope of wireless cellular systems toward embracing two new types of connectivity: massive machine-type communications (mMTCs) and ultrareliable low-latency communications (URLLCs). This article discusses the possible evolution of these two types of connectivity within the umbrella of 6G wireless systems. This article consists of three parts. The first part deals with the connectivity for a massive number of devices. While mMTC research in 5G predominantly focuses on the problem of uncoordinated access in the uplink for a large number of devices, the traffic patterns in 6G may become more symmetric, leading to closed-loop massive connectivity. One of the drivers for this type of traffic pattern is distributed/decentralized learning and inference. The second part of this article discusses the evolution of wireless connectivity for critical services. While latency and reliability are tightly coupled in 5G, 6G will support a variety of safety-critical control applications with different types of timing requirements, as evidenced by the emergence of metrics related to information freshness and information value. In addition, ensuring ultrahigh reliability for safety-critical control applications requires modeling and estimation of the tail statistics of the wireless channel, queue length, and delay. The fulfillment of these stringent requirements calls for the development of novel artificial intelligence (AI)-based techniques, incorporating optimization theory, explainable AI (XAI), generative AI, and digital twins (DTs). The third part analyzes the coexistence of massive connectivity and critical services. Specifically, we consider scenarios in which a massive number of devices need to support traffic patterns of mixed criticality. This is followed by a discussion about the management of wireless resources shared by services with different criticality.",
        "issn": {
            "Print ISSN": "0018-9219",
            "Electronic ISSN": "1558-2256"
        },
        "keywords": {
            "IEEE Keywords": [
                "6G mobile communication",
                "5G mobile communication",
                "Wireless communication",
                "Internet of Things",
                "Ultra reliable low latency communication",
                "Reliability",
                "Wireless sensor networks",
                "Sensors",
                "Artificial intelligence",
                "Traffic control"
            ],
            "Author Keywords": [
                "6G",
                "Internet of Things (IoT)",
                "machine-type communications (MTCs)",
                "massive access",
                "massive connectivity",
                "ultrareliable low-latency communications (URLLC)",
                "wireless networks"
            ]
        },
        "title": "Wireless 6G Connectivity for Massive Number of Devices and Critical Services"
    },
    {
        "authors": [
            "Jiongze Yu",
            "Heqiang Huang",
            "Yuhang Ma",
            "Yueying Wu",
            "Junzhou Chen",
            "Ronghui Zhang",
            "Xuemiao Xu",
            "Zhihan Lv",
            "Guodong Yin"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "06 November 2024",
        "doi": "10.1109/JIOT.2024.3492347",
        "publisher": "IEEE",
        "abstract": "Smoke detection is essential for fire prevention, yet it is significantly hampered by the visual similarities between smoke and fog. To address this challenge, a Split Top-k Attention Transformer Framework (STKformer) is proposed. The STKformer incorporates Split Top-k Attention (STKA), which partitions the attention map for top-k selection to retain informative self-attention values while capturing long-range dependencies. This approach effectively filters out irrelevant attention scores, preventing information loss. Furthermore, the Adaptive Dark-channel-prior Guidance Network (ADGN) is designed to enhance smoke recognition under foggy conditions. ADGN employs pooling operations instead of minimum value filtering, allowing for efficient dark channel extraction with learnable parameters and adaptively reducing the impact of fog. The extracted prior information subsequently guides feature extraction through a Priorformer block, improving model robustness. Additionally, a Cross-Stage Fusion Module (CSFM) is introduced to aggregate features from different stages efficiently, enabling flexible adaptation to smoke features at various scales and enhancing detection accuracy. Comprehensive experiments demonstrate that the proposed method achieves state-of-the-art performance across multiple datasets, with an accuracy of 89.68% on DSDF, 99.76% on CIS, and 99.76% on UIW. The method maintains high speed and lightweight characteristics, validated with an inference speed of 211.46 FPS on an NVIDIA Jetson AGX Orin after TensorRT acceleration, confirming its effectiveness and efficiency for real-world applications. The source code is available at https://github.com/Jiongze-Yu/STKformer",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Accuracy",
                "Real-time systems",
                "Convolutional neural networks",
                "Transformers",
                "Image color analysis",
                "Image classification",
                "Visualization",
                "Adaptive systems",
                "Object detection"
            ],
            "Author Keywords": [
                "Smoke detection",
                "transformer",
                "top-k attention",
                "foggy Internet of Things(IoT) environment",
                "surveillance",
                "lightweight model"
            ]
        },
        "title": "Real-Time Smoke Detection with Split Top-k Transformer and Adaptive Dark Channel Prior in Foggy Environments"
    },
    {
        "authors": [
            "Yasser Qaragoez",
            "Sofie Pollin",
            "Dominique Schreurs"
        ],
        "published_in": "Published in: IEEE Transactions on Microwave Theory and Techniques ( Early Access )",
        "date_of_publication": "17 July 2024",
        "doi": "10.1109/TMTT.2024.3422320",
        "publisher": "IEEE",
        "abstract": "This article introduces an innovative three-tone frequency-shift keying (FSK) modulation technique designed for application in simultaneous wireless information and power transfer (SWIPT) systems. The proposed method aims to improve the power conversion efficiency (PCE) through waveform design without adversely affecting the information transfer performance. Contrary to conventional FSK, which uses a uniform tone power, our method integrates a central tone with constant power and side tones with power proportional to the central tone. This Biased-FSK modulation scheme, combined with an adaptable information reception strategy, uses the frequency dependence of the rectifier’s transfer function to cater to low-and high-power scenarios, respectively. A mathematical model was developed for the PCE of the rectifier under this three-tone signal, highlighting the potential for improved PCE and reduced information distortion. The theoretical derivations are validated by empirical results, demonstrating the feasibility of this approach for enhancing the energy efficiency of SWIPT systems. The rectifier PCE achieved a peak of 62.84%. The use of Biased-FSK modulation enhanced the rectifier PCE by up to 9.04% and reduced the total harmonic distortion (THD) by 6.23% and the BER by a factor of 5.5.",
        "issn": {
            "Print ISSN": "0018-9480",
            "Electronic ISSN": "1557-9670"
        },
        "keywords": {
            "IEEE Keywords": [
                "Frequency shift keying",
                "Schottky diodes",
                "Modulation",
                "Voltage",
                "Simultaneous wireless information and power transfer",
                "Rectifiers",
                "Internet of Things"
            ],
            "Author Keywords": [
                "Data transfer",
                "energy harvesting",
                "frequency shift keying (FSK)",
                "Internet of Things (IoT)",
                "low-power electronics",
                "modulation technique",
                "power conversion efficiency (PCE)",
                "simultaneous wireless information and power transfer (SWIPT)",
                "wireless information transfer (WIT)",
                "wireless power transfer (WPT)"
            ]
        },
        "title": "Biased-FSK Modulation for Simultaneous Wireless Information and Power Transfer"
    },
    {
        "authors": [
            "Kouros Zanbouri",
            "Md. Noor-A-Rahim",
            "Jobish John",
            "Cormac J. Sreenan",
            "H. Vincent Poor",
            "Dirk Pesch"
        ],
        "published_in": "Published in: IEEE Communications Surveys & Tutorials ( Early Access )",
        "date_of_publication": "25 October 2024",
        "doi": "10.1109/COMST.2024.3486618",
        "publisher": "IEEE",
        "abstract": "Time-sensitive networking (TSN) is expected to be a key component of critical machine-type communication networks in areas such as Industry 4.0, robotics and autonomous vehicles. With rising mobility requirements in industrial applications and the prevalence of wireless networks, wireless network integration into TSN is becoming increasingly important. This survey article presents a comprehensive review of the current literature on wireless TSN, including an overview of the architecture of a wireless TSN network and an examination of the various wireless technologies and protocols that can be or are used in such networks. In addition, the article discusses industrial applications of wireless TSN, among them industrial automation, robotics, and autonomous vehicles. The article concludes by summarizing the challenges and open issues related to the integration of TSN into wireless networks, and by offering suggestions for future research directions.",
        "issn": {
            "Electronic ISSN": "1553-877X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Wireless sensor networks",
                "Surveys",
                "Standards",
                "Wireless fidelity",
                "Synchronization",
                "Reliability",
                "Protocols",
                "5G mobile communication",
                "Wireless networks",
                "Quality of service"
            ],
            "Author Keywords": [
                "Wireless TSN",
                "TSN",
                "5G",
                "6G",
                "Wi-Fi 7",
                "IEEE 802.11be",
                "Industrial IoT",
                "Industry 4.0",
                "Industry 5.0",
                "Millimeter-wave",
                "Smart Manufacturing",
                "Smart Factory",
                "Wireless Factory"
            ]
        },
        "title": "A Comprehensive Survey of Wireless Time-Sensitive Networking (TSN): Architecture, Technologies, Applications, and Open Issues"
    },
    {
        "authors": [
            "Hui Zhang",
            "Qiming Jia",
            "Xu Han",
            "Hongde Yu",
            "Jiaxiang Zhao"
        ],
        "published_in": "Published in: IEEE Transactions on Industrial Informatics ( Early Access )",
        "date_of_publication": "02 October 2024",
        "doi": "10.1109/TII.2024.3459025",
        "publisher": "IEEE",
        "abstract": "In the intelligent reflecting surface (IRS)-assisted simultaneous wireless information and power transfer (SWIPT) system, it is critical to improve the system performance by jointly designing the transmit beamforming of base station and IRS. According to this beamforming design problem, the actual phase-related amplitude model is established that is affected by the nonlinear energy loss of IRS, and then a multiparameter joint optimization problem is established to maximize the rate sum of the devices. Furthermore, a PER-SAC algorithm is proposed to solve this problem, which is based on the soft actor–critic (SAC) principle and uses prioritized experience replay (PER) to improve training efficiency. The results show that the PER-SAC algorithm has better stability and convergence effect than other algorithms in this scenario, and the sum rate obtained by the PER-SAC algorithm is also higher than that of the random phase-shift scheme and the random power splitting ratio scheme.",
        "issn": {
            "Print ISSN": "1551-3203",
            "Electronic ISSN": "1941-0050"
        },
        "keywords": {
            "IEEE Keywords": [
                "Array signal processing",
                "Wireless communication",
                "Simultaneous wireless information and power transfer",
                "Receivers",
                "Mathematical models",
                "Energy harvesting",
                "Base stations",
                "Optimization",
                "Data models",
                "Numerical models"
            ],
            "Author Keywords": [
                "Deep reinforcement learning (DRL)",
                "Internet of Things (IoT)",
                "simultaneous wireless information and power transfer (SWIPT)",
                "soft actor–critic (SAC)"
            ]
        },
        "title": "Beamforming Design of IRS-Assisted SWIPT System Based on Deep Reinforcement Learning"
    },
    {
        "authors": [
            "Dae-Ho Kim",
            "Jae-Young Pyun"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "11 November 2024",
        "doi": "10.1109/ACCESS.2024.3494874",
        "publisher": "IEEE",
        "abstract": "Because a typical adaptive data rate (ADR) of the long-range wide-area network (LoRaWAN) is designed for static services such as smart grid, monitoring, and metering, its performance is limited in mobile environments for location-based services (LBS). Indeed, ADR cannot respond to frequent channel environment changes, thus allocates incorrect data rate (DR) settings to devices, resulting in generation of massive packet loss. To improve mobile support in LoRaWAN location-based services, this paper proposes a robust ADR (RADR) based on an artificial intelligence (AI) approach. The proposed RADR enhances the DR allocation by applying an AI model trained on various channel environments of LBS. Its AI model infers efficient DR, ensuring reliable packet delivery even in mobile scenarios. To verify and demonstrate our AI-driven RADR performance, we built a testbed composed of RADR-embedded LoRaWAN network server/end devices and evaluated their performance in field tests assuming LBS. The results showed a significant improvement in the packet success rate, with the proposed RADR scheme achieving an average increase of 30% compared to the typical ADR.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Artificial intelligence",
                "LoRaWAN",
                "Logic gates",
                "Uplink",
                "Signal to noise ratio",
                "Performance evaluation",
                "Data models",
                "Servers",
                "Training",
                "Resource management"
            ],
            "Author Keywords": [
                "Adaptive data rate (ADR)",
                "Deep learning",
                "Internet of Things (IoT)",
                "location-based services (LBS)",
                "Long Range Wide Area Network (LoRaWAN)",
                "Resource allocation"
            ]
        },
        "title": "AI-driven Adaptive Data Rate for LoRaWAN Location-Based Services"
    },
    {
        "authors": [
            "Majid H. Khoshafa",
            "Omar Maraqa",
            "Jules M. Moualeu",
            "Sylvester Aboagye",
            "Telex M. N. Ngatched",
            "Mohamed H. Ahmed",
            "Yasser Gadallah",
            "Marco Di Renzo"
        ],
        "published_in": "Published in: IEEE Communications Surveys & Tutorials ( Early Access )",
        "date_of_publication": "28 October 2024",
        "doi": "10.1109/COMST.2024.3487112",
        "publisher": "IEEE",
        "abstract": "Security and latency are crucial aspects in the design of future wireless networks. Physical layer security (PLS) has received a growing interest from the research community in recent years for its ability to safeguard data confidentiality without relying on key distribution or encryption/decryption, and for its latency advantage over bit-level cryptographic techniques. However, the evolution towards the fifth generation (5G) technology and beyond poses new security challenges that must be addressed in order to fulfill the unprecedented performance requirements of future wireless communication networks. Among the potential key-enabling technologies, reconfigurable intelligent surface (RIS) has attracted extensive attention due to its ability to proactively and intelligently reconfigure the wireless propagation environment to combat dynamic wireless channel impairments. Consequently, the RIS technology can be adopted to improve the information-theoretic security of both radio frequency (RF) and optical wireless communications (OWC) systems. It is worth noting that the configuration of RIS in RF communications is different from the one in optical systems at many levels (e.g., RIS materials, signal characteristics, and functionalities). This survey paper provides a comprehensive overview of the information-theoretic security of RIS-based RF and optical systems. The article first discusses the fundamental concepts of PLS and RIS technologies, followed by their combination in both RF and OWC systems. Subsequently, some optimization techniques are presented in the context of the underlying system model, followed by an assessment of the impact of RIS-assisted PLS through a comprehensive performance analysis. Given that the computational complexity of future communication systems that adopt RIS-assisted PLS is likely to increase rapidly as the number of interactions between the users and infrastructure grows, machine learning (ML) is seen as a promising approach to address ...",
        "issn": {
            "Electronic ISSN": "1553-877X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Wireless communication",
                "Wireless sensor networks",
                "Reconfigurable intelligent surfaces",
                "Communication system security",
                "Optimization",
                "Radio frequency",
                "Ad hoc networks",
                "Surveys",
                "Signal to noise ratio",
                "Probability"
            ],
            "Author Keywords": [
                "Physical layer security (PLS)",
                "reconfigurable intelligent surface (RIS)",
                "smart radio environment",
                "beyond 5G (B5G) networks",
                "multiple-input multiple-output (MIMO)",
                "millimeter wave (mmWave)",
                "terahertz (THz)",
                "unmanned aerial vehicle (UAV)",
                "device-to-device (D2D) communication",
                "cognitive radio networks (CRNs)",
                "simultaneous wireless information and power transfer (SWIPT)",
                "energy harvesting (EH)",
                "mobile edge computing (MEC)",
                "satellite communications",
                "multicast communications",
                "cell-free networks",
                "relay-aided networks",
                "vehicular communications",
                "wireless body area network (WBAN)",
                "integrated sensing and communications (ISAC)",
                "internet-of-everything (IoT)",
                "industrial internet of thing (IIoT)",
                "internet of medical things (IoMT)",
                "backscatter communications",
                "wireless sensor networks (WSNs)",
                "optical wireless communications (OWC)",
                "visible light communications (VLC)",
                "free space optical (FSO)",
                "high-altitude platform systems (HAPs)",
                "coordinated multipoint (CoMP) communications",
                "blockchain technology",
                "Ad-hoc networks",
                "under-water communications",
                "mixed reality",
                "optimization",
                "performance analysis",
                "machine learning (ML)",
                "non-orthogonal multiple access (NOMA)"
            ]
        },
        "title": "RIS-Assisted Physical Layer Security in Emerging RF and Optical Wireless Communication Systems: A Comprehensive Survey"
    },
    {
        "authors": [
            "Norah Ahmed Almubairik",
            "Fakhri Alam Khan"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "23 October 2024",
        "doi": "10.1109/JIOT.2024.3485027",
        "publisher": "IEEE",
        "abstract": "Wearable devices have become increasingly prevalent in workplaces worldwide, offering valuable information and forensic data to dispute false testimonies or track a victim during an incident. However, the use of wearables as sources of digital evidence remains relatively unexplored. Further, there has been no systematic review of data extraction and analysis techniques for wearables. This systematic literature review (SLR) addresses these gaps by (1) exploring methods used by digital investigators to extract data from wearables; (2) surveying prevalent data analysis techniques for wearable digital forensics; (3) examining digital forensics tools used in wearable investigations; (4) proposing a taxonomy integrating data extraction methods, analysis techniques, and forensic tools; and (5) identifying gaps in current wearable forensics research to guide future studies. The SLR covered articles published in the last decade (2012– 2022) on the extraction and analysis of evidence from wearables. Consequently, 50 primary studies relevant to the study’s objectives were identified. Five main extraction techniques were identified: manual, logical, physical, network communication, and electromagnetic. Logical data extraction accounted for approximately 48% of these methods, followed by physical extraction (31%). Notably, 47% employed multiple extraction techniques. Trivial, non-trivial, and anti-forensic techniques were the most commonly used by criminals to evade forensic investigations. Moreover, most tools examined for wearable investigations were from non-wearable domains. The review highlighted several research gaps that require future investigation to develop more sustainable approaches to wearable digital forensics. This comprehensive overview highlighted the need for advancing forensic methodologies to address the unique challenges posed by wearable technology.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Biomedical monitoring",
                "Wearable devices",
                "Digital forensics",
                "Data mining",
                "Wearable Health Monitoring Systems",
                "Databases",
                "Monitoring",
                "Tracking",
                "NIST",
                "Libraries"
            ],
            "Author Keywords": [
                "Acquisition",
                "Extraction",
                "Analysis",
                "Digital evidence",
                "Digital forensics tools",
                "IoT",
                "Wearable digital forensics"
            ]
        },
        "title": "Systematic Literature Review on Wearable Digital Forensics: Acquisition Methods, Analysis Techniques, Tools, and Future Directions"
    },
    {
        "authors": [
            "Seyedpouya Seyedkazemi",
            "M. Emre Gursoy",
            "Yucel Saygin"
        ],
        "published_in": "Published in: IEEE Transactions on Industrial Informatics ( Early Access )",
        "date_of_publication": "08 November 2024",
        "doi": "10.1109/TII.2024.3485800",
        "publisher": "IEEE",
        "abstract": "With the growing popularity of local differential privacy (LDP), there is increasing interest in its deployment in industrial applications, smart homes, and smart cities. However, the main premise of LDP is that data are perturbed to protect privacy, and therefore consumption statistics estimated via LDP are inherently noisy. When noisy estimates are used for capacity planning, they can lead to false positives (false claims of capacity exceedance) or false negatives (actual exceedances are neglected). To address these concerns, this article proposes a system called CAPRI for capacity planning and optimized budget selection in smart city applications under LDP. Based on a specified set of conditions (e.g., number of clients, possible consumption values, LDP protocol) and constraints (e.g., false positive probability should be below 0.01), CAPRI is able to determine the $\\varepsilon$ privacy budget, which simultaneously satisfies the desired constraints and maximizes clients' privacy. To do so, CAPRI proposes an optimization-based problem formulation and a search-based solution, which relies on LDP simulations. We experimentally validate and demonstrate the effectiveness of CAPRI using real-world and synthetic datasets, three popular LDP protocols, and various constraints and conditions.",
        "issn": {
            "Print ISSN": "1551-3203",
            "Electronic ISSN": "1941-0050"
        },
        "keywords": {
            "IEEE Keywords": [
                "Capacity planning",
                "Protocols",
                "Privacy",
                "Servers",
                "Smart homes",
                "Noise measurement",
                "Differential privacy",
                "Smart grids",
                "Smart cities",
                "Planning"
            ],
            "Author Keywords": [
                "Capacity planning",
                "Internet of Things (IoT)",
                "local differential privacy (LDP)",
                "smart cities",
                "smart home"
            ]
        },
        "title": "Capacity Planning Under Local Differential Privacy With Optimized Budget Selection"
    },
    {
        "authors": [
            "Licheng Chen",
            "Yunquan Dong"
        ],
        "published_in": "Published in: IEEE Transactions on Network Science and Engineering ( Early Access )",
        "date_of_publication": "03 September 2024",
        "doi": "10.1109/TNSE.2024.3453959",
        "publisher": "IEEE",
        "abstract": "In this paper, we estimate the average age of information (AoI) of the status updating over a wireless channel with an unknown fading model. Different from most related works which take the distributions of the inter-arrival time and transmission time of updates as known information, we approximate the average AoI of the system by using their first and second-order moments. Note that these distributions are often not accessible or known with inevitable errors while their moments are much easier to obtain, e.g., by using counting and statistics. We model the communications over the fading channel with a continuous transmission model and a discrete transmission model, which use the variable-rate scheme and the fixed-rate scheme, respectively. We assume that the arrival of the continuous transmission model is a Bernoulli process and make no assumptions about the arrival process of the discrete transmission model. Based on these information, we present two pairs of tight lower and upper bounds for the AoI of the two models. We show that obtained bounds are the tightest when the inter-arrival time (or transmission time) follows the degenerate distribution and are the loosest when it follows the two-point distribution, which randomly takes value from two possible outcomes. We also show that tighter bounds can be obtained by using higher order moments.",
        "issn": {
            "Electronic ISSN": "2327-4697"
        },
        "keywords": {
            "IEEE Keywords": [
                "Estimation",
                "Queueing analysis",
                "Fading channels",
                "Accuracy",
                "Computational modeling",
                "Throughput",
                "Measurement"
            ],
            "Author Keywords": [
                "Internet of Things (IoT)",
                "age of information",
                "estimation of age",
                "statistical moments"
            ]
        },
        "title": "Estimating Age of Information in Wireless Systems With Unknown Distributions of Inter-arrival/Service Time"
    },
    {
        "authors": [
            "Mayukh Sarkar",
            "Jitesh Pradhan",
            "Anil Kumar Singh",
            "Hathiram Nenavath"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "04 July 2024",
        "doi": "10.1109/TCE.2024.3423416",
        "publisher": "IEEE",
        "abstract": "Autonomous mobile robots are being used increasingly as consumer devices around the globe, such as for fetching items and cleaning purposes, to name a few, in households and industries. Such robots are being employed to traverse a set of target locations and provide the necessary services. In a 2-dimensional environment, these robots are required to traverse following a Hamiltonian path to reduce energy consumption and time requirements. This problem can be formulated as a Travelling Salesman Problem (TSP), an NP-hard problem. Moreover, upon urgent requirements, these robots must traverse in real-time, demanding speedy path planning from the TSP instance. Among the well-known optimization techniques for solving the TSP problem, Ant Colony Optimization has a good stronghold in providing good approximate solutions. Moreover, ACO not only provides near-optimal solutions for TSP instances but can also output optimal or near-optimal solutions for many other demanding hard optimization problems. However, most of the implementations of Ant Colony Optimization on quantum or hybrid quantum architecture proposed in the literature require conversion of classical data to qubits before being fed to the algorithm, and cannot be automated. But quantum-enabled mobile robots require automated path formation after receiving the commands from the environment. The novelties of the proposed work are many-fold. Firstly, the proposed work allows ACO to be applied as its classical counterparts, allowing automation in path formation in quantum-enabled mobile robots. Secondly, this allows a new way of incorporating quantum processing unit in the research of quantum-enabled mobile robots. Researchers around the globe have been trying to incorporate quantum computing in autonomous mobile robots, and true to the best of authors’ knowledge, no work in path planning for multiple targets in quantum-enabled mobile robots have been found in literature. Thirdly, quantum processing unit has been appli...",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Quantum computing",
                "Mobile robots",
                "Logic gates",
                "Qubit",
                "Ant colony optimization",
                "Service robots",
                "Optimization"
            ],
            "Author Keywords": [
                "Mobile IoT Devices",
                "Path Planning",
                "Travelling Salesman Problem",
                "Ant Colony Optimization",
                "Quantum Computing"
            ]
        },
        "title": "A Novel Hybrid Quantum Architecture for Path Planning in Quantum-Enabled Autonomous Mobile Robots"
    },
    {
        "authors": [
            "Jiazhi Tang",
            "Xiangshuai Meng",
            "Hongyu Shi",
            "Jianjia Yi",
            "Xiaoming Chen",
            "Guan-Long Huang",
            "José Manuel Fernández González",
            "Qiang Cheng"
        ],
        "published_in": "Published in: IEEE Transactions on Microwave Theory and Techniques ( Early Access )",
        "date_of_publication": "16 August 2024",
        "doi": "10.1109/TMTT.2024.3439330",
        "publisher": "IEEE",
        "abstract": "Conventional holographic impedance metasurfaces with large apertures usually have extremely narrow bandwidths, limiting their applications for broadband orbital angular momentum (OAM) waves. In this article, a new wideband, optically transparent low-profile holographic impedance metasurface is proposed for launching multimode OAM vortex waves. To enhance the bandwidth, a metasurface cell composed of materials with high optical transparency (indium tin oxide, polyethylene glycol terephthalate (PET), and borosilicate glass) is put forward for the first time. The design process is presented for wideband OAM implementation based on the distinctive dispersion properties of the devised cell. Moreover, the multimode multiplexed impedance distributions are exhibited. Simulation and measurement results show that the proposed metasurface can effectively generate dual-mode OAM waves from 17 to 21.5 GHz (BW\n=\n22.5%) with a high isolation of over 40 dB. The proposed method provides a comprehensive solution for wideband, optically transparent, easy fabrication, low profile, and multimode OAM launching.",
        "issn": {
            "Print ISSN": "0018-9480",
            "Electronic ISSN": "1557-9670"
        },
        "keywords": {
            "IEEE Keywords": [
                "Metasurfaces",
                "Impedance",
                "Holography",
                "Wideband",
                "Optical vortices",
                "Optical surface waves",
                "Holographic optical components"
            ],
            "Author Keywords": [
                "Holographic impedance metasurface",
                "Internet of Things (IoT)",
                "multimode",
                "optically transparent",
                "orbit angular momentum (OAM)",
                "wideband"
            ]
        },
        "title": "Wideband Optically Transparent Low-Profile Holographic Impedance Metasurface for Multimode OAM Generation"
    },
    {
        "authors": [
            "David E. Ruíz-Guirola",
            "Carlos A. Rodríguez-López",
            "Onel L. A. López",
            "Samuel Montejo-Sánchez",
            "Vitalio Alfonso Reguera",
            "Matti Latva-aho"
        ],
        "published_in": "Published in: IEEE Transactions on Industrial Informatics ( Early Access )",
        "date_of_publication": "23 September 2024",
        "doi": "10.1109/TII.2024.3455010",
        "publisher": "IEEE",
        "abstract": "Discontinuous reception (DRX) is a key technology for reducing the energy consumption of industrial Internet of Things (IIoT) devices. Specifically, DRX allows the devices to operate in a low-power mode when no data reception is scheduled, and its effectiveness depends on the proper configuration of the DRX parameters. In this paper, we characterize the DRX process departing from a semi-Markov chain modeling and detail two ways to set DRX parameters to minimize the device power consumption while meeting a mean delay constraint. The first method exhaustively searches for the optimal configuration, while the second method uses a low-complexity metaheuristic to find a sub-optimal configuration, thus considering ideal and practical DRX configurations. Notably, within the DRX parameters, the inactivity timer (IT) is a caution time that specifies how long a device remains active after the last information exchange as a precedent to a low-power mode. Traditionally, the IT is restarted whenever new data is received, which might sometimes needlessly extend the active time. Herein, we propose a more efficient method in which the transmit base station (BS) explicitly indicates restarting the timer through the control channel only when appropriate. The decision is based on the BS's knowledge about its buffer status. We consider Poisson and bursty traffic models, which are typical in IIoT setups, and verify our proposal's suitability for reducing the devices' energy consumption without significantly compromising the communication latency. Specifically, energy saving gains up to 30% can be obtained regardless of the arrivals rate and delay constraints.",
        "issn": {
            "Print ISSN": "1551-3203",
            "Electronic ISSN": "1941-0050"
        },
        "keywords": {
            "IEEE Keywords": [
                "Industrial Internet of Things",
                "Delays",
                "Downlink",
                "Monitoring",
                "Energy conservation",
                "Symbols",
                "Informatics"
            ],
            "Author Keywords": [
                "Discontinuous reception (DRX mechanism)",
                "energy saving",
                "inactivity timer (IT)",
                "industrial IoT (IIOT)",
                "low-power mode"
            ]
        },
        "title": "Discontinuous Reception With Adjustable Inactivity Timer for IIoT"
    },
    {
        "authors": [
            "Marcel Balle",
            "Wenxiu Xu",
            "Kevin FA Darras",
            "Thomas Cherico Wanger"
        ],
        "published_in": "Published in: IEEE Transactions on AgriFood Electronics ( Early Access )",
        "date_of_publication": "11 November 2024",
        "doi": "10.1109/TAFE.2024.3472493",
        "publisher": "IEEE",
        "abstract": "Recent advances in Internet of Things and artificial intelligence technologies have shifted automated monitoring in smart agriculture toward low power sensors and embedded vision on powerful processing units. Vision-based monitoring devices need an effective power management and control system with system-adapted power input and output capabilities to achieve power-efficient and self-sustainable operation. Here, we present a universal power management solution for automated monitoring devices in agricultural systems, compatible with commonly used off-the-shelf edge processing units (EPUs). The proposed design is specifically adapted for battery-powered EPU systems by incorporating power-matched energy harvesting, a power switch with low-power sleep mode, and simple system integration in an microcontroller unit-less architecture with automated operation. We use a four-month case study to monitor the effects of plastic pollution in agricultural soils on plant growth under 4-mg microplastic exposure, demonstrating that the setup achieved continuous and sustainable operation. In this agricultural application, our power management module is deployed in an embedded vision camera equipped with a 5-W solar panel and five various environmental sensors, effectively monitoring environmental stress and plant growth state. This work highlights the application of the power management board in embedded agricultural monitoring devices for precision farming.",
        "issn": {
            "Electronic ISSN": "2771-9529"
        },
        "keywords": {
            "IEEE Keywords": [
                "Batteries",
                "Universal Serial Bus",
                "Wireless sensor networks",
                "Switching circuits",
                "Power system management",
                "Control systems",
                "Connectors",
                "Power demand",
                "Internet of Things",
                "Environmental monitoring"
            ],
            "Author Keywords": [
                "Agricultural systems monitoring",
                "automatic power management",
                "edge processing unit (EPU)",
                "energy harvesting (EH)",
                "internet of things (IoT)",
                "microplastic (MP)",
                "sensor systems and applications"
            ]
        },
        "title": "A Power Management and Control System for Environmental Monitoring Devices"
    },
    {
        "authors": [
            "Xiaolan Liu",
            "Bin Yang",
            "Jianming Liu",
            "Lintao Xian",
            "Xiaohong Jiang",
            "Tarik Taleb"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "04 September 2024",
        "doi": "10.1109/JIOT.2024.3454365",
        "publisher": "IEEE",
        "abstract": "This paper investigates sum-rate maximization while achieving seamless coverage with the minimum number of UAVs in a device-to-device (D2D)-enabled unmanned aerial vehicle (UAV) network. Toward this end, we formulate it as a nonlinear and nonconvex optimization problem, and then propose a Max-Rate Min-Number (MRMN) scheme to solve this optimization problem. Firstly, we derive UAV’s coverage radius which can depict the maximum coverage for user equipments, and then implement the optimal deployment for UAV swarm by exploiting the disk covering theory. Furthermore, we apply the coalitional game theory to design the cooperative strategy between UAV swarm and ground equipments. Finally, a coalition formation algorithm is presented for achieving maximum system sum-rate while reducing the number of UAVs under seamless coverage constraint. Extensive simulation results are provided to validate the effectiveness of our proposed MRMN scheme, and also illustrate that the scheme can improve the system sum-rate and reduce the number of deployed UAVs. Meanwhile, we further conduct a performance comparison between our scheme and the existing benchmark schemes.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Autonomous aerial vehicles",
                "Device-to-device communication",
                "Disasters",
                "Optimization",
                "Protocols",
                "Cellular networks",
                "Internet of Things"
            ],
            "Author Keywords": [
                "IoT network",
                "UAV technology",
                "D2D communication",
                "seamless coverage",
                "sum-rate"
            ]
        },
        "title": "Sum-Rate Maximization for D2D-Enabled UAV Networks With Seamless Coverage Constraint"
    },
    {
        "authors": [
            "Md. Ali Hasan",
            "M. Humayun Kabir",
            "Md. Shafiqul Islam",
            "Sangmin Han",
            "Wonjae Shin"
        ],
        "published_in": "Published in: IEEE Systems Journal ( Early Access )",
        "date_of_publication": "14 October 2024",
        "doi": "10.1109/JSYST.2024.3457794",
        "publisher": "IEEE",
        "abstract": "In signals of opportunity (SOPs)-based positioning utilizing low Earth orbit (LEO) satellites, ephemeris data derived from two-line element files can introduce increasing error over time. To handle the erroneous measurement, an additional base receiver with a known position is often used to compensate for the effect of ephemeris error when positioning the user terminal (UT). However, this approach is insufficient for long baseline (distance between base receiver and UT) as it fails to adequately correct Doppler shift measurement errors caused by ephemeris inaccuracies, resulting in degraded positioning performance. Moreover, the lack of clock synchronization between the base receiver and UT exacerbates erroneous Doppler shift measurements. To address these challenges, we put forth a robust double-difference Doppler shift-based positioning framework, coined 3DPose, to handle the clock synchronization issue between the base receiver and UT, and positioning degradation due to the long baseline. The proposed 3DPose framework leverages double-difference Doppler shift measurements to eliminate the clock synchronization issue and incorporates a novel ephemeris error correction algorithm to enhance UT positioning accuracy in case of the long baseline. The algorithm specifically characterizes and corrects the Doppler shift measurement errors arising from erroneous ephemeris data, focusing on satellite position errors in the tangential direction. To validate the effectiveness of the proposed framework, we conduct comparative analyses across three different scenarios, contrasting its performance with the existing differential Doppler positioning method. The results demonstrate that the proposed 3DPose framework achieves an average reduction of 90% in 3-dimensional positioning errors compared to the benchmark algorithm.",
        "issn": {
            "Print ISSN": "1932-8184",
            "Electronic ISSN": "1937-9234"
        },
        "keywords": {
            "IEEE Keywords": [
                "Satellite broadcasting",
                "Doppler shift",
                "Low earth orbit satellites",
                "Receivers",
                "Extraterrestrial measurements",
                "Satellites",
                "Global navigation satellite system",
                "Accuracy",
                "Position measurement",
                "Clocks"
            ],
            "Author Keywords": [
                "Differential Doppler positioning",
                "Doppler shift-based positioning",
                "ephemeris error",
                "Internet of Things (IoT)",
                "low Earth orbit (LEO) satellite",
                "signals of opportunity (SOPs)"
            ]
        },
        "title": "A Double-Difference Doppler Shift-Based Positioning Framework With Ephemeris Error Correction of LEO Satellites"
    },
    {
        "authors": [
            "M.M. Hasan Mahfuz",
            "A.M. Mahfouz",
            "Elham Baladi",
            "Ahmed A. Kishk"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "11 November 2024",
        "doi": "10.1109/ACCESS.2024.3496117",
        "publisher": "IEEE",
        "abstract": "Breast cancer is a major killer of women worldwide and one of the leading causes of death overall. It involves the progressive abnormal growth of breast tissue which, if detected at an early stage, can be diagnosed as a tumor. Traditional breast cancer screening methods, such as X-ray mammography, magnetic resonance imaging, and ultrasound scanning, present several drawbacks, making them less than ideal. These drawbacks include high costs, exposure to potentially hazardous radiation, and patient inconvenience. Due to these challenges, researchers have been motivated to seek alternative methods, one of which involves the application of microwave technology. In recent years, wearable and flexible patch antennas have gained popularity due to their appealing characteristics and the potential to develop lightweight, compact, low-cost, and adaptable solutions for biomedical applications. This article provides an overview of microwave approaches for breast tumor detection using microstrip patch antennas. In particular, recent advancements in active microwave imaging and microwave-based methods are reviewed. The primary goal of this work is to offer researchers and medical professionals an understanding of the underlying principles, techniques, and challenges associated with microwave imaging for breast tumor/cancer detection. Additionally, this study aims to highlight the fact that, as of now, commercially available, cost-effective microwave-based technologies for imaging or detecting breast tumors/cancer are relatively scarce. This observation is not meant to imply that microwave technology is ineffective for breast tumor/cancer diagnosis; rather, it seeks to spark a constructive discussion about why, despite years of dedicated research, a widely accessible commercial technology has yet to be made available.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Breast",
                "Antennas",
                "Microwave antennas",
                "Imaging",
                "Microwave imaging",
                "Antenna measurements",
                "Breast cancer",
                "Tumors",
                "Cancer",
                "Reflector antennas"
            ],
            "Author Keywords": [
                "Flexible antennas",
                "ISM band",
                "UWB",
                "breast tumor/cancer",
                "SAR",
                "wearable antenna",
                "breast tissue",
                "biomedical application",
                "IoT"
            ]
        },
        "title": "Microstrip Patch Antennas for Breast Tumor/Cancer Cell Detection–Challenges, Designs and Future Opportunities: A Review"
    },
    {
        "authors": [
            "Osama Wehbi",
            "Sarhad Arisdakessian",
            "Mohsen Guizani",
            "Omar Abdel Wahab",
            "Azzam Mourad",
            "Hadi Otrok",
            "Hoda Al khzaimi",
            "Bassem Ouni"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "09 October 2024",
        "doi": "10.1109/JIOT.2024.3476950",
        "publisher": "IEEE",
        "abstract": "Federated learning is a promising collaborative and privacy-preserving machine learning approach in data-rich smart cities. Nevertheless, the inherent heterogeneity of these urban environments presents a significant challenge in selecting trustworthy clients for collaborative model training. The usage of traditional approaches, such as the random client selection technique, poses several threats to the system’s integrity due to the possibility of malicious client selection. Primarily, the existing literature focuses on assessing the trustworthiness of clients, neglecting the crucial aspect of trust in federated servers. To bridge this gap, in this work, we propose a novel framework that addresses the mutual trustworthiness in federated learning by considering the trust needs of both the client and the server. Our approach entails: (1) Creating preference functions for servers and clients, allowing them to rank each other based on trust scores, (2) Establishing a reputation-based recommendation system leveraging multiple clients to assess newly connected servers, (3) Assigning credibility scores to recommending devices for better server trustworthiness measurement, (4) Developing a trust assessment mechanism for smart devices using a statistical Interquartile Range (IQR) method, (5) Designing intelligent matching algorithms considering the preferences of both parties. Based on simulation and experimental results, our approach outperforms baseline methods by increasing trust levels, global model accuracy, and reducing non-trustworthy clients in the system.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Servers",
                "Federated learning",
                "Smart cities",
                "Training",
                "Internet of Things",
                "Reliability",
                "Performance evaluation",
                "Smart devices",
                "Data privacy",
                "Game theory"
            ],
            "Author Keywords": [
                "Federated Learning",
                "Trustworthiness",
                "Game Theory",
                "Smart-cities",
                "IoT",
                "Bootstrapping",
                "Recommendation Systems"
            ]
        },
        "title": "Enhancing Mutual Trustworthiness in Federated Learning for Data-Rich Smart Cities"
    },
    {
        "authors": [
            "Mengyu Li",
            "Yi Huo",
            "Shuang Song",
            "Wanyuan Qu",
            "Le Ye",
            "Menglian Zhao",
            "Zhichao Tan"
        ],
        "published_in": "Published in: IEEE Transactions on Biomedical Circuits and Systems ( Early Access )",
        "date_of_publication": "04 July 2024",
        "doi": "10.1109/TBCAS.2024.3423366",
        "publisher": "IEEE",
        "abstract": "This paper proposed an event-driven clockless level-crossing ADC (LC-ADC) suitable for biomedical applications. Thanks to the LC loop, the sampling rate of the converter automatically adapts to the input activities. Activity-dependent power consumption and data compression can thus be realized, saving system power, especially during time-sparse signal acquisition. Meanwhile, a SAR-assisted loop is exploited to resolve the loop-delay-induced distortion in conventional LC-ADC. Therefore, the resolution and power efficiency of the LC-ADC are improved effectively while maintaining the event-driven feature. Implemented in a 55nm process, the proposed LC-ADC achieves a scalable power consumption and a peak SNDR of 62.2dB for a 20kHz input. It also achieves a Walden FoM of 29.7fJ/conv.-step and a Schreier FoM of 158.6dB, which is best in class, without using off-chip calibration. Sub μW power is realized when the input frequency is below 1.5kHz. The proposed LC-ADC is also verified by simulated electrocardiogram (ECG), neural spike, and electromyogram (EMG) signals. It provides a ~7X data compression for ECG input, providing an attractive solution for time-sparse signal acquisition in biomedical applications.",
        "issn": {
            "Print ISSN": "1932-4545",
            "Electronic ISSN": "1940-9990"
        },
        "keywords": {
            "IEEE Keywords": [
                "Delays",
                "Signal resolution",
                "Time-frequency analysis",
                "Signal to noise ratio",
                "Jitter",
                "Clocks",
                "Power demand"
            ],
            "Author Keywords": [
                "level-crossing (LC)",
                "event-driven",
                "time-sparse signal",
                "data compression",
                "asynchronous",
                "Internet of Things (IoT)"
            ]
        },
        "title": "A 62.2dB SNDR Event-Driven Level-Crossing ADC with SAR-Assisted Delay Compensation Loop for Time-Sparse Biomedical Signal Acquisition"
    },
    {
        "authors": [
            "Cheng Dai",
            "Shuai Wei",
            "Shengxin Dai",
            "Sahil Garg",
            "Georges Kaddoum",
            "M. Shamim Hossain"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "02 September 2024",
        "doi": "10.1109/JIOT.2024.3453336",
        "publisher": "IEEE",
        "abstract": "Federated learning (FL) is a novel paradigm for distribute edge intelligence for the Internet-of-Vehicles (IoV) application, which can enable superior performance in model training without the need to share local data. However, in the actual architecture of federated learning, the existence of non-independent and identically distributed (non-IID) data at the edge device, along with the involvement of randomly participating distributed nodes, can result in model bias and a subsequent decrease in overall performance. To solve this problem, a new federated self-supervised learning method based on prototypes clustering contrastive learning (FedPCC) is proposed, which can effectively addresses the issue of asynchronous edge training and global model bias by introducing an unsupervised prototypes layer. The prototypes layer maps edge features to a global space and performs clustering, facilitating the new aggregation method of global prototypes on the server. Then, models from other components are aggregated based on data weight. Besides that, during the parameter deployment phase, we replace the prototype layer to acquire global knowledge, while employing momentum updates to preserve the local knowledge of the other components. Finally, to assess the efficacy of our proposed approach, we carried out comprehensive experiments across various datasets. The findings show that our method gains state-of-the-art performance, which also validates its effectiveness.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Prototypes",
                "Servers",
                "Data models",
                "Training",
                "Computational modeling",
                "Biological system modeling",
                "Contrastive learning"
            ],
            "Author Keywords": [
                "Federated learning",
                "Internet of Things (IoT)",
                "Self-supervised learning",
                "Contrastive learning"
            ]
        },
        "title": "Federated Self-Supervised Learning Based on Prototypes Clustering Contrastive Learning for Internet-of-Vehicles Applications"
    },
    {
        "authors": [
            "Agrippina Mwangi",
            "Nadine Kabbara",
            "Patrick Coudray",
            "Mikkel Gryning",
            "Madeleine Gibescu"
        ],
        "published_in": "Published in: IEEE Transactions on Network and Service Management ( Early Access )",
        "date_of_publication": "11 September 2024",
        "doi": "10.1109/TNSM.2024.3458447",
        "publisher": "IEEE",
        "abstract": "Next-generation offshore wind farms are increasingly adopting vendor-agnostic software-defined networking (SDN) to oversee their Industrial Internet of Things Edge (IIoT-Edge) networks. The SDN-enabled IIoT-Edge networks present a promising solution for high availability and consistent performance-demanding environments such as offshore wind farm critical infrastructure monitoring, operation, and maintenance. Inevitably, these networks encounter stochastic failures such as random component malfunctions, software malfunctions, CPU overconsumption, and memory leakages. These stochastic failures result in intermittent network service interruptions, disrupting the real-time exchange of critical, latency-sensitive data essential for offshore wind farm operations. Given the criticality of data transfer in offshore wind farms, this paper investigates the dependability of the SDN-enabled IIoT-Edge networks amid the highlighted stochastic failures using a two-pronged approach to: (i) observe the transient behavior using a proof-of-concept simulation testbed and (ii) quantitatively assess the steady-state behavior using a probabilistic Homogeneous Continuous Time Markov Model (HCTMM) under varying failure and repair conditions. The study finds that network throughput decreases during failures in the transient behavior analysis. After quantitatively analyzing 15 case scenarios with varying failure and repair combinations, steady-state availability ranged from 93% to 98%, nearing the industry-standard SLA of 99.999%, guaranteeing up to 3 years of uninterrupted network service.",
        "issn": {
            "Electronic ISSN": "1932-4537"
        },
        "keywords": {
            "IEEE Keywords": [
                "Wind farms",
                "Sensors",
                "Stochastic processes",
                "Industrial Internet of Things",
                "Wind turbines",
                "Servers",
                "Probabilistic logic"
            ],
            "Author Keywords": [
                "Industrial IoT",
                "software-defined networking",
                "edge computing",
                "IEEE802.1 Time Sensitive Networking",
                "IEC61850",
                "vPAC",
                "Homogeneous CTMM",
                "offshore wind",
                "dependability"
            ]
        },
        "title": "Investigating the Dependability of Software-Defined IIoT-Edge Networks for Next-Generation Offshore Wind Farms"
    },
    {
        "authors": [
            "Katarina Kostelić",
            "Darko Etinger"
        ],
        "published_in": "Published in: IEEE Engineering Management Review ( Early Access )",
        "date_of_publication": "06 September 2024",
        "doi": "10.1109/EMR.2024.3453974",
        "publisher": "IEEE",
        "abstract": "As an emergent virtual shared environment, the metaverse offers a novel approach to digital communication. However, its rapid expansion and development raise complex cybersecurity challenges. To address this issue, the goal of this paper is to map the scholarly contributions of cybersecurity within the metaverse through a comprehensive bibliometric analysis and examine trends, gaps, and future research directions. Following the PRISMA guidelines, a broad range of documents is included in the analysis, and a bibliometric software package was used for science mapping. The results reveal a significant increase in publications and citations in recent years, with contributors from across the globe. The analysis of keywords and abstracts reveals core themes and citation analysis identifies the most prominent sources and articles. The thematic analysis identifies the existing and emerging areas of research that remain to be explored in future studies.",
        "issn": {
            "Print ISSN": "0360-8581",
            "Electronic ISSN": "1937-4178"
        },
        "keywords": {
            "IEEE Keywords": [
                "Metaverse",
                "Computer security",
                "Bibliometrics",
                "Market research",
                "Artificial intelligence",
                "Blockchains",
                "Guidelines"
            ],
            "Author Keywords": [
                "AI",
                "AR",
                "authentication",
                "bibliometric analysis",
                "blockchain",
                "cyber-physical social systems",
                "cybersecurity",
                "iot",
                "metaverse",
                "mr",
                "privacy",
                "security",
                "systematic review",
                "vr"
            ]
        },
        "title": "Securing the Metaverse: A Bibliometric Analysis of Cybersecurity Challenges and Research Trajectories"
    },
    {
        "authors": [
            "Federico Montori",
            "Marek S. Tatara",
            "Pál Varga"
        ],
        "published_in": "Published in: IEEE Transactions on Automation Science and Engineering ( Early Access )",
        "date_of_publication": "12 February 2024",
        "doi": "10.1109/TASE.2024.3362132",
        "publisher": "IEEE",
        "abstract": "Engineering tools support the process of creating, operating, maintaining, and evolving systems throughout their lifecycle. Toolchains are sequences of tools that build on each others’ output during this procedure. The complete chain of tools itself may not even be recognized by the humans who utilize them, people may just recognize the right tool being used at the right place in time. Modern engineering processes, however, do not value such ad-hoc choice of tooling, because of their uncontrolled nature. Building upon the Extended Automation Engineering Model defined by the IEC 81346 standard, this paper proposes to automate the toolchain building and execution process for Cyber-Physical System of Systems (CPSoS), utilizing key principles of the Eclipse Arrowhead framework. The proposed toolchain automation solution addresses issues such as tool interoperability, interaction, automation, and dynamic choreography. The feasibility of this set of integrated concepts is validated through an Arrowhead-based toolchain choreography demonstration. Note to Practitioners —The paper discusses approaches to the automated execution of various industry-related processes. As the processes are becoming more complex and involve numerous systems which have to be orchestrated, a simple and preprogrammed workflow is not enough anymore. Therefore, building on top of the principles of the Eclipse Arrowhead framework, an adequate model of toolchains, allowing for their automated execution, is proposed. Different approaches to supervision of toolchain execution are discussed showing the benefits of reaching higher automation levels. Further, four adoption levels are introduced, which are a measure of the toolchain automation progress. Finally, a simplified demonstrator is shown and steps to elevate it to higher adoption levels are highlighted. To ensure that the approach is industry-oriented, several examples of how the proposed methodology can be used in the industrial context are discus...",
        "issn": {
            "Print ISSN": "1545-5955",
            "Electronic ISSN": "1558-3783"
        },
        "keywords": {
            "IEEE Keywords": [
                "Automation",
                "Interoperability",
                "IEC Standards",
                "Software",
                "Robot sensing systems",
                "Microservice architectures",
                "Europe"
            ],
            "Author Keywords": [
                "Toolchains",
                "industry 4.0",
                "interoperability",
                "engineering process",
                "IoT automation",
                "service oriented architecture",
                "service orchestration",
                "service choreography"
            ]
        },
        "title": "Dynamic Execution of Engineering Processes in Cyber-Physical Systems of Systems Toolchains"
    },
    {
        "authors": [
            "Haijun Liao",
            "Jinchao Fan",
            "Haoyu Ci",
            "Jiahua Gu",
            "Zhenyu Zhou",
            "Bin Liao",
            "Xiaoyan Wang",
            "Shahid Mumtaz"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "11 September 2024",
        "doi": "10.1109/JIOT.2024.3444450",
        "publisher": "IEEE",
        "abstract": "In this paper, we address the key problem of sensing and communication integrated resource allocation for 6G empowered distribution grid hierarchical coordinated control. Firstly, we construct a novel information timeliness metric for electric semantic communication, namely peak age of semantics (PAoS), which covers the entire lifecycle of information sensing, semantic compression, semantic transmission, and semantic decoding. Secondly, we propose a sensing and semantic communication integrated resource allocation algorithm based on Top-N2 and hybrid knowledge-statistic driven fuzzy reinforcement learning. A deep fuzzy neural network is utilized to build a knowledge model between the grid operating state and decision making. The knowledge is embedded into statistic driven model of reinforcement learning to enhance accuracy of upper confidence bound (UCB) utility evaluation. Finally, simulations based on realistic application scenarios indicate that compared with two comparison algorithms, the proposed algorithm reduces average PAoS by 4.72% and 9.49%, and the maximum PAoS by 5.76% and 13.57%. Additionally, its end-to-end delay trend and semantic packet decoding success rate align more closely with semantic importance.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Sensors",
                "Resource management",
                "Decoding",
                "Optimization",
                "Internet of Things",
                "6G mobile communication"
            ],
            "Author Keywords": [
                "6G",
                "IoT",
                "distribution grid",
                "hierarchical collaborative control",
                "integrated sensing and communication",
                "peak age of semantics",
                "fuzzy reinforcement learning"
            ]
        },
        "title": "Electric Semantic Compression Based 6G Wireless Sensing and Communication Integrated Resource Allocation"
    },
    {
        "authors": [
            "Kaddour Messaoudi",
            "Abdullah Baz",
            "Omar Sami Oubbati",
            "Abderrezak Rachedi",
            "Tahar Bendouma",
            "Mohammed Atiquzzaman"
        ],
        "published_in": "Published in: IEEE Transactions on Cognitive Communications and Networking ( Early Access )",
        "date_of_publication": "30 April 2024",
        "doi": "10.1109/TCCN.2024.3394859",
        "publisher": "IEEE",
        "abstract": "Meeting the stringent requirements of real-time Internet of Things (IoT) is a bit challenging than expected. Indeed, IoT devices generate massive amounts of data through their sensing features and face some constraints in timely transmitting sensed data to collectors. To overcome this problem, Unmanned Aerial Vehicles (UAVs) are deployed to act as data collectors for IoT devices as they significantly improve the freshness of collected data in real-time applications. Nevertheless, IoT devices and UAVs have limited energy capacity. In this paper, we tackle the energy concern of IoT devices by utilizing UAVs as data collectors and energy transmitters and by promptly charging IoT devices whenever necessary. As for the energy concern of UAVs, we deploy a set of Unmanned Ground Vehicles (UGVs) to energy supply UAVs, allowing them to complete their tasks successfully. Our objective is to employ a multi-agent reinforcement learning method for optimally controlling the trajectories of both UGVs and UAVs so that it jointly decreases their energy consumption, reduces the Age of Information (AoI) of IoT devices, and timely charges UAVs and avoids their failures. We conducted a series of tests using a simulation tool to validate the effectiveness of the approach.",
        "issn": {
            "Electronic ISSN": "2332-7731"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Autonomous aerial vehicles",
                "Data collection",
                "Task analysis",
                "Charging stations",
                "Trajectory",
                "Monitoring"
            ],
            "Author Keywords": [
                "UGV",
                "Age of Information (AoI)",
                "UAV",
                "Wireless Power Transfer (WPT)",
                "Reinforcement Learning"
            ]
        },
        "title": "UGV Charging Stations for UAV-Assisted AoI-Aware Data Collection"
    },
    {
        "authors": [
            "Jehad Ali",
            "Houbing Herbert Song",
            "Byeong-hee Roh"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "23 September 2024",
        "doi": "10.1109/JIOT.2024.3465609",
        "publisher": "IEEE",
        "abstract": "In 5G and Beyond-based Internet-of-Things (IoT) sensor networks, the end-to-end (E2E) route traverses via multiple heterogeneous network domains, necessitating inter-domain interaction to guarantee and confirm quality-of-service (QoS) for low power IoT devices applications. Moreover, in heterogeneous IoT sensor networks, the E2E path often encompasses domains with diverse QoS parameters or classes. The unique E2E requirements for delay, packet loss ratio (PLR), and other factors present further challenges. However, existing legacy network architectures and typical software-defined networking (SDN) models lack effective strategies for QoS provisioning tailored to the service requests of IoT low power sensor devices. To address these issues, this study proposes a novel multi-objective SDN-based framework for IoT sensors, ensuring E2E QoS across multiple domains with heterogeneous traffic service classes (TSC). A two-layer software-defined networking (SDN) framework is presented to provision QoS for IoT sensors based on their specific service demands at the E2E network level. Central to the framework is the deployment of an optimal additive weighting module (OAWM), facilitating TSC ranking according to their weights and incorporating a priority mechanism for specific service parameters such as delay, PLR, and jitter. Additionally, the global controller statistics enable the provisioning of E2E QoS by mapping the service requests from IoT sensors. Experimental evaluations are conducted to compare the proposed approach with existing schemes. The results validate the effectiveness of our proposed method, demonstrating improved E2E QoS provisioning and meeting the specific requirements of IoT sensors in precision agriculture with low-power IoT devices.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Quality of service",
                "Internet of Things",
                "Precision agriculture",
                "Monitoring",
                "Delays",
                "Collaboration",
                "Real-time systems"
            ],
            "Author Keywords": [
                "Internet-of-Things",
                "SDN",
                "Intelligent networking architecture",
                "QoS",
                "Precision agriculture"
            ]
        },
        "title": "An SDN-Based Framework for E2E QoS Guarantee in Internet-of-Things Devices"
    },
    {
        "authors": [
            "Chi-Hsin Yang",
            "Bernard Mwangi Maina",
            "Shin-Ming Cheng",
            "Hahn-Ming Lee"
        ],
        "published_in": "Published in: IEEE Consumer Electronics Magazine ( Early Access )",
        "date_of_publication": "17 October 2024",
        "doi": "10.1109/MCE.2024.3482700",
        "publisher": "IEEE",
        "abstract": "The proliferation of the Internet of Things (IoT) and Artificial Intelligence (AI) has transformed traditional consumer electronics (CEs) into next-generation devices with enhanced intelligence and connectivity. However, this advancement has exposed CEs to cybersecurity threats such as IoT botnets. Consequently, researchers are employing AI for proactive threat detection and prevention. Unfortunately, AI algorithms are vulnerable to adversarial attacks, necessitating robustness studies, as evaded malware can cause significant damage to already susceptible IoT CEs. This paper presents a case study to evaluate the resilience of AI-based IoT malware detection systems against adversarial attacks. Specifically, our method involves inserting crafted binary code snippets (payloads) into the empty regions of malware executables. We leverage explainable AI (XAI) techniques to guide payload generation, coupled with an optimization procedure to efficiently identify optimal payload sequences. Our method, tested on real-world IoT datasets, yields a robust hybrid detection system with a detection rate of up to 99.11%. Our attack approach achieves evasion rates of up to 100% and generates transferable adversarial examples. The generated samples evade a prominent structural IoT malware detector with an evasion rate of 95.15% at a minimal attack cost. This study underscores the importance of enhancing the robustness of AI-based malware detection systems and implementing diverse strategies to safeguard consumer IoT devices.",
        "issn": {
            "Print ISSN": "2162-2248",
            "Electronic ISSN": "2162-2256"
        },
        "keywords": {
            "IEEE Keywords": [
                "Malware",
                "Detectors",
                "Feature extraction",
                "Internet of Things",
                "Payloads",
                "Consumer electronics",
                "Codes",
                "Semantics",
                "Explainable AI",
                "Training"
            ],
            "Author Keywords": []
        },
        "title": "An Adversarial Attack on Artificial Intelligence Malware Detection in Consumer Internet of Things"
    },
    {
        "authors": [
            "Talha Khan",
            "Sandeep Narayanan Kadan Veedu",
            "Andras Racz",
            "Mehrnaz Afshang",
            "Andreas Hoglund",
            "Johan Bergman"
        ],
        "published_in": "Published in: IEEE Communications Magazine ( Early Access )",
        "date_of_publication": "02 September 2024",
        "doi": "10.1109/MCOM.001.2300795",
        "publisher": "IEEE",
        "abstract": "6G presents new opportunities to enrich the cellular ecosystem by introducing battery-less Zero-energy Internet of things (ZE-IoT) devices, thus unleashing an era of massive, sustainable, and smart connectivity. This explains the increased interest in ZE-IoT in academia and industry. The road to a 6G future empowered by ZE-IoT entails cohesive efforts in the realm of standardization, academic research, and industrial trials, which are synergistic with the anticipated market demands and the dominant technology direction. In this article, we provide a holistic view of a 6G ZE-IoT future, informed by the ongoing standardization activities in the 3rd generation partnership project (3GPP) for ZE-IoT, the role of the emerging technology trends, such as digital twins and artificial intelligence, and the technical challenges in integrating ZE-IoT into the cellular ecosystem. Finally, we present recent research results to address some of the discussed challenges.",
        "issn": {
            "Print ISSN": "0163-6804",
            "Electronic ISSN": "1558-1896"
        },
        "keywords": {
            "IEEE Keywords": [
                "3GPP",
                "Power demand",
                "Internet of Things",
                "Energy storage",
                "6G mobile communication",
                "RF signals",
                "Object recognition"
            ],
            "Author Keywords": []
        },
        "title": "Toward 6G Zero-Energy Internet of Things: Standards, Trends, and Recent Results"
    },
    {
        "authors": [
            "Amirmohammad Pasdar",
            "Nickolaos Koroniotis",
            "Marwa Keshk",
            "Nour Moustafa",
            "Zahir Tari"
        ],
        "published_in": "Published in: IEEE Transactions on Sustainable Computing ( Early Access )",
        "date_of_publication": "14 August 2024",
        "doi": "10.1109/TSUSC.2024.3443256",
        "publisher": "IEEE",
        "abstract": "The Internet of Things (IoT) has enabled pervasive networking and multi-modal sensing, offering various services such as remote operations and augmenting existing processes. The military setting has increasingly and notably adopted IoT technologies, such as sensor-rich drones or autonomous vehicles, which provide military personnel with enhanced situational awareness, faster decision-making capabilities, and improved operational precision. However, integrating IoT into military systems introduces new security challenges due to increased connectivity and susceptibility to vulnerabilities. Cyberattacks on military IoT systems can have severe consequences, including operational disruptions and compromises of sensitive information. This article proposes a new perspective on examining threat models in IoT-enhanced combat systems, emphasising approaches for identifying threats, conducting vulnerability assessments, and suggesting countermeasures. It delves into the characteristics and structures of IoT-enhanced combat systems, exploring technical implementations and technologies. Additionally, it outlines five significant areas of focus, including blockchain, machine learning, game theory, protocols, and algorithms, to enhance understanding of IoT-enhanced combat systems. The insights gained from this analysis can inform the development of secure and resilient military IoT systems, ultimately enhancing the safety and effectiveness of military operations.",
        "issn": {
            "Electronic ISSN": "2377-3782"
        },
        "keywords": {
            "IEEE Keywords": [
                "Security",
                "Internet of Things",
                "Threat modeling",
                "Unified modeling language",
                "Resilience",
                "Prevention and mitigation",
                "Green computing"
            ],
            "Author Keywords": [
                "Combat systems",
                "cyber threat models",
                "internet of battle things",
                "internet of military things",
                "Internet of Things"
            ]
        },
        "title": "Cybersecurity Solutions and Techniques for Internet of Things Integration in Combat Systems"
    },
    {
        "authors": [
            "Son Dinh-Van",
            "Hien Quoc Ngo",
            "Simon L. Cotton",
            "Yuen Kwan Mo",
            "Matthew D. Higgins"
        ],
        "published_in": "Published in: IEEE Open Journal of the Communications Society ( Early Access )",
        "date_of_publication": "05 November 2024",
        "doi": "10.1109/OJCOMS.2024.3491354",
        "publisher": "IEEE",
        "abstract": "This paper considers wireless power transfer (WPT) for powering low-power devices in massive Machine Type Communication (mMTC) using a distributed massive multiple-input multiple-output (MIMO) system. Each Internet of Things (IoT) device can be served by one or more access points (APs) which is equipped with a massive antenna array. During each time slot, each IoT device transmits pilot sequences to enable APs to perform channel estimation. This process is followed by the WPT using conjugate beamforming. The approach to transmission power control is formulated as a non-convex optimization problem aiming to maximize the total accumulated power achieved by all IoT devices while taking into account the power weights at the APs, pilot power control at the IoT devices, and the non-linearity of practical energy harvesting circuits. An alternating optimization approach is adopted to solve it iteratively, achieving convergence within just a few iterations. Furthermore, since the number of IoT devices might be enormous in mMTC networks, we propose a pilot sharing algorithm allowing IoT devices to reuse pilot sequences effectively. Numerical results are provided to validate the effectiveness of the proposed power control algorithms and the pilot sharing scheme. It is shown that by allowing IoT devices to share the pilot sequences instead of employing the orthogonal pilots, the per-user accumulated performance is enhanced considerably, especially when the number of IoT devices is large relative to the coherence interval. The advantage of using distributed massive MIMO compared to its collocated counterpart is demonstrated in terms of the per-user accumulated power.",
        "issn": {
            "Electronic ISSN": "2644-125X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Massive MIMO",
                "Channel estimation",
                "Performance evaluation",
                "Power control",
                "Optimization",
                "Energy harvesting",
                "Batteries",
                "Wireless power transfer",
                "Uplink"
            ],
            "Author Keywords": [
                "5G",
                "Cell-free massive MIMO",
                "Distributed massive MIMO",
                "Internet of Things",
                "mMTC",
                "Wireless Power Transfer"
            ]
        },
        "title": "Distributed Massive MIMO for Wireless Power Transfer in the Industrial Internet of Things"
    },
    {
        "authors": [
            "Yusen Wu",
            "Ye Hu",
            "Mingzhe Chen",
            "Yelena Yesha",
            "Mérouane Debbah"
        ],
        "published_in": "Published in: IEEE Network ( Early Access )",
        "date_of_publication": "10 June 2024",
        "doi": "10.1109/MNET.2024.3410640",
        "publisher": "IEEE",
        "abstract": "Internet of Things (IoT) services necessitate the storage, transmission, and analysis of diverse data for inference, autonomy, and control. Blockchains, with their inherent properties of decentralization and security, offer efficient database solutions for these devices through consensus-based data sharing. However, it’s essential to recognize that not every blockchain system is suitable for specific IoT applications, and some might be more beneficial when excluded with privacy concerns. For example, public blockchains are not suitable for storing sensitive data. This paper presents a detailed review of three distinct blockchains tailored for enhancing IoT applications. We initially delve into the foundational aspects of three blockchain systems, highlighting their strengths, limitations, and implementation needs. Additionally, we discuss the security issues in different blockchains. Subsequently, we explore the blockchain’s application in three pivotal IoT areas: edge AI, communications, and healthcare. We underscore potential challenges and the future directions for integrating different blockchains in IoT. Ultimately, this paper aims to offer a comprehensive perspective on the synergies between blockchains and the IoT ecosystem, highlighting the opportunities and complexities involved.",
        "issn": {
            "Print ISSN": "0890-8044",
            "Electronic ISSN": "1558-156X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Blockchains",
                "Internet of Things",
                "Security",
                "Smart contracts",
                "Data privacy",
                "Artificial intelligence",
                "Training"
            ],
            "Author Keywords": []
        },
        "title": "Blockchains for Internet of Things: Fundamentals, Applications, and Challenges"
    },
    {
        "authors": [
            "Ao Liu",
            "Jing Chen",
            "Ruiying Du",
            "Cong Wu",
            "Yebo Feng",
            "Teng Li",
            "Jianfeng Ma"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "23 October 2024",
        "doi": "10.1109/JIOT.2024.3484996",
        "publisher": "IEEE",
        "abstract": "The rapid expansion of Internet of Things (IoT) has resulted in vast, heterogeneous graphs that capture complex interactions among devices, sensors, and systems. Efficient analysis of these graphs is critical for deriving insights in IoT scenarios such as smart cities, industrial IoT, and intelligent transportation systems. However, the scale and diversity of IoT-generated data present significant challenges, and existing methods often struggle with preserving the structural integrity and semantic richness of these complex graphs. Many current approaches fail to maintain the balance between computational efficiency and the quality of the insights generated, leading to potential loss of critical information necessary for accurate decision-making in IoT applications. We introduce, a novel sampling method designed to address these challenges by preserving the structural integrity, node and edge type distributions, and semantic patterns of IoT-related graphs. works by incorporating the novel top-leader selection, balanced neighborhood expansion, and meta-path guided sampling strategies. The key idea is to leverage the inherent heterogeneous structure and semantic relationships encoded by meta-paths to guide the sampling process. This approach ensures that the resulting subgraphs are representative of the original data while significantly reducing computational overhead. Extensive experiments demonstrate that outperforms state-of-the-art methods, achieving up to 15% higher F1 scores in tasks such as link prediction and node classification, while reducing runtime by 20%.These advantages make a transformative tool for scalable and accurate IoT applications, enabling more effective and efficient analysis of complex IoT systems, ultimately driving advancements in smart cities, industrial IoT, and beyond.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Semantics",
                "Sampling methods",
                "Computational efficiency",
                "Accuracy",
                "Vectors",
                "Smart cities",
                "Representation learning",
                "Motion pictures",
                "Transportation"
            ],
            "Author Keywords": [
                "Heterogeneous graphs",
                "graph sampling",
                "node embedding",
                "graph representation learning"
            ]
        },
        "title": "HETEROSAMPLE: Meta-path Guided Sampling for Heterogeneous Graph Representation Learning"
    },
    {
        "authors": [
            "Jean Pierre Nyakuri",
            "Celestin Nkundineza",
            "Omar Gatera",
            "Kizito Nkurikiyeyezu"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "05 September 2024",
        "doi": "10.1109/ACCESS.2024.3455244",
        "publisher": "IEEE",
        "abstract": "Plant pest and disease management, especially in the early stages of infestation, is a critical challenge that poses significant threats and has potential to devastate agricultural crops, causing total yield loss and food insecurity. Traditional inspection methods are time-consuming and prone to errors due to limited labor expertise. Therefore, to tackle these challenges, harnessing advanced technologies such as artificial intelligence (AI), Machine Learning/Deep Learning (ML/DL), and Internet of Things (IoT) is essential for managing and mitigating agriculture hazards. This research presents a comprehensive review of the state-of-the-art DL architectures integrated with IoT-based systems applied to plant pest and disease detection (PPDD) by investigating different potential approaches that have been employed using DL and IoT up to the year 2024 to address challenges in agriculture. Convolutional Neural Network (CNN) architectures for image recognition, object detection, and their integration with IoT, embedded into mobile devices and unmanned aerial vehicles (UAV) are explored. Moreover, the research discusses the advantages and limitations of these techniques, emphasizing their architecture design, efficiency and accuracy. The findings demonstrate that there is a tradeoff between robustness and complexity among existing techniques, and authors recommend future trends aimed at creating robust models with fewer parameters that are more accurate and easily implementable on small IoT-based and portable devices suitable for in-field and real-time applications. Furthermore, while existing review papers discuss either DL or IoT separately, this research paper uniquely focuses on their combined models, providing a comprehensive overview of the synergistic potential of leveraging IoT-driven technologies alongside advanced DL algorithms to ease the task of researchers in the field of precision agriculture particularly in PPDD.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Crops",
                "Internet of Things",
                "Reviews",
                "Agriculture",
                "Image recognition",
                "Accuracy",
                "Deep learning",
                "Plant diseases",
                "Convolutional neural networks"
            ],
            "Author Keywords": [
                "Deep Learning",
                "Machine Learning",
                "Crop Pest Detection",
                "Crop Disease Detection",
                "Convolutional Neural Networks",
                "Internet of Things"
            ]
        },
        "title": "State-of-the-art Deep Learning Algorithms for Internet of Things-based Detection of Crop Pests and Diseases: A Comprehensive Review"
    },
    {
        "authors": [
            "Danyal Namakshenas",
            "Abbas Yazdinejad",
            "Ali Dehghantanha",
            "Gautam Srivastava"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "14 March 2024",
        "doi": "10.1109/TCE.2024.3377550",
        "publisher": "IEEE",
        "abstract": "The Internet of Things (IoT) has significantly impacted the evolution of consumer-oriented smart environments, primarily due to its capacity for transformative device-to-device communication. While this capability enhances user convenience and experience in the Consumer IoT sector, it also generates vast amounts of data. While beneficial for consumer insight and electronic optimization, this data is vulnerable to security breaches. We focus on Machine Learning-based threat detection systems to address these challenges within the Consumer IoT. While effective in recognizing threats, these systems often overlook crucial privacy considerations, a critical aspect in the realm of consumer devices. To counter this, Federated Learning (FL) emerges as a promising solution for maintaining data privacy in Consumer IoT. However, FL faces its own challenges, especially when dealing with malicious clients. This paper addresses two primary challenges in Consumer IoT threat detection. First, we tackle an unaddressed issue in FL: the rigorous validation of its clients. The advent of quantum computing could render traditional validation techniques obsolete. We introduce a quantum-centric registration and authentication strategy to overcome this, ensuring stringent client validation in an FL framework. The second challenge involves protecting clients’ model weights within FL. We propose the integration of Additive Homomorphic Encryption into our model, offering a robust solution that secures the privacy of FL participants without compromising on computational efficiency. Our empirical results underscore the efficacy of our approach, achieving an average accuracy of 94.93% on the N-baIoT dataset and 91.93% on the Edge-IIoTset dataset, showcasing consistent and robust performance across diverse client configurations. This approach can potentially significantly improve the security and privacy landscape of Consumer IoT.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Privacy",
                "Security",
                "Internet of Things",
                "Data privacy",
                "Data models",
                "Authentication",
                "Fuzzy logic"
            ],
            "Author Keywords": [
                "CIoT",
                "FL",
                "Quantum Computing",
                "Privacy"
            ]
        },
        "title": "Federated Quantum-Based Privacy-Preserving Threat Detection Model for Consumer Internet of Things"
    },
    {
        "authors": [
            "Fangxiao Li",
            "Leyi Shi",
            "Yuchen Zhao",
            "Haoyu Zhang",
            "Zhihao Zhao",
            "Qiang Han"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "03 October 2024",
        "doi": "10.1109/JIOT.2024.3473528",
        "publisher": "IEEE",
        "abstract": "With the development of Internet of Things (IoT), more and more devices are connected to the network. Moving Target Defense (MTD) provides a security solution for many IoT network devices. However, since MTD is deployed in a packet switching network, there are problems such as packet delay and congestion. Traditional follow-up synchronization scheme is not competent for high-rate change of MTD. This poses a serious security threat to IoT devices because slow rate changes are powerless in defending against highly adversarial network attacks. In this paper, we innovatively propose an MTD synchronization method, which uses synchronization certificates to complete synchronization. Furthermore, we establish a fast switching MTD (CMTD) model based on Cryptography Fundamental Logics (CFL). It improves MTD switching rate to milliseconds and solves the problem of synchronization in high speed hopping with high covert requirements. Theoretical analysis and experimental verification show that our method effectively improves the availability and concealment of IoT devices under high-speed hopping. It is of great significance for IoT security protection under high-intensity cyber attacks.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Synchronization",
                "Internet of Things",
                "Authentication",
                "Security",
                "Servers",
                "Denial-of-service attack",
                "Switches",
                "Spread spectrum communication",
                "Protection",
                "Process control"
            ],
            "Author Keywords": [
                "Intnet of Things Security",
                "Moving Target Defense",
                "Cryptography Fundamental Logics",
                "Active Defense",
                "High-speed Hopping"
            ]
        },
        "title": "CMTD:A fast Moving Target Defense Scheme based on CFL Authentication"
    },
    {
        "authors": [
            "Yuhao Hu",
            "Xiaolong Xu",
            "Li Duan",
            "Muhammad Bilal",
            "Qingyang Wang",
            "Wanchun Dou"
        ],
        "published_in": "Published in: IEEE Transactions on Fuzzy Systems ( Early Access )",
        "date_of_publication": "11 June 2024",
        "doi": "10.1109/TFUZZ.2024.3412971",
        "publisher": "IEEE",
        "abstract": "Deep Neural Networks (DNN) has been widely applied in big data-driven Internet of Things (IoT) for excellent learning ability, while the black-box nature of DNN leads to uncertainty of inference results. With higher interpretability, Convolutional Fuzzy Neural Network (CFNN) becomes an alternative choice for the model of IoT applications. IoT applications are often latency-sensitive. By jointly utilizing computing power of IoT devices and edge servers, end-edge collaborative CFNN inference improves the insufficiency of local computing resources and reduces the latency of computing-intensive CFNN inference. However, the calculation amount of fuzzy layers is hard to get directly, bringing difficulty to CFNN partition. Additionally, the profit of service providers is often ignored in existing work on distributed inference. In this paper, an end-edge collaborative inference framework of CFNNs for big data-driven IoT, named DisCFNN, is proposed. Specifically, a novel CFNN structure and a method of fuzzy layer calculation amount assessment are designed at first. Next, computing resource allocation and CFNN partition decisions are generated on each edge server based on deep reinforcement learning. Then, each IoT device sends the request of CFNN inference service to a certain edge server or infer the whole CFNN locally according to the task offloading strategy obtained through many-to-one matching game. Finally, the effectiveness of DisCFNN is evaluated through extensive experiments.",
        "issn": {
            "Print ISSN": "1063-6706",
            "Electronic ISSN": "1941-0034"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Servers",
                "Computational modeling",
                "Collaboration",
                "Task analysis",
                "Image edge detection",
                "Energy consumption"
            ],
            "Author Keywords": [
                "Convolutional Fuzzy Neural Network",
                "big data-driven",
                "Internet of Things",
                "end-edge collaborative inference"
            ]
        },
        "title": "End-Edge Collaborative Inference of Convolutional Fuzzy Neural Networks for Big Data-Driven Internet of Things"
    },
    {
        "authors": [
            "Dongqing Liu",
            "Yongwen Liu",
            "Lyes Khoukhi",
            "Abdelhakim Hafid",
            "Lei Zhang",
            "Bo Wang"
        ],
        "published_in": "Published in: Tsinghua Science and Technology ( Early Access )",
        "date_of_publication": "24 September 2024",
        "doi": "10.26599/TST.2024.9010088",
        "publisher": "TUP",
        "abstract": "Mobile Edge Computing (MEC) has been proposed to enhance the performance of Internet of Things (IoTs) devices by offloading computation-intensive tasks to nearby edge clouds, while Non-Orthogonal Multiple Access (NOMA) enables multiple IoTs devices to share subcarriers with varying power levels, making it ideal for computation offloading. Despite the potential benefits, integrating NOMA with MEC presents complex challenges, including resource allocation, decision optimization, and balancing energy efficiency with completion time. In this paper, we address the computation offloading and resource allocation problem in NOMA-MEC enabled IoT networks, aiming to minimize completion time and maximize energy efficiency while meeting processing latency requirements. Our model supports partial computation offloading, allowing devices to partition tasks for both local execution and offloading to the edge clouds. To this end, we first introduce two processes, i.e., infeasible tasks elimination and admission control, to improve algorithm efficiency. Then, we propose an iterative algorithm, comprising two low-complexity sub-algorithms, to address various optimization aspects, including CPU frequency allocation, offloading decisions, time allocation, transmit power control, and network resource allocation. Extensive simulations validate that our approach outperforms existing methods in terms of completion time, total saved energy, and offloading ratio.",
        "issn": {
            "Electronic ISSN": "1007-0214"
        },
        "keywords": {
            "IEEE Keywords": [],
            "Author Keywords": [
                "computation offloading",
                "Non-Orthogonal Multiple Access (NOMA)",
                "Mobile Edge Computing (MEC)",
                "resource allocation",
                "admission control"
            ]
        },
        "title": "Efficient time and energy optimization in NOMA-enabled mobile edge computing through partial offloading"
    },
    {
        "authors": [
            "Mohamad Arafeh",
            "Ahmad Hammoud",
            "Mohsen Guizani",
            "Azzam Mourad",
            "Hadi Otrok",
            "Hakima Ould-Slimane",
            "Zbigniew Dziong",
            "Chang-Dong Wang",
            "Di Wu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "24 September 2024",
        "doi": "10.1109/JIOT.2024.3467110",
        "publisher": "IEEE",
        "abstract": "Federated learning gained importance in sensitive IoT environments by creating a privacy-preserving ecosystem where participants share machine-learning models instead of raw data. However, federated learning shifts data control away from the server, exposing it to Non-Independent and Identically Distributed (non-IID) problems caused by biased clients (IoT devices). This hinders the learning process by increasing execution time and cost. Current solutions alter the federated learning structure or compromise privacy by offloading clients’ raw data to an external server. To mitigate these limitations, this paper proposes a solution to the non-IID problem by introducing an initialization phase, orchestrated by the server, that constructs high-quality initial models. These models can boost federated learning accuracy and convergence, regardless of whether IoT participants exhibit non-IID properties. Our proposed initialization scheme involves clients training over the same model sequentially, lessening the impact of aggregation, a primary cause of model degradation in federated approaches. Furthermore, a regulator algorithm deployed on the server maintains model integrity and mitigates catastrophic forgetting, enhanced by a client selection process that emphasizes the compatibility of IoT clients to cooperate effectively. Moreover, we devise an optimization scheme based on clustering and genetic algorithms to reduce the selection time while ensuring optimal performance in IoT networks. Experiments on MNIST, KDD, and CIFAR10 datasets show promising results in terms of initial model resiliency against catastrophic forgetting and non-IID settings. Additionally, our findings suggest that our approach can significantly enhance federated learning training in IoT applications by achieving 40% higher initialization accuracy and a 20% average improvement in end results compared to conventional methods, all while reducing computation time by 80% compared to similar approaches.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Federated learning",
                "Data models",
                "Training",
                "Internet of Things",
                "Servers",
                "Biological system modeling",
                "Regulators"
            ],
            "Author Keywords": [
                "Federated Learning",
                "Sequential Learning",
                "Non-IID Problem",
                "Catastrophic Forgetting",
                "Internet of Things"
            ]
        },
        "title": "WFSL: Warmup-Based Federated Sequential Learning"
    },
    {
        "authors": [
            "Danish Attique",
            "Wang Hao",
            "Wang Ping",
            "Danish Javeed",
            "Prabhat Kumar"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "03 April 2024",
        "doi": "10.1109/JIOT.2024.3384374",
        "publisher": "IEEE",
        "abstract": "The Industrial Internet of Things (IIoT) is rapidly evolving, and with this evolution, cyber threats have become a significant issue. IIoT networks, despite improving service quality, are uniquely vulnerable to security threats due to their inherent connectivity and the use of low-power devices. Traditional Deep Learning-based IDS, while accurate, suffer from a “black box” issue that hides the reasoning behind their decisions, leading to a decrease in user trust. To address this, our research presents an Explainable and intelligent mechanism for data-efficient intrusion detection in IIoT. Our proposed IDS enhances data efficiency by employing a Bidirectional Long-Short Term Memory (BiLSTM) model with a self-adaptive attention mechanism. The selfadaptive attention mechanism is a novel feature of our IDS framework, designed specifically for IIoT environments. This mechanism dynamically adjusts its focus to prioritize critical elements within a dataset, allocating more computational resources to data segments likely to contain patterns or anomalies indicative of security threats. When integrated with BiLSTM, which excels at capturing temporal dependencies, the mechanism enhances the IDSs ability to learn efficiently from limited datasets. This focus on significant data features and temporal patterns reduces the need for extensive training datasets, making it particularly effective in IIoT settings where data may be sparse yet complex. In addition, we enhance the proposed IDSs transparency by incorporating the SHapley Additive exPlanations mechanism from Explainable AI, thereby boosting the IDSs trustworthiness and interpretability. Our system exhibits outstanding performance on benchmark datasets such as CICIDS2017 and X-IIoTID, attaining accuracies of 99.92% and 96.54%, respectively.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Industrial Internet of Things",
                "Security",
                "Long short term memory",
                "Industries",
                "Computational modeling",
                "Closed box",
                "Logic gates"
            ],
            "Author Keywords": [
                "Cyber Attacks",
                "Explainable AI (XAI)",
                "Intrusion Detection System (IDS)",
                "IIoT",
                "Proactive Defense"
            ]
        },
        "title": "Explainable and Data-Efficient Deep Learning for Enhanced Attack Detection in IIoT Ecosystem"
    },
    {
        "authors": [
            "Yuqiong Zhang",
            "Yuedan Zhou",
            "Zhongke Lei",
            "Jun Zhou",
            "Pengju Kuang",
            "Yifeng Liu",
            "Chengwei Xian",
            "Gang Li",
            "Guangjun Wen",
            "Yongjun Huang"
        ],
        "published_in": "Published in: IEEE Transactions on Antennas and Propagation ( Early Access )",
        "date_of_publication": "02 October 2024",
        "doi": "10.1109/TAP.2024.3468458",
        "publisher": "IEEE",
        "abstract": "In this paper, a novel optomechanical metasurface for manipulating reflective phase and polarization state of linearly polarized electromagnetic (EM) wave is proposed. The presented MS consists of an electric-LC (ELC) resonators printed on a low-loss flexible printed circuit (FPC) substrate, an air gap in the middle layer, and a metallic substrate on the back ground. The phase modulation is realized via tuning geometrical parameters of ELC, while polarization modulation is achieved by illuminating different power levels of incident EM waves to flexibly control the ratio of polarization conversion. In particular, reconfigurable characteristics of the proposed metasurface resulting from magneto-mechanical coupling are demonstrated and validated respectively. As a proof of concept, a reconfigurable beam deflector and a reconfigurable polarization converter based on proposed optomechanical metasurface are designed, fabricated, and measured. The results show that the proposed reconfigurable beam deflector achieves beam reflection of 34 ° within bandwidth (BW) from 27.84 GHz to 28.43 GHz and the reconfigurable polarization converter enables a linear-to-circular polarization conversion with a 3-dB BW from 26.36 GHz to 28.32 GHz. In addition, two devices will correspondingly generate specular reflective beam and realize linear-to-cross polarization conversion when increasing the incident power of EM waves. We envision optomechanical metasurface proposed here to hold possibilities for beam pointing and message encryption applied in wireless and radar communication.",
        "issn": {
            "Print ISSN": "0018-926X",
            "Electronic ISSN": "1558-2221"
        },
        "keywords": {
            "IEEE Keywords": [
                "Metasurfaces",
                "Force",
                "Resonators",
                "Couplings",
                "Air gaps",
                "Substrates",
                "Structural beams",
                "Deformation",
                "Phase modulation",
                "Flexible printed circuits"
            ],
            "Author Keywords": [
                "Optomechanical metasurface",
                "magneto-echanical coupling",
                "Reconfigurable beam defection",
                "Reconfigurable polarization conversion"
            ]
        },
        "title": "Reconfigurable Beam Deflector and Polarization Converter Using Optomechanical Metasurface"
    },
    {
        "authors": [
            "Longtian Fu",
            "Qi Zhang",
            "Lizhi Lin",
            "Ruel Reyes"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "26 January 2024",
        "doi": "10.1109/ACCESS.2024.3358897",
        "publisher": "IEEE",
        "abstract": "To improve the accuracy and time efficiency of medical brain image segmentation, we propose a deformable registration network EDUNet in the registration stage of multi-atlas segmentation (MAS) of brain images. We perform data preprocessing on the floating and fixed images to minimize external influences. In the registration stage, we use ANTs instead of traditional “coarse” registration, and employ a convolutional neural network (CNN) to improve “fine” registration, estimate the deformation field, and introduce an attention mechanism and a dilated convolution module. We propose a deep learning based multimodal cross-reconstruction inverted pyramid network MCRAIP-Net, which uses multimodal magnetic resonance images as input, extracts features of each modality through three independent encoder structures, and fuses the extracted features at the same resolution level. We use a dual-channel cross-reconstruction attention module to refine and fuse multimodal features. On this basis, we use an inverted pyramid decoder to integrate the features of different resolutions at each stage of the decoder, and complete the task of brain tissue segmentation. We compare the segmentation results of two datasets, and show that the proposed algorithm has simpler interaction and faster speed than the confidence connected algorithm, and also greatly improves the segmentation performance. On the Synapse dataset, we use 18 samples as training sets and 12 samples as testing sets; 5 On the ACDC dataset, we use 70 samples as training set, 10 samples as validation set, and 20 samples as testing set. The experimental results show that compared with the traditional U-Net network and other existing models, our model has better accuracy and feasibility in rib fracture segmentation.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Feature extraction",
                "Convolutional neural networks",
                "Three-dimensional displays",
                "Deep learning",
                "Training",
                "Medical diagnostic imaging",
                "Image processing",
                "Biomedical imaging"
            ],
            "Author Keywords": [
                "Image processing technology",
                "Medical image segmentation",
                "Three-dimensional Convolutional neural network",
                "Inverted pyramid decoder",
                "Medical image segmentation",
                "Deep learning"
            ]
        },
        "title": "A 3D Medical Image Segmentation Method Based on MCRAIP-Net"
    },
    {
        "authors": [
            "Ons Aouedi",
            "Thai-Hoc Vu",
            "Alessio Sacco",
            "Dinh C. Nguyen",
            "Kandaraj Piamrat",
            "Guido Marchetto",
            "Quoc-Viet Pham"
        ],
        "published_in": "Published in: IEEE Communications Surveys & Tutorials ( Early Access )",
        "date_of_publication": "18 July 2024",
        "doi": "10.1109/COMST.2024.3430368",
        "publisher": "IEEE",
        "abstract": "The rapid advances in the Internet of Things (IoT) have promoted a revolution in communication technology and offered various customer services. Artificial intelligence (AI) techniques have been exploited to facilitate IoT operations and maximize their potential in modern application scenarios. In particular, the convergence of IoT and AI has led to a new networking paradigm called Intelligent IoT (IIoT), which has the potential to significantly transform businesses and industrial domains. This paper presents a comprehensive survey of IIoT by investigating its significant applications in mobile networks, as well as its associated security and privacy issues. Specifically, we explore and discuss the roles of IIoT in a wide range of key application domains, from smart healthcare and smart cities to smart transportation and smart industries. Through such extensive discussions, we investigate important security issues in IIoT networks, where network attacks, confidentiality, integrity, and intrusion are analyzed, along with a discussion of potential countermeasures. Privacy issues in IIoT networks were also surveyed and discussed, including data, location, and model privacy leakage. Finally, we outline several key challenges and highlight potential research directions in this important area.",
        "issn": {
            "Electronic ISSN": "1553-877X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Industrial Internet of Things",
                "Artificial intelligence",
                "Privacy",
                "Surveys",
                "Security",
                "Biological system modeling",
                "Reviews"
            ],
            "Author Keywords": [
                "Internet of Things",
                "artificial intelligence",
                "wireless networks",
                "industrial applications",
                "security",
                "privacy"
            ]
        },
        "title": "A Survey on Intelligent Internet of Things: Applications, Security, Privacy, and Future Directions"
    },
    {
        "authors": [
            "S. Balaji",
            "R. Pavithra",
            "D. Arivudainambi",
            "K.A. Varunkumar",
            "A. Suresh",
            "Marwan Marwan Omar",
            "Ali Kashif Bashir"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "22 November 2023",
        "doi": "10.1109/TCE.2023.3335466",
        "publisher": "IEEE",
        "abstract": "The Internet of Things (IoT) has become widely popular due to its rapid progress and broad range of practical uses, which have significantly impacted our daily lives. One such key utilization of IoT is making consumer electronics interactive, interconnected and self-responsible by means of sensors and internet. Sensors play a crucial role as the primary technology that enables the Internet of Things (IoT) and its various applications. The inherent difficulty hinders the further growth of IoT. One such problem is utilizing minimum number of sensors to achieve maximum coverage and connectivity in the network. The most accurate method to encounter this issue is optimal Sensor Placement (OSP). OSP predetermines the sensor’s position for its deployment to attain maximum network coverage and connectivity. Hence, this paper proposes a coverage and connectivity-preserving hierarchical algorithm based on discrete Haar wavelet transform for optimal sensor placement algorithm. The proposed algorithm utilizes discrete Haar wavelet transform, for identifying sensor’s position and to ensure connectivity Breadth first search algorithm is used. Further, to enhance the coverage, proposed algorithm determines redundant sensors and redeploy it in determined spots. The results obtained from the proposed algorithm effectively address the research problem and attain better results compared with the existing efficient methods.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Sensor placement",
                "Monitoring",
                "Discrete wavelet transforms",
                "Base stations",
                "Approximation algorithms",
                "Wireless sensor networks"
            ],
            "Author Keywords": [
                "Breadth first search algorithm",
                "Discrete Haar wavelet transform",
                "Internet of Things",
                "Optimal sensor placement",
                "Target coverage problem"
            ]
        },
        "title": "Towards Efficient Sensor Deployment in Internet of Things for Target Coverage and Sensor Connectivity"
    },
    {
        "authors": [
            "Zixing Zhang",
            "Zhongren Dong",
            "Weixiang Xu",
            "Jing Han"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "18 October 2024",
        "doi": "10.1109/JIOT.2024.3483232",
        "publisher": "IEEE",
        "abstract": "With the increasing implementation of machine learning models on edge or Internet-of-Things (IoT) devices, deploying advanced models on resource-constrained IoT devices remains challenging. Transformer models, a currently dominant neural architecture, have achieved great success in broad domains but their complexity hinders its deployment on IoT devices with limited computation capability and storage size. Although many model compression approaches have been explored, they often suffer from notorious performance degradation. To address this issue, we introduce a new method, namely Transformer Re-parameterization, to boost the performance of lightweight Transformer models. It consists of two processes: the High-Rank Factorization (HRF) process in the training stage and the de-High-Rank Factorization (deHRF) process in the inference stage. In the former process, we insert an additional linear layer before the Feed-Forward Network (FFN) of the lightweight Transformer. It is supposed that the inserted HRF layers can enhance the model learning capability. In the later process, the auxiliary HRF layer will be merged together with the following FFN layer into one linear layer and thus recover the original structure of the lightweight model. To examine the effectiveness of the proposed method, we evaluate it on three widely used Transformer variants, i.e. ConvTransformer, Conformer, and SpeechFormer networks, in the application of speech emotion recognition on the IEMOCAP, MED and DAIC-WOZ datasets. Experimental results show that our proposed method consistently improves the performance of lightweight Transformers, even making them comparable to large models. The proposed re-parameterization approach enables advanced Transformer models to be deployed on resource-constrained IoT devices.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Transformers",
                "Computational modeling",
                "Matrix decomposition",
                "Internet of Things",
                "Biological system modeling",
                "Speech recognition",
                "Performance evaluation",
                "Emotion recognition",
                "Mathematical models",
                "Knowledge engineering"
            ],
            "Author Keywords": [
                "Artificial Internet of Things",
                "Transformer Reconstruction",
                "Speech Emotion Recognition",
                "Model Compression"
            ]
        },
        "title": "Re-Parameterization of Lightweight Transformer for On-Device Speech Emotion Recognition"
    },
    {
        "authors": [
            "Deepti Sharma",
            "Rakesh N. Tiwari",
            "Sachin Kumar",
            "Ajay K. Poddar"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "09 October 2024",
        "doi": "10.1109/JIOT.2024.3476686",
        "publisher": "IEEE",
        "abstract": "The Internet of Things (IoT) has revolutionized the healthcare sector by connecting at-home patients to doctors. In this paper, an improved gain antenna array is proposed to further improve the telehealth monitoring systems on the IoT platform. The proposed 1 × 2 antenna array has ultra-compact footprints of 0.38λ g 2 and operates at 2.4 GHz and 5.8 GHz bands. The antenna array is implanted inside a multi-layer (skin-blood-fat-muscle) canonical phantom model in the simulation environment to validate its performance. Also, the proposed antenna arrayfs performance is checked in the gel-based skin phantom for experimental verification. The improved performance of the designed antenna array is proved by comparing it with the single antenna. The gain improvement in the 1 × 2 antenna array configuration is due to the reduced cross-polarization component and increased gain in the boresight direction. However, a single antenna element has comparatively high cross-polarization, leading to low gain in the boresight direction. Due to this, compared to the single antenna element, a 1 × 2 antenna array has 10.1 dB and 4.1 dB improvement in total gain at 2.45 GHz and 5.8 GHz frequency bands, respectively. And improvements in efficiency and frontto-back ratio (FBR) at 2.45 GHz and 5.8 GHz are 10 % and 28 % and 5.3 dB and 6.4 dB, respectively. The antenna array is chosen over multiple-input-multiple-output (MIMO) as it provides significant benefits for wireless biotelemetry, including directional beamforming, interference reduction, range extension, and energy efficiency, making it particularly suitable for IoT applications. To date, as per the authorsf literature survey, this is the most compact implantable antenna array with significantly improved gain, bandwidth, efficiency, FBR, and low cross-polarization proposed for telehealth monitoring on the IoT platform.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Antenna arrays",
                "Gain",
                "Antennas",
                "Monitoring",
                "Internet of Things",
                "Telemedicine",
                "Patch antennas",
                "Phantoms",
                "Biological tissues",
                "Slot antennas"
            ],
            "Author Keywords": [
                "Antenna array",
                "biomedical",
                "gain",
                "implantable medical devices (IMDs)",
                "industrial",
                "scientific and medical (ISM) band"
            ]
        },
        "title": "An Improved Gain Antenna Array for Telehealth Monitoring on the Internet-of-Things Platform"
    },
    {
        "authors": [
            "Fan Feng",
            "Xinran Li",
            "Wenqiang Wei",
            "Yibo Si",
            "Xiangwei Zhu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "16 October 2024",
        "doi": "10.1109/JIOT.2024.3461228",
        "publisher": "IEEE",
        "abstract": "The security of time synchronization is crucial in the Internet of Things (IoT), as it enables sensors and actuators to collect and transmit various types of data. However, root and critical nodes in the IoT are typically connected to Global Navigation Satellite System (GNSS) receivers, increasing the susceptibility of the system to GNSS-based time synchronization attacks (TSAs). When subjected to TSAs, the scheduling and messages of nodes will conflict, especially in distributed IoT systems. To address this issue, a TSA detection method named the cumulative second-order difference of pseudoranges (CD2-P) is proposed in this paper. Furthermore, we propose an alarm strategy called CD2-P-alarm based on TSA characteristics. When an attacker tries to manipulate the GNSS receivers time, this method can detect the time discrepancy and provide an early warning. This paper verifies that the proposed method outperforms other methods by using software-defined receivers based on the TEXBAT dataset. Compared with the state-of-the-art methods, the proposed method can provide more than a 10% improvement in detection probability. Moreover, the practical applicability of the proposed method on a U-blox receiver and an Android mobile phone is validated through real-world experiments. This indicates that the proposed method can be seamlessly integrated into current commercial receivers without the need for modification. By focusing on processing the output data produced by COTS receivers, the method effectively increases the real-time security of existing IoT systems.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Receivers",
                "Global navigation satellite system",
                "Internet of Things",
                "Synchronization",
                "Sensors",
                "Satellites",
                "Satellite broadcasting",
                "Security",
                "Intelligent sensors",
                "Distortion"
            ],
            "Author Keywords": [
                "GNSS",
                "Internet of Things",
                "time synchronization attack",
                "security"
            ]
        },
        "title": "A GNSS Time Synchronization Attack Detection Method for Commercial Off-the-Shelf Receivers: Cumulative Second-Order Difference of Pseudoranges"
    },
    {
        "authors": [
            "Jing Jin",
            "Xiao Wu",
            "Ian Daly",
            "Weijie Chen",
            "Xinjie He",
            "Xingyu Wang",
            "Andrzej Cichocki"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "31 October 2024",
        "doi": "10.1109/JIOT.2024.3488745",
        "publisher": "IEEE",
        "abstract": "Brain-Computer interface (BCI) technology enables the control of external devices by recognizing user intentions. Steady-state visual evoked potential (SSVEP)-based BCI technology has been widely applied in the field of Internet of things (IoT) device control, including smart healthcare, smart homes, and robotics, and has achieved significant results. However, as the field of BCI-based IoT device control is still in its development stage, there remains considerable room for improvement in terms of accuracy, efficiency, and cost. Therefore, enhancing the classification accuracy of SSVEP decoding using a short time window, reducing both human and material costs, and improving work efficiency are crucial for the theoretical research and engineering applications of BCI technology in IoT device control. Based on this, we propose a novel approach to address the challenge of high accuracy feature extraction within brief timeframes. Our approach integrates a multi-scale convolutional neural network with a squeeze excitation module (SEMSCNN). This fusion leverages CNNs’ local feature learning capacity and the advantageous feature importance distinction offered by the squeeze excitation mechanism. First, the EEG signals are band-pass filtered into distinct frequency bands and frequency band and channel features are extracted by a two-layer convolution. Then, temporal features are extracted via a multi-branch convolution of different scales. Finally, the squeeze and excitation (SE) module is introduced to learn the interdependence between features to improve the quality of the extracted features. The first stage of training exploits statistical commonalities across research participants by learning the global model, and the second stage fine-tunes each participants features separately by exploiting participant-specific differences in features. We evaluate our SEMSCNN model on two large public datasets, Benchmark and BETA, and we compare our model to other state-of-the-art mode...",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Electroencephalography",
                "Internet of Things",
                "Visualization",
                "Accuracy",
                "Target recognition",
                "Frequency modulation",
                "Convolutional neural networks",
                "Decoding",
                "Convolution"
            ],
            "Author Keywords": [
                "Multiscale fusion",
                "convolutional neural network (CNN)",
                "squeeze and excitation module (SEM)",
                "Brain–computer interface (BCI)",
                "steady-state visual evoked potentials (SSVEP)"
            ]
        },
        "title": "Squeeze and Excitation-Based Multiscale CNN for Classification of Steady-State Visual Evoked Potentials"
    },
    {
        "authors": [
            "Hayam Alamro",
            "Wahida MANSOURI",
            "Kawther Saeedi",
            "Menwa Alshammeri",
            "Jawhara Aljabri",
            "Faiz Abdullah Alotaibi",
            "Noha Negm",
            "Mahir Mohammed Sharif"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "17 October 2024",
        "doi": "10.1109/ACCESS.2024.3482876",
        "publisher": "IEEE",
        "abstract": "Security donates itself as one of the largest attacks on the support and development of the Internet of Things (IoT). Security challenges are evident in cyber-security threads that direct the main Internet service provider and weaken a significant part of the complete Internet by benefiting from defective and vulnerable embedded gadgets. Numerous devices inhabit at-home systems with user-administrators unfamiliar with network security best practices, creating simple goals for the attackers. So, security solutions are required to direct the untrusted and insecure public networks by mechanizing defences over affordable and nearby direct network data sharing. The growth of automatic cyberattack classification and detection tools utilizing artificial intelligence (AI) and machine learning (ML) devices become vital to achieving safety in the IoT environment. It is desired that safety issues allied to IoT devices be effectively diminished. This article proposes an Advanced Ensemble Transfer Learning for Cyberthreat Detection in Low Power Systems (AETL-CDLPS) technique. The primary intention of the AETL-CDLPS technique is to automate the detection of cyber-attacks for IoT-assisted resource-constrained systems. The AETL-CDLPS technique utilizes a linear scaling normalization (LSN) model to normalize the input data. Next, the AETL-CDLPS technique employs an improved coati optimization algorithm (ICOA)-based feature selection technique to choose optimal features. For the cyber threat detection process, an ensemble transfer learning (TL) model comprises three classifiers, namely gated recurrent Unit (GRU), deep convolutional neural network (DCNN), and stacked sparse autoencoder (SSAE). Finally, the Bayesian optimization algorithm (BOA) is utilized to optimize the hyperparameter tuning of the three ensemble techniques. The AETL-CDLPS model’s performance validation is performed using the Bot-IoT dataset. The comparison study of the AETL-CDLPS method portrayed superior Accuracy, ...",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Security",
                "Cyberattack",
                "Feature extraction",
                "Optimization",
                "Accuracy",
                "Transfer learning",
                "Convolutional neural networks",
                "Computational modeling",
                "Bayes methods"
            ],
            "Author Keywords": [
                "Ensemble Transfer Learning",
                "Cyberthreat Detection",
                "Low Power Systems",
                "Improved Coati Optimization Algorithm",
                "Internet of Things"
            ]
        },
        "title": "Modelling of Bayesian-Based Optimized Transfer Learning Model for Cyber-Attack Detection in Internet of Things Assisted Resource Constrained Systems"
    },
    {
        "authors": [
            "Soteris Constantinou",
            "Constantinos Costa",
            "Andreas Konstantinidis",
            "Panos K. Chrysanthis",
            "Demetrios Zeinalipour-Yazti"
        ],
        "published_in": "Published in: IEEE Transactions on Sustainable Computing ( Early Access )",
        "date_of_publication": "02 May 2024",
        "doi": "10.1109/TSUSC.2024.3396381",
        "publisher": "IEEE",
        "abstract": "The escalating global energy crisis and the increasing CO 2 emissions have necessitated the optimization of energy efficiency. The proliferation of Internet of Things (IoT) devices, expected to reach 100 billion by 2030, contributed to this energy crisis and subsequently to the global CO 2 emissions increase. Concomitantly, climate and energy targets have paved the way for an escalating adoption of solar photovoltaic power generation in residences. The IoT integration into home energy management systems holds the potential to yield energy and peak demand savings. Optimizing device planning to mitigate CO 2 emissions poses significant challenges due to the complexity of user-defined preferences and consumption patterns. In this paper, we propose an innovative IoT data platform, coined Sustainable Energy Management Framework (SEMF) , which aims to balance the trade-off between the imported energy from the grid, users' comfort, and CO 2 emissions. SEMF incorporates a Green Planning evolutionary algorithm, coined GreenCap\n+\n, to facilitate load shifting of IoT-enabled devices, taking into consideration the integration of renewable energy sources, multiple constraints, peak-demand times, and dynamic pricing. Based on our experimental evaluation utilizing real-world data, our prototype system has outperformed the state-of-the-art approach by up to\n≈\n29% reduction in imported energy,\n≈\n35% increase in self-consumption of renewable energy, and\n≈\n34% decrease in CO 2 emissions, while maintaining a high level of user comfort\n≈\n94%-99%.",
        "issn": {
            "Electronic ISSN": "2377-3782"
        },
        "keywords": {
            "IEEE Keywords": [
                "Green products",
                "Planning",
                "Production",
                "Internet of Things",
                "Energy management",
                "Renewable energy sources",
                "Computational modeling"
            ],
            "Author Keywords": [
                "Green Planning",
                "Rule Automation",
                "Renewable Self-Consumption",
                "Internet-of-Things",
                "Load Shifting"
            ]
        },
        "title": "A Sustainable Energy Management Framework for Smart Homes"
    },
    {
        "authors": [
            "Zeng Zeng",
            "Liyuan Gao",
            "Hongyue Ma",
            "Wenjing Li"
        ],
        "published_in": "Published in: IEEE Open Journal of the Communications Society ( Early Access )",
        "date_of_publication": "08 October 2024",
        "doi": "10.1109/OJCOMS.2024.3476277",
        "publisher": "IEEE",
        "abstract": "Electric power, as a primary source of energy, is crucial to ensuring the healthy development of socio-economic systems. With the rapid growth of the economy, the energy demand continues to rise, necessitating the reduction of environmental pollution. Consequently, renewable energy sources such as wind, photovoltaic, and solar-thermal have been widely adopted. The advancement of microgrid autonomy and smart IoT technologies has improved energy utilization efficiency, optimized the energy consumption structure, reduced transmission losses, and alleviated environmental impacts through the use of renewable sources. Currently, smart power IoT focuses on the real-time collection and distribution of data, facing significant challenges due to the imperfections in edge IoT technologies and reliance on centralized cloud server processing, which substantially increases the computational load on central servers. We propose an edge IoT architecture that enhances the consumption capacity of distributed renewable energy at the microgrid level. By studying fine-grained data fingerprinting and neural network-based renewable energy data edge-analysis technologies, we have enabled edge processing of data, reducing latency and privacy issues, and enhancing the system’s real-time processing capabilities and responsiveness.",
        "issn": {
            "Electronic ISSN": "2644-125X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Renewable energy sources",
                "Fingerprint recognition",
                "Microgrids",
                "Servers",
                "Power systems",
                "Predictive models",
                "Data models",
                "Wind power generation",
                "Real-time systems",
                "Energy consumption"
            ],
            "Author Keywords": [
                "Data fingerprint",
                "Neural network",
                "On-site analyzing",
                "Renewable energy"
            ]
        },
        "title": "Fingerprint-Based Deduplication for Renewable Energy Data On-site Analyzing"
    },
    {
        "authors": [
            "Himanshi Babbar",
            "Shalli Rani",
            "Wadii Boulila"
        ],
        "published_in": "Published in: IEEE Open Journal of the Communications Society ( Early Access )",
        "date_of_publication": "15 July 2024",
        "doi": "10.1109/OJCOMS.2024.3428531",
        "publisher": "IEEE",
        "abstract": "Wireless Sensor Network (WSN)-based manufacturing facilities in the context of the Fourth Industrial Revolution (Industry 5.0) represent advanced Cyber-Physical Production Systems (CPPSs), wherein seamless networking of people, objects, and machines is achieved across the entire supply chain. A significant advantage of such digitization is the facilitation of personalized and agile manufacturing processes. However, this interconnectedness introduces a spectrum of novel threat vectors, enabling sophisticated Distributed Denial-of-Service (DDoS) attacks. One critical vulnerability lies in the Internet of Things (IoT) sensor nodes. These IoT devices, now extensively utilized for sensing, data acquisition, analysis, and communication within manufacturing environments, have concomitantly escalated the risk of cyber threats. To counteract these threats, advanced intrusion detection systems leveraging deep learning algorithms have emerged as scalable and intelligent solutions for safeguarding industrial IoT and WSN infrastructures. This paper introduces a hybrid Convolutional Neural Network-Long Short-Term Memory (CNN-LSTM) model tailored for cybersecurity attack detection in industrial IoT environments, specifically within WSN-based smart manufacturing contexts. The proposed CNN-LSTM model exhibits superior efficacy in identifying DDoS attacks within Industry 5.0 CPS environments, surpassing conventional Artificial Neural Network (ANN), Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU) models in terms of accuracy, precision, recall, and F1-score. Utilizing real-world network traffic datasets, the developed deep learning-based network anomaly detection system enhances the capability to detect and mitigate cyber threats, thereby reinforcing the security and resilience of smart manufacturing systems. The practical benefits of this enhanced cyberattack detection system include improved operational reliability, reduced downtime, and the protection of critical asset...",
        "issn": {
            "Electronic ISSN": "2644-125X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Wireless sensor networks",
                "Industrial Internet of Things",
                "Computer security",
                "Security",
                "Fifth Industrial Revolution",
                "Smart manufacturing",
                "Cyberattack"
            ],
            "Author Keywords": [
                "Cyber security",
                "Industrial Internet of Things 5.0 Attacks",
                "Wireless sensor networks",
                "Smart manufacturing"
            ]
        },
        "title": "Fortifying the Connection: Cybersecurity Tactics for WSN-driven Smart Manufacturing in the Era of Industry 5.0"
    },
    {
        "authors": [
            "Kennedy Chinedu Okafor",
            "Kelvin Anoh",
            "Titus I. Chinebu",
            "Bamidele Adebisi",
            "Gloria Azogini Chukwudebe"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "22 July 2024",
        "doi": "10.1109/JIOT.2024.3431874",
        "publisher": "IEEE",
        "abstract": "Infectious diseases like COVID-19 have remained a primary public and global health concern. Internet of Things (IoT) of networked robots and physiological intervention can be combined to identify and control the spread of the different variants of COVID-19 disease. With this approach, governments and healthcare institutions can plan for such diseases in the future. This paper presents a compact computational model (CCM) to identify and control different COVID-19 variants using IoT-networked robots. The CCM comprises seven physiological variables (PV) and robotic identification (RI) of infected individuals as alternative intervention strategies. The study uses Market Place Service Robots that correctly identify PV and RI for positively infected individuals. The conditions of the existence and the solution of the deterministic model are derived from a compact flow architecture that we develop. We show that the model has COVID-19-free equilibrium and endemic equilibrium. While PV with appropriate isolation and hospital treatment reduces the COVID-19 disease impact by 19% more than RI alone, the study also shows that combining two PV with RI minimises the impact better than PV or RI alone, by 36% and 43%, respectively. When the PV control parameters are increased, up to five, in the presence of IoT and RI, up to 99.99% improvement is seen. With all seven PV control parameters in the presence of IoT and RI, the proposed CCM guarantees an infection-free population.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "COVID-19",
                "Robots",
                "Statistics",
                "Sociology",
                "Diseases",
                "Computational modeling",
                "Internet of Things"
            ],
            "Author Keywords": [
                "Computational Intelligence",
                "Computational Complexity",
                "COVID-19 infection",
                "Global Vaccination",
                "Internet of Things",
                "Market Place Service Robot",
                "Smart Health Infrastructure"
            ]
        },
        "title": "Mitigating COVID-19 Spread in Closed Populations Using Networked Robots and Internet of Things"
    },
    {
        "authors": [
            "Zhuotao Lian",
            "Qingkui Zeng",
            "Zhusen Liu",
            "Haoda Wang",
            "Chuan Ma",
            "Weizhi Meng",
            "Chunhua Su",
            "Kouichi Sakurai"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "08 October 2024",
        "doi": "10.1109/JIOT.2024.3476149",
        "publisher": "IEEE",
        "abstract": "The development of the Internet of Things (IoT) has led to the widespread use of WiFi-enabled consumer electronic devices, which are now common in everyday life. These advancements in IoT have greatly improved data collection and analysis capabilities, especially for health monitoring applications. However, traditional centralized machine learning methods often fall short, raising significant privacy concerns and requiring extensive data collection, which is inefficient. To address these limitations within the distributed IoT environment, this paper presents a federated learning-based WiFi sensing system specifically designed for health monitoring. By enabling local model training, our system prevents the sharing of sensitive data, thus reducing the risk of privacy breaches. We further enhance our system with a secret sharing mechanism coupled with model sparsification to significantly improve privacy. Additionally, our improved Top-k model sparsification algorithm, equipped with adaptive residuals, reduces communication overhead while ensuring high accuracy. Extensive testing across various datasets and models confirms that our system outperforms existing benchmarks in terms of privacy protection and communication efficiency, marking a substantial advancement in health monitoring within the IoT.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Monitoring",
                "Wireless fidelity",
                "Sensors",
                "Servers",
                "Federated learning",
                "Computational modeling",
                "Data models",
                "Adaptation models",
                "Internet of Things",
                "Cryptography"
            ],
            "Author Keywords": [
                "WiFi sensing",
                "federated learning",
                "internet of things",
                "health monitoring",
                "secret sharing",
                "model sparsification"
            ]
        },
        "title": "Privacy-Enhanced Federated WiFi Sensing for Health Monitoring in the Internet of Things"
    },
    {
        "authors": [
            "Yonghui Chen",
            "Linlong Yan",
            "Daxiang Ai"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "07 November 2024",
        "doi": "10.1109/ACCESS.2024.3493112",
        "publisher": "IEEE",
        "abstract": "Combining the Internet of Things (IoT) and federated learning (FL) is a trend. In addition to privacy risks, a long-term operating IoT always faces a hierarchical environment, heterogeneous nodes, and occasional communication and node failures. Blockchain-based FL can improve security, reliability, and tractability compared to conventional FL but faces inference, wire-tapping, and Byzantine attacks, besides consensus-based aggregation problems. These security and privacy protection requirements are particularly prominent in some IoT systems, such as IoMT. This study proposes a secure and efficient blockchain-based hierarchical asynchronous FL (S-BHAFL) for IoT. S-BHAFL treats the smart devices under a gateway as a group and weights them on dataset size. In each group, the gateways use mask differential privacy (DP) to prevent wire-tapping and inference attacks while ensuring zero noise to the global model compared to conventional DP-based schemes. Less noise means more accurate models, fewer iterations, and lower energy consumption. Among the groups, S-BHAFL proposed a novel consensus-based aggregation mechanism with a global testing dataset to resist Byzantine attacks. The normalized dynamic factors reduce the impact of simple weighting on model accuracy. Furthermore, theoretical analysis and experimental results on the S-BHAFL compared with state-of-the-art schemes demonstrate convergence, security, effectiveness, and robustness of SBHAFL. The experiment on datasets MNIST, Fashion-MNIST, CIFAR10, and a real-world Heart Disease dataset shows improvements in accuracy by 0.70%−2.71%, in convergence speed by 8.69%−61.29%. S-BHAFL significantly improved the training efficiency and accuracy and maintained the security.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Security",
                "Logic gates",
                "Privacy",
                "Internet of Things",
                "Consensus protocol",
                "Accuracy",
                "Training",
                "Resists",
                "Noise measurement",
                "Convergence",
                "Federated learning"
            ],
            "Author Keywords": [
                "Federated learning",
                "differential privacy",
                "mask",
                "aggregation",
                "consensus"
            ]
        },
        "title": "An Robust Secure Blockchain-based Hierarchical Asynchronous Federated Learning Scheme for Internet of Things"
    },
    {
        "authors": [
            "Peisong Li",
            "Meng Yi",
            "Muddesar Iqbal",
            "Xinheng Wang",
            "Ziren Xiao"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "08 October 2024",
        "doi": "10.1109/TCE.2024.3476079",
        "publisher": "IEEE",
        "abstract": "The rapid advancement of IoT, AI, and edge computing has led to a significant increase in consumer devices and computation tasks, with new electronics incorporating these technologies to enhance services like VR/AR and autonomous driving, requiring real-time processing for safety and efficiency. However, recent research has focused on optimizing IoT task scheduling and resource allocation through various methods, yet overlooks the dynamic nature of IoT environments, fails to adapt to changing device counts and movements, and often ignores task completion rate in favor of minimizing latency and energy cost. In this context, an adaptive task scheduling and resource allocation strategy is proposed for Edge IoT systems, based on the designed Graph-based Proximal Policy Optimization (GPPO) algorithm. Firstly, the GPPO algorithm enhances PPO for adaptive task scheduling in the fluctuating MEC scenarios, adjusting for the varying number of nearby edge servers. Secondly, it accounts for consumer mobility by opting for local task execution if the consumer risks moving outside the edge server’s range, ensuring result reception. Thirdly, it prioritizes task completion rate to increase the number of tasks finished within their acceptable duration. Experimental results demonstrated that the proposed method outperforms traditional methods.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Servers",
                "Dynamic scheduling",
                "Processor scheduling",
                "Internet of Things",
                "Optimal scheduling",
                "Adaptive scheduling",
                "Resource management",
                "Consumer electronics",
                "Job shop scheduling",
                "Energy consumption"
            ],
            "Author Keywords": [
                "Mobile edge computing",
                "Deep Reinforcement Learning",
                "consumer electronics",
                "task scheduling",
                "GPPO algorithm"
            ]
        },
        "title": "Graph-Based Proximal Policy Optimization Empowered Adaptive Task Scheduling Leveraging Cloud-Edge Collaboration for Consumer Electronics"
    },
    {
        "authors": [
            "Darjo Uršič",
            "Matija Pirc",
            "Marko Jošt",
            "Marko Topič",
            "Marko Jankovec"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "06 November 2024",
        "doi": "10.1109/ACCESS.2024.3492319",
        "publisher": "IEEE",
        "abstract": "In this study, we present a development of a solar-powered Internet-of-Things (IoT) device, that incorporates both light energy harvesting and solar cell monitoring, which we demonstrate by long term monitoring of a single perovskite solar cell in office-like indoor environment. Using off-the-shelf components we engineered a compact, self-sufficient IoT device, with a remarkable 75% efficient energy harvesting (EH) method, at input currents in a range of microamperes. The IoT device acquires environmental data (irradiance, temperature, humidity) and solar cell electrical parameters including its IV curve, which it sends over a Bluetooth low energy (BLE) connection to a nearby access point. A single lab-scale perovskite solar cell was used to evaluate the device in a real-world office setting, over a period of one year. Our findings demonstrate that employing a perovskite solar cell with a 1 cm 2 active area and a 1 F supercapacitor as a charge storage, meets the energy demands for the continuous operation of the developed IoT device at low irradiance conditions. Additionally, irradiance sensor data in combination with the full IV curve measurements of the solar cell are used to monitor the available energy and appropriately react to the environment and solar cell changes, while maintaining an extremely low average power consumption of 6 uW. At the same time, the acquired data provide a valuable information about the solar cell’s electrical behaviour, which makes the developed system an easy to use and versatile long-term monitoring device.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Photovoltaic cells",
                "Internet of Things",
                "Voltage control",
                "Supercapacitors",
                "Photovoltaic systems",
                "Resistance",
                "Energy harvesting",
                "Monitoring",
                "Silicon",
                "Perovskites",
                "Energy harvesting"
            ],
            "Author Keywords": [
                "Data consolidation",
                "energy harvesting",
                "energy storage",
                "indoor photovoltaics",
                "perovskite solar cells"
            ]
        },
        "title": "Ultra-low-power indoor light harvesting and solar cell characterisation system"
    },
    {
        "authors": [
            "Difei Jia",
            "Fengye Hu",
            "Qianqian Zhang",
            "Zhuang Ling",
            "Ying-Chang Liang"
        ],
        "published_in": "Published in: IEEE Transactions on Communications ( Early Access )",
        "date_of_publication": "28 August 2024",
        "doi": "10.1109/TCOMM.2024.3450873",
        "publisher": "IEEE",
        "abstract": "In this paper, we investigate a novel symbiotic radio (SR)-aided high-speed railway (HSR) wireless network, in which the Internet of Things (IoT) device, operating as a secondary transmitter, transmits its own information to the mobile relay (MR) on the HSR by backscattering radio frequency (RF) signals from the base station (BS).With the assistance of SR, the designed network facilitates the transmission of locally collected environmental sensing messages from the IoT network to the HSR, simultaneously enhancing the primary communication between the BS and MRs. Aiming to maximize the sum transmission rate of the primary and the IoT network, we focus on a joint power control and device access (JPCDA) problem. Specifically, each IoT device accesses the network through appropriate time slot selection and appropriate power control, thereby achieving satisfactory overall network performance. However, since the fast channel variations arising from the high mobility of HSRs make it impractical to acquire accurate channel state information (CSI), it is challenging to achieve an optimal resource allocation scheme. To address this challenge, we develop a distributed deep reinforcement learning (DRL)-based algorithm that utilizes historical CSI to infer real-time CSI for decision making. In particular, each computing unit of the agent performs action selection for only one IoT device at one time based on the current local observation information. Numerical results illustrate that our proposed algorithm outperforms other baselines, and still works effectively when the environment changes.",
        "issn": {
            "Print ISSN": "0090-6778",
            "Electronic ISSN": "1558-0857"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Resource management",
                "Rail transportation",
                "Backscatter",
                "Power control",
                "Performance evaluation",
                "Symbiosis"
            ],
            "Author Keywords": [
                "High-speed railway (HSR)",
                "symbiotic radio (SR)",
                "resource allocation",
                "deep reinforcement learning (DRL)"
            ]
        },
        "title": "Distributed Deep Reinforcement Learning-Based Power Control and Device Access for High-Speed Railway Networks with Symbiotic Radios"
    },
    {
        "authors": [
            "Chaowei Wang",
            "Mingliang Pang",
            "Tong Wu",
            "Feifei Gao",
            "Lingli Zhao",
            "Jiabin Chen",
            "Wenyuan Wang",
            "Dongming Wang",
            "Zhi Zhang",
            "Ping Zhang"
        ],
        "published_in": "Published in: IEEE Journal on Selected Areas in Communications ( Early Access )",
        "date_of_publication": "13 September 2024",
        "doi": "10.1109/JSAC.2024.3460030",
        "publisher": "IEEE",
        "abstract": "In the visionary ideals of “Internet of Everything” and “Digital Twins”, the future 6G will deeply integrate diverse heterogeneous networks such as satellite and aerial networks to support seamless connectivity and efficient interoperability, also known as space-air-ground integrated networks (SAGIN), in which the grant-free uplink random access based on Slotted ALOHA (S-ALOHA) can reduce access latency and complexity for massive Internet of Things (IoT) devices. However, with the increasing number of IoT users, the collision probability of S-ALOHA escalates and further degrades the system performance. In this paper, we focus on the massive IoT device uplink access in SAGIN aided by high altitude platform stations (HAPS), investigating power allocation for IoT devices to maximize system access capability and spectral efficiency (SE). Specifically, we first optimize 3D deployment of HAPS. Then the resilient massive access (RMA) based on flexible fusion of S-ALOHA and non-orthogonal multiple access methods is proposed. To maximize system SE with device power constraints, we model the sequential decision problem as a Markov decision process and solve it with the Advantage Actor-Critic (A2C) algorithm. Simulation results demonstrate the proposed RMA can significantly improve the IoT terminal successful access probability and the resource scheduling based on A2C also significantly increases the system SE with low complexity.",
        "issn": {
            "Print ISSN": "0733-8716",
            "Electronic ISSN": "1558-0008"
        },
        "keywords": {
            "IEEE Keywords": [
                "Resource management",
                "Internet of Things",
                "NOMA",
                "Satellites",
                "Autonomous aerial vehicles",
                "Space-air-ground integrated networks",
                "Uplink"
            ],
            "Author Keywords": [
                "Space-air-ground integrated networks",
                "high altitude platform stations",
                "resilient massive access",
                "random access",
                "non-orthogonal multiple access",
                "Advantage Actor-Critic"
            ]
        },
        "title": "Resilient Massive Access for SAGIN: A Deep Reinforcement Learning Approach"
    },
    {
        "authors": [
            "Mohammed Tanvir Masud",
            "Marwa Keshk",
            "Nour Moustafa",
            "Igor Linkov",
            "Darren K. Emge"
        ],
        "published_in": "Published in: IEEE Open Journal of the Communications Society ( Early Access )",
        "date_of_publication": "13 June 2024",
        "doi": "10.1109/OJCOMS.2024.3413790",
        "publisher": "IEEE",
        "abstract": "The performance of Artificial Intelligence (AI) systems reaches or even exceeds that of humans in an increasing number of complicated tasks. Highly effective non-linear AI models are generally employed in a black-box form nested in their complex structures, which means that no information as to what precisely helps them reach appropriate predictions is provided. The lack of transparency and interpretability in existing Artificial Intelligence techniques would reduce human users’ trust in the models used for cyber defence, especially in current scenarios where cyber resilience is becoming increasingly diverse and challenging. Explainable AI (XAI) should be incorporated into developing cybersecurity models to deliver explainable models with high accuracy that human users can understand, trust, and manage. This paper explores the following concepts related to XAI. A summary of current literature on XAI is discussed. Recent taxonomies that help explain different machine learning algorithms are discussed. These include deep learning techniques developed and studied extensively in other IoT taxonomies. The outputs of AI models are crucial for cybersecurity, as experts require more than simple binary outputs for examination to enable the cyber resilience of IoT systems. Examining the available XAI applications and safety-related threat models to explain resilience towards IoT systems also summarises the difficulties and gaps in XAI concerning cybersecurity. Finally, various technical issues and trends are explained, and future studies on technology, applications, security, and privacy are presented, emphasizing the ideas of explainable AI models.",
        "issn": {
            "Electronic ISSN": "2644-125X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Artificial intelligence",
                "Internet of Things",
                "Explainable AI",
                "Security",
                "Computer security",
                "Resilience",
                "Computer crime"
            ],
            "Author Keywords": [
                "Explainable artificial intelligence",
                "cyber resilience",
                "cyber defence",
                "threat model",
                "intrusion detection",
                "cyber threat intelligence"
            ]
        },
        "title": "Explainable Artificial Intelligence for Resilient Security Applications in the Internet of Things"
    },
    {
        "authors": [
            "Huiqing Ao",
            "Hui Tian",
            "Wanli Ni"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "20 September 2024",
        "doi": "10.1109/TCE.2024.3464731",
        "publisher": "IEEE",
        "abstract": "The rapid advancement of the Internet of Things (IoT) and artificial intelligence has significantly increased the number of consumer electronics devices. Traditional federated learning methods typically require all these devices to collaboratively train deep neural networks (DNNs). However, this approach poses a significant challenge due to the substantial communication overhead incurred by uploading and downloading the large DNN models between the base station and resource-constrained IoT devices. To overcome this challenge, we propose a federated split learning (FedSL) framework, along with two variants (FedSL-I and FedSL-II). Furthermore, considering the limitations posed by battery capacity, heterogeneous channels and computing capabilities across all consumer devices, we jointly optimize power allocation, device scheduling and split layer selection to minimize the per-round weighted sum of training delay and energy consumption. We design alternating optimization algorithms to attain a sub-optimal solution with reduced computational complexity. Simulation results demonstrate that our proposed FedSL enables efficient collaborative model training among IoT devices, leading to a significant reduction in communication overhead, and ultimately enhancing the overall performance.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Computational modeling",
                "Data models",
                "Energy consumption",
                "Performance evaluation",
                "Delays",
                "Wireless networks"
            ],
            "Author Keywords": [
                "Federated split learning",
                "resource allocation",
                "device scheduling",
                "edge intelligence",
                "Internet of Things"
            ]
        },
        "title": "Federated Split Learning for Edge Intelligence in Resource-Constrained Wireless Networks"
    },
    {
        "authors": [
            "Tri Ayu Lestari",
            "Sravani Kurma",
            "Anal Paul",
            "Keshav Singh",
            "Simon L. Cotton",
            "Trung Q. Duong"
        ],
        "published_in": "Published in: IEEE Transactions on Communications ( Early Access )",
        "date_of_publication": "02 October 2024",
        "doi": "10.1109/TCOMM.2024.3471968",
        "publisher": "IEEE",
        "abstract": "In the context of ultra-reliable low-latency communication (URLLC) in Internet-of-Things (IoT) networks, conventional half-space coverage limits the flexibility of reconfigurable intelligent surface (RIS) deployment. To overcome these constraints, this paper makes use of active simultaneously transmitting and reflecting RIS (STAR-RIS), which is seamlessly integrated into digital twin (DT) and mobile edge computing (MEC) frameworks. Our primary research objective is to achieve full-space coverage by enabling simultaneous transmission and reflection of the signals while improving uplink data transmission from IoT URLLC user nodes (UNs) to the base station (BS) with the assistance of active STAR-RIS, even in the presence of imperfect channel state information (CSI). We formulate the problem of minimizing total end-to-end (e2e) latency, computed using the alternating optimization (AO) algorithm. Subsequently, we have evaluated the performance of the AO algorithm against the stochastic gradient descent (SGD) algorithm, which serves as the benchmark solution. The simulation outcomes delineate a performance evaluation under perfect and imperfect CSI scenarios. The AO algorithm outperforms SGD with latency reductions of 19.7% at N = 32 and 20.4% at N = 64. Increasing N from 32 to 64 results in a 39.3% latency reduction for AO, surpassing SGD’s 38.8%. However, the SGD algorithm consistently exhibits lower computational complexity compared to the AO algorithm. Additionally, the energy splitting mode achieves the system’s total e2e latency reductions of 28.4% over the mode switching mode and 11.04% over time switching mode. Furthermore, active STAR-RIS optimal beamforming (ARO) achieves ≈10% latency reduction over the predictive optimal beamforming (PRO), which itself surpasses active STAR-RIS with random beamforming (ARR) by ≈9%. This comparison considers key factors such as the power budget, the number of RIS elements, the caching capacity of the edge computing server ...",
        "issn": {
            "Print ISSN": "0090-6778",
            "Electronic ISSN": "1558-0857"
        },
        "keywords": {
            "IEEE Keywords": [
                "Ultra reliable low latency communication",
                "Optimization",
                "Minimization",
                "Array signal processing",
                "Reliability",
                "Reconfigurable intelligent surfaces",
                "Real-time systems",
                "Metaverse",
                "Wireless communication",
                "Industries"
            ],
            "Author Keywords": [
                "alternating optimization",
                "digital twin",
                "imperfect CSI",
                "mobile edge computing",
                "simultaneously transmitting and reflecting",
                "ultra-reliable low-latency communication"
            ]
        },
        "title": "Exploiting Active STAR-RIS to enable URLLC in Digitally-Twinned Internet-of-Things Networks"
    },
    {
        "authors": [
            "Mohammad Shamim Ahsan",
            "Md. Shariful Islam",
            "Md. Shohrab Hossain",
            "Anupam Das"
        ],
        "published_in": "Published in: IEEE Transactions on Dependable and Secure Computing ( Early Access )",
        "date_of_publication": "11 July 2024",
        "doi": "10.1109/TDSC.2024.3424299",
        "publisher": "IEEE",
        "abstract": "Despite the significant benefits of the widespread adoption of smart home Internet of Things (IoT) devices, these devices are known to be vulnerable to active and passive attacks. Existing literature has demonstrated the ability to infer the activities of these devices by analyzing their network traffic. In this study, we introduce a packet-based signature generation and detection system that can identify specific events associated with IoT devices by extracting simple features from raw encrypted network traffic. Unlike existing techniques that depend on specific time windows, our approach automatically determines the optimal number of packets to generate unique signatures, making it more resilient to network jitters. We evaluate the effectiveness, uniqueness, and correctness of our signatures by training and testing our system using four public datasets and an emulated dataset with varying network delays, verifying known signatures and discovering new ones. Our system achieved an average recall and precision of 98-99% and 98-100%, respectively, demonstrating the effectiveness and feasibility of using packet-level signatures to detect IoT device activities.",
        "issn": {
            "Print ISSN": "1545-5971",
            "Electronic ISSN": "1941-0018"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Telecommunication traffic",
                "Object recognition",
                "Feature extraction",
                "Fingerprint recognition",
                "Smart homes",
                "Jitter"
            ],
            "Author Keywords": [
                "Network traffic analysis",
                "Packet-based signature"
            ]
        },
        "title": "Detecting Smart Home Device Activities Using Packet-Level Signatures from Encrypted Traffic"
    },
    {
        "authors": [
            "Hao Wang",
            "Xueguan Song",
            "Chao Zhang"
        ],
        "published_in": "Published in: IEEE Internet Computing ( Early Access )",
        "date_of_publication": "21 October 2024",
        "doi": "10.1109/MIC.2024.3483831",
        "publisher": "IEEE",
        "abstract": "Digital twins (DTs) build the real-time digital mirrors of physical entities, and play an important role in various industrial scenarios. Internet of Thing (IoT) serves as the backbone of collecting real-time data for building DTs to meet the technical requirements on real-time responsiveness and modeling precision. In this study, we propose a multi-fidelity data fusion (MDF) mechanism for digital twins via IoT, called MDF-DT. This mechanism establishes the digital twin of a physical entity by fusing real-time sensor data collected via IoT and historical finite element method (FEM) simulation data. A method called improved hierarchical regression for multi-fidelity data fusion (IHR-MDF), is proposed to predict high-fidelity (HF) responses based on the low-fidelity (LF) samples taken from multiple sources and a small size of HF samples. Numerical experiments show that the NRMSE is less than 0.4 and the computational time is about 0.2 ms/point. The proposed MDF-DT mechanism has high applicability in various DT applications.",
        "issn": {
            "Print ISSN": "1089-7801",
            "Electronic ISSN": "1941-0131"
        },
        "keywords": {
            "IEEE Keywords": [
                "Hafnium",
                "Finite element analysis",
                "Real-time systems",
                "Data models",
                "Internet of Things",
                "Digital twins",
                "Vectors",
                "Boundary conditions",
                "Special issues and sections",
                "Numerical models"
            ],
            "Author Keywords": []
        },
        "title": "Multi-fidelity Data Fusion Mechanism for Digital Twins via Internet of Thing"
    },
    {
        "authors": [
            "Na Wang",
            "Wen Zhou",
            "Qingyun Han",
            "Jianwei Liu",
            "Weilue Liao",
            "Junsong Fu"
        ],
        "published_in": "Published in: IEEE Transactions on Cloud Computing ( Early Access )",
        "date_of_publication": "16 September 2024",
        "doi": "10.1109/TCC.2024.3461732",
        "publisher": "IEEE",
        "abstract": "With the rapid development of cloud computing and Internet of Things (IoT) technologies, large amounts of data collected from IoT devices are encrypted and outsourced to cloud servers for storage and sharing. However, traditional ciphertext retrieval schemes impose high computation and storage overhead on end users. Meanwhile, IoT devices with limited resources are difficult to adapt to large amounts of data computation and transmission, which leads to transmission delay and poor user experience. In this paper, we propose a lightweight privacy-preserving ciphertext retrieval scheme based on edge computing (LPCR) by extending searchable encryption (SE) and ciphertext policy attribute-based encryption (CP-ABE) techniques. First, to avoid network delay and paralysis, we introduce edge servers into LPCR and design a collaboration mechanism between the user side and the edge servers. The user side only needs to accomplish lightweight computation and storage tasks, which greatly reduces their resource consumption. Second, we extend the basic ciphertext policy attribute-based keyword search (CP-ABKS) technique and design the Linear Secret Sharing Scheme (LSSS) access control algorithm with attribute values to hide access policies and attributes. In addition, to improve the retrieval accuracy, the document indexes and query trapdoors are set up by conjunctive keywords to help the cloud server locate exactly the data that the user wishes to query. Formal security analysis verifies that LPCR can achieve the security of chosen plaintext attack (CPA) and chosen keyword attack (CKA), and resist collusion attack. Simulation experiments prove that LPCR is lightweight and feasible.",
        "issn": {
            "Electronic ISSN": "2168-7161"
        },
        "keywords": {
            "IEEE Keywords": [
                "Cloud computing",
                "Servers",
                "Internet of Things",
                "Access control",
                "Encryption",
                "Edge computing",
                "Privacy"
            ],
            "Author Keywords": [
                "Edge computing",
                "privacy protection",
                "searchable encryption"
            ]
        },
        "title": "A Lightweight Privacy-Preserving Ciphertext Retrieval Scheme Based on Edge Computing"
    },
    {
        "authors": [
            "Zyad Yasser",
            "Ahmad Hammoud",
            "Azzam Mourad",
            "Hadi Otrok",
            "Zbigniew Dziong",
            "Mohsen Guizani"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "21 October 2024",
        "doi": "10.1109/JIOT.2024.3484357",
        "publisher": "IEEE",
        "abstract": "In this paper, we tackle the network delays in the Internet of Things (IoT) for an enhanced QoS through a stable and optimized federated fog computing infrastructure. Network delays contribute to a decline in Quality-of-Service (QoS) for IoT applications and may even disrupt time-critical functions. Our paper addresses the challenge of establishing fog federations, which are designed to enhance QoS. However, instabilities within these federations can lead to the withdrawal of providers, thereby diminishing federation profitability and expected QoS. Additionally, the techniques used to form federations could potentially pose data leakage risks to end-users whose data is involved in the process. In response, we propose a stable and comprehensive federated fog architecture that considers federated network profiling of the environment to enhance the QoS for IoT applications. This paper introduces a decentralized evolutionary game theoretic algorithm built on top of a Genetic Algorithm mechanism that addresses the fog federation formation issue. Furthermore, we present a decentralized federated learning algorithm that predicts the QoS between fog servers without the need to expose users’ location to external entities. Such a predictor module enhances the decision-making process when allocating resources during the federation formation phases without exposing the data privacy of the users/servers. Notably, our approach demonstrates superior stability and improved QoS when compared to other benchmark approaches.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Quality of service",
                "Internet of Things",
                "Cloud computing",
                "Servers",
                "Edge computing",
                "Privacy",
                "Genetic algorithms",
                "Games",
                "Federated learning",
                "Accuracy"
            ],
            "Author Keywords": [
                "fog computing",
                "cloud federation",
                "fog federation",
                "federated learning",
                "evolutionary game theory",
                "Nash equilibrium"
            ]
        },
        "title": "Federated Learning and Evolutionary Game Model for Fog Federation Formation"
    },
    {
        "authors": [
            "Bin Xiao",
            "Burak Kantarci",
            "Jiawen Kang",
            "Dusit Niyato",
            "Mohsen Guizani"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "04 October 2024",
        "doi": "10.1109/JIOT.2024.3470210",
        "publisher": "IEEE",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capacities on various tasks, and integrating the capacities of LLMs into the Internet of Things (IoT) applications has drawn much research attention recently. Due to security concerns, many institutions avoid accessing state-of-the-art commercial LLM services, requiring the deployment and utilization of open-source LLMs in a local network setting. However, open-source LLMs usually have more limitations regarding their performance, such as their arithmetic calculation and reasoning capacities, and practical systems of applying LLMs to IoT have yet to be well-explored. Therefore, we propose a LLM-based Generative IoT (GIoT) system deployed in the local network setting in this study. To alleviate the limitations of LLMs and provide service with competitive performance, we apply prompt engineering methods to enhance the capacities of the open-source LLMs, design a Prompt Management Module and a Post-processing Module to manage the tailored prompts for different tasks and process the results generated by the LLMs. To demonstrate the effectiveness of the proposed system, we discuss a challenging Table Question Answering (Table-QA) task as a case study of the proposed system, as tabular data is usually more challenging than plain text because of their complex structures, heterogeneous data types and sometimes huge sizes. We conduct comprehensive experiments on two popular Table-QA datasets, and the results show that our proposal can achieve competitive performance compared with state-of-the-art LLMs, demonstrating that the proposed LLM-based GIoT system can provide competitive performance with tailored prompting methods and is easily extensible to new tasks without training.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Cognition",
                "Python",
                "Artificial intelligence",
                "Structured Query Language",
                "Training",
                "Servers",
                "Question answering (information retrieval)",
                "Security",
                "Performance evaluation"
            ],
            "Author Keywords": [
                "Generative Internet of Things",
                "Table Question Answering",
                "Prompt Engineering",
                "Large Language Model"
            ]
        },
        "title": "Efficient Prompting for LLM-Based Generative Internet of Things"
    },
    {
        "authors": [
            "Farhan Ullah",
            "Leonardo Mostarda",
            "Diletta Cacciagrano",
            "Chien-Ming Chen",
            "Mohammed Amoon",
            "Saru Kumari"
        ],
        "published_in": "Published in: IEEE Consumer Electronics Magazine ( Early Access )",
        "date_of_publication": "30 September 2024",
        "doi": "10.1109/MCE.2024.3467768",
        "publisher": "IEEE",
        "abstract": "The swift expansion of consumer electronics has led to the rapid development of Multi-access Edge Computing (MEC). This technology is indispensable for the real-time processing of data in Internet of Things (IoT) environments. The proximity of MEC to data sources, including autonomous vehicles and IoT sensors, leads to rapid processing and reduced latency. However, this proximity also exposes MEC systems to security vulnerabilities and complex task assignment challenges. This paper presents a robust, metaheuristic-based technique for securing and optimizing task assignments in MEC systems. Our technique solves two issues: effective job distribution and intrusion detection. The Flexible Parameter Grid Search (FPGS) technique optimizes the performance of an Artificial Neural Network (ANN) for intrusion detection. Deep Particle Swarm Optimization (DPSO) dynamically assigns tasks and balances computational loads based on real-time network conditions and device capabilities. The efficiency of the proposed framework is demonstrated by comprehensive experiments using three standard datasets, which show considerable gains in system performance, reduced latency, and efficient resource utilization. These findings demonstrate that metaheuristic algorithms and FPGS may improve MEC security and operational efficiency, enabling robust and effective IoT applications.",
        "issn": {
            "Print ISSN": "2162-2248",
            "Electronic ISSN": "2162-2256"
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Intrusion detection",
                "Security",
                "Consumer electronics",
                "Performance evaluation",
                "Cloud computing",
                "Resource management",
                "Servers",
                "Data models",
                "Real-time systems"
            ],
            "Author Keywords": []
        },
        "title": "Metaheuristic-Driven Secure Task Optimization for Consumer Edge Devices"
    },
    {
        "authors": [
            "Emanuele Pagliari",
            "Luca Davoli",
            "Gianluigi Ferrari"
        ],
        "published_in": "Published in: IEEE Transactions on Aerospace and Electronic Systems ( Early Access )",
        "date_of_publication": "25 July 2024",
        "doi": "10.1109/TAES.2024.3433829",
        "publisher": "IEEE",
        "abstract": "Wi-Fi connectivity for localization purposes has been used for several years in the Internet of Things (IoT) context, where the (general) static nature of IoT devices allows to approximately localize them in known environments with low effort and implementation costs. While the accuracy of Wi-Fi localization for IoT applications can be considered as acceptable, the adoption of Wi-Fi-based localization for (a highly mobile) UAV has received limited attention. In this paper, a low cost and complexity system architecture is proposed and exploited to perform a comparative analysis between two Wi-Fi-based localization approaches: the traditional Received Signal Strength Indicator (RSSI) ranging and the more recent Fine Time Measurement (FTM), based on the IEEE 802.11mc amendment. Our goal is to estimate and compare the efficacy of the proposed system for real-time positioning of a static or moving UAV, evaluating the impact of different filtering solutions on the localization accuracy. The obtained results show that FTM-based localization is more accurate, reducing the positioning error by $37\\%$ with respect to the RSSI-based positioning approach. Our results also confirm the better overall performance of the FTM-based solution for low cost localization applications, discussing its limitations, scalability, and advantages as a viable backup positioning solution in (weak or denied) GNSS-based environments and scenarios.",
        "issn": {
            "Print ISSN": "0018-9251",
            "Electronic ISSN": "1557-9603"
        },
        "keywords": {
            "IEEE Keywords": [
                "Location awareness",
                "Autonomous aerial vehicles",
                "Accuracy",
                "Wireless fidelity",
                "Internet of Things",
                "Estimation error",
                "Distance measurement"
            ],
            "Author Keywords": [
                "UAV localization",
                "Wi-Fi localization",
                "Wi-Fi RSSI",
                "Wi-Fi FTM",
                "Data Filtering",
                "Localization accuracy"
            ]
        },
        "title": "Wi-Fi-based Real-Time UAV Localization: a Comparative Analysis between RSSI-based and FTM-based Approaches"
    },
    {
        "authors": [
            "Jie Cao",
            "Ernest Kurniawan",
            "Amnart Boonkajay",
            "Sumei Sun",
            "Petar Popovski",
            "Xu Zhu"
        ],
        "published_in": "Published in: IEEE Network ( Early Access )",
        "date_of_publication": "14 June 2024",
        "doi": "10.1109/MNET.2024.3414396",
        "publisher": "IEEE",
        "abstract": "Driven by the development goal of network paradigm and demand for various functions in the sixth-generation (6G) mission-critical Internet-of-Things (MC-IoT), we foresee a goal-oriented integration of sensing, communication, computing, and control (GIS3C) in this paper. We first provide an overview of the tasks, requirements, and challenges of MC-IoT. Then we introduce an end-to-end GIS3C architecture, in which goal-oriented communication is leveraged to bridge and empower sensing, communication, control, and computing functionalities. By revealing the interplay among multiple subsystems in terms of key performance indicators and parameters, this paper introduces unified metrics, i.e ., task completion effectiveness and cost, to facilitate S3C co-design in MC-IoT. The preliminary results demonstrate the benefits of GIS3C in improving task completion effectiveness while reducing costs. We also identify and highlight the gaps and challenges in applying GIS3C in the future 6G networks.",
        "issn": {
            "Print ISSN": "0890-8044",
            "Electronic ISSN": "1558-156X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Task analysis",
                "6G mobile communication",
                "Sensors",
                "Measurement",
                "Semantics",
                "Reliability",
                "Monitoring"
            ],
            "Author Keywords": [
                "Goal-oriented communication",
                "integration",
                "mission-critical Internet-of-Things"
            ]
        },
        "title": "Goal-Oriented Integration of Sensing, Communication, Computing, and Control for Mission-Critical Internet-of-Things"
    },
    {
        "authors": [
            "Ankur Jaiswal",
            "Akash Kumar",
            "Abhishek Hazra",
            "Nabajyoti Mazumdar",
            "Jagpreet Singh"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "04 July 2024",
        "doi": "10.1109/TCE.2024.3423435",
        "publisher": "IEEE",
        "abstract": "Integrating Internet of Things (IoT) and Wireless sensor networks (WSN) in E-commerce enhances supply chain visibility and ensures product quality through real-time tracking, revolutionizing inventory management system. IoT devices (IoTDs) gather extensive consumer data, enriching E-commerce databases. Using advanced analytics and machine learning, platforms create personalized recommendations, targeted ads, and customized promotions, enhancing customer engagement and loyalty. While IoT and WSN devices provide valuable insights for E-commerce applications, it’s imperative to address their energy limitations to uphold quality of service. This article explores battery-less IoTDs powered by wireless Power Beacons (PBs). To address this challenge, we employed both omni-directional PBs and directional PBs for powering IoTDs. We have proposed a novel scheme for the optimal number of power beacons (ONPB) which is based on set cover problem. The Omni Directional Cliques (ODC) and Directional Cliques (DC) algorithms are employed to cover the IoTDs, followed by the optimization of PBs placement using the ONPB algorithm, leveraging both omni and directional PBs features.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Wireless sensor networks",
                "Lead",
                "Internet of Things",
                "Electronic commerce",
                "Wireless communication",
                "Consumer electronics",
                "Radio frequency"
            ],
            "Author Keywords": [
                "Wireless Sensor Network",
                "Wireless Power Beacons",
                "Internet of Things",
                "Radio Frequency",
                "E-commerce"
            ]
        },
        "title": "Optimizing Power Beacon Deployment in Battery-Less Wireless Sensor Networks for Transforming E-Commerce"
    },
    {
        "authors": [
            "Chengrong Yang",
            "Qiwen Jin",
            "Yaowei Wang",
            "Yujue Zhou",
            "Dapeng Lan",
            "Yun Yang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "03 October 2024",
        "doi": "10.1109/JIOT.2024.3472079",
        "publisher": "IEEE",
        "abstract": "Pest recognition is of great significance for achieving sustainable development in agriculture. Nevertheless, due to the wide variety of pest species, subtle inter-species differences, and significant intra-species variations, existing artificial intelligence and Internet of Things (IoT) technologies can only recognize a small number of known pests effectively. In this paper, we propose a zero-shot learning pest recognition framework based on ensemble hierarchical attribute prompting, termed EHAPZero. EHAPZero can identify pest images collected by IoT devices, and then transmit the recognition results to the IoT platform for terminal display. Specifically, the image recognition function is implemented by an attribute generation module (AGM), a hierarchical prompting module (HPM), and a semantic-visual interaction module (SVIM). AGM utilizes large language models to construct a knowledge graph of pests. It employs both node importance evaluation algorithms and manual methods to perform dual filtering on attribute nodes within the graph. Inspired by human knowledge reasoning, HPM dynamically predicts different hierarchical attributes of input images within the Transformer intermediate blocks. These predicted attributes are subsequently injected into the intermediate layer features of the Transformer as prompts. To achieve semantic disambiguation and knowledge transfer, SVIM employs a visual-guided semantic representation method and a semantic-guided visual representation method to strengthen cross-domain interaction between semantics and vision. Finally, the final prediction score is derived through ensemble of prediction results across different levels. Extensive experiments show that EHAPZero achieves the new state-of-theart results on the real-word pest recognition benchmark. The codes are available at: https://github.com/jinqiwen/EHAPZero.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Zero shot learning",
                "Visualization",
                "Image recognition",
                "Transformers",
                "Internet of Things",
                "Crops",
                "Knowledge graphs",
                "Real-time systems",
                "Cognition"
            ],
            "Author Keywords": [
                "Zero-shot learning",
                "Attribute generation",
                "Hierarchical attribute prompting",
                "Semantic-visual interaction",
                "Ensemble hierarchical attribute"
            ]
        },
        "title": "EHAPZero: Ensemble Hierarchical Attribute Prompting Based Zero-Shot Learning for Pest Recognition"
    },
    {
        "authors": [
            "Xiaoli Liu",
            "Xiang Su",
            "Guillermo Del Campo",
            "Jacky Cao",
            "Boyu Fan",
            "Edgar Saavedra",
            "Asunción Santamaría",
            "Juha Röning",
            "Pan Hui",
            "Sasu Tarkoma"
        ],
        "published_in": "Published in: IEEE Network ( Early Access )",
        "date_of_publication": "03 October 2024",
        "doi": "10.1109/MNET.2024.3469988",
        "publisher": "IEEE",
        "abstract": "Industry 4.0, leveraging the Internet of Things (IoT) and Artificial Intelligence (AI), is a key enabler for many automated processes in modernized industrial applications. This paper addresses significant challenges pertaining to sensing and data analytics by connecting a large number of industrial IoT (IIoT) devices and deploying federated learning on 5G edge networks. We envision a federated learning-based 5G edge architecture for IIoT and develop an AI algorithm, i.e., an LSTM autoencoder algorithm for anomaly detection, on the 5G edge.We conduct comprehensive scalability analytics of communication and computation resources on our 5G edge IoT testbed. Our experimentation verifies that 1) federated AI algorithms can be deployed on 5G edge servers for latency-sensitive analytics, and 2) 5G edge supports scalable deployment of IIoT devices with low latency.",
        "issn": {
            "Print ISSN": "0890-8044",
            "Electronic ISSN": "1558-156X"
        },
        "keywords": {
            "IEEE Keywords": [
                "5G mobile communication",
                "Industrial Internet of Things",
                "Reliability",
                "Servers",
                "Image edge detection",
                "Artificial intelligence",
                "Anomaly detection",
                "Decision making",
                "Data privacy",
                "Training"
            ],
            "Author Keywords": [
                "Internet of Things",
                "networked edge systems",
                "federated learning",
                "anomaly detection",
                "5G"
            ]
        },
        "title": "Federated Learning on 5G Edge for Industrial Internet of Things"
    },
    {
        "authors": [
            "Liqiong Chang",
            "Guodong Xie",
            "Yuqi Zhang",
            "Xiaofeng Yang",
            "Ju Wang"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "19 August 2024",
        "doi": "10.1109/JSEN.2024.3439376",
        "publisher": "IEEE",
        "abstract": "With the rapid development of Internet of Things (IoT) technology, gesture recognition based on wireless signals such as Wi-Fi has become an important method of human-computer interaction. However, existing gesture recognition methods rely on devices with rich storage resources, such as smartphones and laptops, to achieve high recognition accuracy. Due to cost reasons, most IoT devices are usually equipped with limited storage space, which makes it impossible to store sufficient gesture samples to train high-precision, multi-category gesture recognition models. In this paper, we propose an incremental, memory-efficient gesture recognition system based on Wi-Fi, iMe, which only uses a small amount of data to achieve incremental recognition of multiple categories gestures, and solves the challenge of implementing multi-category and high-precision gesture recognition systems on IoT devices. iMe uses a Conditional Variational AutoEncoder (CVAE) generative network with a dynamic updating structure to generate the old data required for gesture incremental recognition. Adopting an incremental learning framework based on knowledge distillation to address catastrophic forgetting during gesture incremental learning. The results demonstrate that iMe achieves optimal performance in both recognition accuracy and storage cost, outperforming state-of-the-art Fine-Tune, iCaRL, and EWC methods.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Gesture recognition",
                "Internet of Things",
                "Wireless fidelity",
                "Sensors",
                "Accuracy",
                "Knowledge engineering",
                "Continuing education"
            ],
            "Author Keywords": [
                "Gesture recognition",
                "Generative network",
                "Knowledge distillation"
            ]
        },
        "title": "iMe: Incremental, Memory-Efficient Hand Gesture Recognition System With Wi-Fi"
    },
    {
        "authors": [
            "Tinh T. Bui",
            "Long D. Nguyen",
            "Berk Canberk",
            "Vishal Sharma",
            "Octavia A. Dobre",
            "Hyundong Shin",
            "Trung Q. Duong"
        ],
        "published_in": "Published in: IEEE Communications Magazine ( Early Access )",
        "date_of_publication": "16 August 2024",
        "doi": "10.1109/MCOM.001.2300545",
        "publisher": "IEEE",
        "abstract": "Integrated satellite-terrestrial networks (ISTNs) technology in the sixth generation (6G) wireless networks has been considered a promising candidate for global coverage and seamless connectivity for the Internet of things (IoT). However, integrating these two complex systems poses many deployment, management, and maintenance issues. With the fundamental principle of building a virtual live representation of the networks, the digital twin (DT) technology can be used in complex ISTNs to provide a reliable environment for designing or testing, reducing risk and latency, recovering the networks quickly, and optimizing resource allocation for IoT devices in real-time. In this article, we propose an ISTN framework empowered by DT technology, and discuss its promising models, benefits, potential technologies, research challenges, and future research direction for 6G IoT.",
        "issn": {
            "Print ISSN": "0163-6804",
            "Electronic ISSN": "1558-1896"
        },
        "keywords": {
            "IEEE Keywords": [
                "Satellites",
                "6G mobile communication",
                "Real-time systems",
                "Satellite broadcasting",
                "Low earth orbit satellites",
                "Internet of Things",
                "Servers"
            ],
            "Author Keywords": []
        },
        "title": "Digital Twin-Empowered Integrated Satellite-Terrestrial Networks toward 6G Internet of Things"
    },
    {
        "authors": [
            "Haifeng Sun",
            "Yuqiang Zhou",
            "Hui Zhang",
            "Laha Ale",
            "Hongning Dai",
            "Ning Zhang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "10 September 2024",
        "doi": "10.1109/JIOT.2024.3456846",
        "publisher": "IEEE",
        "abstract": "The 6G network is expected to accommodate a wide array of connected devices, supporting diverse services from any location at any time. In this paper, we introduce an aerial Mobile Edge Computing (MEC) framework composed of High-Altitude Platforms (HAPs) and low-altitude Unmanned Aerial Vehicles (UAVs), to cater to computing offloading for Internet of Things (IoT) devices, particularly in rural/remote areas or disaster zones. The framework accommodates various types of tasks, each computed by the corresponding Docker container. The objective is to achieve optimal workload fairness for UAVs while simultaneously minimizing the weighted processing costs among IoT devices in terms of task computation latency and energy consumption over the long term. This is achieved by jointly optimizing the flight trajectories and Docker image caching decisions of the UAVs with limited storage capacities, alongside ensuring service fairness for IoT devices. We tailor a Multi-Agent Deep Deterministic Policy Gradient (MADDPG)-based approach to solve the long-term joint optimization problem, normalizing continuous actions and sampling discrete actions by generalizing the Gumbel-Softmax reparameterization trick. Experimental results indicate that our approach significantly outperforms benchmark schemes in terms of processing delay, energy consumption, and fairness.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Containers",
                "Optimization",
                "Autonomous aerial vehicles",
                "Servers",
                "Computational modeling",
                "Wireless communication"
            ],
            "Author Keywords": [
                "Mobile Edge Computing",
                "task offloading",
                "Aerial Access Networks",
                "joint optimization",
                "MADDPG"
            ]
        },
        "title": "Joint Optimization of Caching, Computing and Trajectory Planning in Aerial Mobile Edge Computing Networks: A MADDPG Approach"
    },
    {
        "authors": [
            "Yating Xu",
            "Gongpu Wang",
            "Rongtao Xu",
            "Yuanwei Liu",
            "Chintha Tellambura",
            "Bingcheng Liu"
        ],
        "published_in": "Published in: IEEE Transactions on Vehicular Technology ( Early Access )",
        "date_of_publication": "22 October 2024",
        "doi": "10.1109/TVT.2024.3484759",
        "publisher": "IEEE",
        "abstract": "Recently backscatter communications and Ambient Internet of Things (A-IoT) arouse extensive interest from both academia and industry. One key parameter for A-IoT is frequency offset (FO), including carrier frequency offset (CFO) and sampling frequency offset (SFO), which is often neglected by existing studies. In this paper, we investigate the channel estimation and optimal pilot design for A-IoT with frequency offsets. Specifically, we first propose a joint FOs and channel estimator and then derive the corresponding Cramer-Rao Bounds (CRBs) for the channel estimates. We then suggest the optimal pilot design for the tag with on-off keying and binary phase shift keying modulation schemes. Finally, extensive numerical results are provided to corroborate our proposed studies.",
        "issn": {
            "Print ISSN": "0018-9545",
            "Electronic ISSN": "1939-9359"
        },
        "keywords": {
            "IEEE Keywords": [
                "Channel estimation",
                "Backscatter",
                "Frequency estimation",
                "Oscillators",
                "Baseband",
                "Symbols",
                "Sensors",
                "Internet of Things",
                "Transforms",
                "Transceivers"
            ],
            "Author Keywords": [
                "Ambient Internet of Things",
                "backscatter communication",
                "channel estimation",
                "frequency offsets",
                "optimal pilot design",
                "training sequence"
            ]
        },
        "title": "Channel Estimation and Pilot Design for Ambient Internet of Things with Frequency Offsets"
    },
    {
        "authors": [
            "Runxin Zhang",
            "Jianpeng Ma",
            "Shun Zhang",
            "Octavia A. Dobre"
        ],
        "published_in": "Published in: IEEE Journal of Selected Topics in Signal Processing ( Early Access )",
        "date_of_publication": "30 August 2024",
        "doi": "10.1109/JSTSP.2024.3451290",
        "publisher": "IEEE",
        "abstract": "Low earth orbit (LEO) satellites are bringing new opportunities for the integration between terrestrial Internet-of-Things (IoT) and satellite IoT. Due to its high robustness against large time delays and Doppler shifts, chirp spread spectrum (CSS) modulation, i.e., the key technology of the Long-Range (LoRa), is expected to empower the satellite link. However, the ALOHA protocol employed by LoRa will inevitably lead to collisions over the satellite channels. In this paper, we focus on the concurrent uplink transmission over the LEO satellite IoT, which is based on CSS. We carefully analyze the relationship between the chirp rate and its spreading factor (SF). Then, we propose the fractional chirp rate based CSS modulation, and support terrestrial users to achieve the non-orthogonal multiple access with the same SF, which ensures that the users possess the same noise immunity. We derive the bit error rate (BER) for both the synchronous and asynchronous scenarios. The performance of our scheme is tested by simulation. Results show that our scheme can achieve the multiple access while maintaining a satisfactory BER performance and is robust over the asynchronous scenario. Furthermore, we build a hardware system using the field-programmable gate array (FPGA) devices to validate the feasibility of this system.",
        "issn": {
            "Print ISSN": "1932-4553",
            "Electronic ISSN": "1941-0484"
        },
        "keywords": {
            "IEEE Keywords": [
                "Satellites",
                "Chirp",
                "Modulation",
                "Low earth orbit satellites",
                "LoRa",
                "Frequency modulation",
                "Bit error rate"
            ],
            "Author Keywords": [
                "CSS",
                "multiple access",
                "BER performance analysis",
                "LEO satellite"
            ]
        },
        "title": "Fractional Chirp Rate Based CSS Division Multiple Access over LEO Satellite Internet-of-Things"
    },
    {
        "authors": [
            "Alessio Mostaccio",
            "Gaetano Marrocco"
        ],
        "published_in": "Published in: IEEE Antennas and Propagation Magazine ( Early Access )",
        "date_of_publication": "25 September 2024",
        "doi": "10.1109/MAP.2024.3457309",
        "publisher": "IEEE",
        "abstract": "Laser-induced graphene (LIG) is a widespread technology for the manufacturing of low-cost and eco-friendly sensors that can be also exploited for the fabrication of conductorless antennas and RF devices for the personal and industrial Internet of Things (IoT). In this case, additional issues must be accounted for, such as the much larger lasing area, the moderate conductivity of LIG, and the durability versus environmental and mechanical conditions in real applications, but also new opportunities to provide antennas with nonconventional features. This article provides a unitary representation of the state-of-the-art knowledge for the modeling, design, fabrication, and testing of LIG-based antennas at some frequencies of IoT systems. The reader will find information about the selection of materials, how to configure the laser parameters to minimize the sheet resistance, and how to account for the moderate conductivity in numerical solvers. The upper bound performances are identified and related to the optimal antenna size. As IoT devices include sensors, the integration with the antenna can benefit from nonuniform lasing to dump unwanted RF currents on the sensor while preserving the communication capability. Finally, the immunity of LIG antennas to external stimuli is reviewed to quantify the expected degradation of performance. This tutorial hence provides a multidisciplinary background to activate a new research line, as well as to conduct experiments with the new concept of antennas engraved on substrates.",
        "issn": {
            "Print ISSN": "1045-9243",
            "Electronic ISSN": "1558-4143"
        },
        "keywords": {
            "IEEE Keywords": [
                "Antennas",
                "Graphene",
                "Resistance",
                "Lasers",
                "Laser beams",
                "Surface emitting lasers",
                "Substrates"
            ],
            "Author Keywords": []
        },
        "title": "Design, Prototyping, and Characterization of Laser-Induced Graphene Antennas on Flexible Substrates: Consolidating current knowledge"
    },
    {
        "authors": [
            "Yao Xiao",
            "Lei Xu",
            "Yan Wu",
            "Jiahang Sun",
            "Liehuang Zhu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "03 September 2024",
        "doi": "10.1109/JIOT.2024.3454087",
        "publisher": "IEEE",
        "abstract": "With the help of artificial intelligence, the large amount of data generated by IoT has unleashed significant value. Federated learning is emerging as a novel paradigm which can be applied to solve the privacy issues caused by analyzing IoT data. However, traditional federated learning protocols are vulnerable to inference and poisoning attacks. Various solutions have been proposed to enhance data privacy and robustness. Nonetheless, most of these solutions are usually centralized and rely on unrealistic security assumptions. Furthermore, the recently proposed blockchain-based decentralized solutions generally incur high costs, which is unaffordable for resource-constrained IoT devices. In this paper, we propose a practical secure federated learning system named PrSeFL. We utilize blockchain to decentralize federated learning process so that the security assumptions are easier to achieve in practice. To preserve data privacy, we implement secure multiparty computation based secure aggregation in blockchain environment. To guarantee practical robustness, we enforce norm constraints on the masked updates via zero-knowledge proof. Moreover, we propose a modified dynamic accumulator which is utilized to realize lightweight anonymous authentication of users. Simulation results show that, compared with state-of-the-art systems, PrSeFL has superior performance on authentication and model training. And the advantage of PrSeFL becomes more significant as the number of users grows.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Federated learning",
                "Blockchains",
                "Servers",
                "Internet of Things",
                "Authentication",
                "Data privacy",
                "Robustness"
            ],
            "Author Keywords": [
                "federated learning",
                "privacy preserving",
                "robustness",
                "blockchain"
            ]
        },
        "title": "PrSeFL: Achieving Practical Privacy and Robustness in Blockchain-Based Federated Learning"
    },
    {
        "authors": [
            "Yan Liu",
            "Bin Guo",
            "Nuo Li",
            "Yasan Ding",
            "Zhouyangzi Zhang",
            "Zhiwen Yu"
        ],
        "published_in": "Published in: IEEE Communications Surveys & Tutorials ( Early Access )",
        "date_of_publication": "04 July 2024",
        "doi": "10.1109/COMST.2024.3423319",
        "publisher": "IEEE",
        "abstract": "Artificial Intelligence of Things (AIoT) is an emerging frontier based on the deep fusion of Internet of Things (IoT) and Artificial Intelligence (AI) technologies. The fundamental goal of AIoT is to establish a self-organizing, self-learning, self-adaptive, and continuous-evolving AIoT system by orchestrating intelligent connections among Humans, Machines, and IoT devices. Although advanced deep learning techniques enhance the efficient data processing and intelligent analysis of complex IoT data, they still suffer from notable challenges when deployed to practical AIoT applications, such as constrained resources, dynamic environments, and diverse task requirements. Knowledge transfer, a popular and promising area in machine learning, is an effective method to enhance learning performance by avoiding the exorbitant costs associated with data recollection and model retraining. Notably, although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances of various knowledge transfer techniques for AIoT field. This survey endeavors to introduce a new concept of knowledge transfer, referred to as Crowd Knowledge Transfer (CrowdTransfer), which aims to transfer prior knowledge learned from a crowd of agents to reduce the training cost and as well as improve the performance of the model in real-world complicated scenarios. Particularly, we present four transfer modes from the perspective of crowd intelligence, including derivation, sharing, evolution and fusion modes. Building upon conventional transfer learning methods, we further delve into advanced crowd knowledge transfer models from three perspectives for various AIoT applications: intra-agent knowledge transfer, centralized inter-agent knowledge transfer, and decentralized inter-agent knowledge transfer. Furthermore, we explore some applications of AIoT areas, such as human activity recognition, urban comp...",
        "issn": {
            "Electronic ISSN": "1553-877X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Knowledge transfer",
                "Artificial intelligence",
                "Transfer learning",
                "Task analysis",
                "Surveys",
                "Deep learning"
            ],
            "Author Keywords": [
                "AIoT",
                "crowd intelligence",
                "crowd knowledge transfer",
                "transfer learning"
            ]
        },
        "title": "CrowdTransfer: Enabling Crowd Knowledge Transfer in AIoT Community"
    },
    {
        "authors": [
            "Gyaneshwar Singh",
            "Saurabh Rana",
            "Dheerendra Mishra",
            "Muhammad Khurram Khan"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "18 June 2024",
        "doi": "10.1109/TCE.2024.3416402",
        "publisher": "IEEE",
        "abstract": "In consumer electronics, Industry 5.0 offers the potential to combine human expertise with advanced technologies. This convergence creates opportunities for real-time healthcare services to be provided to remote consumers. Internet of Things (IoT) devices also play a crucial role in capturing and transmitting consumer monitoring data for analysis and review. However, the traditional approach to centralize IoT data processing and storage comes with risks such as data manipulation and privacy breaches. Blockchain technology provides a solution to these challenges. By decentralizing computation and storage for IoT data, blockchain has the potential to revolutionize the e-healthcare industry. We focus on proposing a blockchain-based solution for securely sharing user health data among authorized stakeholders. Our access control architecture, based on blockchain technology, allows authorized parties to access data independently, eliminating the need for trusted intermediaries. This approach is particularly advantageous in scenarios where third-party dependencies could hinder real-time data sharing. We have rigorously tested and validated our design to ensure its effectiveness and reliability.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Medical services",
                "Blockchains",
                "Internet of Things",
                "Consumer electronics",
                "Fifth Industrial Revolution",
                "Electric potential",
                "Access control"
            ],
            "Author Keywords": [
                "E-Healthcare",
                "Internet of things",
                "Blockchain",
                "Access control of devices",
                "Security"
            ]
        },
        "title": "Blockchain-Based Access Control Architecture for Enhancing Authorized E-Healthcare Data Sharing Services in Industry 5.0"
    },
    {
        "authors": [
            "Zhanyuan Xie",
            "Randall Li",
            "Zheng Jiang",
            "Xiaoming She",
            "Peng Chen",
            "Zhu Han"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "28 October 2024",
        "doi": "10.1109/JIOT.2024.3487023",
        "publisher": "IEEE",
        "abstract": "High reliability is crucial for stable and accurate control in wireless Internet of Things (IoT) systems. Although average outage probability is a widely adopted metric for evaluating the reliability of wireless IoT systems, it does not account for the sparsity of outage occurrences, which can be interpreted as the frequency of outage occurrence within a certain time. Compared to an isolated single outage, clustered outages can significantly impact stability. To address this problem, we introduce the concepts of virtual queue and consecutively effective throughput. The virtual queue treats outage packets as arrivals, with its service rate determined by the desired frequency of the single outage. In contrast to the traditional throughput, consecutively effective throughput measures the effectiveness of consecutive success of transmissions. Based on these concepts, we then consider maximizing the consecutively effective throughput under virtual queue constraints. Theoretical analysis is conducted to derive the optimal rate for this problem. Specifically, we explore two scenarios: outages caused by packet decoding errors and outages due to packet decoding errors and latency violations. Considering the non-asymptotic property of the virtual queue, queueing theory is utilized to provide closed-form expressions for virtual queue constraints. Based on the theoretical analysis, efficient and effective algorithms are proposed to obtain the optimal rate based on the above analysis. Numerical comparisons between grid searches and our algorithms validate the correctness of our theoretical analysis. This study underscores the importance of specific scheduling designs to control clustered outages, rather than merely enhancing throughput in wireless IoT systems.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Queueing analysis",
                "Wireless communication",
                "Throughput",
                "Decoding",
                "Internet of Things",
                "Transmitters",
                "Reliability theory",
                "Ultra reliable low latency communication",
                "Bandwidth",
                "6G mobile communication"
            ],
            "Author Keywords": [
                "Reliability",
                "outage pattern",
                "sparse outage control",
                "virtual queue",
                "consecutively effective throughput",
                "queueing theory",
                "Internet of Things"
            ]
        },
        "title": "Sparse Outage Control for Wireless Internet of Things Systems Through Virtual Queue and Consecutively Effective Throughput"
    },
    {
        "authors": [
            "Jiahui Xiang",
            "Lirong Fu",
            "Tong Ye",
            "Peiyu Liu",
            "Huan Le",
            "Liming Zhu",
            "Wenhai Wang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/JIOT.2024.3490661",
        "publisher": "IEEE",
        "abstract": "The diversity of web configuration interfaces for IoT devices has exacerbated issues such as inadequate permission controls and insecure interfaces, resulting in various vulnerabilities. Owing to the varying interface configurations across various devices, the existing methods are inadequate for identifying these vulnerabilities precisely and comprehensively. This study addresses these issues by introducing an automated vulnerability detection system, called LuaTaint. It is designed for the commonly used web configuration interface of IoT devices. LuaTaint combines static taint analysis with a large language model (LLM) to achieve widespread and high-precision detection. The extensive traversal of the static analysis ensures the comprehensiveness of the detection. The system also incorporates rules related to page handler control logic within the taint detection process to enhance its precision and extensibility. Moreover, we leverage the prodigious abilities of LLM for code analysis tasks. By utilizing LLM in the process of pruning false alarms, the precision of LuaTaint is enhanced while significantly reducing its dependence on manual analysis. We develop a prototype of LuaTaint and evaluate it using 2,447 IoT firmware samples from 11 renowned vendors. LuaTaint has discovered 111 vulnerabilities. Moreover, LuaTaint exhibits a vulnerability detection precision rate of up to 89.29%.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Microprogramming",
                "Static analysis",
                "Security",
                "Codes",
                "Uniform resource locators",
                "Performance evaluation",
                "Dispatching",
                "Bandwidth",
                "Web servers"
            ],
            "Author Keywords": [
                "Device Security",
                "Web Configuration Interface",
                "Static Analysis",
                "LLMs"
            ]
        },
        "title": "LuaTaint: A Static Analysis System for Web Configuration Interface Vulnerability of Internet of Things Device"
    },
    {
        "authors": [
            "Feng Yang",
            "Duy-Hieu Bui",
            "Yang Zhao",
            "Liang Qi",
            "Jinghua Zhang",
            "Xuan-Tu Tran",
            "Yongfu Li"
        ],
        "published_in": "Published in: Integrated Circuits and Systems ( Early Access )",
        "date_of_publication": "17 October 2024",
        "doi": "10.23919/ICS.2024.3482310",
        "publisher": "SJTU",
        "abstract": "The rapid growth of Internet-of-Things (IoT) applications necessitates the development of cost-effective solutions and accelerated design cycles. While digital circuit design has witnessed significant automation, analog design still heavily relies on experienced engineers. To bridge this gap, synthesizable solutions that integrate digital and analog design automation are crucial for efficient IoT development. This paper explores the historical context and current challenges in circuit automation and electronic design automation (EDA) tools, specifically focusing on analog and mixed-signal circuits. As successive approximation register (SAR) ADCs play a critical role in IoT applications, it is important to critically examines the state-of-the-art synthesizable SAR ADCs, discussing the strengths and limitations of different design techniques for various circuit blocks. These findings provide valuable insights for researchers and industry practitioners, informing future research directions in the field of synthesizable SAR ADC design automation.",
        "issn": {
            "Print ISSN": "2995-1968"
        },
        "keywords": {
            "IEEE Keywords": [
                "Circuits",
                "Computer architecture",
                "Microprocessors",
                "Standards",
                "Optimization",
                "Design methodology",
                "Libraries",
                "Internet of Things",
                "Design automation",
                "Calibration"
            ],
            "Author Keywords": [
                "successive-approximation-register (SAR)",
                "analog-to-digital converter (ADC)",
                "digital-to-analog converter (DAC)",
                "standard cell",
                "custom design"
            ]
        },
        "title": "Understanding Synthesizable Design Methodologies for Mixed-Signal SAR ADC Circuits"
    },
    {
        "authors": [
            "Demeke Shumeye Lakew",
            "Anh-Tien Tran",
            "Nhu-Ngoc Dao",
            "Sungrae Cho"
        ],
        "published_in": "Published in: IEEE Transactions on Network Science and Engineering ( Early Access )",
        "date_of_publication": "03 January 2024",
        "doi": "10.1109/TNSE.2023.3349321",
        "publisher": "IEEE",
        "abstract": "Given the notable surge in Internet of Things (IoT) devices, low Earth orbit (LEO) satellites and unmanned aerial vehicles (UAVs) have emerged as promising networking components to supplement the network capacity and ensure seamless coverage in 6G, especially over remote areas. However, task offloading and resource management are challenging to realize because of the limited connectivity duration of LEO satellites attributable to their high mobility and UAVs limited resources. Thus, this paper proposes a network model in which mobile edge computing (MEC)-enabled multiple LEO satellites in-orbit provide computational services for a resource-constrained energy harvesting UAV (EH-UAV). The EH-UAV collects data from remote IoT/sensor devices and periodically generates a computational task. To optimize the system model, we formulate a joint LEO-MEC server selection, transmission power allocation, and partial task offloading decision-making problem to maximize the service satisfaction and alleviate energy dissipation under the constraints of connectivity duration, task deadline, and available energy. To circumvent the non-convexity and dynamicity of the problem, it is reformulated as a reinforcement learning problem and solved using a novel mixed discrete-continuous control deep reinforcement learning ( $MDC^{2}-DRL$ ) based algorithm with an action shaping function. Simulation results demonstrate that $MDC^{2}-DRL$ effectively converges and outperforms the existing methods.",
        "issn": {
            "Electronic ISSN": "2327-4697"
        },
        "keywords": {
            "IEEE Keywords": [
                "Satellites",
                "Task analysis",
                "Low earth orbit satellites",
                "Autonomous aerial vehicles",
                "Resource management",
                "Servers",
                "Internet of Things"
            ],
            "Author Keywords": [
                "LEO satellite",
                "deep reinforcement learning",
                "computation offloading",
                "resource allocation",
                "UAV",
                "energy harvesting"
            ]
        },
        "title": "Intelligent Self-Optimization for Task Offloading in LEO-MEC-Assisted Energy-Harvesting-UAV Systems"
    },
    {
        "authors": [
            "Eman A. Al-Shahari",
            "Ghadah Aldehim",
            "Mohammed Aljebreen",
            "Jehad Saad Alqurni",
            "Ahmed S. Salama",
            "Sitelbanat Abdelbagi"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "08 May 2024",
        "doi": "10.1109/ACCESS.2024.3397619",
        "publisher": "IEEE",
        "abstract": "The integration of Internet of Things (IoT) technology with deep learning (DL) algorithms has revolutionized plant disease detection and crop management and paved the way for sustainable agricultural practices. Real-time information on soil moisture, plant health, and environmental conditions can be collected by deploying a network of connected devices and sensors in agricultural fields. DL algorithms, specifically convolutional neural networks (CNN), analyze this massive dataset, facilitating timely and accurate recognition of plant diseases. This early detection allows farmers to implement targeted interventions, like adjustment to irrigation or precision application of pesticides, maximizing crop yield, and minimizing resource wastage. Therefore, this article develops an automated Plant Disease Detection and Crop Management using a spotted hyena optimizer with deep learning (APDDCM-SHODL) technique for Sustainable Agriculture. The APDDCM-SHODL approach aims to detect the existence of plant diseases and improve crop productivity in the IoT infrastructure. To achieve this, the APDDCM-SHODL method primarily employs the Vector Median Filter (VMF) technique. In addition, the Densely Connected Networks (DenseNet201) model is deployed for feature extraction. In addition, the SHO technique is exploited for optimum hyperparameter tuning of the DenseNet201 model. Furthermore, the classification algorithm is implemented by using the recurrent spiking neural network (RSNN) model. A brief set of experiments has been made to determine the experimental validation of the APDDCM-SHODL model. The comprehensive results inferred that the APDDCM-SHODL method reaches remarkable performance over other existing methods with the highest accuracy of 98.60%.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Crops",
                "Vectors",
                "Plant diseases",
                "Agriculture",
                "Internet of Things",
                "Tuning",
                "Detection algorithms",
                "Hyperparameter optimization",
                "Sustainable development",
                "Deep learning"
            ],
            "Author Keywords": [
                "Plant Disease Detection",
                "Internet of Things",
                "hyperparameter tuning",
                "Sustainable Agriculture",
                "Deep Learning"
            ]
        },
        "title": "Internet of Things Assisted Plant Disease Detection and Crop Management using Deep Learning for Sustainable Agriculture"
    },
    {
        "authors": [
            "Inam Ullah",
            "Deepak Adhikari",
            "Farhad Ali",
            "Ahmad Ali",
            "Habib Khan",
            "Amin Sharafian",
            "Suresh Manic Kesavan",
            "Xiaoshan Bai"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "10 June 2024",
        "doi": "10.1109/TCE.2024.3411606",
        "publisher": "IEEE",
        "abstract": "In the context of the Internet of Things (IoT), wireless sensor networks (WSNs) play a significant role by collecting and analyzing real-time data on consumer behavior, product availability, and other environmental factors, thus enhancing e-commerce operations. Researchers in the domain of consumer electronics are working actively to increase the sustainability of WSNs and decrease their ecological footprint by developing energy-efficient algorithms, data compression techniques, and sensor designs. These advancements are directed toward optimizing energy consumption and minimizing environmental consequences while ensuring the smooth functionality of IoT applications. Organizations are emphasizing the need for collaboration between designers, business organizations, developers, and regulatory authorities to implement eco-friendly industrial laws to enhance the effectiveness of the e-commerce industry. Decision-making in the e-commerce sector is complicated by information overload, subjectivity of preferences, trust issues, product variety, lack of personalized assistance, return and exchange concerns, dynamic pricing and discounts, limited sensory experience, and complex decision-making processes. To address these challenges, an entropy-based multicriteria decision-making (MCDM) approach is proposed to assist personalization in e-commerce and improve user interfaces, recommendation systems, product information transparency, and consumer trust. The utilization of the MCDM technique facilitates the customization of electronic commerce platforms, whereby affordable products can be identified and retrieved according to individual user preferences. The study investigates the impact of electronic gadgets and energy-efficient WSNs on inventory management, customer preferences, and online shopping sustainability, highlighting the intricate relationship between data privacy, energy efficiency, and governance. The proposed method is compared to other state-of-the-art methodolo...",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Electronic commerce",
                "Wireless sensor networks",
                "Consumer electronics",
                "Energy efficiency",
                "Cloud computing",
                "Internet of Things",
                "Energy consumption"
            ],
            "Author Keywords": [
                "Energy-Efficient Wireless Sensor Networks",
                "Multi-Criteria Decision Making",
                "Consumer Electronics",
                "Cloud Computing",
                "Internet of Things",
                "E-commerce",
                "Personalization in E-commerce"
            ]
        },
        "title": "Revolutionizing E-Commerce With Consumer-Driven Energy-Efficient WSNs: A Multi-Characteristics Approach"
    },
    {
        "authors": [
            "Chenggang Shan",
            "Runze Gao",
            "Qinghua Han",
            "Tian Liu",
            "Zhen Yang",
            "Jinhui Zhang",
            "Yuanqing Xia"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "23 September 2024",
        "doi": "10.1109/JIOT.2024.3466231",
        "publisher": "IEEE",
        "abstract": "As more IoT applications gradually move towards the cloud-edge collaborative model, the containerized scheduling of workflows extends from the cloud to the edge. However, given the high delay of the communication network, loose coupling of structure, and resource heterogeneity between the cloud and the edge, workflow containerization scheduling in the cloud-edge scenarios faces the difficulty of resource coordination and application collaboration management. To address these two issues, we propose a KubeEdge-Cloud-Edge-Scheduling scheme named KCES. This workflow containerization scheduling scheme includes a cloud-edge workflow scheduling engine for KubeEdge and incorporates workflow scheduling strategies for tasks’ horizontal roaming and vertical offloading. This paper proposes a cloud-edge workflow scheduling model and node model, as well as a workflow scheduling engine designed to maximize cloud-edge resource utilization under the constraint of workflow task delay. A cloud-edge resource hybrid management technology is used to devise the cloud-edge resource evaluation and resource allocation algorithms to achieve cloud-edge resource collaboration. Based on the ideas of distributed functional roles and the hierarchical division of computing power, the horizontal roaming among the edges and cloud-edge vertical offloading strategies for workflow tasks are designed to realize cloud-edge application collaboration. Experimental results using a customized IoT application workflow instance demonstrate that KCES outperforms three comparing algorithms in total workflow time, average workflow time, and resource usage and features horizontal roaming and vertical offloading of workflow tasks.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Cloud computing",
                "Collaboration",
                "Scheduling",
                "Job shop scheduling",
                "Processor scheduling",
                "Containers",
                "Image edge detection"
            ],
            "Author Keywords": [
                "Workflow scheduling",
                "cloud-edge collaboration",
                "resource collaboration",
                "application collaboration",
                "horizontal roaming",
                "vertical offloading"
            ]
        },
        "title": "KCES: A Workflow Containerization Scheduling Scheme Under Cloud-Edge Collaboration Framework"
    },
    {
        "authors": [
            "Sujit Bebortta",
            "Subhranshu Sekhar Tripathy",
            "Surbhi Bhatia Khan",
            "Maryam M. Al Dabel",
            "Ahlam Almusharraf",
            "Ali Kashif Bashir"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "21 August 2024",
        "doi": "10.1109/TCE.2024.3445290",
        "publisher": "IEEE",
        "abstract": "Recently, there has been a rise in the use of Unmanned Areal Vehicles (UAVs) in consumer electronics, particularly for the critical situations. Internet of Things (IoT) technology and the accessibility of inexpensive edge computing devices present novel prospects for enhanced functionality in various domains through the utilization of IoT-based UAVs. One major difficulty of this perspective is the challenges of computation offloading between resource-constrained edge devices, and UAVs. This paper proposes an innovative framework to solve the computation offloading problem using a multi-objective Deep reinforcement learning (DRL) technique. The proposed approach helps in finding a balance between delays and energy consumption by using the concept of Tiny Machine Learning (TinyML). It develops a low complexity frameworks that make it feasible for offloading tasks to edge devices. Catering to the dynamic nature of edge-based UAV networks, TinyDeepUAV suggests a vector reinforcement that can change weights dynamically based on various user preferences. It is further conjectured that the structure can be enhanced by Double Dueling Deep Q Network (D3QN) for optimal improvement of the optimization problem. The simulation results depicts a trade-off between delay and energy consumption, enabling more effective offloading decisions while outperforming benchmark approaches.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Task analysis",
                "Autonomous aerial vehicles",
                "Optimization",
                "Delays",
                "Energy consumption",
                "Deep reinforcement learning",
                "Consumer electronics"
            ],
            "Author Keywords": [
                "Unmanned Aerial Vehicles",
                "Internet of Things",
                "TinyML",
                "Deep Reinforcement Learning",
                "Edge Intelligence",
                "Multi Objective Optimization",
                "Energy Efficiency",
                "Delay Minimization"
            ]
        },
        "title": "TinyDeepUAV: A Tiny Deep Reinforcement Learning Framework for UAV Task Offloading in Edge-Based Consumer Electronics"
    },
    {
        "authors": [
            "Haoyi Zhang",
            "Jiahao Song",
            "Haoyang Luo",
            "Xiyuan Tang",
            "Yuan Wang",
            "Runsheng Wang",
            "Ru Huang"
        ],
        "published_in": "Published in: IEEE Transactions on Circuits and Systems II: Express Briefs ( Early Access )",
        "date_of_publication": "25 July 2024",
        "doi": "10.1109/TCSII.2024.3433543",
        "publisher": "IEEE",
        "abstract": "This letter presents a NOR-structured physically unclonable function (PUF) tailored for low-cost Internet of Things (IoT) applications. The proposed NOR-structured PUF utilizes a single minimum-sized differential NMOS pair, capitalizing on threshold-voltage mismatch as the entropy source. Fabricated in 65nm CMOS, the basic PUF cell is a 58F2 differential NMOS pair, demonstrating a raw bit error rate (BER) of 0.31%. To further enhance the stability and achieve an ultralow BER, we introduce an area-efficient redundancy strategy. By incorporating 4x redundancy cells (266F2 in total), the prototype chip achieves an ultra-low BER (zero error in 20M bits), over a wide temperature range (-20 to 125° C) and supply voltage variations (0.8 to 1.2V). The core energy consumption is only 63fJ/bit, offering a low-cost and highly stable solution for IoT applications.",
        "issn": {
            "Print ISSN": "1549-7747",
            "Electronic ISSN": "1558-3791"
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer architecture",
                "Microprocessors",
                "Redundancy",
                "Bit error rate",
                "Circuits",
                "Termination of employment",
                "Transistors"
            ],
            "Author Keywords": [
                "NOR-structured",
                "physically unclonable function (PUF)",
                "redundancy strategy",
                "ultra-low BER",
                "low-cost"
            ]
        },
        "title": "A 266F2 Ultra Stable Differential NOR-Structured Physically Unclonable Function with < 6× 10-9 Bit Error Rate Through Efficient Redundancy Strategy"
    },
    {
        "authors": [
            "Thinh Q. Dinh",
            "Son Hoang Dau",
            "Eva Lagunas",
            "Symeon Chatzinotas",
            "Diep N. Nguyen",
            "Dinh Thai Hoang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "16 October 2024",
        "doi": "10.1109/JIOT.2024.3481373",
        "publisher": "IEEE",
        "abstract": "Satellite communication (SatCom) systems play a vital role in providing global connectivity and enable a wide range of applications, including Internet of Things (IoT) connectivity for remote areas such as forests and oceans. Two crucial resource allocation challenges in SatCom are Beam Placement (BP) and Frequency Assignment (FA) problems, which involve the clique cover (CC) and graph coloring (GC) problems, respectively. Conventional solutions for these problems incur excessive computational cost, which is intractable for classical computers. A promising approach is to formulate these problemsusing the Ising model, construct their Hamiltonians, and then solve them efficiently by a quantum computer. However, the current quantum computers have very limited hardware and can only handle rather small inputs. To overcome this limitation, we propose a hybrid quantum-classical computational pipeline where an efficient Hamiltonian Reduction method is the key for solving large CC/GC instances. Through experiments on real quantum computers, our reduction method outperforms commercial solutions, allowing quantum annealers to handle significantly larger BP/FA instances while maintaining high probability to achieve feasible solutions and near-optimal performance. Although the inherent hardness of the CC/GC problems cannot be overcome by quantum computing, our research contributes to the early exploration of quantum computing in the context of the complex optimization problems in SatCom systems, particularly in the realm of IoT connectivity for remote areas.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Quantum computing",
                "Satellites",
                "Computers",
                "Annealing",
                "Quantum annealing",
                "Optimization",
                "Pipelines",
                "Internet of Things",
                "Qubit",
                "Low earth orbit satellites"
            ],
            "Author Keywords": [
                "Beam Placement",
                "Quantum Computing",
                "Satellite Communication",
                "Low Earth Orbit"
            ]
        },
        "title": "Quantum Annealing for Complex Optimization in Satellite Communication Systems"
    },
    {
        "authors": [
            "Hua Wei",
            "Junfeng Miao",
            "Jianhui Lv",
            "Chien-Ming Chen",
            "Saru Kumari",
            "Mohammed Amoon"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "01 October 2024",
        "doi": "10.1109/TCE.2024.3471573",
        "publisher": "IEEE",
        "abstract": "The integration of Artificial Intelligence (AI) with the Internet of Things (IoT) marks a considerable advancement in data collection and utilization across various sectors. In the realm of complex scenarios, dance-consumer electronics (DCE) employ this technology to provide detailed and trustworthy documentation and three-dimensional digital capture of dancers’ movements. However, the handling of sensitive physiological data poses significant security and trustworthiness risks in such consumer electronics, including potential breaches that could be exploited for location tracking and behavior prediction. To address the challenges of encrypted traffic detection and the need for trustworthiness in IoT, this paper introduces the AI-Enhanced Anonymous Traffic Filtering Framework, which features terminal anonymization. This framework employs the Inception-SelfAttBiGRU model to filter malicious traffic and includes a credibility-based filtering mechanism prior to this module to reduce network load and enhance system efficiency for those consumer electronics. The evaluation concentrated on two main areas: assessing the access credibility-based filtering mechanism’s ability to identify and block malicious DCE endpoints and evaluating the effectiveness of the encrypted traffic classification technique. Using simulated environments and public datasets, this comprehensive assessment demonstrated that the dual-layer protection mechanism effectively detects and blocks malicious traffic, thereby enhancing system security and maintaining network integrity.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Cryptography",
                "Feature extraction",
                "Internet of Things",
                "Artificial intelligence",
                "Humanities",
                "Consumer electronics",
                "Deep learning",
                "Information integrity",
                "Information filtering",
                "Data privacy"
            ],
            "Author Keywords": [
                "AIoT",
                "consumer electronics",
                "security and trustworthiness",
                "encrypted traffic filtering"
            ]
        },
        "title": "Secure and Trustworthy Data Management Mechanism for Dance-Consumer Electronics in AIoT"
    },
    {
        "authors": [
            "Hongyang Xie",
            "Haoqiang Liu",
            "Huiming Chen",
            "Shaohan Feng",
            "Zhaobin Wei",
            "Yonghong Zeng"
        ],
        "published_in": "Published in: IEEE Transactions on Vehicular Technology ( Early Access )",
        "date_of_publication": "04 September 2024",
        "doi": "10.1109/TVT.2024.3454771",
        "publisher": "IEEE",
        "abstract": "Advanced in the proliferation of the Internet of Things (IoT), a plethora of functions have been integrated in vehicular networks and thereby transfered it into a smart network. However, the contradiction between the limited on-vehicle computing resource and the massive data collected by these IoT devices hinders the broader adoption of vehicular network as a vast variety of on-vehicle applications are latency-sensitive. To address this issue, vehicular edge computing has become a promising technology as it can offload a large number of tasks from its proximal vehicles. However, the offloading methods recently utilized are inefficient while dealing with multi-user vehicular networks under dynamic scenarios. To design a superior offloading method that can effectively and efficiently offload tasks from vehicles to servers, multiple objectives and constraints with various topologies should be considered. In this paper, instead of constructing a typical multi-user and multi-server vehicular edge computing scenario, a complex scenario with more uncertainties, i.e. urban scenario, is modeled. We propose a Hybrid Architecture Matching Algorithm (HAMA) to minimize the average time latency subject to the constraint on energy consumption and evaluate the proposed algorithm in the above two scenarios. Moreover, HAMA is constructed based on hybrid centralized-distributed architecture, which can process the centralized collected information on a distributed manner. Experimental results demonstrate that the matching algorithm can significantly reduce average time latency, achieving up to a 68% improvement compared to local execution.",
        "issn": {
            "Print ISSN": "0018-9545",
            "Electronic ISSN": "1939-9359"
        },
        "keywords": {
            "IEEE Keywords": [
                "Servers",
                "Delays",
                "Computational modeling",
                "Vehicle dynamics",
                "Urban areas",
                "Roads",
                "Handover"
            ],
            "Author Keywords": [
                "Vehicular edge computing",
                "matching algorithm",
                "resource allocation",
                "optimization algorithm"
            ]
        },
        "title": "Efficient Multi-User Resource Allocation for Urban Vehicular Edge Computing: A Hybrid Architecture Matching Approach"
    },
    {
        "authors": [
            "Junwei Tang",
            "Sijie Zhou",
            "Tao Peng",
            "Xiaoyun Yan",
            "Xinrong Hu",
            "Wenlong Tian"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "09 October 2024",
        "doi": "10.1109/JIOT.2024.3477442",
        "publisher": "IEEE",
        "abstract": "Android has occupied an important share of the operating system of intelligent terminal devices in the Internet of Things (IoT), and the malicious applications of Android have increased rapidly, posing a serious threat to the security of IoT. Machine learning has advanced significantly in the detection of android malware. In order to protect intellectual property, Android developers have begun to use packing techniques to enhance the security of their applications. However, attackers can also pack their malware, which may make feature extraction ineffective and interfere with the prediction results of learning-based classifiers. For this issue, we have designed and implemented a tool by dynamically loading the original DEX using a shell DexClassLoader to generate a batch of packed Android applications. And we have verified that several existing methods fail when faced with packed samples. Therefore, we propose a novel malware detection method called DTDroid that can resist code packing. DTDroid automatically captures network traffic characteristics of target samples based on fuzzy testing and network traffic packet extraction. At the same time, the dynamic behavior characteristics of the target application can be obtained by monitoring the corresponding runtime function calls and system status. The extracted two types of features are contextually spliced and converted into grayscale images, and then detected based on deep learning model. Experimental results show that the detection accuracy of our method reaches 94.22% and 95.14% respectively on two kinds of packed datasets, indicating that DTDroid has better robustness for packed samples than the existing methods.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Malware",
                "Feature extraction",
                "Telecommunication traffic",
                "Operating systems",
                "Machine learning",
                "Static analysis",
                "Security",
                "Codes",
                "Internet of Things",
                "Gray-scale"
            ],
            "Author Keywords": [
                "Packed Malware",
                "Grayscale Image",
                "Deep Learning",
                "Traffic Feature",
                "Dynamic Analysis"
            ]
        },
        "title": "DTDroid: Adversarial Packed Android Malware Detection Based on Traffic and Dynamic Behavioral"
    },
    {
        "authors": [
            "Xiankun Fu",
            "Li Pan",
            "Shijun Liu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "29 August 2024",
        "doi": "10.1109/JIOT.2024.3451496",
        "publisher": "IEEE",
        "abstract": "Nowadays, the internet is rapidly evolving towards the future of the Internet of Things (IoT), where billions or even trillions of edge devices may be interconnected. The proliferation of network cameras and the advancement of IoT technologies have provided broader opportunities for data collection and utilization. In the past, the massive real-time videos generated by network cameras were mostly transmitted over the network to the cloud for analysis. However, due to network speed limitations, the latency incurred by uploading all videos to the cloud makes it difficult to meet the real-time requirements of video analysis. While edge computing significantly reduces latency, the computational capabilities of edge devices are limited, making it difficult to handle large amounts of real-time video data. In this paper, we introduce a real-time video processing framework called DeepVA, which utilizes cloud-edge collaboration technology to reduce latency in real-time video processing and enhance the accuracy of analysis. The DeepVA framework incorporates the DRLVA video frame distribution algorithm based on deep reinforcement learning, which dynamically determines whether to distribute video frames for processing at the cloud or edge. To evaluate the performance of the proposed DRLVA algorithm, we first verify that it is superior to several other deep reinforcement learning-based distribution algorithms on the Gym environment. We also evaluate the performance of DeepVA on the MOT2015 dataset, MOTSynth dataset, and real campus surveillance videos. The experiments show that our DeepVA outperforms both cloud-only and edge-only solutions in terms of reducing latency and improving accuracy.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Cloud computing",
                "Streaming media",
                "Real-time systems",
                "Edge computing",
                "Cameras",
                "Bandwidth",
                "Computational modeling"
            ],
            "Author Keywords": [
                "Real-time video analysis",
                "cloud-edge collaboration",
                "edge computing",
                "deep reinforcement learning",
                "the Internet of Things"
            ]
        },
        "title": "A DRL-Based Real-Time Video Processing Framework in Cloud-Edge Systems"
    },
    {
        "authors": [
            "Ying Nie",
            "Zheng-Yi Chai",
            "Li Lu",
            "Ya-Lun Li"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "30 September 2024",
        "doi": "10.1109/JIOT.2024.3459019",
        "publisher": "IEEE",
        "abstract": "With the rapid development of Internet of Things (IoT) technology, the number of IoT devices has increased dramatically and a large amount of data has been generated. In order to further reduce the resource cost required for task offloading, it is necessary to design task offloading methods with high energy efficiency and low latency. Considering the correlation between task offloading process and time in real-time interactive scenarios, we propose an evolutionary algorithm framework with online load prediction based on CNN-GRU hybrid model and channel Attention Mechanism (AM). In the model construction stage, we firstly combined Convolutional Neural Network (CNN) and Gated Recurrent Unit (GRU) to learn the features and patterns of historical data. In order to reduce the loss of historical information, the channel attention mechanism is introduced into the CNN-GRU model to enhance the influence of important features between information. In the model training stage, the optimal individual training model generated by the evolutionary algorithm is used to further optimize the training accuracy and training effect of CNN-GRU-AM. In the test phase, the optimized CNN-GRU-AM network is used to predict the task load online and dynamically allocate computing resources while training the model online iteratively, which further reduces the delay and energy consumption of the task and improves the offloading performance of the system. The simulation results show that the proposed algorithm effectively reduces the system delay and the overall energy consumption.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling",
                "Delays",
                "Predictive models",
                "Convolutional neural networks",
                "Mathematical models",
                "Prediction algorithms",
                "Internet of Things",
                "Training",
                "Energy consumption",
                "Edge computing"
            ],
            "Author Keywords": [
                "task offloading",
                "resource allocation",
                "prediction",
                "evolutionary algorithm",
                "hybrid neural network"
            ]
        },
        "title": "Task Offloading in Edge Computing: An Evolutionary Algorithm With Multi-Model Online Prediction"
    },
    {
        "authors": [
            "Peng Qin",
            "Min Fu",
            "Yang Fu",
            "Rui Ding",
            "Xiongwen Zhao"
        ],
        "published_in": "Published in: IEEE Transactions on Wireless Communications ( Early Access )",
        "date_of_publication": "27 September 2024",
        "doi": "10.1109/TWC.2024.3464610",
        "publisher": "IEEE",
        "abstract": "Through deploying satellites and unmanned aerial vehicles (UAVs) with onboard processing capability, the space-air-ground edge computing network (SAGECN) is poised to support ubiquitous access and computation offloading for Internet of Things (IoT) terminals deployed in remote areas. However, the current SAGECN faces several challenges in realizing its full potential, such as scarce spectrum resources, diverse computational demands, and dynamic network circumstances. To meet these challenges, we propose a cluster-non-orthogonal multiple access (C-NOMA)-enabled SAGECN model, where a satellite and multiple UAVs act as collaborative edge servers to execute tasks from IoT terminals. Since each offloaded task should be processed via a specific program, the edge servers carry out program caching, whilst transfer the tasks that do not match the cached programs to another server in a multi-hop manner. Considering the delay-sensitive requirements of computation tasks, we formulate a joint task offloading, communication-computation-cache resource assignment, and routing plan problem, aimed at minimizing the average system latency. To cope with this challenging issue, we partition it into three subproblems. First, a multi-agent learning-based approach is developed to collaboratively train the task offloading, flight trajectory, and program caching. As a step further, two optimization subroutines are embedded to perform routing plan, subchannel allocation, and power control, thereby rendering the overall solution. Experimental results reveal that our approach achieves outstanding performance in terms of system delay and spectrum efficiency.",
        "issn": {
            "Print ISSN": "1536-1276",
            "Electronic ISSN": "1558-2248"
        },
        "keywords": {
            "IEEE Keywords": [
                "Autonomous aerial vehicles",
                "Delays",
                "Routing",
                "Satellites",
                "Servers",
                "Wireless communication",
                "Trajectory",
                "NOMA",
                "Resource management",
                "Optimization"
            ],
            "Author Keywords": [
                "Space-air-ground edge computing network (SAGECN)",
                "task offloading",
                "program caching",
                "trajectory design",
                "routing plan"
            ]
        },
        "title": "Collaborative Edge Computing and Program Caching With Routing Plan in C-NOMA-Enabled Space-Air-Ground Network"
    },
    {
        "authors": [
            "Xinyu Fan",
            "Jie Hu",
            "Kun Yang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "06 November 2024",
        "doi": "10.1109/JIOT.2024.3491358",
        "publisher": "IEEE",
        "abstract": "Wireless energy transfer (WET) technology enables internet of things (IoT) networks to have longer lifetime without the need of frequent battery replacements. This paper proposes a payload-adaptive hybrid MAC (PAH-MAC) protocol by considering both contention and time-slot allocation among sensors for large scale data collection scenarios within WET enhanced IoT networks. The PAH-MAC protocol employs different access strategies based on the payload size of data packets. Furthermore, leveraging synchronization mechanisms, the coordinator periodically dispatches energy packets to replenish the battery of all sensors. The stationary performance of PAH-MAC protocol is analyzed by invoking Markov chains. Considering the traffic variations caused by changes in the number of sensors in the network, a slot adaptive adjustment algorithm is proposed to maximize the throughput and energy performance. Therefore, the coordinator adaptively adjust the duration of the contention period and the energy transmission period within the next superframe according to access conditions observed in current superframe. Another transmission power adjustment algorithm is proposed for sensors to improve their energy efficiency. According to our simulation, our proposed protocol consume less energy in low-traffic scenarios than the classic CSMA/CA and another baseline protocol. Furthermore, in high-traffic scenarios, our proposed protocol achieves higher throughput, lower latency, and lower energy consumption than its counterpart.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Protocols",
                "Media Access Control",
                "Internet of Things",
                "Wireless sensor networks",
                "Throughput",
                "Batteries",
                "Synchronization",
                "Sensor phenomena and characterization",
                "Resource management"
            ],
            "Author Keywords": [
                "Data and energy integrated network",
                "CSMA",
                "hybrid protocol",
                "SWIPT",
                "Markov",
                "adaptive adjustment"
            ]
        },
        "title": "Payload-Adaptive Hybrid MAC Protocol for Sustainable Internet of Things Networks: Protocol Design and Adaptive Adjustment Mechanisms"
    },
    {
        "authors": [
            "Jiao Zhang",
            "Xiong Li",
            "Pandi Vijayakumar",
            "Wei Liang",
            "Victor Chang",
            "Brij B. Gupta"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "07 June 2024",
        "doi": "10.1109/TCE.2024.3411551",
        "publisher": "IEEE",
        "abstract": "Federated learning enables model training for the consumer-driven Internet of Things (IoT) in a distributed manner without violating individual privacy. Several secure aggregation protocols have been proposed for large-scale federated learning models in IoT scenarios. However, the communication and computational overhead grow quadratically with the number of clients, which becomes a significant obstacle to these secure aggregation protocols. To address this problem, some work utilizes deterministic graphs of logarithmic degrees, such as the Harary graph or Erd˝=os-Rényi graph, instead of the complete communication graph. The graph generated under the given fixed conditions is unique and invariant throughout the federated learning process. In this paper, we propose SparsiFL, a graph sparsification-based secure aggregation protocol for federated learning, which significantly reduces communication and computational overhead while maintaining correctness and privacy. The SparsiFL takes a complete graph as input and formulates the optimization problem as an uncertain graph sparsification task, which reduces the number of edges and redistributes the probabilities attached to them. In the process, SparsiFL also preserves the underlying structure. The graph can accurately and efficiently approximate the secure-sharing task in secure aggregation. Theoretical analysis shows correctness and privacy. Experiments show that SparsiFL reduces the communication and computational overheads up to 6.10× and 3.16× as compared to other related approaches.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Protocols",
                "Servers",
                "Federated learning",
                "Internet of Things",
                "Privacy",
                "Computational modeling",
                "Cryptography"
            ],
            "Author Keywords": [
                "Federated learning",
                "Secure aggregation",
                "Graph sparsification",
                "Communication-efficient learning"
            ]
        },
        "title": "Graph Sparsification-Based Secure Federated Learning for Consumer-Driven Internet of Things"
    },
    {
        "authors": [
            "Yang Wei",
            "Kim-Fung Tsang"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "01 July 2024",
        "doi": "10.1109/TCE.2024.3421546",
        "publisher": "IEEE",
        "abstract": "The emerging of LoRaWAN creates huge benefits for various IoT applications in consumer markets, given its advantages of long range and low power consumption. Generally, different applications have different quality of service (QoS) requirements (e.g., PLR, latency, etc.). The diversity of QoS requirements makes it difficult to comprehensively and uniformly evaluate LoRaWAN performance. Additionally, the conventional QoS metrics do not directly reflect the users satisfaction for the specific application. To address these challenges, the Satisfaction IoT Index (SDex) based on the IEEE 2668 standard is proposed to evaluate LoRaWAN performance comprehensively and uniformly. Based on defined QoS metrics (i.e., PLR, latency, energy consumption, and coverage efficiency), SDex is established as a user-centric index to evaluate the users satisfaction degree for the specific application by measuring difference between practical and required performances. Besides, SDex-based fuzzy comprehensive evaluation (SDex-FCE) scheme is developed to grade the SDex score taking attribution uncertainty at adjacent levels into consideration. Finally, the best LoRaWAN configuration solution can be identified as the one with the highest SDex score. Experiments for case studies (i.e., smart metering and smart healthcare) are performed to illustrate the effectiveness of SDex and provide guidance for developers to choose the best LoRaWAN configuration solution.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Quality of service",
                "Uplink",
                "Measurement",
                "Logic gates",
                "Internet of Things",
                "Downlink",
                "Performance evaluation"
            ],
            "Author Keywords": [
                "QoS",
                "LoRaWAN",
                "IEEE 2668",
                "fuzzy comprehensive evaluation"
            ]
        },
        "title": "QoS Management and Performance Evaluation of LoRaWAN Using IEEE 2668 With Fuzzification"
    },
    {
        "authors": [
            "Guosheng Kang",
            "Hangyu Cheng",
            "Jianxun Liu",
            "Yiping Wen",
            "Jun Peng"
        ],
        "published_in": "Published in: IEEE Transactions on Automation Science and Engineering ( Early Access )",
        "date_of_publication": "28 March 2024",
        "doi": "10.1109/TASE.2024.3354929",
        "publisher": "IEEE",
        "abstract": "Business process modeling is widely used in modern organizations for business description. Business Process Modeling Notation (BPMN), as a de-facto modeling standard, represents business process models in graphical notations. Nevertheless, BPMN lacks intuitive modeling tasks for Industrial Internet application scenarios (e.g., IoT tasks and multi-instance tasks with constraints). Although there are some works on extending BPMN elements to improve the model representation, most of them stay in the conceptual model only without tool support, or they are confined to specific domains. In this paper, we extend both BPMN elements and attributes for application in the Industrial Internet context, and two modeling tools are implemented in a client version and Web version to support business process modeling via low-code, enabling the BPMN extension model from conceptual to executable level. Two real-world case studies in Industrial Internet are conducted to show the usefulness of process models with BPMN extension. Furthermore, a comprehensive user experiment is conducted to evaluate the extended process models and tools, and the experimental results show that process models with extension have better quality compared with traditional process models and provided tools are effective for business process modeling in Industrial Internet applications Note to Practitioners —This article was motivated by the problem of business process modeling for Industrial Internet. Practically, existing methods extend BPMN elements and add text annotations to enhance the representation of process models. Although these methods facilitate process participants to understand the process models in detail, most of them focus on conceptual modeling and increase the complexity of process models. Moreover, the process models and multi-instance business constraints of business processes involving IoT elements are difficult to be represented by using native BPMN. This article proposes a comprehensive ...",
        "issn": {
            "Print ISSN": "1545-5955",
            "Electronic ISSN": "1558-3783"
        },
        "keywords": {
            "IEEE Keywords": [
                "Business",
                "Process modeling",
                "Internet",
                "Task analysis",
                "Computational modeling",
                "Security",
                "Internet of Things"
            ],
            "Author Keywords": [
                "Business process modeling",
                "BPMN extension",
                "low-code",
                "Industrial Internet"
            ]
        },
        "title": "Business Process Modeling for Industrial Internet Application via BPMN Extension"
    },
    {
        "authors": [
            "I-An Lin",
            "Yuan-Wei Cheng",
            "Trong-Yen Lee"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "17 October 2024",
        "doi": "10.1109/JSEN.2024.3478810",
        "publisher": "IEEE",
        "abstract": "In recent years, the rapid development of the Internet of Things (IoT) and AI deep learning technologies has enhanced communication between devices and increased the availability of image data. In smart agriculture, sensors remotely monitor crop information, and cameras detect leaf growth and health, thereby enhancing farm productivity and quality. However, most object detection algorithms, such as YOLOv4, have too many parameters for real-time deployment on edge computing devices. To address this, we propose the MobileNetv3-YOLOv4 (MobYOLv4) architecture, which uses MobileNetv3 as the backbone and Depthwise Separable Convolution to reduce complexity. This approach balances accuracy and detection speed. Experimental results reveal that the lightest MobYOLv4 architecture reduces parameters by 82.31%, achieves 29.48 FPS (a 38.14% increase), and has an accuracy of 97.74%. The MobYOLv4 with widths 1.4 architecture, optimized with width multiplier, reduces parameters by 78.03%, achieves 28.87 FPS, and has an accuracy of 98.84%. The MobYOLv4 with widths 1.8 architecture achieves the highest accuracy at 99.65%, reduces parameters by 72.62%, and achieves 27.91 FPS. Furthermore, the MQTT IoT transmission protocol is used to manage farm data and facilitate device communication. The proposed adaptive width multiplier strategies in MobileNetv3-YOLOv4 for lightweight object detection balance accuracy improvement per additional parameter with overall model accuracy, effectively optimizing the model. The width multiplier can significantly impact the model’s parameters and actual detection accuracy.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Accuracy",
                "Computer architecture",
                "Computational modeling",
                "Neural networks",
                "Internet of Things",
                "YOLO",
                "Real-time systems",
                "Smart agriculture",
                "Intelligent sensors",
                "Transfer learning"
            ],
            "Author Keywords": [
                "Sensors",
                "YOLOv4",
                "Real-Time",
                "MobileNetv3",
                "Lightweight Object Detection"
            ]
        },
        "title": "Enhancing Smart Agriculture with Lightweight Object Detection: MobileNetv3-YOLOv4 and Adaptive Width Multipliers"
    },
    {
        "authors": [
            "Haowen Zhang",
            "Jinwang Feng",
            "Juan Li",
            "Qing Yao"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "16 October 2024",
        "doi": "10.1109/JSEN.2024.3478214",
        "publisher": "IEEE",
        "abstract": "The proliferation of IoT devices generates vast amounts of sensor data, making sensor data mining an increasingly important research topic in the IoT domain. The Top-k similarity search plays a crucial role in sensor data mining, forming the foundation for advanced techniques such as sensor data classification, clustering, and anomaly detection. The need for efficient algorithms to enhance search speed drives our research. This paper introduces an efficient method named PIPDTW for Top-k similarity search of sensor data using Dynamic Time Warping (DTW) distance, a prevalent but computationally intensive similarity measure in time-series data mining. To address the high computational complexity, PIPDTW first employs perceptually important points (PIPs) to transform lengthy sensor data into concise vector representations. We then introduce a novel distance measure, DTWIV, based on PIP representations to quickly estimate DTW distances between the query q and all sensor data. Finally, during the online search, we apply two levels of filtering based on approximate ranking and the lower-bound technique to efficiently prune dissimilar data, reducing the need for exact DTW distance computations and thereby achieving efficient querying. Experiments conducted on synthetic and real-world publicly available datasets, compared with relevant competitors, demonstrate that PIPDTW slightly sacrifices retrieval accuracy but significantly reduces runtime.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Data mining",
                "Filtering",
                "Accuracy",
                "Time series analysis",
                "Approximation algorithms",
                "Trajectory",
                "Internet of Things",
                "Intelligent sensors",
                "Computational efficiency"
            ],
            "Author Keywords": [
                "Top-k similarity search",
                "sensor data",
                "dynamic time warping",
                "perceptually important points",
                "dual-bound filtering"
            ]
        },
        "title": "Efficient Top-k DTW-based Sensor Data Similarity Search using Perceptually Important Points and Dual-Bound Filtering"
    },
    {
        "authors": [
            "Jing Zhang",
            "Jiaxuan Zhang",
            "Fei Shen",
            "Feng Yan",
            "Zhiyong Bu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "11 September 2024",
        "doi": "10.1109/JIOT.2024.3457855",
        "publisher": "IEEE",
        "abstract": "The Space-Air-Ground Integrated Network (SA-GIN) integrates satellites, Unmanned Aerial Vehicles (UAVs), and terrestrial remote clouds to provide seamless network access and high-volume computing services for remote Internet of Things (IoT) devices, thus alleviating geographic and resource constraints. Existing methods typically focus on the network dynamics while overlooking the comprehensive consideration of device dynamics, namely the time-varying task performance weights, task sizes, and task processing demands. Moreover, the centralized learning-based offloading schemes often lead to substantial signaling overhead. To bridge these gaps, this paper proposes a distributed dynamic task offloading mechanism with game-theoretic Multi-Agent Stochastic Learning (MASL). Technically, a stochastic game is formulated with each device as a player minimizing its weighted sum cost of latency and energy. We prove the existence of Nash equilibrium (NE) for our proposed game and propose a Multi-Agent Entropy-enhanced Stochastic Learning (MESL) algorithm in a fully distributed manner with no information exchange among IoT devices. By introducing the entropy of decision probability for each device, MESL increases decision dimensions, accelerates convergence, and facilitates optimal strategy achievement. Experimental results show that the MESL algorithm significantly reduces the overall cost and greatly enhances the convergence speed in dynamic SAGIN environments compared to existing algorithms.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Heuristic algorithms",
                "Games",
                "Dynamic scheduling",
                "Satellites",
                "Space-air-ground integrated networks",
                "Performance evaluation"
            ],
            "Author Keywords": [
                "Space-air-ground integrated network (SAGIN)",
                "distributed computing offloading",
                "potential game",
                "Nash equilibrium",
                "multi-agent stochastic learning"
            ]
        },
        "title": "DOGS: Dynamic Task Offloading in Space-Air-Ground Integrated Networks With Game-Theoretic Stochastic Learning"
    },
    {
        "authors": [
            "Shanpeng Xiao",
            "Zhiqing Wei",
            "Zhiqiang Wu",
            "Qianli Liu",
            "Shang Liu",
            "Weixi Gu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "16 October 2024",
        "doi": "10.1109/JIOT.2024.3480992",
        "publisher": "IEEE",
        "abstract": "In the edge learning process of B5G IoT systems, the allocation strategy of edge sensors will directly affect the learning results of the system. This paper proposes multiple methods to allocate edge devices that can learn and predict the usage of spectrum data, aiming to improve the efficiency and fairness of edge learning. We design an efficient edge device allocation strategy to enhance the edge learning efficiency and propose a metric called Ineffective Transmission Parameter (ITP) to evaluate its performance. To solve the optimization problem, mathematical analysis is performed and closed-form expressions are obtained. The scenarios considered include: devices with different learning performance and the same learning performance on the same band. We propose three edge device allocation methods: iterative hierarchical Hungarian allocation, bow allocation, and category-divided allocation to ensure the fairness of edge learning among sub-bands. The fairness of the system is measured by evaluating the lowest learning performance in the sub-band. To adapt to the actual scenario, we enhance fairness by introducing band attribute parameters (considering the priority, anti-interference ability, and congestion level of the main users of the sub-band). Simulation results show that the proposed strategy significantly improves the ITP of edge learning in IoT systems, especially in the case of varying band utilization. The fairness scheme improves the overall edge learning fairness of the system.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Sensors",
                "Performance evaluation",
                "Resource management",
                "Internet of Things",
                "Monitoring",
                "Real-time systems",
                "Wideband",
                "Simulation",
                "Protection"
            ],
            "Author Keywords": []
        },
        "title": "Edge Learning Based Sensor Allocation Strategy in Internet of Things System"
    },
    {
        "authors": [
            "Daniela Annunziata",
            "Diletta Chiaro",
            "Pian Qi",
            "Francesco Piccialli"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "31 October 2024",
        "doi": "10.1109/JIOT.2024.3488855",
        "publisher": "IEEE",
        "abstract": "The paradigm of Augmented Intelligence of Things (AIoT) aims to empower IoT devices with intelligent capabilities to analyze data, make informed decisions, and execute actions autonomously. This study focuses on enhancing collaboration between vehicles and road infrastructure within the AIoT framework, particularly in the context of self-driving cars and smart city environments. A Proof of Concept (PoC) is presented, introducing a vehicle road cooperation framework tailored for Online-VIC (Vehicle-Infrastructure Cooperation) forecasting tasks. This framework enables real-time information exchange and trajectory prediction of target agents by leveraging IoT sensor technologies and incorporating two layers of cooperation: ego-vehicles and infrastructures. Experimental results demonstrate that the integration of information from both layers enhances prediction metrics compared to approaches focusing on individual layers. Comparative analysis with existing method, PP-VIC, underscores the superiority of the proposed framework in trajectory prediction. This research offers a promising avenue for enhancing communication and collaboration between infrastructure and autonomous vehicles, thereby contributing to the development of more efficient and safer transportation systems in smart cities.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Roads",
                "Internet of Things",
                "Autonomous vehicles",
                "Forecasting",
                "Cameras",
                "Artificial intelligence",
                "Vehicle-to-everything",
                "Pedestrians",
                "Real-time systems"
            ],
            "Author Keywords": [
                "Augmented Intelligence of Things (AIoT)",
                "Vehicle Road Cooperation",
                "V2X",
                "Ego-vehicles",
                "Trajectory prediction",
                "Online-VIC forecasting"
            ]
        },
        "title": "On the Road to AIoT: A Framework for Vehicle Road Cooperation"
    },
    {
        "authors": [
            "Yingxue Liu",
            "Jing Sun",
            "Zhuo Chen",
            "Feng Gao",
            "Xiangbo Yuan",
            "Zijian Zhang",
            "Lei Zhang",
            "Meng li",
            "Qi Gao",
            "Liehuang Zhu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "06 November 2024",
        "doi": "10.1109/JIOT.2024.3486994",
        "publisher": "IEEE",
        "abstract": "The Internet of Things (IoT) system collects data from various smart devices and sensors to make tailored decisions. However, in complex IoT setups, privacy can be compromised through data collection, processing, and sharing. Thus, safeguarding data during transmission is crucial. Combining blockchain technology with covert communication has made progress in addressing these issues, but challenges like low data embedding rates and identifiable blockchain transactions with covert data remain. To tackle this, this paper analyzes Ethereum transaction field formats, identifying the input data field as the target for high-capacity embedding. A data covert transmission scheme using hybrid embedding in contract fields is proposed, employing LSB steganography to embed covert data in images and incorporating the image URL into the input data field of Ethereum smart contracts, increasing embedding rates. Additionally, a new method for data embedding based on contract relationships is introduced, enabling covert data transmission through the invocation relationships of smart contracts, rather than directly embedding data in transactions. This makes transactions theoretically indistinguishable from regular ones, enhancing security. Finally, evaluations of undetectability, embedding rate, and scalability show that the proposed schemes excel in all three areas.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Smart contracts",
                "Steganography",
                "InterPlanetary File System",
                "Data privacy",
                "Scalability",
                "Data communication",
                "Bitcoin",
                "Sun",
                "Smart devices"
            ],
            "Author Keywords": [
                "Convert channel",
                "Blockchain",
                "Ethereum",
                "Smart Contracts",
                "Embedding rate",
                "Undetectability"
            ]
        },
        "title": "Covert Transmission via Steganography and Smart Contract"
    },
    {
        "authors": [
            "Jiayan Wang",
            "Jing Zhao",
            "Li Li",
            "Zichi Wang",
            "Hanzhou Wu",
            "Deyang Wu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "03 September 2024",
        "doi": "10.1109/JIOT.2024.3453960",
        "publisher": "IEEE",
        "abstract": "Video Internet of things (IoT) is widely used in the fields of safe city, smart transportation and logistics warehousing, which facilitates the acquisition of important environmental and semantic information. However, the tampering of unauthorized video data may seriously violate user privacy and even harm society. Although the existing video watermarking technology provides an effective solution for copyright protection, it still faces challenges to achieve robust copyright authentication in the complex IOT environment. In this paper, a robust blind video watermarking based on ring Tensor and BCH coding is proposed. Firstly, ring subbands of different sizes are constructed in the spatial domain of the video, and the ring subbands of consecutive video frames are combined into a ring tensor for copyright watermark embedding. Secondly, to balance the imperceptibility and robustness of the copyright watermark, an adaptive BCH coding scheme is developed, which uses the modified differential entropy to calculate the video complexity and automatically selects the appropriate watermark coding parameters. Finally, a quaternary synchronisation watermark embedding strategy is designed to solve the time synchronization destruction caused by video frame rate conversion. A synchronization ring is constructed within each video frame using the strong correlation between adjacent frames. When the video is subjected to temporal synchronization attacks, the synchronization watermark is extracted from the synchronization ring to restore the synchronization of the copyright watermark. Extensive experimental results demonstrate that the proposed scheme can effectively resist common video processing while exhibiting excellent robustness against video attacks in complex internet environments.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Watermarking",
                "Tensors",
                "Encoding",
                "Robustness",
                "Frequency-domain analysis",
                "Feature extraction",
                "Synchronization"
            ],
            "Author Keywords": [
                "Video watermarking",
                "Ring tensor",
                "Blind watermarking",
                "BCH coding",
                "Double watermarking"
            ]
        },
        "title": "Robust Blind Video Watermarking Based on Ring Tensor and BCH Coding"
    },
    {
        "authors": [
            "Deqin Xu",
            "Weixin Bian",
            "Qingde Li",
            "Dong Xie",
            "Jun Zhao",
            "Yao Hu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "24 October 2024",
        "doi": "10.1109/JIOT.2024.3486005",
        "publisher": "IEEE",
        "abstract": "The 5G networks can provide high data rates, ultra-low latency and huge network capacity. In 5G networks environment, the popularity of the Internet of Things (IoT) has led to a rapid increase in the amount of data. Multi-server distributed cloud computing technology provides an excellent solution to alleviate network pressure caused by the rapid growth of data. However, this technology serves as a two-edged weapon, which not only makes various IoT applications possible, but also brings growing concerns for user privacy and ever pressing security challenges. To ensure the high security of 5G network-based applications, we design a secure user anonymity-preserving biometrics and PUFs-based multi-server authentication scheme with key agreement. In our method, we make full use of the inherent security features of user fingerprint and smart device PUF to design a secure multi-server authentication scheme with key agreement in 5G Networks. The proposed scheme is able to resist recognized attacks and its robustness has been verified by security analysis.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Authentication",
                "Biometrics",
                "Passwords",
                "Physical unclonable function",
                "Servers",
                "Security",
                "Smart cards",
                "5G mobile communication",
                "Internet of Things",
                "Reliability"
            ],
            "Author Keywords": [
                "Multi-server",
                "Mutual authentication",
                "Biometrics",
                "Physically unclonable functions"
            ]
        },
        "title": "A Secure User Anonymity-Preserving Biometrics and PUFs-Based Multi-Server Authentication Scheme With Key Agreement in 5G Networks"
    },
    {
        "authors": [
            "Bangyong Sun",
            "Changyu Wu",
            "Mengying Yu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "24 July 2024",
        "doi": "10.1109/JIOT.2024.3432975",
        "publisher": "IEEE",
        "abstract": "Spectral imaging acquires more analyzable and distinguishable information than RGB imaging and has become an emerging technique powering Internet of Things (IoT). Thus, generating spectral images from existing RGB cameras is instrumental to high level vision tasks. In this paper, an IoT-oriented spectral reconstruction model with parallel fusion of Convolutional Neural Network (CNN) and Transformer (PFCT) is proposed to efficiently recover hyperspectral images (HSI) from RGB counterparts, facilitating low-cost and non-hardware-specific spectral image acquisition. Recent works mainly utilize CNNs to extract local features by stacking more layers, ignoring the latent correlations of global features. In our PFCT network, we take advantage of lightweight CNNs to efficiently perceive local details, and exploit Transformer blocks to fully capture the global context. Based on the architecture, a parallel fusion module is further designed to deeply interact and fuse the features obtained by CNN and Transformer in both directions. The final output spectral image is generated with same spatial sizes of the original image based on the deeply fused features. Consequently, the proposed PFCT network achieved high performance on four benchmark datasets compared to several state-of-the-art networks with a relatively small number of parameters.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Hyperspectral imaging",
                "Convolutional neural networks",
                "Feature extraction",
                "Transformers",
                "Imaging",
                "Reconstruction algorithms"
            ],
            "Author Keywords": [
                "Convolutional neural network",
                "Internet of Things",
                "spectral reconstruction",
                "Transformer"
            ]
        },
        "title": "Spectral Reconstruction for Internet of Things Based on Parallel Fusion of CNN and Transformer"
    },
    {
        "authors": [
            "Zheyi Chen",
            "Yunjing Ren",
            "Yujie Xue",
            "Alireza Jolfaei",
            "Amr Tolba",
            "Keping Yu",
            "Hailin Feng"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "04 October 2024",
        "doi": "10.1109/TCE.2024.3472468",
        "publisher": "IEEE",
        "abstract": "IoT-enabled consumer electronics can collect and analyze data to improve functionality and user experiences, increasingly becoming part of edge computing networks. Decentralized federated learning is envisioned as a promising AIoT framework to leverage the data generated by these interconnected consumer electronics without compromising user privacy, where computations are performed closer to the data source rather than in a centralized cloud-based data center. However, a number of obstacles bottleneck the deployment of decentralized learning frameworks under a large-scale AIoT scenario, such as heterogeneity issues, communication efficiency, large-scale issues and others. Specifically, heterogeneity issues include diverse learning models, non-i.i.d. data distribution and unbalanced datasets. In this paper, we propose a trustworthy and adaptive knowledge distillation based decentralized federated learning framework to overcome a number of heterogeneity and large-scale issues. To achieve this goal, we formulate a decentralized optimization problem which formally presents various heterogeneity issues. The proposed framework is composed of two components, including a knowledge distillation based algorithm to transfer knowledge over diverse learning models as well as a real-time evaluation module to adapt client drift issues. Convergence analysis demonstrates that the convergence of the proposed framework is guaranteed. Finally, we conduct extensive experiments over three public datasets to validate the effectiveness and efficiency of proposed framework in the large-scale IoT scenario, illustrating that the proposed framework could significantly improve the learning performance over various metrics and outperforming state-of-the-art baseline approaches.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Federated learning",
                "Adaptation models",
                "Biological system modeling",
                "Training",
                "Data models",
                "Consumer electronics",
                "Peer-to-peer computing",
                "Convergence",
                "Optimization",
                "Internet of Things"
            ],
            "Author Keywords": [
                "AIoT",
                "decentralized federated learning",
                "knowledge distillation",
                "large-scale AI"
            ]
        },
        "title": "A Trustworthy Decentralized Federated Learning Framework for Consumer Electronics: Mitigating Large-Scale AIoT Heterogeneity through Transfer Knowledge Integration"
    },
    {
        "authors": [
            "Jian Chen",
            "Yuzhu Hu",
            "Lalit Garg",
            "Thippa Reddy Gadekallu",
            "Gautam Srivastava",
            "Wei Wang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "18 July 2024",
        "doi": "10.1109/JIOT.2024.3430297",
        "publisher": "IEEE",
        "abstract": "Internet of Things (IoT) devices like wearable devices have enabled quick monitoring of ECG signals with lower resources than multi-electrode ECG devices, opening up development opportunities for sustainable ECG-based emotion recognition. However, existing methods that rely on pre-designed features extracted from single-lead ECG signals cannot automatically extract effective features from the original ECG signal collected by IoT devices. To address this limitation, we propose a novel approach leveraging signal transformation and graph representation learning for ECG-based emotion recognition. The signal graph learning process can be divided into local subgraph learning for ECG representation learning and signal enhancement graph to derive the graph-enhanced representation. We employ a designed loss function by calculating cosine similarity to extract an effective representation of the original signal from the transformed signal in the local subgraph learning. Additionally, we utilize a graph convolution model based on the signal enhancement graph to obtain a graph-enhanced representation of the ECG signal. The method incorporates six signal transformations and constructs a self-signal transformation graph. For emotion recognition, we design a classification network comprising convolutional neural networks (CNNs) and long short-term memory (LSTM) networks. Experiments on public datasets show the superiority of our method among other baselines. Ablation studies are conducted to verify the performance.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Electrocardiography",
                "Emotion recognition",
                "Feature extraction",
                "Long short term memory",
                "Representation learning",
                "Internet of Things",
                "Wearable devices"
            ],
            "Author Keywords": [
                "AIoT",
                "ECG signal",
                "Emotion Recognition",
                "Wearable devices",
                "Affective computing"
            ]
        },
        "title": "Graph Enhanced Low-Resource ECG Representation Learning for Emotion Recognition Based on Wearable Internet of Things"
    },
    {
        "authors": [
            "Yunfan Zhang",
            "Feihuang Chu",
            "Luliang Jia",
            "Miao Yu",
            "Wenting Cao"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "16 August 2024",
        "doi": "10.1109/TCE.2024.3412166",
        "publisher": "IEEE",
        "abstract": "The Internet of Things (IoT) is a key factor driving the widespread use of smart consumer electronics, yet remote areas are difficult to access. Satellite Internet of Things (SIoT), as a complement and extension of IoT, has the advantages of being able to achieve global coverage and the deployment of sensors that are virtually unrestricted by space. However, due to its highly exposed star-ground links, SIoT is highly susceptible to jamming attacks. Therefore, this paper investigates the dynamic anti-jamming scheme for SIoT to decrease consumption in the jamming environment. A hierarchical anti-jamming Stackelberg game (HASG) is proposed to characterize the adversarial interactions between the jamming satellites and the ground cells, and it has been demonstrated that a Stackelberg equilibrium exists in the proposed HASG. Subsequently, to avoid the dimensional catastrophe associated with exhaustive enumeration, the leader subgame and follower subgame are described as many-to-one matching and one-to-one matching problems, respectively. A low-complexity matching strategy is proposed, and the existence of stable suboptimal solutions is proved. Simulation results show that the proposed anti-jamming matching strategy can realize efficient communication.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Satellites",
                "Jamming",
                "Games",
                "Low earth orbit satellites",
                "Satellite antennas",
                "Transmitting antennas",
                "Interference"
            ],
            "Author Keywords": [
                "Anti-jamming communication",
                "satellite communication",
                "Stackelberg game",
                "matching game"
            ]
        },
        "title": "Dynamic Anti-Jamming Strategy in SIoT: A Stackelberg-Matching Game Approach"
    },
    {
        "authors": [
            "Zhimin He",
            "Weijie Tan",
            "Yangyang Long",
            "Yuling Chen",
            "Kun Niu",
            "Chunguo Li",
            "Weiqiang Tan"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "11 September 2024",
        "doi": "10.1109/JIOT.2024.3457827",
        "publisher": "IEEE",
        "abstract": "The Internet of Vehicles (IoV) is a specific instance of the Internet of Things (IoT) in the transportation field, driven by application requirements such as intelligent traffic services and automatic vehicle control, can improve road safety and enhancing transmission efficiency. However, highly open networks tend to bring more security threats, and secure authentication becomes an important guarantee for reliable communication. Traditional IoT authentication and key agreement methods are costly, inefficient, and rely on third-party trusted institutions, making them unsuitable for direct application in IoV systems. To meet the security authentication needs of IoV, and improve authentication efficiency and anonymity, this paper proposes a verifiable commitment-based mutual authentication and key agreement protocol, called VC-MAKA. In VC-MAKA, we construct a verifiable commitment scheme where the verifier can verify the committed secret. Furthermore, based on this verifiable commitment scheme, we implement secure authentication and session key agreement, allowing vehicles to freely negotiate secure session keys and achieving conditional anonymous protection. Additionally, the proposed VC-MAKA also achieves rapid session key updates, enhancing the security of the session keys. We have conducted formal and informal security analysis, and the results show that VC-MAKA meets security requirements such as mutual authentication, anonymity, traceability, and untraceability. Moreover, we have used the ProVerif tool for security experiment and performance comparison analysis, and the results indicate that compared to other schemes, the VC-MAKA protocol offers higher security and better efficiency.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Authentication",
                "Security",
                "Protocols",
                "Internet of Things",
                "Cryptography",
                "Computational efficiency",
                "Intelligent sensors"
            ],
            "Author Keywords": [
                "Authentication",
                "Key Agreement",
                "Internet of Vehicles",
                "PUF",
                "Commitment"
            ]
        },
        "title": "VC-MAKA: Mutual Authentication and Key Agreement Protocol Based on Verifiable Commitment for Internet of Vehicles"
    },
    {
        "authors": [
            "Phu Tran Tin",
            "Minh-Sang Van Nguyen",
            "Dinh-Hieu Tran",
            "Cong T. Nguyen",
            "Symeon Chatzinotas",
            "Zhiguo Ding",
            "Miroslav Voznak"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "06 August 2024",
        "doi": "10.1109/JIOT.2024.3439377",
        "publisher": "IEEE",
        "abstract": "This paper investigates the combination of active reconfigurable intelligent surfaces (aRIS) with cognitive radio networks (CRn) enabled by non-orthogonal multiple access (NOMA) to enhance the capability of aRIS-based Internet of Things (IoT) systems. The proposed system model enhances overall performance and energy allocation by combining aRIS and NOMA. In this proposed system paradigm, the secondary source (SS) controls the information transmission to two secondary users (SUs) via the aRIS element. Transmission power limitations are put in place to lessen the impression that base stations are interfering with the main purpose. This paper evaluates critical system performance indicators such as outage probability (OP), achievable ergodic rate (AER), throughput, and energy efficiency (EE). Specifically, the impact of the distance between the aRIS unit and the base station on AER is explored. The examination of the correlation among SS-aRIS-Users in the presence of the Nakagami-m fading scenario is further investigated. Such discoveries offer significant perspectives for the enhancement and configuration of aRIS-NOMA frameworks in CRn, contributing to the advancement of adaptable communication infrastructures. In contrast to the traditional method that uses orthogonal multiple access (OMA), the aRIS-NOMA system proposed in CRn appears to be a promising way to improve the performance of IoT networks based on aRIS. Simulations have demonstrated significant improvements in spectral efficiency, with gains as high as 10%–20% observed. This suggests that performance has significantly improved, particularly for OP and AER. Finally, the Monte Carlo simulation confirms and strengthens these findings.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "NOMA",
                "Throughput",
                "Reconfigurable intelligent surfaces",
                "Internet of Things",
                "6G mobile communication",
                "Wireless communication",
                "System performance"
            ],
            "Author Keywords": [
                "active reconfigurable intelligent surfaces",
                "cognitive radio",
                "energy efficiency",
                "achievable ergodic rate",
                "non-orthogonal multiple access",
                "outage probability"
            ]
        },
        "title": "Performance Analysis of User Pairing for Active RIS-Enabled Cooperative NOMA in 6G Cognitive Radio Networks"
    },
    {
        "authors": [
            "Yuchen Li",
            "Weifa Liang",
            "Zichuan Xu",
            "Wenzheng Xu",
            "Xiaohua Jia"
        ],
        "published_in": "Published in: IEEE Transactions on Mobile Computing ( Early Access )",
        "date_of_publication": "06 September 2024",
        "doi": "10.1109/TMC.2024.3455357",
        "publisher": "IEEE",
        "abstract": "With the advance of mobile edge computing (MEC) and the Internet of Things (IoT), digital twin (DT) has become an emerging technology for provisioning IoT services between the real world and the cyber world. In this paper, we consider the state updating of DTs in an MEC network through synchronizing DTs with their physical objects, by proposing a DT state synchronization framework and synchronization algorithms. We make use of an energy-constrained UAV for data collection in a sensor network, as an illustrative example for the DT state updating of each object (sensor), and then use the DT data of objects (sensors) later for fidelity-aware query services. To this end, we first formulate a novel DT state staleness minimization problem with the aim to minimize the average DT state staleness of all objects for a finite time horizon, under a given update budget per update round. We then propose an optimal algorithm for a special case of the problem where the budget per update round is exactly $K$ objects synchronizing with their DTs. Inspired by the solution of this special case, we then devise an algorithm for the DT state staleness minimization problem by reducing to the award collection maximization problem, assuming that the volume of the update data generated by each object per update round is given. Otherwise, we adopt a deep learning method to predict the volume of the update data. To demonstrate the importance of the DT state staleness in practical applications, we consider fidelity-aware query services in the MEC network where the fidelity of the query result is determined by its source data freshness (or the DT state staleness of querying objects), and we develop a cost-effective evaluation plan for each query. We finally evaluate the performance of the proposed algorithms through simulations. Simulation results demonstrate that the proposed algorithms are promising, where the average DT state staleness by the proposed algorithms are 44.92% less than the base...",
        "issn": {
            "Print ISSN": "1536-1233",
            "Electronic ISSN": "1558-0660"
        },
        "keywords": {
            "IEEE Keywords": [
                "Synchronization",
                "Autonomous aerial vehicles",
                "Internet of Things",
                "Costs",
                "Heuristic algorithms",
                "Minimization",
                "Data collection"
            ],
            "Author Keywords": [
                "Digital twins",
                "DT state staleness",
                "mobile edge computing",
                "DT synchronizations with physical objects",
                "cost modelling",
                "algorithm design and analysis",
                "prediction mechanism"
            ]
        },
        "title": "Budget-Constrained Digital Twin Synchronization and Its Application on Fidelity-Aware Queries in Edge Computing"
    },
    {
        "authors": [
            "Kadir Ozlem",
            "Asli Tuncay Atalay",
            "Ozgur Atalay",
            "Gökhan Ince"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/JIOT.2024.3490981",
        "publisher": "IEEE",
        "abstract": "Textile products are present in almost every aspect of human life. With the introduction of electronic textiles, textile products have become capable of converting various physiological and environmental stimuli into electrical signals, many of which are of vital importance to humans. Therefore, these products require real-time (low-latency) and robust computing systems. However, due to comfort considerations, they cannot accommodate powerful computing resources. In this study, a novel fog computing-based framework (FogETex) is proposed to meet the needs of e-textile applications. FogETex is a Platform-as-a-Service model that is cross-platform supported, scalable, and operates in real-time. This framework encompasses end-to-end integration of the system including textile-based Internet of Things (T-IoT) device, fog devices, and the cloud. Fog devices consist of a broker that manages the fog node and a worker that handles incoming computation requests. Sensor data is transmitted to the fog node through a mobile application, and system architecture can be monitored through developed user interfaces. Resource usage from broker devices is monitored in real time to prevent worker devices from experiencing overload. For the system case study, a deep learning-based gait phase analysis application using textile-based capacitive sensors is employed. FogETex was evaluated in terms of time characteristics, resource usage, and network bandwidth usage using a mock client to determine the ideal system performance and an actual client to conduct real-world tests. The fog devices outperformed the cloud system in these metrics. Besides being developed primarily for electronic textile applications, FogETex framework can accommodate other IoT devices as well.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Smart textiles",
                "Sensors",
                "Internet of Things",
                "Edge computing",
                "Cloud computing",
                "Real-time systems",
                "Wearable devices",
                "Textiles",
                "Monitoring",
                "Medical services"
            ],
            "Author Keywords": [
                "Fog computing",
                "electronic textile",
                "gait phase analysis",
                "deep learning",
                "socket programming",
                "raspberry pi"
            ]
        },
        "title": "FogETex: Fog Computing Framework for Electronic Textile Applications"
    },
    {
        "authors": [
            "Ismail Adam",
            "Meshari D. Alanazi",
            "Sheroz Khan",
            "Mashkuri Yaacob",
            "Anis Nurashikin Nordin",
            "Hasmah Mansor",
            "Mohamed Hadi Habaebi"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "04 October 2024",
        "doi": "10.1109/ACCESS.2024.3460877",
        "publisher": "IEEE",
        "abstract": "Inductive Power Transfer (IPT) finds applications in various fields that require episodic rather than continuous power supplies, such as implantable medical devices, consumer electronics, IoT applications, civil structure monitoring, and electric vehicle charging. The efficiency of IPT systems depends on the operating frequency which is determined by the coupling coefficient and is affected by the distance between the transmitting and receiving coils. Therefore, accurate estimation of the coupling coefficient and resonance frequency of the IPT is essential to ensure maximum power transfer. This paper presents a novel yet straightforward method for estimating the series-to-series resonant frequency of Inductive Resonant Power Transfer (IRPT) systems. The proof utilizes post-processed experimental data, combining Total Harmonic Distortion (THD) and RMS voltage (VRMS) values evaluated on the transmitter side and obtained through Fast Fourier Transform (FFT). The research shows that the resonant frequency can be determined by identifying the points of lowest THD and highest VRMS on the transmitting side. The analytical plots are experimentally validated by establishing a transmitter unit with a variable frequency pulse generator to drive a DC-to-AC converter connected to a primary coil and a capacitor. The setup includes a display unit and multiple input switches for manually adjusting frequency settings, as well as activating and deactivating the DC-to-AC converter in 10Hz, 100Hz, and 1kHz frequency steps. The study shows promising results in determining the IPT resonance frequency to maximize power transfer, paving the way for advances in inductive resonant Power Transfer applications, such as move-and-charge Wireless Power Transfer (WPT). The novelty of this study not only lies in the simplicity of the proposed technique but also its feasibility and applicability to real-world application scenarios. The results of this work have applications in many wireless technolo...",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Resonant frequency",
                "Resonance",
                "Coils",
                "Inductance",
                "Wireless power transfer",
                "Vehicle dynamics",
                "RLC circuits",
                "Capacitors",
                "Internet of Things",
                "Frequency modulation"
            ],
            "Author Keywords": [
                "Fast Fourier Transform",
                "Total Harmonic Distortion",
                "Wireless Power Transfer",
                "Inductive Resonant Series-to-series WPT",
                "Resonance Frequency"
            ]
        },
        "title": "Resonance Frequency Estimation in Series-to-Series Inductive Power Transfer"
    },
    {
        "authors": [
            "Shaobo Jia",
            "Rong Wang",
            "Yi Lou",
            "Ning Wang",
            "Di Zhang",
            "Keshav Singh",
            "Shahid Mumtaz"
        ],
        "published_in": "Published in: IEEE Transactions on Wireless Communications ( Early Access )",
        "date_of_publication": "24 September 2024",
        "doi": "10.1109/TWC.2024.3461800",
        "publisher": "IEEE",
        "abstract": "Ambient backscatter communication (AmBC) has emerged as a paradigm distinguished by its energy-efficient attributes and low-power dynamics, ideally suited to address the vast expanse of the Internet of Things (IoT). Unmanned aerial vehicles (UAVs) deployed with flexibility can effectively establish wireless connections for isolated IoT devices through AmBC. This paper delves into the exploration of secure transmission within a UAV-assisted AmBC network, particularly addressing the challenges posed by the presence of a passive eavesdropper. Specifically, a UAV is utilized as an aerial base station to offer services to an isolated ground user, an AmBC tag transmits its information to its associated receivers by leveraging the UAV’s radio frequency (RF) signals. Furthermore, a multi-antenna cooperative jammer is integrated within the system to intentionally interfere with the eavesdropper without affecting legitimate receivers. To characterize the secrecy performance, the expressions of secrecy outage probability of the air-ground link and backscatter link are both deduced leveraging a two-layer Gaussian-Chebyshev quadrature. Moreover, the asymptotic behaviors under the high signal-to-noise ratio (SNR) regime are also analyzed. Monte Carlo simulations are performed to validate the correctness and effectiveness of the analytical results.",
        "issn": {
            "Print ISSN": "1536-1276",
            "Electronic ISSN": "1558-2248"
        },
        "keywords": {
            "IEEE Keywords": [
                "Backscatter",
                "Autonomous aerial vehicles",
                "Internet of Things",
                "RF signals",
                "Jamming",
                "Radio frequency",
                "Wireless networks"
            ],
            "Author Keywords": [
                "Ambient backscatter communication",
                "UAV",
                "physical layer security",
                "cooperative jamming",
                "secrecy outage probability"
            ]
        },
        "title": "Secrecy Performance Analysis of UAV-assisted Ambient Backscatter Communications with Jamming"
    },
    {
        "authors": [
            "Guangyao Ding",
            "Guanding Yu",
            "Jiantao Yuan",
            "Shengli Liu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "14 August 2024",
        "doi": "10.1109/JIOT.2024.3444455",
        "publisher": "IEEE",
        "abstract": "To satisfy the requirements of many industrial applications, realizing ultra-reliable low-latency communication (URLLC) has become one of the major challenges for future wireless networks. This paper considers a downlink multi-user multiple-input single-output (MISO) system in the Internet-of-Things (IoT) networks, in which a multi-antenna base station (BS) serves multiple delay-sensitive IoT users, each equipped with a single antenna. To minimize the overall end-to-end delay, we jointly optimize the beamforming vectors and the packet blocklength to balance the queuing delay and the transmission delay. The problem is formulated as a Markov decision process (MDP), whose optimal solution can be theoretically found. However, the complexity on finding the optimal resource allocation and blocklength selection strategy is prohibitively high for real-system deployments due to the large state and action space. To overcome this issue, we simplify the original problem and develop an iterative algorithm to solve the simplified problem based on the uplink-downlink duality theory. Since solving the simplified problem would result in sub-optimal solutions and may degrade the latency performance, we further develop a deep reinforcement learning (DRL)-based beamforming and blocklength selection framework to efficiently learn the optimal strategy of the original MDP. Simulation results demonstrate that the proposed algorithms can effectively improve the latency performance compared with the benchmark algorithm.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Delays",
                "Ultra reliable low latency communication",
                "Array signal processing",
                "Downlink",
                "MISO communication",
                "Reliability",
                "Vectors"
            ],
            "Author Keywords": [
                "5G",
                "ultra-reliable low-latency communications (URLLC)",
                "resource allocation",
                "beamforming"
            ]
        },
        "title": "Joint Beamforming Design and Blocklength Optimization for Low-Latency Multi-User MISO URLLC Systems"
    },
    {
        "authors": [
            "Ayshika Kapoor",
            "Dheeraj Kumar"
        ],
        "published_in": "Published in: IEEE Communications Surveys & Tutorials ( Early Access )",
        "date_of_publication": "29 July 2024",
        "doi": "10.1109/COMST.2024.3434510",
        "publisher": "IEEE",
        "abstract": "In recent years, advancements in Artificial Intelligence (AI), the Internet of Things (IoT) and wireless technologies have propelled the evolution of smart cities. Urban sensing systems collect real-time data from urban areas for various applications, such as environmental monitoring, healthcare, and intelligent transportation, that contribute to the growth of smart cities. In urban sensing, the active participation of users gives rise to participatory sensing, where individuals contribute real-time data through their smartphones or IoT devices, but it encounters bottlenecks in communication, network latency, and user privacy with an exponential rise in data. A prominent characteristic of urban sensing applications is the highly individualized and personal nature of the data, e.g., location and time. Hence, adequate privacy and security provisions are required for these applications to succeed on a high scale. Conventional centralised machine learning approaches expose participants to potential vulnerabilities from malicious tasking servers or inference based on anonymized data. Federated learning (FL) has been proposed as the most viable alternative that leverages the advances in modern-day smartphones’ computation and communication capabilities by allowing participants to train local models on their devices. These models are aggregated by the application server to form a global model without the need for users to share their private data. However, large-scale FL-based urban sensing systems are still not practical due to various challenges associated with their real-life implementation. This paper presents a comprehensive survey addressing practical challenges in implementing FL-based urban sensing applications, e.g., inference attacks, poisoning attacks, and fair incentivization to participants while preserving privacy. We then provide an extensive survey on the use of FL in various urban sensing applications, highlighting that current applications do not simultan...",
        "issn": {
            "Electronic ISSN": "1553-877X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Surveys",
                "Data models",
                "Smart cities",
                "Internet of Things",
                "Servers",
                "Security"
            ],
            "Author Keywords": [
                "Federated learning",
                "urban sensing",
                "smart city",
                "inference attack",
                "poisoning attack",
                "incentive mechanism"
            ]
        },
        "title": "Federated Learning for Urban Sensing Systems: A Comprehensive Survey on Attacks, Defences, Incentive Mechanisms, and Applications"
    },
    {
        "authors": [
            "Anastasios Zafeiropoulos",
            "Nikos Filinis",
            "Eleni Fotopoulou",
            "Symeon Papavassiliou"
        ],
        "published_in": "Published in: IEEE Communications Magazine ( Early Access )",
        "date_of_publication": "15 July 2024",
        "doi": "10.1109/MCOM.001.2200583",
        "publisher": "IEEE",
        "abstract": "The dominance of microservices-based development frameworks and the development of complex distributed applications, along with the massive production of powerful Internet of things (IoT) and edge computing devices, are paving the way toward the transition from centralized to distributed computing paradigms, where applications are deployed across the computing continuum. In the computing continuum, we consider the management of resources from the IoT to the edge to the cloud part of the infrastructure. To achieve effective management of resources and application workloads, a set of challenges have been identified. These challenges revolve around the need to introduce automation and distributed intelligence characteristics into emerging orchestration mechanisms. To treat this, the design of synergetic or cooperative orchestration mechanisms that take advantage of artificial intelligence techniques is arising as a promising approach. In this article, we describe the design and development of a synergetic orchestration mechanism, powered by multi-agent reinforcement learning (MARL), to guide the autoscaling of distributed applications that are deployed across the computing continuum. A MARL environment and a corresponding agent have been developed and applied in real and simulated environments to guide scaling actions, while indicative evaluation results are provided.",
        "issn": {
            "Print ISSN": "0163-6804",
            "Electronic ISSN": "1558-1896"
        },
        "keywords": {
            "IEEE Keywords": [
                "Artificial intelligence",
                "Microservice architectures",
                "Training",
                "Decision making",
                "Cloud computing",
                "Reinforcement learning",
                "Edge computing"
            ],
            "Author Keywords": []
        },
        "title": "AI-Assisted Synergetic Orchestration Mechanisms for Autoscaling in Computing Continuum Systems"
    },
    {
        "authors": [
            "Muhammad Yahya",
            "Muhammad Naeem",
            "Zeeshan Kaleem",
            "Ali H. Alenezi",
            "Waleed Ejaz"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "19 September 2024",
        "doi": "10.1109/JIOT.2024.3462899",
        "publisher": "IEEE",
        "abstract": "Unmanned Aerial Vehicles (UAVs) have been gaining much attention in the next-generation wireless networks due to their ability to enhance coverage and provide advanced services, particularly for first responders. UAVs equipped with Mobile Edge Computing (MEC) capabilities can migrate computational resources to airborne platforms. However, it is crucial to manage resources efficiently to optimize overall network performance. Moreover, in public safety scenarios, UAVs can help charge low-power Internet of Things (IoT) devices to sustain system operations. A holistic approach to managing communication, computation, caching, and energy resources is necessary to leverage UAV-assisted MEC networks fully. We formulated an optimization problem to minimize latency and reduce resource costs associated with communication, computation, caching, and energy harvesting while maximizing the number of IoT devices served by UAVs. Therefore, we integrated digital twin technology to analyze the latency. The optimization problem is challenging as it involves a mixed-integer non-linear programming problem. To address this complexity, we propose a multi-stage offloading algorithm named penalty function method heuristic algorithm that combines a learning algorithm with an interior-point method, ultimately delivering a practical solution. Our simulation results validate the performance of the proposed algorithm, which yields superior results compared to the simple relaxation heuristic algorithm.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Heuristic algorithms",
                "Quality of service",
                "Optimization",
                "Autonomous aerial vehicles",
                "Throughput",
                "Resource management"
            ],
            "Author Keywords": [
                "Digital twin",
                "mobile edge computing",
                "optimization",
                "resource management",
                "unmanned aerial vehicles"
            ]
        },
        "title": "Robust Multi-Criterion Offloading in Digital Twin-Assisted UAVs Networks"
    },
    {
        "authors": [
            "Preetam Suman",
            "Sasmita Padhy",
            "Naween Kumar",
            "Amrit Suman",
            "Akansha Singh",
            "Krishna Kant Singh",
            "Ángel Kuc Castilla",
            "Turki Saad S. AL-Zahrani"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "07 October 2024",
        "doi": "10.1109/TCE.2024.3475823",
        "publisher": "IEEE",
        "abstract": "The research area of Vehicular Ad-Hoc Network (VANET) is rapidly expanding for different reasons. The main factors responsible for this are the unstable topology, fast vehicle movement, and security vulnerabilities. Identifying malicious attacks in vehicular ad-hoc networks is necessary to enhance the security and reliability of communication between all vehicles in the system. The intruders carry out numerous malicious attacks. The focus of this paper is to identify and mitigate attacks like Botnet, Sybil, DoS, Wormhole, PortScan, Blackhole, and BruteForce. The present work proposes an Improved LeeNET (I-LeeNet) architecture to identify and mitigate unidentified attack types. The suggested architecture intelligently blends Convolutional Neural Networks (CNN) and Adaptive Neuro-Fuzzy Inference Systems (ANFIS) to provide real-time solutions to unidentified attacks. The proposed approach includes a module KIDS for known attack detection and another module UIDS for learning and identifying previously unidentified attacks. The KIDS module uses ANFIS classification to identify destructive attacks. ANFIS and CNN use the UIDS module to identify unknown attacks on VANET further. The efficacy of the proposed I-LeeNet is tested on three different datasets, such as i-VANET, ToN-IoT, and CIC-IDS 2017. The results section discusses the testing in a regressive manner. In addition, the experimental results are compared to other advanced methods. The average accuracy achieved by the proposed method is 97.21% on i-VANET, 97.75% on ToN-IoT, and 96.66% on the CIC-IDS 2017 dataset. The analysis suggests the proposed method is promising and can be applied in real-time.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Vehicular ad hoc networks",
                "Security",
                "Wireless sensor networks",
                "Intrusion detection",
                "Wireless communication",
                "Convolutional neural networks",
                "Computer architecture",
                "Deep learning",
                "Authentication",
                "Machine learning algorithms"
            ],
            "Author Keywords": [
                "IDS VANET i-VANET DEEP Learning ANFIS CNN"
            ]
        },
        "title": "An Improved Deep Learning-Based Intrusion Detection for Reliable Communication in VANET"
    },
    {
        "authors": [
            "Peng Qin",
            "Jinghan Li",
            "Jing Zhang",
            "Yang Fu"
        ],
        "published_in": "Published in: IEEE Transactions on Network Science and Engineering ( Early Access )",
        "date_of_publication": "15 October 2024",
        "doi": "10.1109/TNSE.2024.3481061",
        "publisher": "IEEE",
        "abstract": "With the proliferation of Internet of Things (IoT), compute-intensive and latency-critical applications continue to emerge. However, IoT devices in isolated locations have insufficient energy storage as well as computing resources and may fall outside the service range of ground communication networks. To overcome the constraints of communication coverage and terminal resource, this paper proposes a multiple Unmanned Aerial Vehicle (UAV)-assisted air-ground collaborative edge computing network model, which comprises associated UAVs, auxiliary UAVs, ground user devices (GDs), and base stations (BSs), intending to minimize the overall system energy consumption. It delves into task offloading, UAV trajectory planning and edge resource allocation, which thus is classified as a Mixed-Integer Nonlinear Programming (MINLP) problem. Worse still, the coupling of long-term task queuing delay and short-term offloading decision makes it challenging to address the original issue directly. Therefore, we employ Lyapunov optimization to transform it into two sub-problems. The first involves task offloading for GDs, trajectory optimization for associated UAVs as well as auxiliary UAVs, which is tackled using Deep Reinforcement Learning (DRL), while the second deals with task partitioning and computing resource allocation, which we address via convex optimization. Through numerical simulations, we verify that the proposed approach outperforms other benchmark methods regarding overall system energy consumption.",
        "issn": {
            "Electronic ISSN": "2327-4697"
        },
        "keywords": {
            "IEEE Keywords": [
                "Autonomous aerial vehicles",
                "Resource management",
                "Gold",
                "Air to ground communication",
                "Collaboration",
                "Energy consumption",
                "Computational modeling",
                "Internet of Things",
                "Edge computing",
                "Delays"
            ],
            "Author Keywords": [
                "Multi-UAV collaborative air-ground edge computing",
                "trajectory optimization",
                "resource allocation",
                "Lyapunov optimization",
                "Multi-Agent Deep Deterministic Policy Gradients (MADDPG)"
            ]
        },
        "title": "Joint Task Allocation and Trajectory Optimization for Multi-UAV Collaborative Air-Ground Edge Computing"
    },
    {
        "authors": [
            "Mehbub Alam",
            "Nurzaman Ahmed",
            "Shyamal Ghosh",
            "Rakesh Matam",
            "Ferdous Ahmed Barbhuiya"
        ],
        "published_in": "Published in: IEEE Transactions on Services Computing ( Early Access )",
        "date_of_publication": "13 June 2024",
        "doi": "10.1109/TSC.2024.3414371",
        "publisher": "IEEE",
        "abstract": "The primary objective of fog computing is to minimize the reliance of IoT devices on the cloud by leveraging the resources of fog network. Typically, IoT devices offload computation tasks to fog to meet different task requirements such as latency in task execution, computation costs, etc. So, selecting such a fog node that meets task requirements is a crucial challenge. To choose an optimal fog node, access to each node's resource availability information is essential. Existing approaches often assume state availability or depend on a subset of state information to design mechanisms tailored to different task requirements. In this paper, OptiFog: a cluster-based fog computing architecture for acquiring the state information followed by optimal fog node selection and task offloading mechanism is proposed. Additionally, a continuous time Markov chain based stochastic model for predicting the resource availability on fog nodes is proposed. This model prevents the need to frequently synchronize the resource availability status of fog nodes, and allows to maintain an updated state information. Extensive simulation results show that OptiFog lowers task execution latency considerably, and schedules almost all the tasks at the fog layer compared to the existing state-of-the-art.",
        "issn": {
            "Electronic ISSN": "1939-1374"
        },
        "keywords": {
            "IEEE Keywords": [
                "Task analysis",
                "Internet of Things",
                "Costs",
                "Edge computing",
                "Computer architecture",
                "Cloud computing",
                "Predictive models"
            ],
            "Author Keywords": [
                "CTMC",
                "fog computing",
                "load balanced network",
                "optimal fog node selection",
                "task latency",
                "task offloading"
            ]
        },
        "title": "OptiFog: A Framework for Acquiring State Information and Predicting Resource Availability for Task Offloading in Cooperative Fog-Networks"
    },
    {
        "authors": [
            "Xiaoyue Ji",
            "Yi Chen",
            "Junfan Wang",
            "Guangdong Zhou",
            "Chun Sing Lai",
            "Zhekang Dong"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "23 August 2024",
        "doi": "10.1109/JIOT.2024.3448350",
        "publisher": "IEEE",
        "abstract": "With the rapid adoption of Internet of things (IoT) and artificial intelligence (AI), lithium-ion battery state-of-health (SOH) estimation plays an important role in guaranteeing the secure and stable functioning of various domains. However, the majority of the existing methods are constrained by factors such as transmission latency, computational energy, and computing speed. To address these challenges, we develop a time-frequency hybrid neuromorphic computing architecture for battery SOH estimation. Specifically, an eco-friendly, biodegradable memristor crossbar array is designed, enabling high energy efficiency and high-performance density in the proposed system. To improve the understanding of the designed time-frequency hybrid neuromorphic computing system, a local information extraction module, a time-frequency feature fusion module, and a global information perception module are proposed. Furthermore, the proposed system is validated on two publicly available battery ageing datasets (i.e., the CALCE-CS2 dataset and the NASA dataset). The experimental results show that the system exhibits superior performance to that of the state-of-the-art (SOTA) methods in terms of estimation accuracy (highest estimation accuracy), time consumption (approximately 8 12 times faster), and transmission latency (approximately 10 times faster). This study is expected to promote the advancement and evolution of next-generation computing systems, enabling the realization of low power consumption and high-density information processing in IoT scenarios.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Time-frequency analysis",
                "Memristors",
                "Estimation",
                "Neuromorphic engineering",
                "Computer architecture",
                "Internet of Things",
                "Feature extraction"
            ],
            "Author Keywords": [
                "Neuromorphic computing",
                "circuit design",
                "memristor",
                "lithium-ion battery",
                "state-of-health estimation"
            ]
        },
        "title": "Time-Frequency Hybrid Neuromorphic Computing Architecture Development for Battery State-of-Health Estimation"
    },
    {
        "authors": [
            "Sungjin Yu",
            "Ashok Kumar Das",
            "Youngho Park"
        ],
        "published_in": "Published in: IEEE Transactions on Intelligent Transportation Systems ( Early Access )",
        "date_of_publication": "24 October 2024",
        "doi": "10.1109/TITS.2024.3480029",
        "publisher": "IEEE",
        "abstract": "Unmanned aerial vehicles (UAVs) integrated with the internet of things (IoT) guarantee useful advantages such as facilitating ground communications in regions where the availability of connectivity is restricted owing to physical obstacles. However, the data transmitted by sensors and IoT embedded in UAVs are facing new security issues and privacy challenges with the known security attacks over time. To address these security attacks and threats and meet lightweight UAV communication requirements, a secure and lightweight authentication and key agreement (AKA) scheme is essential. Recently, researchers have designed a lightweight blockchain-enabled AKA scheme with privacy-preserving for UAVs to provide useful and reliable services. However, we prove that the existing scheme is fragile to various security attacks and does not ensure mutual authentication. Thus, we propose a robust and lightweight blockchain-based AKA scheme for PUF-enabled UAVs, called RLBA-UAV to enhance the security problems of the existing scheme. We demonstrate the security of RLBA-UAV by using informal/formal security analyses such as the ROR oracle model and AVISPA simulation. Moreover, we demonstrate the performance comparison analysis between RLBA-UAV and related schemes for UAVs. We demonstrate an implementation of a network simulator (NS) 3 compliant with IEEE 802.11p standards to show its validation and feasibility that RLBA-UAV is appropriate for practical UAVs. Thus, RLBA-UAV offers enhanced security and operational efficiency compared to related schemes and can be applied to practical blockchain-based AKA systems for UAVs.",
        "issn": {
            "Print ISSN": "1524-9050",
            "Electronic ISSN": "1558-0016"
        },
        "keywords": {
            "IEEE Keywords": [
                "Security",
                "Autonomous aerial vehicles",
                "Physical unclonable function",
                "Authentication",
                "Servers",
                "Smart cities",
                "Blockchains",
                "Internet of Things",
                "Privacy",
                "Thermal sensors"
            ],
            "Author Keywords": [
                "Unmanned aerial vehicles (UAVs)",
                "blockchain",
                "physical unclonable functions (PUF)",
                "authentication"
            ]
        },
        "title": "RLBA-UAV: A Robust and Lightweight Blockchain-Based Authentication and Key Agreement Scheme for PUF-Enabled UAVs"
    },
    {
        "authors": [
            "Ze Wei",
            "Rongxi He",
            "Haotian Liu",
            "Chengzhi Song"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "12 September 2024",
        "doi": "10.1109/JIOT.2024.3459098",
        "publisher": "IEEE",
        "abstract": "Mobile edge computing (MEC) powered by renewable energy, is promising to provide green computing for the Internet of Things (IoT). However, the unpredictable renewable energy and computing demands usually cause a mismatch between system requirements and energy supply, resulting in wasted surplus energy or energy supply shortage. Hence, it is crucial to improve energy efficiency and system performance, that is“, make the best use of generated energy” and “make the best use of system’s talents” simultaneously. In this paper, we focus on some islands far from the mainland, with growing computation requirements for environmental monitoring and navigation safety, and propose a device-to-device (D2D) collaboration based SDN-MEC framework in Island IoT employing tidal energy. Following that, we formulate a multi-objective energy scheduling system performance association (MESPA) problem to minimize the long-term average task execution loss (TEL), including energy consumption per bit executed, overall execution latency, and energy waste caused by underutilization of tidal energy, with the constraints of energy queue stability, peak transmission power, and CPU-cycle frequency. To address this challenging problem, we propose a Lyapunov-based multi-dimensional resource allocation and computation offloading (LMDRACO) algorithm and transform the original problem into several individual subproblems in each time slot. These subproblems are then solved using convex decomposition and submodular methods. Theoretical research shows that the LMDRACO algorithm can achieve a O(1/V), O(V) tradeoff between TEL and energy queue length. Numerical results show that the proposed algorithm significantly improves both system performance and energy efficiency compared to baseline schemes.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Industrial Internet of Things",
                "Device-to-device communication",
                "Tidal energy",
                "Resource management",
                "System performance",
                "Energy consumption",
                "Energy efficiency"
            ],
            "Author Keywords": [
                "Computation offloading",
                "device-to-device",
                "wireless power transmission",
                "green energy scheduling",
                "Lyapunov optimization"
            ]
        },
        "title": "Joint Computation Offloading and Resource Allocation in Green MEC-Assisted Software-Defined Island Internet of Things"
    },
    {
        "authors": [
            "MohammadHossein Alishahi",
            "Paul Fortier",
            "Ming Zeng",
            "Quoc-Viet Pham",
            "Thien Huynh-The"
        ],
        "published_in": "Published in: IEEE Transactions on Vehicular Technology ( Early Access )",
        "date_of_publication": "16 October 2024",
        "doi": "10.1109/TVT.2024.3481462",
        "publisher": "IEEE",
        "abstract": "Due to the limited battery capacity and computational power of Internet-of-Things (IoT) devices, emerging IoT applications depend on mobile edge computing (MEC) networks for computational offloading and wireless power transfer (WPT) to sustain energy levels, with transfer performance influenced by the propagation environment and transmission strategies. Compared to the traditional reflecting-only reconfigurable intelligent surface (RIS), simultaneously transmitting and reflecting (STAR)-RIS extends coverage from half-space to full-space, significantly enhancing transferring efficiency. This paper compares the performance of three different uplink transmission schemes on a STAR-RIS WPT MEC network. On this basis, for each scenario, a non-convex optimization problem is formulated to maximize the total computational bits for the system, where energy transfer, local processing, uplink transmission allocated resources, and phase shift vectors of STAR-RIS are jointly optimized. To address the non-convexity, each scenario is decoupled into two main subproblems: phase shift vectors of STAR-RIS optimization and joint power, time, and computing frequency optimization. While a closed-form expression is derived to optimize the phase shift vector of STAR-RIS in time division multiple access (TDMA), semi-definite relaxation is employed to optimize the phase shift vectors under Hybrid TDMA- non-orthogonal multiple access (NOMA) and NOMA. Furthermore, block coordinate descent is employed to address the remaining resources. Simulation results indicate the superiority of the proposed scheme for all scenarios in terms of total computational bits compared to benchmark schemes. Moreover, among the three uplink transmission schemes, TDMA performs the best in total computational bits and user fairness owing to its ability to flexibly adapt STAR-RIS phase shift vectors.",
        "issn": {
            "Print ISSN": "0018-9545",
            "Electronic ISSN": "1939-9359"
        },
        "keywords": {
            "IEEE Keywords": [
                "NOMA",
                "Vectors",
                "Time division multiple access",
                "Internet of Things",
                "Uplink",
                "Energy efficiency",
                "Reflection",
                "Resource management",
                "Wireless communication",
                "Reconfigurable intelligent surfaces"
            ],
            "Author Keywords": [
                "Computational bits",
                "NOMA",
                "MEC",
                "Optimization",
                "WPT",
                "STAR-RIS",
                "TDMA"
            ]
        },
        "title": "Total Computational Bits Maximization for STAR-RIS Aided Wireless Power Transfer Mobile Edge Computing Networks: TDMA or NOMA?"
    },
    {
        "authors": [
            "Eduardo Ortega",
            "Fei Su",
            "Rita Chattopadhyay",
            "Krishnendu Chakrabarty"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "30 September 2024",
        "doi": "10.1109/JIOT.2024.3468950",
        "publisher": "IEEE",
        "abstract": "Memory and compute constraints make anomaly detection model training infeasible on Internet-of-Things (IoT) resource-limited edge devices. Many solutions train anomaly detection models (e.g., deep neural networks or DNN) on the cloud and deploy them on IoT edge devices for inferencing. However, cloud-based training does not address overall communication latency and potential data leakage. Moreover, because anomalies rarely occur, using labels to define anomalies is impractical. Hence, supervised learning mechanisms are unsuitable for anomaly detection. There is a need for effective unsupervised anomaly detection for resource-constrained edge devices. We present the Discretized Isolation Forest to address memory-and compute-efficient unsupervised anomaly detection for resource-constrained edge devices. We also present a discretization function, based on information entropy, to inform the growth of the Isolation Forest ensemble to create the Discretized Isolation Forest. The Discretized Isolation Forest reduces the training time (memory usage) of the original Isolation Forest by 79.38× (166.66×). We test the Discretized Isolation Forest against general anomaly detection benchmarks and edge-anomaly detection benchmarks. The edge-anomaly detection benchmarks were curated from the built-in iPhone edge sensors. Across all edge-sensor anomaly detection datasets and against all other considered models, the Discretized Isolation resulted in the lowest training time, lowest memory usage, preserved anomaly detection performance, and highest inference speeds. In addition, across all general anomaly detection benchmarks and against all considered models, Discretized Isolation Forest incurs lower training time and memory usage while retaining competitive inferencing execution time and anomaly detection performance.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Anomaly detection",
                "Training",
                "Image edge detection",
                "Forestry",
                "Memory management",
                "Mathematical models",
                "Internet of Things",
                "Benchmark testing",
                "Computational modeling",
                "Artificial neural networks"
            ],
            "Author Keywords": [
                "Anomaly Detection",
                "Compute Efficiency",
                "Decision Tree",
                "Isolation Forest",
                "Internet of Things",
                "Memory Efficiency"
            ]
        },
        "title": "Discretized-Isolation Forest: Memory-& Compute-Efficient Unsupervised Anomaly Detection for Resource-Constrained Internet-of-Things Edge Devices"
    },
    {
        "authors": [
            "Waltenegus Dargie",
            "Paulo Padrao",
            "Leonardo Bobadilla",
            "Christian Poellabauer"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "09 October 2024",
        "doi": "10.1109/JSEN.2024.3472850",
        "publisher": "IEEE",
        "abstract": "Low-power IoT sensing nodes can be deployed on the surface of different water bodies for various purposes, including water quality monitoring and pollution detection. Two of the most formidable challenges towards such goals are (1) making the nodes resilient against rough water and extreme weather conditions, and (2) enabling the nodes to establish reliable wireless links. In this paper we share our experience in deploying low-power and resilient IoT nodes on the surface of different water bodies – on a small lake, North Biscayne Bay, Crandon Beach, and South Beach, in Miami, Florida. Furthermore, the paper closely examines how link quality was affected by pre-deployment configurations as well as the characteristics and the motion of the waters. Based on the analyses of a vast amount of statistics, the paper establishes a theoretical (mathematical) and generalized model to characterize and predict link quality fluctuations. We shall show that the realization of the model using the Kalman Filter enables link quality prediction with accuracy exceeding 90%.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Wireless sensor networks",
                "Monitoring",
                "Water quality",
                "Wireless communication",
                "Surface impedance",
                "Water pollution",
                "Lakes",
                "Surface roughness",
                "Rough surfaces"
            ],
            "Author Keywords": [
                "Water quality monitoring",
                "wireless sensor networks",
                "Internet of Things",
                "link quality fluctuation",
                "RSSI. surface water deployment"
            ]
        },
        "title": "Link Quality Fluctuation in Wireless Networks Deployed on the Surface of Different Water Bodies"
    },
    {
        "authors": [
            "Zhaolong Ning",
            "Hongjing Ji",
            "Xiaojie Wang",
            "Edith C. H. Ngai",
            "Lei Guo",
            "Jiangchuan Liu"
        ],
        "published_in": "Published in: IEEE Transactions on Mobile Computing ( Early Access )",
        "date_of_publication": "30 September 2024",
        "doi": "10.1109/TMC.2024.3470831",
        "publisher": "IEEE",
        "abstract": "The development of Internet of Things (IoT) technology has led to the emergence of a large number of Intelligent Sensing Devices (ISDs). Since their limited physical sizes constrain the battery capacity, wireless powered IoT networks assisted by Unmanned Aerial Vehicles (UAVs) for energy transfer and data acquisition have attracted great interest. In this paper, we formulate an optimization problem to maximize system energy efficiency while satisfying the constraints of UAV mobility and safety, ISD quality of service and task completion time. The formulated problem is constructed as a Constrained Markov Decision Process (CMDP) model, and a Multi-agent Constrained Deep Reinforcement Learning (MCDRL) algorithm is proposed to learn the optimal UAV movement policy. In addition, an ISD-UAV connection assignment algorithm is designed to manage the connection in the UAV sensing range. Finally, performance evaluations and analysis based on real-world data demonstrate the superiority of our solution.",
        "issn": {
            "Print ISSN": "1536-1233",
            "Electronic ISSN": "1558-0660"
        },
        "keywords": {
            "IEEE Keywords": [
                "Autonomous aerial vehicles",
                "Internet of Things",
                "Safety",
                "Wireless sensor networks",
                "Sensors",
                "Data acquisition",
                "Resource management",
                "Trajectory planning",
                "Three-dimensional displays",
                "Mobile computing"
            ],
            "Author Keywords": [
                "Constrained markov decision process",
                "data acquisition",
                "UAV",
                "wireless power transfer"
            ]
        },
        "title": "Joint Optimization of Data Acquisition and Trajectory Planning for UAV-Assisted Wireless Powered Internet of Things"
    },
    {
        "authors": [
            "B. Sainath",
            "Sai Kartik Tadinada"
        ],
        "published_in": "Published in: IEEE Journal on Miniaturization for Air and Space Systems ( Early Access )",
        "date_of_publication": "28 August 2024",
        "doi": "10.1109/JMASS.2024.3451011",
        "publisher": "IEEE",
        "abstract": "Rapid advancements in internet-of-things (IoT), unmanned aerial vehicles (UAVs), and energy harvesting (EH) technologies can be leveraged to design and develop green and reliable cooperative Cube satellite communication (CSC) systems and networks. In this work, we propose a novel cooperative CSC system model comprising green UAVs as intelligent relays equipped with IoT sensors, intelligent processing and EH modules, and transceivers. Using a novel and intelligent probabilistic transmission policy (PTP) that we propose, CubeSats can conserve energy by deactivating transmissions in unfavorable weather conditions based on control signals from the smart UAV via a telemetry link. We extend this model to include multiple CubeSats and analyze it by deriving and evaluating network energy efficiency and its lower bound. Our numerical plots show that the proposed PTP significantly outperforms the continuous transmission policy (CTP). At a specific transmission probability of 0.125, PTP is 40 times more energy efficient than CTP. We extend the work and develop a novel and insightful performance analysis for energy efficiency outage (EEO) probability. Specifically, we derive closed-form approximate expressions for EEO probability and present numerical results. Furthermore, we analyze the performance of clustered CSC networks and present numerical results to assess EEO probability, providing valuable insights for future large-scale green CSC network design and deployment.",
        "issn": {
            "Electronic ISSN": "2576-3164"
        },
        "keywords": {
            "IEEE Keywords": [
                "Autonomous aerial vehicles",
                "CubeSat",
                "Green products",
                "Satellites",
                "Payloads",
                "Meteorology",
                "Probabilistic logic"
            ],
            "Author Keywords": [
                "CubeSat communications",
                "unmanned aerial vehicles",
                "weather sensors",
                "probabilistic transmission",
                "energy efficiency",
                "outage probability",
                "clustering"
            ]
        },
        "title": "Environment-Aware Green UAV-Assisted, CubeSat Communication Network Energy Efficiency and Outage Probability Analysis"
    },
    {
        "authors": [
            "Hu Xiong",
            "Lingxiao Gong",
            "Ruoxia Li",
            "Saru Kumari",
            "Chien-Ming Chen",
            "Mohammed Amoon"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "11 July 2024",
        "doi": "10.1109/TCE.2024.3426101",
        "publisher": "IEEE",
        "abstract": "The rise in popularity of consumer electronics has led to concerns regarding security and privacy issues in the Consumer Internet of Things (IoT). Blockchain is an ideal technological method to solve the above issues. When implementing consumer IoT with blockchain, the privacy protection in consumer electronics is a core issue that needs to be addressed. The threshold identity-based ring signature (TIBRS) not only prevents the data stored on the blockchain from being altered but also offers privacy protection for consumer electronics. However, most existing TIBRS protocols rely on key generation center (KGC), which makes these protocols vulnerable to system intrusion. Furthermore, only a handful of protocols succeed in achieving identity abort. To deal with these problems, we propose a blockchain-enabled distributed identity-based ring signature with identity abort for consumer electronics (DIBRS-CE). By leveraging the benefits of both MuSig and threshold identity-based ring signature, we construct an efficient threshold identity-based ring signature protocol that eliminates the need for KGC. Meanwhile, by introducing a commitment mechanism, we have successfully achieved the feature of identity abort. Moreover, the communication complexity of DIBRS-CE reaches O(nlogn). The security analysis and simulation experimental results demonstrate that our protocol is more efficient while maintaining security.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Protocols",
                "Consumer electronics",
                "Public key",
                "Blockchains",
                "Security",
                "Polynomials",
                "Privacy"
            ],
            "Author Keywords": [
                "Consumer electronics",
                "Blockchain",
                "Threshold Identity-Based ring signature",
                "Join-Shamir-RSS",
                "Identity abort"
            ]
        },
        "title": "Blockchain-Enabled Distributed Identity-Based Ring Signature With Identity Abort for Consumer Electronics"
    },
    {
        "authors": [
            "Suyel Namasudra",
            "Sangjukta Das",
            "Ruben Gonzalez Crespo",
            "David Taniar",
            "Nageswara Rao Moparthi"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "03 October 2024",
        "doi": "10.1109/TCE.2024.3473739",
        "publisher": "IEEE",
        "abstract": "The growing consumer interest in product quality and originality has made the drug supply chain management system complex and critical. Moreover, fraudulent activities, such as drug contamination, have become global issues in the drug supply chain. Traditional centralized systems struggle to maintain transparency and traceability throughout the supply chain, often resulting in inefficiencies and vulnerabilities. Here, consumer electronics devices can be integrated with the ecosystem to extend the reach of the drug supply chain. In this paper, a solution for traditional drug supply chain systems is presented by using blockchain technology and Internet of Things (IoT) with a focus on consumer electronics. Here, blockchain supports to store drug records immutably, while keeping information transparent to all entities involved in the network. This proposed scheme is deployed over the Ethereum blockchain network and uses IoT technology to provide the key features of blockchain, such as drug traceability, security, and transparency. Here, smart contracts automate the entire process in the proposed supply chain network, and all the drug batch’s transfer details are stored on the blockchain network. The security analysis and performance analysis of the proposed scheme are performed to evaluate the efficiency of the proposed scheme over some existing schemes.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Drugs",
                "Supply chains",
                "Blockchains",
                "Smart contracts",
                "Consumer electronics",
                "Cloud computing",
                "Security",
                "Supply chain management",
                "Data privacy",
                "Stakeholders"
            ],
            "Author Keywords": [
                "Scalability",
                "Decentralized System",
                "Smart Contract",
                "Ethereum"
            ]
        },
        "title": "DrugBlock: An Advanced System to Secure Drug Supply Chain Using Internet of Things and Blockchain-Enabled Consumer Electronics"
    },
    {
        "authors": [
            "Jun Chen",
            "Zhuo Wang",
            "Kun Xiao",
            "Mario Ferraro",
            "Nikolai Ushakov",
            "Santosh Kumar",
            "Fengxiang Ge",
            "Xiaoli Li",
            "Rui Min"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "24 October 2024",
        "doi": "10.1109/JIOT.2024.3485614",
        "publisher": "IEEE",
        "abstract": "Remote healthcare monitoring is a crucial component in the field of medical Internet of Things (IoT), which effectively achieves remote monitoring, collection, and transmission of physiological data by combining AI algorithms with intelligent health monitoring systems to improve people’s quality of life and health. In this work, an AI-enabled scalable smartphone photonic sensing system is developed for remote healthcare monitoring using fiber optic sensors and a smartphone. The smartphone serves as both the light source and interrogator for the system, with the ability to connect to the network for integration with the IoT. Scalability is achieved through a multi-channel framework, and by modifying the connector design, the system can incorporate more sensors to monitor multiple physiological parameters in real-time. In addition to acquiring basic respiratory and heartbeat signals and various gait parameters, the system successfully implemented the recognition of various gait patterns and fatigue monitoring using an adapted MobileNetV3 neural network structure. The accuracy of 98.5% for the gait pattern recognition task, and 94% and 95.8% for the mental and muscle fatigue monitoring tasks, respectively, demonstrates the system’s potential as a telemedicine tool. Additionally, the low cost, non-invasiveness, and portability of this innovative sensor system make it highly generalizable.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Monitoring",
                "Optical fibers",
                "Optical fiber sensors",
                "Sensor systems",
                "Biomedical monitoring",
                "Intelligent sensors",
                "Medical services",
                "Biomedical optical imaging",
                "Optical pulses"
            ],
            "Author Keywords": [
                "Artificial intelligence",
                "Smartphone-based sensing",
                "Photonic sensing",
                "Healthcare monitoring",
                "Deep learning"
            ]
        },
        "title": "AI-Enabled Scalable Smartphone Photonic Sensing System for Remote Healthcare Monitoring"
    },
    {
        "authors": [
            "Yan Guo",
            "Min Lin",
            "Yiwen Liu",
            "Huaicong Kong",
            "Jun-Bo Wang",
            "Jiangzhou Wang"
        ],
        "published_in": "Published in: IEEE Transactions on Aerospace and Electronic Systems ( Early Access )",
        "date_of_publication": "29 August 2024",
        "doi": "10.1109/TAES.2024.3451455",
        "publisher": "IEEE",
        "abstract": "This paper investigates uplink transmission in satellite Internet of Things (IoTs) networks, where a low earth orbit satellite having a uniform planar array provides services simultaneously for an earth station (ES) and multiple IoT devices (IoTDs). Specifically, we use the statistical channel state information and propose two age of information (AoI)-aware uplink access schemes, namely sleep threshold-based access (STA) and forcing update threshold-based access (FUTA), so that many IoTDs can share the same channel with the ES through cognitive-radio inspired non-orthogonal multiple access (CR-NOMA) technology. By assuming that the satellite channels obey $\\kappa - \\mu$ shadowed distribution, we exploit the discrete-time Markov chain model and derive the average AoI closed-form expression to assess the information freshness of the proposed schemes. Simulation results are given to confirm the correctness of the theoretical analysis and the superiority of the schemes we have proposed. Moreover, it is shown that compared with STA scheme, the FUTA can achieve better average AoI performance, and exhibits robustness against fluctuations in the transmission probability of IoTDs, enabling it to consistently maintain a lower average AoI.",
        "issn": {
            "Print ISSN": "0018-9251",
            "Electronic ISSN": "1557-9603"
        },
        "keywords": {
            "IEEE Keywords": [
                "Satellites",
                "Internet of Things",
                "Uplink",
                "NOMA",
                "Low earth orbit satellites",
                "Aerospace and electronic systems",
                "Transmitting antennas"
            ],
            "Author Keywords": [
                "Age of information",
                "Satellite Internet of Things",
                "Non-orthogonal multiple access",
                "$\\kappa - \\mu$ shadowed distribution"
            ]
        },
        "title": "AoI-Aware Uplink CR-NOMA Schemes in Satellite Internet of Things Networks"
    },
    {
        "authors": [
            "Weidong Wang",
            "Ping Li",
            "Siqi Li",
            "Jihao Zhang",
            "Zijiao Zhou",
            "Dapeng Oliver Wu",
            "Duk Kyung Kim",
            "Guangwei Zhang",
            "Peng Gong"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "20 June 2024",
        "doi": "10.1109/JIOT.2024.3416943",
        "publisher": "IEEE",
        "abstract": "As the Internet of Things (IoT) technology and Artificial Intelligence (AI) technology continue to evolve, many envisaged concepts regarding smart cities are gradually becoming a reality. However, the proliferation of numerous IoT devices in smart cities has led to several challenges. The existing 5G networks are incapable of meeting the requirements of these devices in terms of channel capacity and network coverage. Additionally, traditional cloud-based centralized machine learning methods fail to ensure the privacy of user data. At this juncture, Space-Air-Ground Information Network, along with Federated Learning (FL), are perceived as viable solutions to address these issues. This paper focuses on addressing FL challenges in smart cities using the Space-Air-Ground Information Network. Here, data distribution heterogeneity leads to increased federated training time and higher energy costs. The paper begins by analyzing the reasons for the non-independent and non-identically distributed (NON-IID) data collected by devices in this scenario. Subsequently, from the perspective of device selection, the paper proposes a node selection model based on near-edge strategy optimization, termed \"Low Node Selection in Federated Learning\" (LCNSFL). Finally, the LCNSFL algorithm is compared with federated av-eraging algorithms based on random selection strategies and the FedProx algorithm. Experimental results demonstrate that the federated learning model aided by the LCNSFL algorithm achieves the target accuracy with fewer communication rounds, considerably reducing the required training time and energy costs compared to the other two algorithms.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Data models",
                "Internet of Things",
                "Smart cities",
                "Training",
                "Computational modeling",
                "Federated learning",
                "Costs"
            ],
            "Author Keywords": [
                "smart cities",
                "space-air-ground information network",
                "federated learning",
                "non-IID data",
                "node selection model"
            ]
        },
        "title": "Optimizing Proximity Strategy for Federated Learning Node Selection in the Space-Air-Ground Information Network for Smart Cities"
    },
    {
        "authors": [
            "Sohail Abbas",
            "Muhammad Fayaz",
            "Abdulrahman Ghandoura",
            "Muhammad Zahid Khan",
            "Ateeq Ur Rehman"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "12 August 2024",
        "doi": "10.1109/TCE.2024.3442568",
        "publisher": "IEEE",
        "abstract": "The Consumer Internet of Things (CIoT), a key aspect of the IoT, aims to integrate smart technologies into everyday life. In order to improve the spectral efficiency and provide massive connectivity to IoT networks, non-orthogonal multiple access (NOMA) variants like semi-grant-free (SGF) NOMA are employed. This paper aims to maximize secrecy energy efficiency (EE) for SGF-NOMA enabled CIoT in the presence of untrusted users (eavesdroppers) by utilizing a single-agent multi-agent deep reinforcement learning (SAMA-DRL) algorithm to overcome scalability and expensive learning issues. Given the limited long-distance transmission capabilities of CIoT devices, which typically have low transmit power, relay nodes are used to decode and forward data from grant-free (GF) users to the base station. Moreover, to enhance the coverage for GF users, the K-nearest neighbors (KNN) algorithm is utilized to place the relay nodes at an optimal positions. Furthermore, we design a collaborative contribution reward system to discourage user (agent) laziness. Simulation results show that the proposed SAMA-DRL-based SGF-NOMA algorithm for CIoT networks is more effective than baseline algorithms, achieving a 20% increase in secrecy EE compared to DRL-based SGF-NOMA without KNN. Moreover, the proposed scheme outperforms benchmark schemes in terms of EE across different radii. Additionally, we show that the proposed algorithm with quality of service based successive interference cancellation (SIC) is more power efficient as compared to conventional SIC decoding order.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "NOMA",
                "Internet of Things",
                "Relays",
                "Interference cancellation",
                "Resource management",
                "Quality of service",
                "Nearest neighbor methods"
            ],
            "Author Keywords": [
                "Non-orthogonal multiple access",
                "grant-free",
                "Internet of things",
                "grant-based",
                "deep reinforcement learning"
            ]
        },
        "title": "Secure Energy Aware Power Control in Consumer Internet of Things With Semi Grant Free NOMA"
    },
    {
        "authors": [
            "Ye Liu",
            "Pei Tian",
            "Carlo Alberto Boano",
            "Xiaoyuan Ma",
            "Qing Yang",
            "Honggang Wang"
        ],
        "published_in": "Published in: IEEE Transactions on Cognitive Communications and Networking ( Early Access )",
        "date_of_publication": "16 September 2024",
        "doi": "10.1109/TCCN.2024.3461671",
        "publisher": "IEEE",
        "abstract": "Low-power wide area network (LPWAN) testbeds are essential for cognitive communications and networking, as they provide practical and controlled environments for testing, validating, and advancing cognitive technologies in the cognitive Internet of Things (IoT). However, establishing extensive outdoor testbeds faces a significant challenge due to the lack of robust infrastructure, limiting testing to indoor settings or a small number of devices. This constraint prevents adequate testing of cognitive communications and networking techniques. In this article, we introduce ChirpBox: an innovative, infrastructure-free, and cost-effective LPWAN testbed that revolutionizes the utilization of LoRa nodes. Beyond their conventional role in experimentation, these nodes in ChirpBox orchestrate all operations, from disseminating firmware for testing to collecting log traces at the conclusion of each test cycle. This holistic approach is enabled by our development of an all-to-all multi-channel protocol that leverages concurrent transmissions for efficient communication across multi-hop LoRa networks. Following a detailed presentation of ChirpBox’s design and implementation, we demonstrate its capabilities through a practical deployment. This evaluation offers experimental insights into the testbed’s performance, illustrating its operations and highlighting its potential to advance cognitive IoT research and development.",
        "issn": {
            "Electronic ISSN": "2332-7731"
        },
        "keywords": {
            "IEEE Keywords": [
                "Chirp",
                "LoRa",
                "Microprogramming",
                "Hardware",
                "Testing",
                "Internet of Things",
                "Global navigation satellite system"
            ],
            "Author Keywords": [
                "Cognitive Internet of Things",
                "Infrastructure-Less Network Testbed",
                "Experimental Tool",
                "LoRa"
            ]
        },
        "title": "A Low-Cost and Infrastructure-Less LoRa Wireless Network Testbed for Cognitive Internet of Things"
    },
    {
        "authors": [
            "Xinya Li",
            "Hongji Xu",
            "Yang Wang",
            "Jiaqi Zeng",
            "Yiran Li",
            "Xiaoman Li",
            "Wentao Ai",
            "Hao Zheng",
            "Yupeng Duan"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "05 November 2024",
        "doi": "10.1109/JIOT.2024.3491362",
        "publisher": "IEEE",
        "abstract": "As the Internet of Things (IoT) technology advances, human activity recognition (HAR) using IoT devices including wearable sensors has become prevalent in various applications. Nevertheless, many sensor-based HAR methods still struggle to balance recognition accuracy with network complexity. Meanwhile, most existing sensor-based HAR networks fail to achieve an effective fusion of multidimensional features. To address the above issues, a hierarchical multiscale time-frequency and channel feature adaptive fusion (HMTF-CFAF) network is put forward. The HMTF-CFAF efficiently extracts unique multiscale time-frequency and channel features in sensor data using hierarchical connectivity. Furthermore, it incorporates a feature fusion mechanism to integrate and exchange multiscale and multidimensional features, providing more comprehensive and richer features. To evaluate the HMTF-CFAF network, we utilize three datasets: the University of California Irvine HAR (UCI-HAR), physical activity monitoring for aging people (PAMAP2), and self-collected household behavior (HB) dataset. The HMTF-CFAF network achieves the accuracy of 97.66%, 98.75%, and 98.80% on the above three datasets, respectively, demonstrating its excellent performance.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Human activity recognition",
                "Internet of Things",
                "Time-frequency analysis",
                "Floors",
                "Data mining",
                "Convolution",
                "Time series analysis",
                "Long short term memory",
                "Kernel"
            ],
            "Author Keywords": [
                "Human activity recognition",
                "hierarchical multiscale and multidimensional feature extraction",
                "sensor data",
                "time-frequency domain feature"
            ]
        },
        "title": "An Efficient Hierarchical Multiscale and Multidimensional Feature Adaptive Fusion Network for Human Activity Recognition Using Wearable Sensors"
    },
    {
        "authors": [
            "Chen Zhang",
            "Yusha Liu",
            "Jie Hu",
            "Kun Yang"
        ],
        "published_in": "Published in: IEEE Journal on Selected Areas in Communications ( Early Access )",
        "date_of_publication": "16 September 2024",
        "doi": "10.1109/JSAC.2024.3460059",
        "publisher": "IEEE",
        "abstract": "Satellite Internet of things (S-IoT) aims to provide globally covered network services. In this paper, we conceive an uplink grant-free random access scheme for S-IoT network, where ground devices transmit data packets to the low Earth orbit (LEO) satellite, reducing signaling cost and making efficient use of spectrum resources by employing the non-orthogonal multiple access scheme. The impact of high operational speed of the LEO satellite is also taken into account. We further propose an iterative Gaussian approximated message passing-aided sparse Bayesian learning (GAMP-SBL) algorithm to address the joint channel estimation (CE), active user identification (UID) and data detection (DD) problem, where the three steps interacts with each other during the iterative process. Simulation results have demonstrated that our proposed joint receiver design outperforms the existing AMP-based schemes in terms of bit error rate (BER), convergence speed, as well as false alarm rate (FAR).",
        "issn": {
            "Print ISSN": "0733-8716",
            "Electronic ISSN": "1558-0008"
        },
        "keywords": {
            "IEEE Keywords": [
                "Satellites",
                "Low earth orbit satellites",
                "Receivers",
                "NOMA",
                "Message passing",
                "Channel estimation",
                "Multiuser detection"
            ],
            "Author Keywords": [
                "Grant-free non-orthogonal multi access (GF-NOMA)",
                "low Earth orbit (LEO) satellite communications",
                "message passing",
                "sparse Bayesian learning (SBL)"
            ]
        },
        "title": "Joint User Identification, Channel Estimation and Data Detection for Grant-Free NOMA in LEO Satellite Communications"
    },
    {
        "authors": [
            "Vivek Kapil",
            "Sheetla Prasad"
        ],
        "published_in": "Published in: IEEE Transactions on Power Systems ( Early Access )",
        "date_of_publication": "09 May 2024",
        "doi": "10.1109/TPWRS.2024.3398693",
        "publisher": "IEEE",
        "abstract": "The power grids are becoming smarter with adoption of various IoT Technologies (Internet of Things). IoT calls for sensors, which are transacting real time data through communication channels, installed at various locations in power system. These communication channels not only become overloaded by high volume of state-estimation data from these sensors but are also prone to cyber-attacks. The cyber-attack may infirmly affect the load frequency control (LFC) schemes which are implemented across multi-area grids using state estimation techniques. The cyber-attack may modify system frequency via signal false data injections in system and therefore deteriorates system performance or may even lead to blackout. In this work, artificial intelligent (AI) estimator and nonlinear sliding mode controller (SMC) are combined together to form event-triggered intelligent control scheme which not only optimise the use of communication infrastructure but also attenuates cyber intrusions. The AI observer predicts grid states and also senses cyber intrusions via the back propagation error algorithm in presence of cyber-attacks. The proposed event-triggered SMC rejects the undesirable impact of cyber intrusions on the plant trajectories. The proposed event-triggered control strategy when subjected to cyber attack and power penetration from the distributed renewable resources is demonstrated through MATLAB simulations.",
        "issn": {
            "Print ISSN": "0885-8950",
            "Electronic ISSN": "1558-0679"
        },
        "keywords": {
            "IEEE Keywords": [
                "Cyberattack",
                "Uncertainty",
                "Power system stability",
                "Mathematical models",
                "Frequency control",
                "Artificial intelligence",
                "Power grids"
            ],
            "Author Keywords": [
                "Event-triggered",
                "cyber-attack",
                "artificial intelligent (AI) estimator",
                "sliding mode control (SMC) and load frequency control (LFC)"
            ]
        },
        "title": "Event-triggered Intelligent Control Scheme for Data Integrity Attack Mitigation in Multi-area Power Grids"
    },
    {
        "authors": [
            "Mehdi Naderi Soorki",
            "Hossein Aghajari",
            "Sajad Ahmadinabi",
            "Hamed Bakhtiari Babadegani",
            "Christina Chaccour",
            "Walid Saad"
        ],
        "published_in": "Published in: IEEE Transactions on Mobile Computing ( Early Access )",
        "date_of_publication": "26 September 2024",
        "doi": "10.1109/TMC.2024.3468382",
        "publisher": "IEEE",
        "abstract": "Long-range (LoRa) wireless networks have been widely proposed as efficient wireless access networks for battery-constrained Internet of Things (IoT) devices. However, applying the LoRa-based IoT network in search-and-rescue (SAR) operations will have limited coverage caused by high signal attenuation due to terrestrial blockages, especially in highly remote areas. To overcome this challenge, using unmanned aerial vehicles (UAVs) as a flying LoRa gateway to transfer messages from ground LoRa nodes to the ground rescue station can be a promising solution. In this paper, an artificial intelligence-empowered SAR operation framework using a UAV-assisted LoRa network in different unknown search environments is designed and implemented. The problem of the flying LoRa (FL) gateway control policy is modeled as a partially observable Markov decision process to move the UAV towards the LoRa transmitter carried by a lost person in the known remote search area. A deep reinforcement learning (RL)-based policy is designed to determine the adaptive FL gateway trajectory in a given search environment. Then, as a general solution, a deep meta-RL framework is used for SAR in any new and unknown environments. The proposed deep meta-RL framework integrates the information of the prior FL gateway experience in the previous SAR environments to the new environment and then rapidly adapts the UAV control policy model for SAR operation in a new and unknown environment. To analyze the performance of the proposed framework in real-world scenarios, the proposed SAR system is experimentally tested in three environments: a university campus, a wide plain, and a slotted canyon at Mongasht mountain ranges, Iran. Experimental results show that if the deep meta-RL-based control policy is applied instead of the deep RL-based one, the number of SAR time slots decreases from 141 to 50. Moreover, in the slotted canyon environment, the UAV energy consumption under the deep meta-RL policy is respectively 5...",
        "issn": {
            "Print ISSN": "1536-1233",
            "Electronic ISSN": "1558-0660"
        },
        "keywords": {
            "IEEE Keywords": [
                "LoRa",
                "Logic gates",
                "Autonomous aerial vehicles",
                "Location awareness",
                "Internet of Things",
                "Adaptation models",
                "Greedy algorithms",
                "Wireless networks",
                "Tracking",
                "Three-dimensional displays"
            ],
            "Author Keywords": [
                "Deep meta-reinforcement learning",
                "LoRa technology",
                "search-and-rescue operation",
                "unmanned aerial vehicle"
            ]
        },
        "title": "Catch Me If You Can: Deep Meta-RL for Search-and-Rescue Using LoRa UAV Networks"
    },
    {
        "authors": [
            "Ze Zhang",
            "Pan Wang",
            "Tianqi Zhang",
            "Minyao Liu",
            "Xiaokang Zhou"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "03 October 2024",
        "doi": "10.1109/TCE.2024.3473304",
        "publisher": "IEEE",
        "abstract": "The application of Artificial Intelligence (AI) and Internet of Things (IoT) technologies enhances device smart decision-making, yet it also makes them more susceptible to cyber-attacks. AI-enabled Network Intrusion Detection System (NIDS) has been regarded as an effective way to mitigate such attacks. However, large-scale attacks data collection and labeling is prohibitively costly and time-consuming. Therefore, effective Few-Shot Learning based NIDS for IoT are crucial. Variational Autoencoders and Generative Adversarial Networks are widely used in generative few-shot learning but struggle with limited sample diversity and unstable training. Meanwhile, Deep Learning-based NIDS often lacks trustworthiness due to the \"black box\" problem. In response to these challenges, we propose a network traffic sample generation method that leverages the Conditional Denoising Diffusion Probabilistic Model (CDDPM) to tackle the issues of samples. Then, we introduce an Intrusion Detection method named CNNBiGRU that combines Convolutional Neural Networks with Bidirectional Gated Recurrent Units based on the synthetic data generated by CDDPM. Furthermore, we apply the SHAP method to interpret the results to ensure model users’ trust. Finally, the proposed method is evaluated on the two public datasets, which illustrate that our approach can successfully generates high-quality samples and improves the performance of NIDS.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Consumer electronics",
                "Data models",
                "Network intrusion detection",
                "Feature extraction",
                "Internet of Things",
                "Few shot learning",
                "Diffusion models",
                "Training",
                "Artificial intelligence",
                "Telecommunication traffic"
            ],
            "Author Keywords": [
                "Internet of Things",
                "Consumer Electronics",
                "Diffusion Models",
                "Network Intrusion Detection",
                "Generative Few-Shot Learning",
                "Model interpretability"
            ]
        },
        "title": "Trustworthy Generative Few-Shot Learning Based Intrusion Detection Method in Internet of Things"
    },
    {
        "authors": [
            "Jiashuo He",
            "Sai Huang",
            "Yuting Chen",
            "Shuo Chang",
            "Yifan Zhang",
            "Zhiyong Feng"
        ],
        "published_in": "Published in: IEEE Transactions on Cognitive Communications and Networking ( Early Access )",
        "date_of_publication": "28 May 2024",
        "doi": "10.1109/TCCN.2024.3405481",
        "publisher": "IEEE",
        "abstract": "Due to the unprecedented growth of the Internet of Things (IoT), safeguarding IoT devices of wireless connection has now become more challenging than ever. As a new paradigm of security mechanisms, radio frequency fingerprint identification (RFFI) has attracted lots of research attention since it can be used to authenticate authorized users by exploiting the transmitter hardware characteristics. However, the channel variations will cause the distribution shift of the received signals and thus lead to an unreliable classification performance for RFFI in unknown channel environments. For this reason, this paper proposes a robust RFFI method to resist the unknown channel effects. Specifically, we first propose a signal preprocessing method named second-order spectral circular shift bidirectional division (SoSCSBD) to convert the received signals into the second-order spectral quotient (SoSQ) sequences, where the channel effects can be deeply suppressed. Secondly, considering the division-based algorithm will induce the outliers, we present the median absolute deviation aided outlier filter (MADAOF) to remove them so that the statistical stability of the filtered SoSQ sequences will be enhanced. Then, a low-dimension RFF extraction method using the principal component analysis (PCA) algorithm is proposed to construct feature samples with device-specific information. Finally, the feature samples collected under different channel conditions are sent to the multi-class support vector machine (SVM) classifiers for training and testing. Furthermore, we carry out extensive experiments to evaluate the performance of the proposed RFFI method. Numerical results suggest that the newly developed RFFI method exhibits strong generalization, achieving robust and superior performance in comparison to several existing methods under unknown multipath fading channels.",
        "issn": {
            "Electronic ISSN": "2332-7731"
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Fading channels",
                "Wireless communication",
                "Training",
                "OFDM",
                "Symbols",
                "Support vector machines"
            ],
            "Author Keywords": [
                "Multipath fading channel",
                "OFDM system",
                "physical layer security",
                "radio frequency fingerprint"
            ]
        },
        "title": "Radio Frequency Fingerprint Identification for OFDM System Considering Unknown Multipath Fading Channel"
    },
    {
        "authors": [
            "Hao Tieng",
            "Ting-Chia Ou",
            "Tsung-Han Tsai",
            "Yu-Yong Li",
            "Min-Hsiung Hung",
            "Fan-Tien Cheng"
        ],
        "published_in": "Published in: IEEE Transactions on Automation Science and Engineering ( Early Access )",
        "date_of_publication": "12 December 2023",
        "doi": "10.1109/TASE.2023.3340149",
        "publisher": "IEEE",
        "abstract": "Industry 4.0 is accelerating, with manufacturing enterprises embracing digital transformation and intelligent manufacturing (iM) to enhance competitiveness. Manufacturing enterprises must also improve their environmental performance to meet the goal of net zero by 2050 and to avoid border carbon tax imposed by large economies. Thus, green intelligent manufacturing (GiM), i.e., conducting iM while pursuing to maximize energy conservation and carbon reduction, has become the key development trend and a fundamental challenge for the modern manufacturing industry. To address the challenges of green intelligent manufacturing (GiM), this paper proposes a novel framework called Industry 4.2 for GiM (I4.2-GiM). This framework builds on the Intelligent Factory Automation (\ni\nFA) platform, which the authors developed to achieve zero-defect manufacturing (i.e., Industry 4.1). I4.2-GiM uses various IoT devices, called Cyber-Physical Agents (CPAs), to collect and integrate large amounts of data. It also includes two interrelated systems, an intelligent carbon emission management system (\ni\nCMS) and an intelligent energy management system (\ni\nEMS), simultaneously tackling carbon reduction and energy saving. Existing factory EMSs typically save less than 10% of energy, but I4.2-GiM has been shown to conserve 10.9% of energy and reduce carbon emissions by 12.55% while conducting iM in daily production. I4.2-GiM is a promising new framework that can help manufacturing enterprises approach net zero intelligently. Note to Practitioners —This paper proposes a new green intelligent manufacturing (GiM) framework called I4.2-GiM (Industry 4.2 for GiM). I4.2-GiM builds on the Intelligent Factory Automation (\ni\nFA) platform, which the authors developed to achieve zero-defect manufacturing (i.e., Industry 4.1). I4.2-GiM also uses various IoT devices called Cyber-Physical Agents (CPAs) to collect data from multiple sources. It also includes two interrelated systems, an intell...",
        "issn": {
            "Print ISSN": "1545-5955",
            "Electronic ISSN": "1558-3783"
        },
        "keywords": {
            "IEEE Keywords": [
                "Carbon dioxide",
                "Manufacturing",
                "Net zero",
                "Carbon footprint",
                "Green products",
                "Internet of Things",
                "Energy management systems"
            ],
            "Author Keywords": [
                "Net zero",
                "cyber-physical agent (CPA)",
                "intelligent carbon-emission management system ( $i$ CMS)",
                "intelligent energy management system ( $i$ EMS)",
                "industry 4.2 for green intelligent manufacturing (I4.2-GiM)"
            ]
        },
        "title": "I4.2-GiM: A Novel Green Intelligent Manufacturing Framework for Net Zero"
    },
    {
        "authors": [
            "Yuanyuan Wang",
            "Chi Zhang",
            "Taiheng Ge",
            "Miao Pan"
        ],
        "published_in": "Published in: IEEE Transactions on Network Science and Engineering ( Early Access )",
        "date_of_publication": "19 April 2024",
        "doi": "10.1109/TNSE.2024.3391289",
        "publisher": "IEEE",
        "abstract": "The exponential growth of Internet of Things (IoT) devices and emerging applications have significantly increased the requirements for ubiquitous connectivity and efficient computing paradigms. Traditional terrestrial edge computing architectures cannot provide massive IoT connectivity worldwide. In this paper, we propose an aerial hierarchical mobile edge computing system composed of high-altitude platforms (HAPs) and unmanned aerial vehicles (UAVs). In particular, we consider non-divisible tasks and formulate a task offloading problem to minimize the long-term processing cost of tasks while satisfying the queueing mechanism in the offloading procedure and processing procedure of tasks. We propose a multi-agent deep reinforcement learning (DRL) based computation offloading algorithm in which each device can make its offloading decision according to local observations. Due to the limited computing resources of UAVs, high task loads of UAVs will increase the ratio of abandoning offloaded tasks. To increase the success ratio of completing tasks, the convolutional LSTM (ConvLSTM) network is utilized to estimate the future task loads of UAVs. In addition, a prioritized experience replay (PER) method is proposed to increase the convergence speed and improve the training stability. The experimental results demonstrate that the proposed computation offloading algorithm outperforms other benchmark methods.",
        "issn": {
            "Electronic ISSN": "2327-4697"
        },
        "keywords": {
            "IEEE Keywords": [
                "Task analysis",
                "Internet of Things",
                "Autonomous aerial vehicles",
                "Delays",
                "Costs",
                "Resource management",
                "Disasters"
            ],
            "Author Keywords": [
                "Aerial computing",
                "mobile edge computing",
                "deep reinforcement learning",
                "computation offloading"
            ]
        },
        "title": "Computation Offloading Via Multi-Agent Deep Reinforcement Learning in Aerial Hierarchical Edge Computing Systems"
    },
    {
        "authors": [
            "Chuan Sun",
            "Xiongwei Wu",
            "Xiuhua Li",
            "Qilin Fan",
            "Junhao Wen",
            "Victor C.M. Leung"
        ],
        "published_in": "Published in: IEEE Transactions on Network Science and Engineering ( Early Access )",
        "date_of_publication": "30 April 2021",
        "doi": "10.1109/TNSE.2021.3076795",
        "publisher": "IEEE",
        "abstract": "Driven by numerous emerging services and applications of mobile devices, multi-access edge computing (MEC) is regarded as a promising technique for massive Internet of Things (IoT) with 6G mobile networks to alleviate core network congestion and reduce service latency. However, the conventional MEC suffers from the infrastructure without the cloud server (CS) and cooperation of multiple edge servers (ESs), which cannot deal with the large-scale computation tasks in the ultra-dense smart environments. This paper investigates the issue of the cooperative computation offloading for MEC in the 6G era. The proposed MEC system allows the cooperation of edge-cloud and the cooperation of edge-edge to address the limitation of single ES and the nonuniform distribution of computation task arrival among multiple ESs. To support low-latency services, we model the cooperative computation offloading problem as a Markov decision process, and propose two intelligent computation offloading algorithms based on Soft Actor Critic (SAC), i.e., centralized SAC offloading and decentralized SAC offloading. Evaluation results show that the proposed algorithms outperform the existing computation offloading algorithms in terms of service latency.",
        "issn": {
            "Electronic ISSN": "2327-4697"
        },
        "keywords": {
            "IEEE Keywords": [
                "Task analysis",
                "6G mobile communication",
                "Edge computing",
                "Cloud computing",
                "Computer architecture",
                "Servers",
                "Resource management"
            ],
            "Author Keywords": [
                "Multi-access edge computing",
                "6G",
                "cooperative computation offloading",
                "service latency",
                "Soft Actor Critic"
            ]
        },
        "title": "Cooperative Computation Offloading for Multi-Access Edge Computing in 6G Mobile Networks via Soft Actor Critic"
    },
    {
        "authors": [
            "Sudhir K. Routray",
            "Sasmita Mohanty"
        ],
        "published_in": "Published in: IEEE Potentials ( Early Access )",
        "date_of_publication": "03 June 2024",
        "doi": "10.1109/MPOT.2024.3404114",
        "publisher": "IEEE",
        "abstract": "Information and communications technology (ICT) is among the main tools of the scientific world in modern times. It includes all of the technologies used for processing, transmitting, and storing information and facilitating communication. In fact, ICT includes a wide range of technologies and tools that are used to manage and exchange information, such as computers, software, networks, telecommunications equipment, and other associated digital devices. It plays crucial roles in various aspects of modern life, including business, education, government, health care, entertainment, and many more. It has transformed the way people communicate, access information, conduct business, and interact with each other. Large-scale ICT applications include the Internet; the Internet of Things (IoT); mobile phones; social media; e-mail; videoconferencing; cloud computing; and various software applications for data analysis, graphics design, programming, and digitalization.",
        "issn": {
            "Print ISSN": "0278-6648",
            "Electronic ISSN": "1558-1772"
        },
        "keywords": {
            "IEEE Keywords": [
                "Meteorology",
                "Economics",
                "Sustainable development",
                "Climate change",
                "Ecosystems",
                "Education",
                "Greenhouse gases"
            ],
            "Author Keywords": []
        },
        "title": "Information and communications technology for a sustainable world"
    },
    {
        "authors": [
            "Yudai Mita",
            "Yuta Hiraoka",
            "Tomofumi Hikasa",
            "Shigeru Tomisato",
            "Satoshi Denno",
            "Kazuhiro Uehara"
        ],
        "published_in": "Published in: IEICE Communications Express ( Early Access )",
        "date_of_publication": "12 November 2024",
        "doi": "10.23919/comex.2024XBL0173",
        "publisher": "IEICE",
        "abstract": "With the arrival of the IoT era, the problem of interference has become even more serious, and the challenge is to establish techniques for separating and demodulating collide signals. To achieve this, we are conducting research on stored data batch signal processing technology using short-time Fourier transform (STFT). In this paper, we used a feature quantity demodulation method proposed as a method for stored data batch signal processing to evaluate the effect of IQ imbalance, which is a factor in signal degradation in the transmitter, on signal separation and demodulation performance. Furthermore, we proposed a receiver configuration that estimates the amount of degradation due to IQ imbalance from the extracted feature quantities and compensates for the signal degradation. Computer simulation results confirmed that the proposed compensation method can improve signal separation and demodulation performance when degraded by IQ imbalance.",
        "issn": {
            "Electronic ISSN": "2187-0136"
        },
        "keywords": {
            "IEEE Keywords": [
                "Demodulation",
                "Source separation",
                "Feature extraction",
                "Degradation",
                "Signal processing",
                "Receivers",
                "Bandwidth",
                "Transmitters",
                "Symbols",
                "Wireless communication"
            ],
            "Author Keywords": [
                "stored data batch signal processing",
                "short-time Fourier transform",
                "IQ imbalance",
                "IQ Imbalance compensation"
            ]
        },
        "title": "Study of signal separation and demodulation techniques considering IQ imbalance by stored data batch signal processing"
    },
    {
        "authors": [
            "Chang Kyung Kim",
            "TaeYoung Kim",
            "SuKyoung Lee"
        ],
        "published_in": "Published in: IEEE Transactions on Vehicular Technology ( Early Access )",
        "date_of_publication": "07 October 2024",
        "doi": "10.1109/TVT.2024.3475585",
        "publisher": "IEEE",
        "abstract": "Edge service caching enables computational tasks to be processed at nearby edge nodes (ENs). caching necessary programs rather than in the distant cloud. However, with the continually increasing popularity of IoT devices, resource-constrained ENs cannot ensure low-latency services for tasks generated by these devices. To address this issue, in this study, we propose a cooperative service caching mechanism that caches highly demanded services and processes tasks requiring those services across multiple ENs. Considering the waiting time for tasks to be processed at the EN, we formulate a delay minimization problem in cooperative service caching. To solve this problem, we devise an ant colony optimization-based algorithm. The simulation results show that the proposed method can achieve a lower total delay than existing methods.",
        "issn": {
            "Print ISSN": "0018-9545",
            "Electronic ISSN": "1939-9359"
        },
        "keywords": {
            "IEEE Keywords": [
                "Delays",
                "Wireless communication",
                "Internet of Things",
                "Navigation",
                "Noise",
                "Interference",
                "Costs",
                "Autonomous vehicles",
                "Ant colony optimization",
                "Visualization"
            ],
            "Author Keywords": [
                "Cooperative service caching",
                "waiting time",
                "total delay",
                "multiple ENs",
                "ant colony optimization"
            ]
        },
        "title": "Cooperative Service Caching for Reducing Delay in Multi-Edge Networks"
    },
    {
        "authors": [
            "Trung-Dung Ha",
            "Xuecong Nie",
            "Mobayode Akinsolu",
            "Bo Liu",
            "Pai-Yen Chen"
        ],
        "published_in": "Published in: IEEE Antennas and Wireless Propagation Letters ( Early Access )",
        "date_of_publication": "22 August 2024",
        "doi": "10.1109/LAWP.2024.3447693",
        "publisher": "IEEE",
        "abstract": "In this letter, we propose and experimentally demonstrate compact, low-profile, and optically-transparent antennas for multi-band and multi-range wireless power transfer (WPT) applications. Specifically, we put forward new types of transparent multi-band antennas that can perform the near-field reactive WPT (13.56 MHz), as well as the far-field radiative WPT (980 MHz and 2.45 GHz) within a single device. Further, such an antenna is integrated with compact, frequency-scalable rectifying circuits to form an unseeable multi-mode WPT device. We show that a hybrid inductive (13.56 MHz) and radiative (980 MHz and 2.4 GHz) WPT device can be realized with a modified inverted-F antenna (IFA) structure connected to spiral-coil virtual ground. To meet the stringent design requirements of this unobtrusive multi-band antenna, a state-of-the-art machine learning-assisted global optimization method (parallel surrogate model-assisted hybrid differential evolution for antenna optimization or PSADEA) is exploited for global optimization. We envision that the proposed transparent and flexible WPT and energy harvesting devices can be beneficial for many applications, including ubiquitous wireless charging based on smart windows and glasses, solar-radio frequency (RF) integrated power supply, wearable or textile electronics, and internet-of-things (IoTs).",
        "issn": {
            "Print ISSN": "1536-1225",
            "Electronic ISSN": "1548-5757"
        },
        "keywords": {
            "IEEE Keywords": [
                "Antennas",
                "Antenna measurements",
                "Optimization",
                "Rectennas",
                "Microstrip antennas",
                "Reflector antennas",
                "Reflection coefficient"
            ],
            "Author Keywords": [
                "transparent antennas",
                "AI-assisted antenna design optimization",
                "antenna synthesis",
                "differential evolution",
                "efficient global optimization",
                "energy harvesting"
            ]
        },
        "title": "An Artificial Intelligence-Assisted Optimization of Imperceptible Multi-Mode Rectenna"
    },
    {
        "authors": [
            "Yuejun Zhang",
            "Lixun Wang",
            "Jiawei Wang",
            "Yongzhong Wen",
            "Huihong Zhang",
            "Gang Li",
            "Pengjun Wang"
        ],
        "published_in": "Published in: IEEE Transactions on Very Large Scale Integration (VLSI) Systems ( Early Access )",
        "date_of_publication": "06 November 2024",
        "doi": "10.1109/TVLSI.2024.3486368",
        "publisher": "IEEE",
        "abstract": "Security and cost efficiency are of utmost importance for embedded processors when it comes to limiting hardware resources in IoT applications. This brief presents a security reduced instruction set computer-five (RISC-V) specific instruction set extension (ISE) designed based on hardware-assisted orthogonal obfuscation for hardware security. The orthogonal obfuscation defines an architecture geared toward high-security processors that supports a Pay-Per-ISE function using a key management unit (KMU), thus capable of supporting the customization of the key for a user’s partially authorized ISE and controlling the unlocking of the specific ISE. The proposed security RISC-V test chip is fabricated in a 65-nm CMOS technology with a core area occupying about 0.739 mm $^2$ . The measured results demonstrate that our processor realizes the instruction set authorization function. The results show an average power of 52.8 mW at 1.2 V, a hardware overhead of $<$ 3% at 50 MHz, and a 30% improvement in security.",
        "issn": {
            "Print ISSN": "1063-8210",
            "Electronic ISSN": "1557-9999"
        },
        "keywords": {
            "IEEE Keywords": [
                "Security",
                "Hardware",
                "Instruction sets",
                "Registers",
                "Vectors",
                "Computer architecture",
                "Encryption",
                "Decoding",
                "Very large scale integration",
                "Software"
            ],
            "Author Keywords": [
                "Instruction set authorization",
                "orthogonal obfuscation",
                "Pay-per-instruction set extension (ISE)",
                "reduced instruction set computer-five (RISC-V)"
            ]
        },
        "title": "A Pay-Per-ISE RISC-V Processor With Hardware-Assisted Orthogonal Obfuscation"
    },
    {
        "authors": [
            "Jian-Xin Chen",
            "Jian Zhou",
            "Jing Cai",
            "Wen-Wen Yang"
        ],
        "published_in": "Published in: IEEE Antennas and Wireless Propagation Letters ( Early Access )",
        "date_of_publication": "16 August 2024",
        "doi": "10.1109/LAWP.2024.3444793",
        "publisher": "IEEE",
        "abstract": "A bandwidth-enhanced frequency-agile stacked patch antenna sharing common tuning element is proposed. Benefiting from the stacked patch structure, the proposed antenna obtains the enhanced bandwidth and significantly reduces the antenna size. Based on the strongest E-fields on both edges which perpendicular to the polarized direction, a pair of microstrip lines is embedded between the edges of the two patches. Besides, the application of two varactors implanted in the microstrip lines enables the proposed antenna to synchronously tune the two resonant frequencies. The shared tuning element makes the proposed antenna to obtain a stable enhanced bandwidth of about 15% over the entire frequency tuning range of 33.4% (1.87-2.62 GHz). The gain and peak efficiency vary from 3 to 5.5 dBi and 76% to 88% over the entire band, respectively. The remarkable performances of the proposed antenna make it suitable for low-cost Internet of Things (IoT) devices.",
        "issn": {
            "Print ISSN": "1536-1225",
            "Electronic ISSN": "1548-5757"
        },
        "keywords": {
            "IEEE Keywords": [
                "Antennas",
                "Resonant frequency",
                "Tuning",
                "Varactors",
                "Microstrip antennas",
                "Substrates",
                "Bandwidth"
            ],
            "Author Keywords": [
                "Stacked patch antenna",
                "common tuning element",
                "bandwidth-enhanced antenna",
                "frequency-agile antenna"
            ]
        },
        "title": "Bandwidth-Enhanced Frequency-Agile Stacked Patch Antenna Sharing Common Tuning Element"
    },
    {
        "authors": [
            "Wai-Kong Lee",
            "Seog Chung Seo",
            "Hwajeong Seo",
            "Dong Cheon Kim",
            "Seong Oun Hwang"
        ],
        "published_in": "Published in: IEEE Embedded Systems Letters ( Early Access )",
        "date_of_publication": "05 June 2024",
        "doi": "10.1109/LES.2024.3409725",
        "publisher": "IEEE",
        "abstract": "The Advanced Encryption Standard (AES) has been widely used to protect digital data in various applications, such as secure IoT communication, files encryption, and pseudo-random number generation. The efficient implementation of AES on parallel architecture such as GPU, has attracted considerable interest over the past decade. These prior studies mainly implemented the AES electronics code book (ECB) and counter (CTR) mode using the table-based approach. In this brief, we set a new speed record of AES-ECB and AES-CTR on Graphics Processing Unit (GPU) based on the proposed bit-sliced implementation techniques. Our implementation achieved 2.6% (ECB) and 9% (CTR) faster than the state-of-the-art table-based implementation on a RTX3080 GPU. Our work evaluated on an embedded GPU (Jetson Orin Nano) can also achieve high throughput at 60 Gbps, which is 1.9% (ECB) and 7% (CTR) faster than state-of-the-art.",
        "issn": {
            "Print ISSN": "1943-0663",
            "Electronic ISSN": "1943-0671"
        },
        "keywords": {
            "IEEE Keywords": [
                "Graphics processing units",
                "Registers",
                "Encryption",
                "Instruction sets",
                "Throughput",
                "Computer architecture",
                "Standards"
            ],
            "Author Keywords": [
                "Advanced Encryption Standard",
                "Graphics Processing Unit",
                "Bit-sliced Implementation"
            ]
        },
        "title": "Speed Record of AES-CTR and AES-ECB Bit-Sliced Implementation on GPUs"
    },
    {
        "authors": [
            "Siyou Guo",
            "Qilei Li",
            "Mingliang Gao",
            "Guisheng Zhang",
            "Gwanggil Jeon"
        ],
        "published_in": "Published in: IEEE Consumer Electronics Magazine ( Early Access )",
        "date_of_publication": "07 November 2024",
        "doi": "10.1109/MCE.2024.3493776",
        "publisher": "IEEE",
        "abstract": "The rapid and widespread adoption of consumer electronics and technology, such as smartphones, smart speakers, and IoT devices, has fundamentally changed how we access and consume information. This technological revolution has not only enhanced our daily convenience but has also driven the development of smart cities and brought significant advancements in urban living. However, this digital convenience comes with challenges, notably the production of fake news. Through various consumer electronics platforms, false information can be quickly produced and spread, undermining public trust and social order. Although fake news-detecting technology has advanced rapidly benefitting from deep-learning techniques, it often fails to consider feature interactions. To address this issue, we propose an Enhanced Feature Interactions Network (EFI-Net) for fake news detection. Specifically, the EFI-Net introduces an Efficient Additive Learning (EAL) module to enhance feature interaction for language models at different scales. Experiments were conducted using the ARG fake news detection dataset, and the proposed network achieves an accuracy of 88.9% on English fake news and 78.7% on Chinese fake news, which outperforms the state-of-the-art (SOTA) method by a large margin. This work has substantial implications for consumer electronics and technology. Users can benefit from more reliable information filtering and verification by integrating EFI-Net into various consumer electronic platforms.",
        "issn": {
            "Print ISSN": "2162-2248",
            "Electronic ISSN": "2162-2256"
        },
        "keywords": {
            "IEEE Keywords": [
                "Fake news",
                "Consumer electronics",
                "Smart cities",
                "Feature extraction",
                "Additives",
                "Cognition",
                "Vectors",
                "Smart phones",
                "Security",
                "Semantics"
            ],
            "Author Keywords": []
        },
        "title": "Smart City Security: Fake News Detection in Consumer Electronics"
    },
    {
        "authors": [
            "Miroslav Hutar",
            "Alejandro Gil-Martínez",
            "José Antonio López-Pastor",
            "Peter Brida",
            "José Luis Gómez-Tornero"
        ],
        "published_in": "Published in: IEEE Sensors Letters ( Early Access )",
        "date_of_publication": "24 October 2024",
        "doi": "10.1109/LSENS.2024.3486343",
        "publisher": "IEEE",
        "abstract": "In this paper, we investigate different criteria for selecting the mechanical tilting angle needed in amplitude-monopulse antenna arrays, with application to direction finding using wireless local area networks (WLANs). The antenna system consists of two tilted panel antennas used to compare the received signal strength indicator (RSSI) of the signals coming from IoT (Internet of Things) mobile devices equipped with WiFi Network Interface Controllers (NIC). This comparison is employed to estimate the direction of arrival (DoA) for real-time indoor localization and tracking applications. A DoA monopulse antenna system designed for the 2.4 GHz WiFi band is used as a practical example. Three different tilting angles are investigated to explain the benefits and drawbacks when operating using the conventional half-power beam-crossover point or when different tilting angles are applied. Measurements are carried out in an outdoor multipath scenario, concluding that accuracy and field of view are highly dependent on the tilting angle.",
        "issn": {
            "Electronic ISSN": "2475-1472"
        },
        "keywords": {
            "IEEE Keywords": [
                "Direction-of-arrival estimation",
                "Wireless fidelity",
                "Antennas",
                "Antenna measurements",
                "Internet of Things",
                "Location awareness",
                "Transmitters",
                "Receiving antennas",
                "Directive antennas",
                "Antenna radiation patterns"
            ],
            "Author Keywords": [
                "direction of arrival",
                "WiFi",
                "localization",
                "amplitude monopulse comparison"
            ]
        },
        "title": "Amplitude-monopulse design considerations for WiFi direction finding applications"
    },
    {
        "authors": [
            "Jiaming Pei",
            "John Feng"
        ],
        "published_in": "Published in: IEEE Consumer Electronics Magazine ( Early Access )",
        "date_of_publication": "19 August 2024",
        "doi": "10.1109/MCE.2024.3445595",
        "publisher": "IEEE",
        "abstract": "In the era of digital content abundance, the consumer electronics field is rapidly evolving with the continuous emergence of new products and technologies such as smart home devices, mobile devices, and IoT devices. Recommendation systems play a crucial role in customizing online content according to individual user preferences, significantly enhancing user experience and indirectly boosting the profits of content service providers. These systems are increasingly integrated into consumer electronics, providing personalized experiences such as recommending media content on smart TVs, suggesting apps on smartphones, and optimizing smart home environments based on user habits. However, they face challenges including protecting user privacy, ensuring data security, and enhancing the personalization and accuracy of recommendations. In response, we propose a vision for the next generation of recommendation systems that addresses these challenges by combining federated learning with cognitive learning approaches. Federated learning enhances efficiency and strengthens privacy by processing data locally on user devices, while cognitive learning allows systems to dynamically adapt to user preferences using client-side feedback. This article explores the current state of recommendation systems, their evolution, demands, and obstacles, and outlines a blueprint for next-generation systems. By integrating these advanced technologies, we aim to create more efficient, secure, and personalized recommendation systems that can be seamlessly embedded into a wide array of consumer electronics, ultimately enhancing user experience and driving innovation in the industry.",
        "issn": {
            "Print ISSN": "2162-2248",
            "Electronic ISSN": "2162-2256"
        },
        "keywords": {
            "IEEE Keywords": [
                "Recommender systems",
                "Computational modeling",
                "Cognitive systems",
                "Consumer electronics",
                "Data privacy",
                "Data models",
                "Security"
            ],
            "Author Keywords": []
        },
        "title": "Federated Cognitive Learning For Next Generation Recommendation System"
    },
    {
        "authors": [
            "Tsen-Fang Lin",
            "Liang-Bi Chen"
        ],
        "published_in": "Published in: IEEE Potentials ( Early Access )",
        "date_of_publication": "10 October 2024",
        "doi": "10.1109/MPOT.2024.3470344",
        "publisher": "IEEE",
        "abstract": "This article introduces a non-invasive pressure therapy method using monaural beats (MB) sonic resonance to promote relaxation and reduce stress. Unlike binaural beats (BB), MB requires no headphones, enhancing accessibility. By targeting ultralow brainwave frequencies, MB effectively relieves anxiety and stress. Compared to BB and other infrasound methods, MB offers advantages in terms of cost, comfort, and flexibility. Potential applications in IoT and AIoT systems for adaptive therapies are also discussed, with further research needed to realize their therapeutic potential fully.",
        "issn": {
            "Print ISSN": "0278-6648",
            "Electronic ISSN": "1558-1772"
        },
        "keywords": {
            "IEEE Keywords": [
                "Medical treatment",
                "Ear",
                "Resonance",
                "Music",
                "Headphones",
                "Resonant frequency",
                "Brainstem",
                "Anxiety disorders",
                "Frequency modulation",
                "Brain modeling"
            ],
            "Author Keywords": []
        },
        "title": "A pressure therapy method: Monaural beats sonic resonance"
    },
    {
        "authors": [
            "Elena García",
            "Aurora Andújar",
            "Joan L. Pijoan",
            "Jaume Anguera"
        ],
        "published_in": "Published in: IEEE Antennas and Wireless Propagation Letters ( Early Access )",
        "date_of_publication": "29 July 2024",
        "doi": "10.1109/LAWP.2024.3434579",
        "publisher": "IEEE",
        "abstract": "Wearables, trackers, and sensors all fall under the category of IoT (Internet of Things) devices. Despite their differences in size, the efficiency of their antennas is crucial, especially considering the need for compactness and effectiveness across multiple frequency bands. To address these requirements, a tunable architecture is proposed, featuring a single switch SP4T (Single-Pole 4-Throw) that operates seamlessly from 698 MHz to 960 MHz and from 1710 MHz to 2170 MHz. In contrast to conventional methods relying on intricate geometric antenna shapes for achieving multiband performance, this design adopts a 30 mm x 3 mm x 1 mm (0.07λ) non-resonant element called an antenna booster. Coupled with a multiband matching network, this approach ensures efficient operation across multiple frequency bands. This design maintains consistency across 5 PCBs (Printed Circuit Boards) with different form factors, demonstrating robustness without changing the bill of materials (BoM). This design philosophy presents a streamlined alternative for incorporating antennas into various devices without the need for separate implementations",
        "issn": {
            "Print ISSN": "1536-1225",
            "Electronic ISSN": "1548-5757"
        },
        "keywords": {
            "IEEE Keywords": [
                "Antennas",
                "Radio frequency",
                "Internet of Things",
                "Bandwidth",
                "Impedance",
                "Antenna accessories",
                "Inductors"
            ],
            "Author Keywords": [
                "reconfigurable antennas",
                "compact antennas",
                "matching networks",
                "tunable solutions",
                "multiband",
                "antenna boosters"
            ]
        },
        "title": "Tunable Architecture for Multiple Form Factors with an Antenna Booster Element for Multiband Operation"
    },
    {
        "authors": [
            "Chenfei Xie",
            "Yonghui Li",
            "Lu Chen",
            "Weimin Shi",
            "Zhongpei Zhang",
            "Yue Xiu"
        ],
        "published_in": "Published in: IEEE Communications Letters ( Early Access )",
        "date_of_publication": "14 October 2024",
        "doi": "10.1109/LCOMM.2024.3479233",
        "publisher": "IEEE",
        "abstract": "As the Internet of Things (IoT) continue to advance, the integration of massive devices into wireless networks for communication, sensing, and energy harvesting tasks is becoming increasingly prevalent. This trend has led to the emergence of Integrated Sensing, Communication, and Power Transmission (ISCPT) systems. However, with the proliferation of functionalities within these systems, power consumption has become a critical concern. In this paper we aim to minimize power consumption by optimizing transmit beamforming while ensuring quality of service (QoS), meeting the Cramér–Rao bound (CRB) for radar target sensing accuracy, subject to energy harvesting constraints. To solve the non-convex problem, semidefinite relaxation (SDR) is proposed. Numerical results show that our proposed algorithm can reduce power consumption and satisfy the QoS and CRB constraints.",
        "issn": {
            "Print ISSN": "1089-7798",
            "Electronic ISSN": "1558-2558"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Signal to noise ratio",
                "Power demand",
                "Array signal processing",
                "Radar cross-sections",
                "Minimization",
                "Integrated sensing and communication",
                "Direction-of-arrival estimation",
                "Optimization",
                "Interference"
            ],
            "Author Keywords": [
                "Internet of Things",
                "integrating sensing",
                "communication, and power transfer",
                "Cramér–Rao bound",
                "semidefinite relaxation"
            ]
        },
        "title": "Power Minimization for Integrated Sensing, Communication, and Power Transmission Systems"
    },
    {
        "authors": [
            "Plínio S. Dester",
            "Maice Costa",
            "Pedro H. J. Nardelli",
            "Pedro E. Gória Silva",
            "Jules M. Moualeu"
        ],
        "published_in": "Published in: IEEE Wireless Communications Letters ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/LWC.2024.3490695",
        "publisher": "IEEE",
        "abstract": "In this letter, we revisit the notion of energy efficiency widely adopted in age-centric network-based systems. To this end, we consider two traditional ALOHA random-access protocols (unslotted ALOHA and slotted ALOHA) wherein their total energy consumption is analyzed. The importance of considering the total energy needed to deliver timely information stems from the increasing data consumption through resource-constrained devices, such as sensors in an application of the Internet of Things (IoT). We propose a novel perspective on the energy consumption, taking into account synchronization costs for a fair comparison between the access schemes.",
        "issn": {
            "Print ISSN": "2162-2337",
            "Electronic ISSN": "2162-2345"
        },
        "keywords": {
            "IEEE Keywords": [
                "Costs",
                "Synchronization",
                "Energy efficiency",
                "Throughput",
                "Internet of Things",
                "Energy consumption",
                "Wireless sensor networks",
                "Wireless networks",
                "Stationary state",
                "Measurement"
            ],
            "Author Keywords": [
                "Age of Information",
                "energy dissipation",
                "packet generation",
                "random access protocols"
            ]
        },
        "title": "Novel Energy-Centric Analysis for Random Access Networks"
    },
    {
        "authors": [
            "Ryunosuke Yamamoto",
            "Keigo Matsumoto",
            "Yoshiaki Inoue",
            "Yuko Hara",
            "Kazuki Maruta",
            "Yu Nakayama",
            "Daisuke Hisano"
        ],
        "published_in": "Published in: IEICE Communications Express ( Early Access )",
        "date_of_publication": "09 August 2024",
        "doi": "10.23919/comex.2024COL0014",
        "publisher": "IEICE",
        "abstract": "With the development of 5G technology and the proliferation of IoT devices, Deep Joint Source-Channel Coding (DeepJSCC) has attracted attention for efficiently transmitting video and image data. DeepJSCC can maintain a good peak signal-to-noise ratio (PSNR) of images even at a meager signal-to-noise ratio (SNR). In cellular communication systems, the compression ratio must adapt to channel fluctuations, requiring multiple training models at the base station. However, the optimal SNR and compression ratio combination during training has yet to be reported. This paper investigates the necessary number of training models by stepwise varying SNR and compression ratio during training.",
        "issn": {
            "Electronic ISSN": "2187-0136"
        },
        "keywords": {
            "IEEE Keywords": [
                "Signal to noise ratio",
                "Symbols",
                "Decoding",
                "Channel coding",
                "Wireless communication",
                "Reliability",
                "Real-time systems"
            ],
            "Author Keywords": [
                "5G",
                "Joint Source-Channel Coding",
                "Deep Learning",
                "Semantic Communication",
                "Image Transmission"
            ]
        },
        "title": "Impact of training models on deep joint source-channel coding applicable to 5G systems"
    },
    {
        "authors": [
            "Shiqi Han",
            "Zhan Wang",
            "Yuandan Dong"
        ],
        "published_in": "Published in: IEEE Antennas and Wireless Propagation Letters ( Early Access )",
        "date_of_publication": "02 August 2024",
        "doi": "10.1109/LAWP.2024.3437485",
        "publisher": "IEEE",
        "abstract": "This paper proposes a pattern-reconfigurable dielectric resonator (DR) antenna based on a hybrid excitation method. It intelligently combines the equivalent Huygens source and the biased feed, to effectively reduce the antenna dimensions while providing rich radiation patterns. Four PIN diodes are placed into the feeding network and the slot, respectively. By controlling the ON/OFF states of these PIN diodes, the radiation beams can be switched between five different states, including four states of the tilted beams and one state of the broadside beam. A highlight of this work is that most horizontally polarized (HP) antennas have only one-dimensional reconfigurable patterns, while this design achieves two-dimensional reconfigurability of the HP beam. The dimensions of the antenna are 0.35 λ 0 × 0.31 λ 0 × 0.15 λ 0 . (where λ 0 is the wavelength in free space at 2.42 GHz). A prototype is fabricated and measured. Good agreement between the measured and simulated results is achieved. The measured results show that the -10 dB impedance bandwidth can cover 2.4-2.48 GHz in all five states. The antenna has a stable gain throughout the overlapping bandwidth, with a peak gain of 5.15 dBi. The proposed antenna is characterized by small size, low cost, and flexible beam-steering, making it a good candidate for intelligent IoT/6G applications in the WI-FI band.",
        "issn": {
            "Print ISSN": "1536-1225",
            "Electronic ISSN": "1548-5757"
        },
        "keywords": {
            "IEEE Keywords": [
                "Antenna radiation patterns",
                "Antenna measurements",
                "Antennas",
                "Switches",
                "PIN photodiodes",
                "Dielectric resonator antennas",
                "Probes"
            ],
            "Author Keywords": [
                "Biased feed",
                "dielectric resonator antenna (DRA)",
                "Huygens source",
                "pattern-reconfigurable antenna"
            ]
        },
        "title": "Planar Beam-Steering Dielectric Resonator Antenna Based on Hybrid Excitation Method"
    },
    {
        "authors": [
            "Braj Kishore Jha",
            "Sunder Ali Khowaja",
            "Kapal dev",
            "Ankur Pandey"
        ],
        "published_in": "Published in: IEEE Consumer Electronics Magazine ( Early Access )",
        "date_of_publication": "14 August 2024",
        "doi": "10.1109/MCE.2024.3443543",
        "publisher": "IEEE",
        "abstract": "The unprecedented success of Artificial Intelligence (AI), Machine Learning (ML), Deep Learning (DL), and Internet of Things (IoT) technologies in the medical field has revolutionized healthcare systems across the globe. As a powerful computer vision and image processing tool, AI has leveraged early and accurate diagnosis of diseases facilitating early interventions of medical professionals. The Consumer Electronics (CE) in the medical arena range from smart wearable devices to sophisticated tools, that monitor medical signals such as Electroencephalogram (EEG) and electrocardiogram (ECG), and capture radiographic images such as Magnetic Resonance Imaging (MRI) that act as biomarkers for medical analysis. However, with its emergence, an associated problem has also surfaced in the form of its vulnerability to adversarial attacks. This work highlights different ways in which undetectable adversarial attacks pose a threat to the medical CE ecosystem. We simultaneously explore methods to mitigate the vulnerability of our medical domain against such attacks.",
        "issn": {
            "Print ISSN": "2162-2248",
            "Electronic ISSN": "2162-2256"
        },
        "keywords": {
            "IEEE Keywords": [
                "Artificial intelligence",
                "Medical diagnostic imaging",
                "Brain modeling",
                "Medical services",
                "Training",
                "Perturbation methods",
                "Consumer electronics"
            ],
            "Author Keywords": []
        },
        "title": "Safeguarding Medical AI: Insights and Addressing Adversarial Threats in Consumer Electronics"
    },
    {
        "authors": [
            "Nguyen Tien Hoa",
            "Can Thi Thanh Hai",
            "Hoang Le Hung",
            "Nguyen Cong Luong",
            "Dusit Niyato"
        ],
        "published_in": "Published in: IEEE Communications Letters ( Early Access )",
        "date_of_publication": "12 November 2024",
        "doi": "10.1109/LCOMM.2024.3496534",
        "publisher": "IEEE",
        "abstract": "In this letter, we investigate a joint edge computing and semantic communication in the UAV-enabled network. Therein, a UAV executes tasks offloaded from ground user equipments (UEs). Meanwhile, it acts as an IoT device to provide image data to a Metaverse platform through a ground base station (BS). A semantic communication (SemCom) technique is implemented at the UAV to extract scene graphs from captured images. The UAV then transmits the scene graphs (rather than the orignal images) to the BS. The small size of the scene graphs allows the UAV to transmit multiple images to the BS within a short duration. However, the scene graph extraction consumes computing resource, which may reduce the performance of the task offloading of the UEs. Therefore, we aim to optimize the UAV’s computing resource allocation and the fractions of the tasks offloaded from the UEs to achieve the min-max between i) the total latency of image collection, scene graph extraction, and scene graph communication, and ii) the computation offloading latency over the ground UEs. Given the dynamics and uncertainty of the wireless channels, the distances between the UAV and UEs, and the computing resources of the UEs, we leverage the Advantage Actor-Critic (A2C) and Proximal Policy Optimization (PPO) to solve it.",
        "issn": {
            "Print ISSN": "1089-7798",
            "Electronic ISSN": "1558-2558"
        },
        "keywords": {
            "IEEE Keywords": [
                "Autonomous aerial vehicles",
                "Metaverse",
                "Optimization",
                "Image edge detection",
                "Edge computing",
                "Servers",
                "Resource management",
                "Internet of Things",
                "Data mining",
                "Wireless communication"
            ],
            "Author Keywords": [
                "UAV",
                "semantic communication",
                "edge computing",
                "advantage actor-critic",
                "proximal policy optimization"
            ]
        },
        "title": "Joint Edge Computing and Semantic Communication in UAV-Enabled Networks"
    },
    {
        "authors": [
            "Priyan Malarvizhi Kumar",
            "Latif U. Khan",
            "Choong Seon Hong"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "22 February 2022",
        "doi": "10.1109/JSEN.2022.3153410",
        "publisher": "IEEE",
        "abstract": "Notice of Violation of IEEE Publication Principles \"Affirmative Fusion Process for Improving Wearable Sensor Data Availability in Artificial Intelligence of Medical Things,\" by P. M. Kumar, L. U. Khan and C. S. Hong, in IEEE Sensors Journal, Early Access After careful and considered review of the content and authorship of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE’s Publication Principles. The submitting author, Priyan Malarvizhi Kumar, added the coauthors Latif U. Khan and Choong Seon Hong without their consent. Due to the nature of this violation, the Editor in Chief has decided the article will not be published in an issue of IEEE Sensors Journal. Artificial Intelligence of Medical Things (AIoMT) is a hybridized outcome of Internet of Things (IoT), machine learning (ML) paradigms, and data analytics procedures for sophisticated healthcare services and applications. However, the fluctuating or lacking wearable sensors (WSs) data cause trivial computing errors that lead to incomplete diagnosis/ recommendation in healthcare applications. This article proposes a novel Affirmative Fusion Process (AFP) to enable high quality WS data with fewer fluctuations in in medical diagnosis. The proposed process assimilates sensed data with the existing datasets for avoiding discrete availability of WS data during the analysis. In this fusion process, based on the dataset inputs, the discreteness in the sensed data is identified. The discreteness is mitigated through precise replacement consideration from the existing datasets, preventing computational errors. The fusion process is monitored using simulated annealing and neural learning for output approximation and identification. The fused output with and without discreteness is identified for which annealing-based approximation is performed. In this process, the recurrence of the learning iterates is confined to identifying the final best solution. The proposed p...",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [],
            "Author Keywords": []
        },
        "title": "Notice of Violation of IEEE Publication Principles: Affirmative Fusion Process for Improving Wearable Sensor Data Availability in Artificial Intelligence of Medical Things"
    },
    {
        "authors": [
            "Ning Liu",
            "Xiangyan Liu",
            "Xianjun Sheng",
            "Xiao Li",
            "Haoyong Ding"
        ],
        "published_in": "Published in: IEEE Antennas and Wireless Propagation Letters ( Early Access )",
        "date_of_publication": "19 March 2024",
        "doi": "10.1109/LAWP.2024.3379221",
        "publisher": "IEEE",
        "abstract": "In this letter, a dual-polarized, notched ambient electromagnetic (EM) energy harvester array based on frequency selective surface (FSS) is presented. The proposed unit structure consists of a notched solid patch with two through-vias. By introducing the notches into the solid patch, the current aggregation at the through-vias is enhanced. As a result, the EM energy flow is constrained and high harvesting efficiency is achieved. Then, a 9×9 harvester array was fabricated and measured. Simulated and experimental results demonstrate that the proposed harvester features dual-polarization and wide-angle insensitive characteristics. At a 60° incidence angle of both TE and TM polarization, the harvesting efficiency exceeds 78%. Also, the half power bandwidth maintains above 100MHz, which covers the WIFI band. Besides, it is also shown that the notched harvester has a small operating frequency offset of 30MHz (1.23%). The proposed harvester can be used to construct a high-efficiency EM energy harvesting system for the wireless power supply of IoT sensors.",
        "issn": {
            "Print ISSN": "1536-1225",
            "Electronic ISSN": "1548-5757"
        },
        "keywords": {
            "IEEE Keywords": [
                "Integrated circuit modeling",
                "Equivalent circuits",
                "Solids",
                "Rectennas",
                "Impedance",
                "Metals",
                "Energy loss"
            ],
            "Author Keywords": [
                "Energy harvester",
                "dual-polarization",
                "wide-angle insensitivity",
                "frequency selective surface (FSS)",
                "wireless power supply"
            ]
        },
        "title": "A Dual-Polarized, Notched Electromagnetic Energy Harvester Array Based on FSS"
    },
    {
        "authors": [
            "Debashis Das",
            "Uttam Ghosh",
            "Pushpita Chatterjee",
            "Mohammed S. Al-Numay"
        ],
        "published_in": "Published in: IEEE Consumer Electronics Magazine ( Early Access )",
        "date_of_publication": "10 September 2024",
        "doi": "10.1109/MCE.2024.3456965",
        "publisher": "IEEE",
        "abstract": "The evolution of Context-Aware Computing (CAC) from its traditional roots to advanced implementations marks a significant milestone in human-machine interactions (HMI). With the rise of smart devices and IoT technologies, there is a greater need for advanced systems that can adapt to diverse contexts in consumer electronics (CE). Therefore, an advanced CAC approach is proposed to redefine human-machine interaction for more intuitive, personalized, and adaptable contexts. This approach utilizes cutting-edge technologies like machine learning and sensor fusion for real-time and accurate context recognition as well as enables more seamless and responsive interactions for enhancing user experiences by delivering contextually relevant content and services.",
        "issn": {
            "Print ISSN": "2162-2248",
            "Electronic ISSN": "2162-2256"
        },
        "keywords": {
            "IEEE Keywords": [
                "Hidden Markov models",
                "Real-time systems",
                "Adaptation models",
                "Data models",
                "Consumer electronics",
                "Accuracy",
                "Sensor fusion"
            ],
            "Author Keywords": []
        },
        "title": "Advanced Context-aware Computing for Human Machine Interaction in Consumer Electronics"
    },
    {
        "authors": [
            "Yujun Wang",
            "Shixing Yu",
            "Na Kou"
        ],
        "published_in": "Published in: IEEE Antennas and Wireless Propagation Letters ( Early Access )",
        "date_of_publication": "09 July 2024",
        "doi": "10.1109/LAWP.2024.3425649",
        "publisher": "IEEE",
        "abstract": "In this paper, a compact spherical rectifier antenna array is proposed. The rectifier antenna unit cell integrates a double-layer slit-coupled circularly polarized microstrip antenna with rectifier circuit loaded with a diode. The proposed rectified antenna unit achieves a conversion efficiency of 64.3% at 5.8GHz with an input power of 13 dBm and a load of 500Ω. To broaden the coverage area in wireless energy harvesting system, we propose to use spherical array to realize wide angle covered rectifier antenna array which consists of 31 units. At each direction of feed horn antenna in measurement, several unit cells are activated for harvesting the RF energy. When feed horn is located normally to the array, the DC conversion efficiency of 41.87% and converted DC voltage of 5.42V can be obtained when the input power is 17dBm and the load is 1400Ω. Furthermore, the rectified voltages of the array with feed horn at different elevation angles are measured. The rectified voltage was stabilized at about 5V when the input power and load resistance were fixed. This indicates that the designed spherical rectifier antenna array has good omnidirectional reception capability which is suitable for IoT sensor applications",
        "issn": {
            "Print ISSN": "1536-1225",
            "Electronic ISSN": "1548-5757"
        },
        "keywords": {
            "IEEE Keywords": [
                "Rectifiers",
                "Antenna arrays",
                "Receiving antennas",
                "Antenna measurements",
                "Radio frequency",
                "Loaded antennas",
                "Rectennas"
            ],
            "Author Keywords": [
                "Circular polarization",
                "antenna array",
                "wireless power transfer (WPT)",
                "three-dimensional spherical"
            ]
        },
        "title": "A Compact Spherical Rectifier Antenna Array for Directionally Insensitive Wireless Energy Harvesting Systems"
    },
    {
        "authors": [
            "Tomasz Szydlo",
            "Marcin Nagy"
        ],
        "published_in": "Published in: IEEE Micro ( Early Access )",
        "date_of_publication": "29 March 2024",
        "doi": "10.1109/MM.2024.3382488",
        "publisher": "IEEE",
        "abstract": "Internet of Things systems are used in many aspects of our lives. Thanks to TinyML algorithms, they provide several new and smart functionalities that were impossible before. However, implementing such a solution on a large scale and maintaining it over time is a big challenge. This is due to the need not only to update the device firmware to remove errors and extend the functionality of the devices but also to update the ML models. This imposes several requirements for device monitoring and management mechanisms. In this work, we discuss not only the required aspects that an IoT system should meet, but we also show how they fit into the ML model management process.",
        "issn": {
            "Print ISSN": "0272-1732",
            "Electronic ISSN": "1937-4143"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Fraud",
                "Protocols",
                "Microprogramming",
                "Monitoring",
                "Data models",
                "Random access memory"
            ],
            "Author Keywords": []
        },
        "title": "Management of TinyML Enabled Internet of Things Devices"
    },
    {
        "authors": [
            "Suraj Kumar Gupta",
            "Ashwani Sharma"
        ],
        "published_in": "Published in: IEEE Microwave and Wireless Technology Letters ( Early Access )",
        "date_of_publication": "24 October 2024",
        "doi": "10.1109/LMWT.2024.3479326",
        "publisher": "IEEE",
        "abstract": "Wireless sensor networks face a challenge with the limited battery capacity in IoT sensor nodes and low communication quality, hindering network sustainability and increasing maintenance costs. To simultaneously address these issues, a novel octa-port antenna/rectenna with dual-polarization capabilities is proposed for simultaneous wireless information and power transfer (SWIPT) operation. The four ports of the design realize a rectenna for wireless power transfer (WPT), and the other four ports are dedicated to MIMO operation for wireless information transfer (WIT). The rectenna has a gain of $7.2$ dBi with a dc beamwidth of $72.2^\\circ$ . The rectenna achieves a good PCE of $50.92\\%$ at $1700~\\Omega$ , translating to a dc power harvesting capacity of $72.47~\\mu$ W at $-8.46$ -dBm RF input power. The MIMO antennas maintain a bandwidth of $200$ MHz, with $S_{11}$ less than $-10$ dB centered around $5.8$ GHz. The mutual coupling between co-polarized antenna/rectenna ports and among the MIMO antenna ports is less than $-12$ and $-25$ dB, respectively.",
        "issn": {
            "Electronic ISSN": "2771-9588",
            "Print ISSN": "2771-957X"
        },
        "keywords": {
            "IEEE Keywords": [
                "IP networks",
                "Antennas",
                "Impedance",
                "Signal to noise ratio",
                "Rectifiers",
                "Gain",
                "Wireless communication",
                "Radio frequency",
                "Simultaneous wireless information and power transfer",
                "Antenna measurements"
            ],
            "Author Keywords": [
                "Dual polarization",
                "MIMO",
                "rectenna",
                "simultaneous wireless information and power transfer (SWIPT)"
            ]
        },
        "title": "Octa-Port Dual-Polarized Antenna/Rectenna for MIMO Simultaneous Wireless Information and Power Transfer (SWIPT)"
    },
    {
        "authors": [
            "Quan Yuan",
            "Dezhi Li",
            "Zhenyong Wang",
            "Chang Liu",
            "Ci He"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "14 March 2019",
        "doi": "10.1109/ACCESS.2019.2904990",
        "publisher": "IEEE",
        "abstract": "Channel estimation is one of the most important aspects of wireless communication. Especially in Sparse Code Multiple Access (SCMA) system, the accuracy of channel estimation has a significant impact on decoding performance. Various methods, so far, have been developed for channel estimation. Most of these methods regard channel estimation as a parameter estimation problem of linear models. However, these methods require lots of time-frequency resources to ensure high estimation accuracy. In massive connection scenarios, high pilot overhead makes the spectrum resource more scarce. Therefore, the drawback of conventional channel estimation methods limits the further improvement of system capacity in the Internet of Things(IoT) when time-frequency resources is restricted. To address this problem, in this paper, we propose an efficient channel estimation scheme and sparse pilot structure design method in SCMA system based on complex-valued sparse autoencoder which is effective to learn features of wireless channel. Complex-valued sparse autoencoder is a kind of neural network with complex-valued weights. It contains two parts: encoder and decoder. In our work, the encoder part is used to realize pilot design. Channel estimation is implemented by the decoder. Complex-valued weights obtained from training are used as baseband pilots. Compared with maximum likelihood channel estimation (MLE) of linear model, the proposed method can achieve higher channel estimation accuracy with more sparse pilot structure. The bit-error rates performance of the SCMA receiver in our work is very close to that of the perfect channel state information (CSI).",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Channel estimation",
                "Biological neural networks",
                "Neurons",
                "Uplink",
                "Time-frequency analysis",
                "Baseband"
            ],
            "Author Keywords": [
                "Sparse Code Multiple Access",
                "channel estimation",
                "sparse pilot structure",
                "Internet of Things",
                "massive connection",
                "complex-valued sparse autoencoder",
                "maximum likelihood"
            ]
        },
        "title": "Channel Estimation and Pilot Design for Uplink Sparse Code Multiple Access System based on Complex-Valued Sparse Autoencoder"
    },
    {
        "authors": [
            "Prince Bahoumina",
            "Hamida Hallil-Abbas",
            "Jean-Luc Lachaud",
            "Dominique Rebiere",
            "Corinne Dejous",
            "Aymen Abdelghani",
            "Kamel Frigui",
            "Stephane Bila",
            "Dominique Baillargeat",
            "Qing Zhang",
            "Philippe Coquet",
            "Carlos Alberto Paragua",
            "Emmanuelle Pichonat",
            "Henri Happy"
        ],
        "published_in": "Published in: IEEE Transactions on Nanotechnology ( Early Access )",
        "date_of_publication": "18 April 2018",
        "doi": "10.1109/TNANO.2018.2828302",
        "publisher": "IEEE",
        "abstract": "This paper presents the feasibility of a low-cost fully inkjet printed capacitive microwave flexible gas sensor based on a resonant electromagnetic transducer in micro-strip technology with poly (3,4-ethylenedioxythiophene) polystyrene sulfonate and multi wall carbon nanotubes (PEDOT:PSS-MWCNTs) as sensitive material for Volatile Organic Compounds (VOCs) detection. The design and simulation results of the device on paper substrate for differential measurements are first described. This theoretical study based on both analytical and Finite Element Model approaches, validates the operating principle. The fabrication process and experimental devices are then presented, as well as electrical characterization results, both in air and under selected vapor analytes. These experimental results show the good sensor repeatability and sensitivity according to the transmission S parameter resonant frequency shift equal to -2.153 kHz/ppm and -1.855 kHz/ppm for ethanol and toluene vapor concentrations from 500 to 1300 ppm, respectively. This leads to conclude on promising future of such passive sensors and further integration into real-time multi-sensing platform adaptable for the Internet of Things (IoT).",
        "issn": {
            "Print ISSN": "1536-125X",
            "Electronic ISSN": "1941-0085"
        },
        "keywords": {
            "IEEE Keywords": [
                "Transducers",
                "Polymers",
                "Conductivity",
                "Gas detectors",
                "Microwave devices",
                "Internet of Things",
                "Temperature measurement"
            ],
            "Author Keywords": [
                "Ink-jet printed chemical gas sensor",
                "volatile organic compounds detection",
                "electromagnetic transduction",
                "microwave resonator",
                "carbon materials",
                "composite polymer"
            ]
        },
        "title": "VOCs monitoring using differential microwave capacitive resonant transducer and conductive PEDOT:PSS-MWCNTs nanocomposite film for environmental applications"
    },
    {
        "authors": [
            "Muhammed Golec",
            "Sukhpal Singh Gill",
            "Felix Cuadrado",
            "Ajith Kumar Parlikad",
            "Minxian Xu",
            "Huaming Wu",
            "Steve Uhlig"
        ],
        "published_in": "Published in: IEEE Transactions on Sustainable Computing ( Early Access )",
        "date_of_publication": "29 December 2023",
        "doi": "10.1109/TSUSC.2023.3348157",
        "publisher": "IEEE",
        "abstract": "Serverless edge computing decreases unnecessary resource usage on end devices with limited processing power and storage capacity. Despite its benefits, serverless edge computing's zero scalability is the major source of the cold start delay, which is yet unsolved. This latency is unacceptable for time-sensitive Internet of Things (IoT) applications like autonomous cars. Most existing approaches need containers to idle and use extra computing resources. Edge devices have fewer resources than cloud-based systems, requiring new sustainable solutions. Therefore, we propose an AI-powered, sustainable resource management framework called ATOM for serverless edge computing. ATOM utilizes a deep reinforcement learning model to predict exactly when cold start latency will happen. We create a cold start dataset using a heart disease risk scenario and deploy using Google Cloud Functions. To demonstrate the superiority of ATOM, its performance is compared with two different baselines, which use the warm-start containers and a two-layer adaptive approach. The experimental results showed that although the ATOM required more calculation time of 118.76 seconds, it performed better in predicting cold start than baseline models with an RMSE ratio of 148.76. Additionally, the energy consumption and $CO_{2}$ emission amount of these models are evaluated and compared for the training and prediction phases.",
        "issn": {
            "Electronic ISSN": "2377-3782"
        },
        "keywords": {
            "IEEE Keywords": [
                "Containers",
                "Edge computing",
                "Computational modeling",
                "Internet of Things",
                "Green computing",
                "Scalability",
                "Predictive models"
            ],
            "Author Keywords": [
                "Cold start",
                "deep reinforcement learning",
                "Internet of Things",
                "serverless edge computing",
                "sustainable resource management"
            ]
        },
        "title": "ATOM: AI-Powered Sustainable Resource Management for Serverless Edge Computing Environments"
    },
    {
        "authors": [
            "Long Cheng",
            "Yan Gu",
            "Qingzhi Liu",
            "Lei Yang",
            "Cheng Liu",
            "Ying Wang"
        ],
        "published_in": "Published in: IEEE Transactions on Sustainable Computing ( Early Access )",
        "date_of_publication": "12 January 2024",
        "doi": "10.1109/TSUSC.2024.3353176",
        "publisher": "IEEE",
        "abstract": "The amalgamation of artificial intelligence with Internet of Things (AIoT) devices have seen a rapid surge in growth, largely due to the effective implementation of deep neural network (DNN) models across various domains. However, the deployment of DNNs on such devices comes with its own set of challenges, primarily related to computational capacity, storage, and energy efficiency. This survey offers an exhaustive review of techniques designed to accelerate DNN inference on AIoT devices, addressing these challenges head-on. We delve into critical model compression techniques designed to adapt to the limitations of devices and hardware optimization strategies that aim to boost efficiency. Furthermore, we examine parallelization methods that leverage parallel computing for swift inference, as well as novel optimization strategies that fine-tune the execution process. This survey also casts a future-forward glance at emerging trends, including advancements in mobile hardware, the co-design of software and hardware, privacy and security considerations, and DNN inference on AIoT devices with constrained resources. All in all, this survey aspires to serve as a holistic guide to advancements in the acceleration of DNN inference on AIoT devices, aiming to provide sustainable computing for upcoming IoT applications driven by artificial intelligence.",
        "issn": {
            "Electronic ISSN": "2377-3782"
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling",
                "Hardware",
                "Artificial neural networks",
                "Optimization",
                "Internet of Things",
                "Adaptation models",
                "Data models"
            ],
            "Author Keywords": [
                "AIoT devices",
                "DNN inference",
                "model compression",
                "parallel computing",
                "performance optimization",
                "survey"
            ]
        },
        "title": "Advancements in Accelerating Deep Neural Network Inference on AIoT Devices: A Survey"
    },
    {
        "authors": [
            "Wajahat Ali",
            "Ikram Ud Din",
            "Ahmad Almogren",
            "Joel J. P. C. Rodrigues"
        ],
        "published_in": "Published in: IEEE Transactions on Vehicular Technology ( Early Access )",
        "date_of_publication": "07 March 2024",
        "doi": "10.1109/TVT.2024.3368439",
        "publisher": "IEEE",
        "abstract": "In the contemporary landscape of vehicular communications, the role of vehicular ad-hoc networks (VANETs) has become increasingly pivotal, transcending the capabilities of traditional mobile ad-hoc networks (MANETs). These advancements in VANETs play a critical role in enhancing traffic management systems, promoting collision prevention, bolstering road safety, and efficiently handling emergency scenarios. Modern vehicles, equipped with advanced data collection tools, accumulate extensive information encompassing vehicle health, fuel requirements, and comprehensive location histories. This rich data repository is instrumental in forecasting future destinations and facilitating timely arrangements, embodying the essence of ambient intelligence within the Internet of Things (IoT) framework. In emergency contexts, the rapid analysis of vehicle data is crucial for identifying the nearest emergency facilities. This paper proposes an innovative approach that leverages machine learning and edge computing techniques to predict vehicles' subsequent locations using large-scale data, concurrently prioritizing user privacy. We employ federated learning for processing at the network's edge and integrate a blockchain-based distributed database to ensure robust data privacy and security. The application of blockchain and federated learning in training models on expansive datasets is particularly effective in estimating the proximity to medical facilities and emergency services. Furthermore, this study introduces an optimization method to monitor vehicle speed and outlines a comprehensive attack model, along with effective protection measures.",
        "issn": {
            "Print ISSN": "0018-9545",
            "Electronic ISSN": "1939-9359"
        },
        "keywords": {
            "IEEE Keywords": [
                "Blockchains",
                "Vehicular ad hoc networks",
                "Privacy",
                "Data models",
                "Federated learning",
                "Predictive models",
                "Data privacy"
            ],
            "Author Keywords": [
                "Large-Scale Data Models",
                "Blockchain",
                "Federated Learning",
                "Location Prediction",
                "Internet of Vehicles"
            ]
        },
        "title": "Federated Learning-based Privacy-aware Location Prediction Model for Internet of Vehicular Things"
    },
    {
        "authors": [
            "Abdelfattah A. Eladl",
            "Magda I. El-Afifi",
            "Magdi M. El-Saadawi",
            "Bishoy E. Sedhom"
        ],
        "published_in": "Published in: CSEE Journal of Power and Energy Systems ( Early Access )",
        "date_of_publication": "08 September 2023",
        "doi": "10.17775/CSEEJPES.2023.00670",
        "publisher": "CSEE",
        "abstract": "This paper proposes an IoT-Fog-Cloud distributed consensus algorithm for solving the energy hub (EH) dispatch problem with packet-dropping communication links and some of the EH elements' uncertainties. Every generating and consumption unit in this algorithm is required to estimate the total power generated, the total load, and the power mismatches. Energy node coordination is accomplished using a distributed approach. Such a distributed approach wins in work sharing, enduring a single link failure, effective decision-making, the quickest convergence, and autonomy for the global power nodes. The method works with all grid types in connected and islanded modes. Minimizing total operation cost and emissions while meeting the total demand and system constraints are the most crucial contributions of this paper. Two case studies are applied to explain the performance and effectiveness of the proposed algorithm with different packet loss scenarios. Under uncertainty, the sensitivity of the system was evaluated. The results show that the mismatch between the generated and consumed power is improved by 100% in the electricity grid, 99.94% in the heating grid, and 99.91% in the gas grid. Also, total operating cost, total emissions, and emissions cost decreased by 8.6%, 13.48%, and 18.73%, respectively.",
        "issn": {},
        "keywords": {
            "IEEE Keywords": [
                "Resistance heating",
                "Heating systems",
                "Energy management",
                "Costs",
                "Consensus algorithm",
                "Mathematical models",
                "Cogeneration"
            ],
            "Author Keywords": [
                "Multi-energy systems",
                "energy hubs",
                "Packet drops",
                "consensus algorithm",
                "CO2 emissions"
            ]
        },
        "title": "Distributed optimal dispatch of smart multi-agent energy hubs based on consensus algorithm considering lossy communication network and uncertainty"
    },
    {
        "authors": [
            "Navin kumar Agrawal",
            "Rijwan Khan",
            "Preeti Rani",
            "Ajeet Kumar Srivastava",
            "Rohit Sharma",
            "Kusum Yadav",
            "Ahmed Alkhayyat",
            "Arwa N. Aledaily"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "20 December 2023",
        "doi": "10.1109/TCE.2023.3344129",
        "publisher": "IEEE",
        "abstract": "The Internet of Vehicles (IOV) allows vehicles to communicate with each other in the Internet of Things (IOT). As vehicle nodes are considered to be always in motion, their topology frequently changes. The dynamic topology changes that result in these changes have caused IOV to face major issues, including scalability, shortest-path routing, and dynamic topology changes. Clustering can be used to solve such problems. Clustering is based on an optimization approach based on transmission range, node density, speed, and direction. This paper presents a method for calculating and evaluating an optimal cluster head (CH) using ant colony and Firefly optimization algorithms. Massively interconnected networks with heterogeneous data generated at the edge of networks require distributed machine-learning techniques that can take advantage of this data. A three-layer federated learning model is proposed in this study to take advantage of the distributed end-edge-cloud architecture typical of a 5G/6G environment to increase learning efficiency and accuracy while protecting data privacy and reducing communications overhead. Our experimental and evaluation results demonstrate our proposed method’s outstanding performance in improving convergence speed and learning accuracy for 5G/6G-supported IoV applications. The proposed TFL-IHOA model enhanced the number of clusters in the grid by 3-5%, reduced the computation time by 1.5-2%, and had 12-20% less packet loss than the existing algorithms.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Vehicles",
                "Optimization",
                "Routing",
                "Vehicular ad hoc networks",
                "Vehicle dynamics",
                "Surveys",
                "Heuristic algorithms"
            ],
            "Author Keywords": []
        },
        "title": "TFL-IHOA:Three-Layer Federated Learning Based Intelligent Hybrid Optimization Algorithm for Internet of Vehicle"
    },
    {
        "authors": [
            "Bong-Hyun Kim",
            "Anandakumar Haldorai",
            "Suprakash S"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "07 May 2024",
        "doi": "10.1109/TCE.2024.3397714",
        "publisher": "IEEE",
        "abstract": "The extensive utilization of mobile consumer electronic technology in several areas highlights the importance of maximizing battery energy efficiency. Users may want to ensure a specific duration of battery life to allow continuous use of their smart mobile devices. This research introduces a novel method to tackle the issues associated with micro power batteries in consumer devices by utilising a sensor-based smart IoT system. The study also examines the use of Split Learning, an innovative algorithmic framework that improves the accuracy and effectiveness of state-of-charge monitoring in micro power batteries. This approach reduces power usage and enhances the precision of estimating battery life. The circuits that monitor battery temperature and current remain in a low-power mode, only activating when the controller needs to make measurements, leading to a substantial reduction in power consumption. Integrating Split Learning algorithms is a major advancement in battery monitoring technology, offering exceptional power efficiency and accuracy for upcoming micro power battery applications in consumer devices. The proposed approach accurately predicts the remaining battery charge under various discharge scenarios. The device also includes an automated learning system using Split Learning to safeguard the battery from excessive current consumption, ensuring the longevity and reliability of these crucial power sources. The results of the current research were 0.9340 for testing data and 0.9376 for training data.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Batteries",
                "Monitoring",
                "Temperature measurement",
                "State of charge",
                "Voltage measurement",
                "Consumer electronics",
                "Battery charge measurement"
            ],
            "Author Keywords": [
                "Battery Monitoring",
                "Split Learning",
                "Smart Phone",
                "Power Consumption"
            ]
        },
        "title": "A Battery Lifetime Monitoring and Estimation using Split Learning Algorithm in Smart Mobile Consumer Electronics"
    },
    {
        "authors": [
            "Arun Kumar Sangaiah",
            "Fan-Nong Yu",
            "Yi-Bing Lin",
            "Wan-Chi Shen",
            "Akashdeep Sharma"
        ],
        "published_in": "Published in: IEEE Transactions on Network Science and Engineering ( Early Access )",
        "date_of_publication": "10 January 2024",
        "doi": "10.1109/TNSE.2024.3350640",
        "publisher": "IEEE",
        "abstract": "The paddy agronomy in the Asia-pacific region has gained a prominent role in connection with the major rice production area in over the decades. The research aims to investigate the aerial computing techniques to improve the sky farming techniques. Recently, the enhancement of unmanned aerial vehicle (UAV) and Internet of Things (IoT) with Deep Learning (DL) in paddy agronomy research has ensured the impact on data availability and predictive analytics. In this research, we focus on Deep Learning (DL) for identifying weeds, regions of crop failure, and crop health in paddy crops. Therefore, a DL architecture suitable for application in aerial computing UAV onboard intelligence is necessary. Furthermore, the DL architecture should be stable and consume as few computational resources as possible, given that it is applied on the UAV's onboard system. This paper proposes to use Tiny YOLO (T-Yolo)V4 as the base detector via following modules: (a) YOLO detection layer is added to the T-YOLO v4 to make the network more capable of detecting small objects. (b) Spatial pyramid pooling (SPP), convolutional block attention module (CBAM), Sand Clock Feature Extraction Module (SCFEM), Ghost modules, and more convolutional layers are added to the network to increase the accuracy of the network. Subsequently, a rice leaf diseases data set which contains the labeled images of rice leaf diseases such as Bacterial leaf blight, Rice blast, and brown spot is obtained. In addition, the image augmentations is applied to produce more samples of the three classes to create our own rice leaf diseases data set. Finally, the enhanced UAV Tiny Yolo Rice (UAV T-yolo-Rice) network has obtained the testing mean average precision (mAP) as $86 \\%$ by training the proposed rice leaves' disease data set. More experimental results reveal that our proposed method outperforms the Rice Leaves' Diseases detection model by using the proposed UAV T-yolo-Rice network set can obtain the highest testing Mean ...",
        "issn": {
            "Electronic ISSN": "2327-4697"
        },
        "keywords": {
            "IEEE Keywords": [
                "Diseases",
                "Autonomous aerial vehicles",
                "Feature extraction",
                "Computer architecture",
                "Deep learning",
                "Crops",
                "Plant diseases"
            ],
            "Author Keywords": [
                "Aerial computing",
                "deep learning (DL)",
                "machine learning (ML)",
                "rice leaf diseases",
                "Tiny Yolo V4",
                "unmanned aerial vehicle (UAV)"
            ]
        },
        "title": "UAV T-YOLO-Rice: An Enhanced Tiny Yolo Networks for Rice Leaves Diseases Detection in Paddy Agronomy"
    },
    {
        "authors": [
            "Aisha Alabsi",
            "Ammar Hawbani",
            "Xingfu Wang",
            "Ahmed Al- Dubai",
            "Jiankun Hu",
            "Samah Abdel Aziz",
            "Santosh Kumar",
            "Liang Zhao",
            "Alexey V. Shvetsov",
            "Saeed Hamood Alsamhi"
        ],
        "published_in": "Published in: IEEE Transactions on Sustainable Computing ( Early Access )",
        "date_of_publication": "22 March 2024",
        "doi": "10.1109/TSUSC.2024.3380607",
        "publisher": "IEEE",
        "abstract": "Wireless Power Transfer (WPT) is a disruptive technology that allows wireless energy provisioning for energy-limited IoT devices, thus decreasing the over-reliance on batteries and wires. WPT could replace conventional energy provisioning (e.g., energy harvesting) and expand to be deployed in many of our daily-life applications, including but not limited to healthcare, transportation, automation, and smart cities. As a new rising technology, WPT has attracted many researchers from academia and industry about WPT technologies and wireless charging scheduling algorithms. Therefore, in this paper, we review the most recent studies related to WPT, including classifications, advantages, disadvantages, and main domains of application. Furthermore, we review the recently designed wireless charging scheduling algorithms (schemes) for wireless sensor networks. Our study provides a detailed survey of wireless charging scheduling schemes covering the main scheme classifications, evaluation metrics, application domains, advantages, and disadvantages of each charging scheme. We further summarize trends and opportunities for applying WPT at some intersections.",
        "issn": {
            "Electronic ISSN": "2377-3782"
        },
        "keywords": {
            "IEEE Keywords": [
                "Wireless sensor networks",
                "Wireless communication",
                "Reviews",
                "Market research",
                "Green computing",
                "Wireless power transfer",
                "Surveys"
            ],
            "Author Keywords": [
                "wireless power transfer",
                "wireless sensor networks",
                "smart homes",
                "healthcare",
                "industrial",
                "charging schemes"
            ]
        },
        "title": "Wireless Power Transfer Technologies, Applications, and Future Trends: A Review"
    },
    {
        "authors": [
            "Sunil Prajapat",
            "Pankaj Kumar",
            "Dheeraj Kumar",
            "Ashok Kumar Das",
            "M. Shamim Hossain",
            "Joel J. P. C. Rodrigues"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "22 August 2024",
        "doi": "10.1109/JIOT.2024.3448212",
        "publisher": "IEEE",
        "abstract": "The Internet of Medical Things (IoMT) is a compelling networking paradigm integrating wireless communications sensors, connected devices, and embedded computing technologies. The IoMT involves the collection of real-time health data using sophisticated medical sensors. In recent years, the IoMT has become increasingly significant within the broader context of the Internet of Things (IoT). It provides accessibility for health monitoring and poses security obstacles to safeguarding the confidentiality and privacy of patient data. Therefore, this article presents a blockchain-integrated quantum authentication scheme in sensor-assisted IoMT networks. The proposed concept utilizes blockchain technology to achieve efficient patient authentication without the need for third-party entities. In addition, a secure quantum authentication scheme is designed not to require patients to authenticate themselves when communicating with multiple doctors simultaneously. This protocol explicitly addresses how clinicians can misuse their professional roles toward patients in IoMT networks. An evaluation analysis assesses the proposed technique’s efficacy compared to existing authentication schemes. The performance analyses demonstrate that the proposed protocol is resilient against various security attacks. Also, the practical usability of the quantum authentication scheme proved its importance as a significant improvement in communication security for IoMT networks.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Authentication",
                "Security",
                "Medical services",
                "Protocols",
                "Blockchains",
                "Wireless sensor networks",
                "Sensors"
            ],
            "Author Keywords": [
                "Quantum Computing",
                "Authentication",
                "Blockchain",
                "Internet of Medical Things (IoMT)",
                "Scyther tool",
                "Quantum-Sim"
            ]
        },
        "title": "Quantum Secure Authentication Scheme for Internet of Medical Things Using Blockchain"
    },
    {
        "authors": [
            "Ali Ranjha",
            "Diala Naboulsi",
            "Mohamed El Emary",
            "Francois Gagnon"
        ],
        "published_in": "Published in: IEEE Transactions on Reliability ( Early Access )",
        "date_of_publication": "09 February 2024",
        "doi": "10.1109/TR.2024.3357356",
        "publisher": "IEEE",
        "abstract": "The futuristic sixth-generation (6-G) networks will empower ultrareliable and low latency communications (URLLC), enabling a wide array of mission-critical applications such as mobile edge computing (MEC) systems, which are largely unsupported by fixed communication infrastructure. To remedy this issue, unmanned aerial vehicle (UAV) has recently come to the limelight to facilitate MEC for internet of things (IoT) devices as they provide desirable line-of-sight (LoS) communications compared to fixed terrestrial networks, thanks to their added flexibility and 3-D positioning. In this article, we consider UAV-enabled relaying for MEC systems for uplink transmissions in 6-G networks, and we aim to optimize mission completion time subject to the constraints of resource allocation, including UAV transmit power, UAV CPU frequency, decoding error rate, blocklength, communication bandwidth, and task partitioning as well as 3-D UAV positioning. Moreover, to solve the nonconvex optimization problem, we propose three different algorithms, including successive convex approximations, altered genetic algorithm (AGA), and smart exhaustive search. Thereafter, based on time-complexity, execution time, and convergence analysis, we select AGA to solve the given optimization problem. Simulation results demonstrate that the proposed algorithm can successfully minimize the mission completion time, perform power allocation at the UAV side to mitigate information leakage and eavesdropping as well as map a 3-D UAV positioning, yielding better results compared to the fixed benchmark submethods. Lastly, subject to 3-D UAV positioning, AGA can also effectively reduce the decoding error rate for supporting URLLC services.",
        "issn": {
            "Print ISSN": "0018-9529",
            "Electronic ISSN": "1558-1721"
        },
        "keywords": {
            "IEEE Keywords": [
                "Autonomous aerial vehicles",
                "Task analysis",
                "Internet of Things",
                "Ultra reliable low latency communication",
                "Resource management",
                "Optimization",
                "Bandwidth"
            ],
            "Author Keywords": [
                "Mobile edge computing (MEC) systems",
                "sixth-generation (6-G) networks",
                "unmanned aerial vehicle (UAV)-enabled relaying",
                "ultrareliable and low latency communications (URLLC)"
            ]
        },
        "title": "Facilitating URLLC vis-á-vis UAV-Enabled Relaying for MEC Systems in 6-G Networks"
    },
    {
        "authors": [
            "Yueying Zou",
            "Enwen Hu",
            "Zhongliang Deng",
            "Cong Jin"
        ],
        "published_in": "Published in: IEEE Transactions on Intelligent Vehicles ( Early Access )",
        "date_of_publication": "05 December 2023",
        "doi": "10.1109/TIV.2023.3325806",
        "publisher": "IEEE",
        "abstract": "In recent years, unmanned aerial vehicles (UAVs), especially swarming UAVs, have been widely used in various Internet of Things (IoT) scenarios. Multi-Dimensional Scaling (MDS) localization is sensitive to noise and requires communication between each UAV, which is not feasible in reality. Moreover, the MDS method cannot be applied to dynamic UAV clusters due to the high computational complexity, which introduces a large amount of communication overhead. To address the above problems, this paper proposes a fast-clustering localization method named Dynamic Cluster-MDS (DC-MDS), which is able to cluster UAV swarms according to the energy of UAVs in a short period of time, localize among clusters, and merge clusters and clusters upon completion of localization. This algorithm can greatly improve the accuracy of localization and the robustness of the algorithm. In addition, the DC-MDS method can reduce the computational complexity and prevent the optimization from falling into the local optimum to maintain the stability of the algorithm by introducing the learning rate. The scaling by majorizing a complicated function (SMACOF) algorithm based on MDS optimization is sensitive to initialization, and the initialization of DC-MDS uses the core map fusion algorithm, which can improve the accuracy of node initialization and reduce the number of subsequent calculations. Finally, simulation results show that the algorithm proposed in this paper has significantly improved the accuracy and algorithm robustness for UAV cluster localization and can be used for dynamic UAV swarm localization.",
        "issn": {
            "Electronic ISSN": "2379-8904",
            "Print ISSN": "2379-8858"
        },
        "keywords": {
            "IEEE Keywords": [
                "Autonomous aerial vehicles",
                "Location awareness",
                "Clustering algorithms",
                "Heuristic algorithms",
                "Approximation algorithms",
                "Robustness",
                "Computational complexity"
            ],
            "Author Keywords": [
                "MDS",
                "localization",
                "SMACOF",
                "UAV swarms"
            ]
        },
        "title": "Multidimensional Scaling Algorithm for Mobile Swarming UAVs Localization"
    },
    {
        "authors": [
            "Bing Shi",
            "Zhifeng Chen",
            "Zhuohan Xu"
        ],
        "published_in": "Published in: IEEE Transactions on Network and Service Management ( Early Access )",
        "date_of_publication": "06 February 2024",
        "doi": "10.1109/TNSM.2024.3362949",
        "publisher": "IEEE",
        "abstract": "Mobile Edge Computing (MEC) is a technology that shows great promise in enhancing the computational power of smart devices (SDs) in the Internet of Things (IoT). However, the fixed location and limited coverage of MEC servers constrain their performance. To overcome this issue, this paper explores a multiple unmanned aerial vehicle (UAV) assisted MEC system. The proposed system considers a scenario where multiple UAVs work together to provide computing services while dynamically adjusting their frequency based on the task size, under the constraint of limited energy. This paper aims to maximize computation bits, SDs’ fairness, and UAVs’ load balancing in multi-UAV assisted MEC system by jointly optimizing the trajectory and frequency. To address this challenge, we model it as a Partially Observable Markov Decision Process and propose a joint optimization strategy based on multi-agent deep reinforcement learning. The effectiveness of the proposed strategy is evaluated on both synthetic and realistic datasets. The results demonstrate that our strategy outperforms other benchmark strategies.",
        "issn": {
            "Electronic ISSN": "1932-4537"
        },
        "keywords": {
            "IEEE Keywords": [
                "Autonomous aerial vehicles",
                "Task analysis",
                "Trajectory",
                "Optimization",
                "Servers",
                "Computer architecture",
                "Computational modeling"
            ],
            "Author Keywords": [
                "Mobile Edge Computing",
                "Unmanned Aerial Vehicle",
                "Multi-Agent Deep Reinforcement Learning"
            ]
        },
        "title": "A Deep Reinforcement Learning Based Approach for Optimizing Trajectory and Frequency in Energy Constrained Multi-UAV Assisted MEC System"
    },
    {
        "authors": [
            "Karan Gupta",
            "Koyel Datta Gupta",
            "Devender Kumar",
            "Gautam Srivastava",
            "Deepak Kumar Sharma"
        ],
        "published_in": "Published in: IEEE Journal of Biomedical and Health Informatics ( Early Access )",
        "date_of_publication": "19 October 2023",
        "doi": "10.1109/JBHI.2023.3325964",
        "publisher": "IEEE",
        "abstract": "The benefits of the Internet of Medical Things (IoMT) in providing seamless healthcare to the world are at the forefront of technological advancement. However, security concerns of any IoMT systems are high since they threaten to compromise personal information of patients and can even cause health hazards. Researchers are exploring the use of various techniques to ensure a high level of security of IoMT systems. One key concern is that the computing power of any Internet of Things (IoT) device is relatively low, hence mechanisms that require low computational power are appropriate for designing Intrusion Detection Systems (IDS). In this research work, a blockchain IDS coalition is proposed for securing IoMT networks and devices. The blockchain ledger is compact and uses less processing resources. Additionally, the ledger requires less communication overhead. The cryptographic hashes in the suggested architecture ensure complete data secrecy and integrity between parties who are trusted and those who are untrustworthy. Peer-to-peer networks in both central and cluster networks are also included in this work for complete decentralization. The proposed model can counter various attacks, including Denial of Service (DoS), anonymity attacks, impersonation attacks, Man-In-The-Middle (MITM), and Cross-Site Scripting (XSS). The proposed method achieved an F1- score as high as 100% and reported an AUC value of over 99%.",
        "issn": {
            "Print ISSN": "2168-2194",
            "Electronic ISSN": "2168-2208"
        },
        "keywords": {
            "IEEE Keywords": [
                "Blockchains",
                "Medical services",
                "Security",
                "Computer architecture",
                "Bioinformatics",
                "Peer-to-peer computing",
                "Internet of Medical Things"
            ],
            "Author Keywords": [
                "Internet of Medical Things",
                "Intrusion Detection",
                "Tree Classifier",
                "Security",
                "Blockchain"
            ]
        },
        "title": "BIDS: Blockchain and Intrusion Detection System Coalition for Securing Internet of Medical Things Networks"
    },
    {
        "authors": [
            "Xiaojie Wang",
            "Jiameng Li",
            "Jun Wu",
            "Lei Guo",
            "Zhaolong Ning"
        ],
        "published_in": "Published in: IEEE Journal of Selected Topics in Signal Processing ( Early Access )",
        "date_of_publication": "30 August 2024",
        "doi": "10.1109/JSTSP.2024.3452501",
        "publisher": "IEEE",
        "abstract": "With the surge in the number of Internet of Things (IoT) devices and latency-sensitive services such as smart cities and smart factories, Next Generation Multiple Access (NGMA) technologies (e.g., Intelligent Reflecting Surface (IRS) and millimeter wave), which can efficiently process a large number of user accesses and low-latency services, have gained much attention. Among them, due to the ability to optimize wireless channels and improve data and energy transmission efficiency, IRS has been applied to Unmanned Aerial Vehicle (UAV)-assisted wireless powered edge networks. However, scheduling multi-dimensional resources in multi-UAVs, multi-IRSs and multi-devices coexistence scenarios always leads to a large number of highly coupled variables and complicated optimization problems. To address the above challenges, we propose a multi-agent Deep Reinforcement Learning (DRL)-based distributed scheduling algorithm for IRS and UAV-assisted wireless powered edge networks to jointly optimize charging time, phase shift matrices of IRSs, association scheduling of UAVs and UAV trajectories. First, to satisfy UAV time constraints and device energy consumption constraints, we formulate an energy efficiency maximization problem and represent it as a corresponding Markov Decision Process (MDP). Then, we propose a lightweight scheduling algorithm based on multi-agent DRL with value function decomposition. Finally, experiments show that the proposed algorithm has significant advantages in terms of algorithm convergence and system energy efficiency.",
        "issn": {
            "Print ISSN": "1932-4553",
            "Electronic ISSN": "1941-0484"
        },
        "keywords": {
            "IEEE Keywords": [
                "Wireless communication",
                "Autonomous aerial vehicles",
                "Internet of Things",
                "Communication system security",
                "Performance evaluation",
                "Resource management",
                "Data collection"
            ],
            "Author Keywords": [
                "Intelligent reflecting surface",
                "multi-agent deep reinforcement learning",
                "next generation multiple access",
                "unmanned aerial vehicle"
            ]
        },
        "title": "Energy Efficiency Optimization of IRS and UAV-Assisted Wireless Powered Edge Networks"
    },
    {
        "authors": [
            "Luca Davoli",
            "Laura Belli",
            "Luca Veltri",
            "Gianluigi Ferrari"
        ],
        "published_in": "Published in: IEEE Cloud Computing ( Early Access )",
        "date_of_publication": "22 December 2017",
        "doi": "10.1109/MCC.2017.455155318",
        "publisher": "IEEE",
        "abstract": "In order to make cloud services attractive for several IT organizations, it is necessary to provide access control and to implement safe and reliable mechanisms of Identity and Access Management (IAM). In this work, we focus on security issues and challenges in the design and implementation of cloud architectures and, in particular, for the management of Big Stream applications in Internet of Things (IoT) scenarios. The proposed work introduces a new set of modules allowing a federated access control policy for cloud users. An analysis of possible threats and attacks against the proposed Big Stream platform is presented, investigating the system performance in terms of detection and elimination of malicious nodes. In particular, we propose a new module, denoted as Traffic Handler Orchestrator & Rapid Intervention (THORIN), which is very efficient in counteracting botnet-based threats.",
        "issn": {
            "Electronic ISSN": "2325-6095"
        },
        "keywords": {
            "IEEE Keywords": [
                "Cloud computing",
                "Computer architecture",
                "Organizations",
                "Access control",
                "Authentication",
                "Servers"
            ],
            "Author Keywords": [
                "big stream",
                "Cloud Computing",
                "access control",
                "federated authentication",
                "threat mitigation",
                "security issues"
            ]
        },
        "title": "THORIN: an Efficient Module for Federated Access and Threat Mitigation in Big Stream Cloud Architectures"
    },
    {
        "authors": [
            "Xiaoqiang Zhu",
            "Jiqiang Liu",
            "Lingyun Lu",
            "Tao Zhang",
            "Tie Qiu",
            "Chunpeng Wang",
            "Yuan Liu"
        ],
        "published_in": "Published in: IEEE Communications Surveys & Tutorials ( Early Access )",
        "date_of_publication": "24 July 2024",
        "doi": "10.1109/COMST.2024.3432871",
        "publisher": "IEEE",
        "abstract": "The rapid growth of intelligent sensing capabilities and super computation power in 6G mobile communication systems has facilitated their application in diverse domains such as smart health, smart factories, and the industrial Internet of Things. Integrated Sensing and Communication (ISAC), as a core technology, has merged with artificial intelligence (AI) to enable intelligent connectivity, leading to a paradigm shift in traditional communication modes. This paper presents a visionary design for an ISAC-oriented unified IoT architecture that integrates software-defined communication and super-intelligent agents. By leveraging dynamic adaptability, self-learning, and optimization, the ISAC system can intelligently and flexibly respond to evolving requirements and environments. The architecture is redefined into three layers: the hardware layer, the omniscient layer, and the application layer. Furthermore, a retrospective survey of ISAC technology development over the past decade is conducted, highlighting new design principles for AI-empowered networks and multi-modals that support “intelligent connectivity\" across various application scenarios and reinforce the security of ISAC. This paper categorizes the related works according to the different layer structures of the proposed architecture, and some important physical and machine learning models are introduced. Additionally, we summarize the current technological bottlenecks associated with ISAC and propose future research directions and potential solutions that lay the foundation for the future development of secure and intelligent communication networks.",
        "issn": {
            "Electronic ISSN": "1553-877X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Integrated sensing and communication",
                "Computer architecture",
                "Artificial intelligence",
                "Surveys",
                "6G mobile communication",
                "Indexes",
                "Wireless communication"
            ],
            "Author Keywords": [
                "6G",
                "ISAC",
                "AI",
                "Software-defined communication",
                "Super intelligent agent"
            ]
        },
        "title": "Enabling Intelligent Connectivity: A Survey of Secure ISAC in 6G Networks"
    },
    {
        "authors": [
            "Baofeng Ji",
            "Jingjing Zhang",
            "Hui Zhang",
            "YangYu",
            "Gaoyuan Zhang",
            "Huitao Fan",
            "Shahid Mumtaz"
        ],
        "published_in": "Published in: IEEE Transactions on Cognitive Communications and Networking ( Early Access )",
        "date_of_publication": "12 August 2024",
        "doi": "10.1109/TCCN.2024.3431925",
        "publisher": "IEEE",
        "abstract": "With the continuous development of information and communication technology, it is critical to design novel spectrum, energy and cost-efficient wireless communication technologies to support massive Internet of Things (IoT) devices over a large area. Symbiotic communication that integrates active and passive communications has emerged as a promising technology to fulfill such demand. This paper focuses on using reconfigurable intelligent surfaces (RIS) to address the communication challenges faced by vehicles, such as data privacy concerns, network security threats, and interference issues. The key idea of symbiotic communication is to modify and reflect the incident signal from the active main transmitter. Therefore, the base model of this paper is a transmitter consisting of radio frequency (RF) signal and RIS that transmits a signal to one of the vehicles. In this process, inevitable threats of interference or eavesdropping arise. To ensure secure communication in Internet of Vehicles (IoV) systems, we establish a functional relationship between security performance of the system and variables such as the number of RIS elements and the aggregate interferences. We validate the accuracy of the analytical results through Monte Carlo simulations. Simulation outcomes demonstrate that increasing the number of RIS elements can mitigate adverse effects from aggregate interference or eavesdropping, thereby enhancing the security performance of IoV systems.",
        "issn": {
            "Electronic ISSN": "2332-7731"
        },
        "keywords": {
            "IEEE Keywords": [
                "Interference",
                "Security",
                "Eavesdropping",
                "Wireless communication",
                "Symbiosis",
                "Aggregates",
                "Research and development"
            ],
            "Author Keywords": [
                "Symbiotic communication",
                "reconfigurable intelligent surfaces",
                "aggregate interference",
                "Internet of Vehicles",
                "security performance",
                "Monte Carlo simulations"
            ]
        },
        "title": "Performance Analysis of RIS-Enhanced Secure Transmission for Symbiotic IoV Systems"
    },
    {
        "authors": [
            "Ilias Siniosoglou",
            "Vasileios Argyriou",
            "George Fragulis",
            "Panagiotis Fouliras",
            "Georgios Th. Papadopoulos",
            "Anastasios Lytos",
            "Panagiotis Sarigiannidis"
        ],
        "published_in": "Published in: IEEE Open Journal of the Communications Society ( Early Access )",
        "date_of_publication": "11 September 2024",
        "doi": "10.1109/OJCOMS.2024.3457803",
        "publisher": "IEEE",
        "abstract": "The time-consuming nature of training and deploying complicated Machine and Deep Learning (DL) models for a variety of applications continues to pose significant challenges in the field of Machine Learning (ML). These challenges are heightened in the federated domain, where optimizing models for individual nodes is particularly difficult. Many methods have been developed to tackle this problem, aiming to reduce training expenses and time while maintaining efficient optimisation. Three suggested strategies to tackle this challenge include Active Learning, Knowledge Distillation, and Local Memorization. These methods enable the adaptive finetuning of the leveraged AI models allowing for model personalization with local data, thereby improving the effectiveness of current models. The present study delves into the fundamental principles of these three approaches and proposes an advanced Federated Learning System that utilises different Personalization methods towards improving the accuracy of AI models and enhancing user experience in real-time NG-IoT applications, investigating the efficacy of these techniques in the local and federated domain. The results of the original and optimised models are then compared in both local and federated contexts using a comparison analysis. The analysis reveals promising results for optimizing and personalizing models using the proposed techniques.",
        "issn": {
            "Electronic ISSN": "2644-125X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Biological system modeling",
                "Data models",
                "Artificial intelligence",
                "Training",
                "Federated learning",
                "Adaptation models",
                "Deep learning"
            ],
            "Author Keywords": [
                "Deep Learning",
                "Model Optimisation",
                "Model Personalization",
                "Knowledge Distillation",
                "Forecasting",
                "Dataset",
                "Transformers",
                "LSTM"
            ]
        },
        "title": "Applied Federated Model Personalization in the Industrial Domain: A Comparative Study"
    },
    {
        "authors": [
            "Thulitha Senevirathna",
            "Vinh Hoa La",
            "Samuel Marchal",
            "Bartlomiej Siniarski",
            "Madhusanka Liyanage",
            "Shen Wang"
        ],
        "published_in": "Published in: IEEE Communications Surveys & Tutorials ( Early Access )",
        "date_of_publication": "02 August 2024",
        "doi": "10.1109/COMST.2024.3437248",
        "publisher": "IEEE",
        "abstract": "With the advent of 5G commercialization, the need for more reliable, faster, and intelligent telecommunication systems is envisaged for the next generation beyond 5G (B5G) radio access technologies. Artificial Intelligence (AI) and Machine Learning (ML) are immensely popular in service layer applications and have been proposed as essential enablers in many aspects of 5G and beyond networks, from IoT devices and edge computing to cloud-based infrastructures. However, existing 5G ML-based security surveys tend to emphasize AI/ML model performance and accuracy more than the models’ accountability and trustworthiness. In contrast, this paper explores the potential of Explainable AI (XAI) methods, which would allow stakeholders in 5G and beyond to inspect intelligent black-box systems used to secure next-generation networks. The goal of using XAI in the security domain of 5G and beyond is to allow the decision-making processes of ML-based security systems to be transparent and comprehensible to 5G and beyond stakeholders, making the systems accountable for automated actions. In every facet of the forthcoming B5G era, including B5G technologies such as ORAN, zero-touch network management, and end-to-end slicing, this survey emphasizes the role of XAI in them that the general users would ultimately enjoy. Furthermore, we presented the lessons from recent efforts and future research directions on top of the currently conducted projects involving XAI.",
        "issn": {
            "Electronic ISSN": "1553-877X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Artificial intelligence",
                "5G mobile communication",
                "Explainable AI",
                "Security",
                "Surveys",
                "6G mobile communication",
                "Wireless sensor networks"
            ],
            "Author Keywords": [
                "B5G",
                "5G",
                "XAI",
                "AI security",
                "cyber-security",
                "6G mobile communication",
                "Accountability",
                "Trustworthy AI",
                "Explainable security"
            ]
        },
        "title": "A Survey on XAI for 5G and Beyond Security: Technical Aspects, Challenges and Research Directions"
    },
    {
        "authors": [
            "Jinbo Wang",
            "Ruijin Wang",
            "Guangquan Xu",
            "Donglin He",
            "Xikai Pei",
            "Fengli Zhang",
            "Jie Gan"
        ],
        "published_in": "Published in: IEEE Transactions on Sustainable Computing ( Early Access )",
        "date_of_publication": "06 March 2024",
        "doi": "10.1109/TSUSC.2024.3374049",
        "publisher": "IEEE",
        "abstract": "Federated learning is a distributed learning paradigm, which is usually combined with edge computing to meet the joint training of IoT devices. A significant challenge in federated learning lies in the statistical heterogeneity, characterized by non-independent and identically distributed (non-IID) local data across diverse parties. This heterogeneity can result in inconsistent optimization within individual local models. Although previous research has endeavored to tackle issues stemming from heterogeneous data, our findings indicate that these attempts have not yielded high-performance neural network models. To overcome this fundamental challenge, we introduce the framework called FedPKR in this paper, which facilitates efficient federated learning through knowledge review. The core principle of FedPKR involves leveraging the knowledge representation generated by the global and local model layers to conduct periodic layer-by-layer comparative learning in a reciprocal manner. This strategy rectifies local model training, leading to enhanced outcomes. Our experimental results and subsequent analysis substantiate that FedPKR effectively augments model accuracy in image classification tasks, meanwhile demonstrating resilience to statistical heterogeneity across all participating entities. Code is available at https://github.com/jbwangnb/FedPKR .",
        "issn": {
            "Electronic ISSN": "2377-3782"
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling",
                "Internet of Things",
                "Data models",
                "Training",
                "Biological system modeling",
                "Servers",
                "Federated learning"
            ],
            "Author Keywords": [
                "Edge computing",
                "federated learning",
                "heterogeneity data",
                "image classification"
            ]
        },
        "title": "FedPKR: Federated Learning With Non-IID Data Via Periodic Knowledge Review in Edge Computing"
    },
    {
        "authors": [
            "Wang Zhengqiang",
            "Chang Ruifei",
            "Wan Xiaoyu",
            "Fan Zifu",
            "Duo Bin"
        ],
        "published_in": "Published in: China Communications ( Early Access )",
        "date_of_publication": "09 April 2024",
        "doi": "10.23919/JCC.ea.2021-0842.202401",
        "publisher": "IEEE",
        "abstract": "The massive connectivity and limited energy pose significant challenges to deploy the enormous devices in energy-efficient and environmentally friendly in the Internet of Things (IoT). Motivated by these challenges, this paper investigates the energy efficiency (EE) maximization problem for downlink cooperative non-orthogonal multiple access (C-NOMA) systems with hardware impairments (HIs). The base station (BS) communicates with several users via a half-duplex (HD) amplified-and-forward (AF) relay. First, we formulate the EE maximization problem of the system under HIs by jointly optimizing transmit power and power allocated coefficient (PAC) at BS, and transmit power at the relay. The original EE maximization problem is a non-convex problem, which is challenging to give the optimal solution directly. First, we use fractional programming to convert the EE maximization problem as a series of subtraction form subproblems. Then, variable substitution and block coordinate descent (BCD) method are used to handle the sub-problems. Next, a resource allocation algorithm is proposed to maximize the EE of the systems. Finally, simulation results show that the proposed algorithm outperforms the downlink cooperative orthogonal multiple access (C-OMA) scheme.",
        "issn": {},
        "keywords": {
            "IEEE Keywords": [
                "NOMA",
                "Relays",
                "Downlink",
                "Resource management",
                "Energy efficiency",
                "Noise",
                "Hardware"
            ],
            "Author Keywords": [
                "block coordinate descent",
                "cooperative non-orthogonal multiple access",
                "energy efficiency",
                "hardware impairments",
                "resource allocation"
            ]
        },
        "title": "Energy efficiency maximization for cooperative NOMA with hardware impairments"
    },
    {
        "authors": [
            "Aryan Nikul Patel",
            "Gautam Srivastava",
            "Praveen Kumar Reddy Maddikunta",
            "Ramalingam Murugan",
            "Gokul Yenduri",
            "Thippa Reddy Gadekallu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "06 August 2024",
        "doi": "10.1109/JIOT.2024.3439228",
        "publisher": "IEEE",
        "abstract": "With the rapid growth of the Internet of Things, sensors have become integral components of smart homes, enabling real-time monitoring and control of various aspects ranging from energy consumption to security. In this context, we cannot underestimate the importance of sensor-based data in ensuring the safety and well-being of occupants, particularly in scenarios involving early detection of fire outbreaks. We propose a novel Federated Learning (FL) Framework in this study to address the crucial issue of rapid fire smoke detection at the edge of smart home environments. The proposed framework employs three distinct FL algorithms, namely Federated Averaging, Federated Adaptive Moment Estimation, and Federated Proximal, for global aggregation of machine learning predictions based on data from various IoT sensors. This framework allows for early prediction by utilizing the computational capabilities at the edge, thereby improving the responsiveness and efficiency of fire safety systems. Furthermore, to improve trust and transparency in the FL framework, eXplainable Artificial Intelligence techniques such as Local Interpretable Model-agnostic Explanations (LIME) and Shapley Additive exPlanations (SHAP) are integrated. We unveil pivotal features driving predictive outcomes through LIME and SHAP analyses, offering users valuable insights into model decision-making processes.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Smart homes",
                "Accuracy",
                "Internet of Things",
                "Computational modeling",
                "Privacy",
                "Safety",
                "Image edge detection"
            ],
            "Author Keywords": [
                "Internet of Things",
                "Sensor-based data",
                "Federated Learning",
                "Edge computing",
                "eXplainable Artificial Intelligence"
            ]
        },
        "title": "A Trustable Federated Learning Framework for Rapid Fire Smoke Detection at the Edge in Smart Home Environments"
    },
    {
        "authors": [
            "Jianhui Wu",
            "Yuanfa Ji",
            "Xiyan Sun",
            "Wentao Fu",
            "Songke Zhao"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "23 September 2024",
        "doi": "10.1109/JIOT.2024.3465881",
        "publisher": "IEEE",
        "abstract": "The collaborative control algorithms of multi-agents system have been applied to many Internet of Things (IoT) devices. The anonymous flocking algorithm of multi-agents is a core technology in the collaborative control research of multi-agents system. It does not need agents to distinguish other agents and obstacles, but most existing researches have specific constraints on the obstacle shape, which limits its practical applications. The obstacle boundary points contain the shape characteristics of the obstacle. To relax the obstacle shape constraint, we assume that all agents can only perceive the position of obstacle boundary points within their sensing radius, and propose an anonymous flocking algorithm with obstacle avoidance via the position of obstacle boundary point. In this algorithm, the consensus term is divided into velocity consensus term and velocity unit direction consensus term. The velocity consensus term is designed to tow agents that perceive the obstacle boundary points preventing them from following to bypass obstacles, and the velocity unit direction consensus term is designed to achieve the matching of velocity unit direction. Additionally, the gradient-based term is designed to realize the separation and aggregation between agents and obstacle boundary points, and the navigational feedback term is designed to lead all agents to realize the group objective following. Furthermore, it is verified through simulations that the proposed algorithm can relax obstacle shape constraint and has better environmental adaptability.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Vectors",
                "Sensors",
                "Internet of Things",
                "Collision avoidance",
                "Collaboration",
                "Navigation"
            ],
            "Author Keywords": [
                "Anonymous flocking",
                "multi-agents",
                "obstacle avoidance",
                "obstacle boundary point",
                "expected velocity"
            ]
        },
        "title": "Anonymous Flocking With Obstacle Avoidance via the Position of Obstacle Boundary Point"
    },
    {
        "authors": [
            "Demin Gao",
            "Liyuan Ou",
            "Yongrui Chen",
            "Xiuzhen Guo",
            "Ruofeng Liu",
            "Yunhuai Liu",
            "Tian He"
        ],
        "published_in": "Published in: IEEE Transactions on Mobile Computing ( Early Access )",
        "date_of_publication": "17 September 2024",
        "doi": "10.1109/TMC.2024.3462941",
        "publisher": "IEEE",
        "abstract": "Wi-Fi is the de facto standard for providing wireless access to the Internet in the 2.4 GHz ISM band. Tens of billions of Wi-Fi devices (e.g., smartphones) have been shipped worldwide with limited types of wireless radios operating only when Wi-Fi connectivity is available, making it challenging to access data in heterogeneous IoT devices. However, the direct connection between Wireless Personal Area Network (WPAN) technologies, such as Bluetooth, and Wi-Fi presents challenges due to the inherent distinct physical layer. In our work, a novel communication method called BlueWi has been introduced, which serves as a cross technology communication method that enables BLE devices to establish connections and engage in communication with Wi-Fi based WPAN networks. We let BLE signals hitchhike on ongoing Wi-Fi signals, enabling Wi-Fi to recognize specific BLE signal waveforms in the frequency domain. By analyzing the decoded Wi-Fi payload, BlueWi can retrieve the BLE data, ensuring this method remains fully compatible with existing commodity Wi-Fi hardware. The direct sequence spread spectrum scheme is appended to handle general BLE frames and can be considered as “ COPY ” operation, which allows for better correlation and detection of the signal at the receiver. Evaluations conducted using both USRP and commodity devices have demonstrated that BlueWi can achieve concurrent wireless communication from BLE commercial chips to Wi-Fi networks with a frame reception rate exceeding 96%.",
        "issn": {
            "Print ISSN": "1536-1233",
            "Electronic ISSN": "1558-0660"
        },
        "keywords": {
            "IEEE Keywords": [
                "Wireless fidelity",
                "Internet of Things",
                "Wireless communication",
                "Smart phones",
                "IEEE 802.11ax Standard",
                "Payloads",
                "Hardware"
            ],
            "Author Keywords": [
                "Cross Technology Communication",
                "Wi-Fi",
                "BLE"
            ]
        },
        "title": "Physical-layer CTC from BLE to Wi-Fi with IEEE 802.11ax"
    },
    {
        "authors": [
            "Dandan Zhang",
            "Ziniu Wu",
            "Jin Zheng",
            "Yifan Li",
            "Zheng Dong",
            "Jialin Lin"
        ],
        "published_in": "Published in: IEEE Robotics & Automation Magazine ( Early Access )",
        "date_of_publication": "28 August 2024",
        "doi": "10.1109/MRA.2024.3417090",
        "publisher": "IEEE",
        "abstract": "Although the Internet of Robotic Things (IoRT) has enhanced the productivity of robotic systems in conjunction with the Internet of Things (IoT), it does not inherently support seamless human–robot collaboration. This article presents HuBotVerse, a unified framework designed to foster the evolution of the Internet of Human and Intelligent Robotic Things (IoHIRT). HuBotVerse is advantageous due to its unique features, including security, user-friendliness, manageability, and its open source nature. Moreover, this framework can seamlessly integrate various human–robot interaction (HRI) interfaces to facilitate collaborative control between humans and robots. Here, we emphasize a digital twin (DT)-based mixed reality (MR) interface, which enhances teleoperation efficiency by offering users an intuitive and immersive way to interact with. To evaluate the effectiveness of HuBotVerse, we conducted user studies based on a pick-and-place task. Feedback was gathered through questionnaires, complemented by a quantitative analysis of key performance metrics, user experience, and the National Aeronautics and Space Administration Task Load Index (NASA-TLX). Results indicate that the fusion of MR and HuBotVerse within a comprehensive framework significantly improves the efficiency and user experience of teleoperation. Moreover, the follow-up questionnaires reflect the advantages of the HuBotVerse framework in terms of evident user-friendliness, manageability, and usability in home-care or health-care applications. For codes, project videos, tutorials, technical details, case studies, and Q&A, please check our website ( https://sites.google.com/view/iohirtplusmr/home ).",
        "issn": {
            "Print ISSN": "1070-9932",
            "Electronic ISSN": "1558-223X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Robots",
                "Robot sensing systems",
                "Task analysis",
                "Human-robot interaction",
                "Sensors",
                "Physical layer",
                "Collaboration"
            ],
            "Author Keywords": []
        },
        "title": "HuBotVerse: Toward Internet of Human and Intelligent Robotic Things With a Digital Twin-Based Mixed Reality Framework"
    },
    {
        "authors": [
            "Chunwei Lou",
            "Mingsheng Cao",
            "Yaohua Luo",
            "Hua Xu",
            "Yuan Gao",
            "Weiwei Wang",
            "Dong Wang"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "16 March 2020",
        "doi": "10.1109/ACCESS.2020.2980886",
        "publisher": "IEEE",
        "abstract": "Key-aggregate keyword retrieval primitive enables a cloud server on behalf of delegated users to securely perform a keyword search over the data encrypted with various public keys. However, existing key-aggregate searchable encryption schemes are insecure against keyword guessing attacks, which result in the privacy leakage of keyword ciphertext and trapdoor. In this paper, we for the first time formulate a secure and efficient key-aggregate searchable encryption for cloud-assisted IoT applications, which not only enables a data owner to securely share the selected documents to multiple users, but also supports data users in delegating the capability of keyword search to a cloud server for searching the desired documents without leaking any privacy of keyword ciphertext and trapdoor. Then, the security of the proposed scheme is formally defined and proven to be secure against the indistinguishable selective-file chosen keyword attacks. The flexibility and practicability of the formulated scheme is also demonstrated by theoretical evaluations and experimental simulations.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Encryption",
                "Keyword search",
                "Servers",
                "Public key",
                "Cloud computing"
            ],
            "Author Keywords": [
                "Key-aggregate",
                "keyword retrieval",
                "privacy",
                "practicability"
            ]
        },
        "title": "A Secure Key-Aggregate Keyword Retrieval Scheme Over Encrypted Data in Cloud Computing"
    },
    {
        "authors": [
            "Anik Islam",
            "Hadis Karimipour",
            "Thippa Reddy Gadekallu",
            "Yaodong Zhu"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "03 May 2024",
        "doi": "10.1109/TCE.2024.3396723",
        "publisher": "IEEE",
        "abstract": "In consumer electronics, integrating the Internet of Things (IoT) and Artificial Intelligence (AI) has transformed everyday devices into smart, interconnected systems. However, this progress brings significant challenges in resource management, privacy, and security, particularly with the increasing reliance on data-centric technologies like Deep Learning (DL). The introduction of the Right to Be Forgotten (RBF) policy further complicates data management in DL models. This paper presents a new method for automating consumer electronic devices using Federated Learning (FL). This approach involves training devices with the help of a Digital Twin (DT) and securely storing data on a redactable blockchain after each training cycle. An unlearning mechanism in FL is adapted to meet RBF policy requirements, with the redactable blockchain facilitating the necessary data adjustments. Dual authentication methods are used to prevent malicious attacks: a hampel filter and performance checks during training, and a two-phase system comprising an XoR filter and continuous counter checks for request validation. A proof of concept confirms the system’s effectiveness, demonstrating its superior performance compared to existing methods.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Blockchains",
                "Training",
                "Servers",
                "Data models",
                "Artificial intelligence",
                "Security",
                "Data privacy"
            ],
            "Author Keywords": [
                "Consumer electronics",
                "digital twin",
                "edge computing",
                "federated unlearning",
                "home edge computing",
                "redacted blockchain"
            ]
        },
        "title": "A Federated Unlearning-Based Secure Management Scheme to Enable Automation in Smart Consumer Electronics Facilitated by Digital Twin"
    },
    {
        "authors": [
            "Chen Xu",
            "Gang Wang",
            "Mingzhu Wei",
            "Ping Zhang",
            "Bei Peng"
        ],
        "published_in": "Published in: IEEE Transactions on Vehicular Technology ( Early Access )",
        "date_of_publication": "26 April 2024",
        "doi": "10.1109/TVT.2024.3393940",
        "publisher": "IEEE",
        "abstract": "With the continuous development of intelligent transportation systems, achieving efficient vehicle-road coordination and task scheduling has become increasingly important for enhancing safety and efficiency. In order to effectively solve the problem that it is difficult to adapt to the intelligent transportation system due to the heterogeneity of cloud and edge computing resources and the complexity of communication, this paper proposes an intelligent transportation vehicle road collaboration and task scheduling method based on deep learning in the enhanced Internet of Things (IoT) environment. The proposed method utilizes a Collaborative Task Placement Deep Reinforcement Learning strategy (CTPDRL) and builds a cloud-edge collaborative computing framework controlled by deep reinforcement learning. Then, from the perspective of the interests of users and service providers, the task model under cloud edge collaborative computing is analyzed, and a system service quality model is established. Through Deep Q Networks (DQN) and Q-tables, CTPDRL can optimize the allocation of computing and communication resources, achieving effective task placement. The experimental results show that our proposed method can effectively reduce the overall cost of cloud and edge cloud computing, improve the practicality of the system in multi edge cloud environments, and help solve the problem of image perception blind spots in intelligent transportation, thereby improving the safety and efficiency of intelligent driving.",
        "issn": {
            "Print ISSN": "0018-9545",
            "Electronic ISSN": "1939-9359"
        },
        "keywords": {
            "IEEE Keywords": [
                "Task analysis",
                "Cloud computing",
                "Transportation",
                "Reliability",
                "Collaboration",
                "Roads",
                "Real-time systems"
            ],
            "Author Keywords": [
                "Augmented Internet of Things",
                "Deep Reinforcement Learning",
                "Vehicle Road Collaboration",
                "Task Offloading",
                "Vehicle to Vehicle"
            ]
        },
        "title": "Intelligent Transportation Vehicle Road Collaboration and Task Scheduling Based on Deep Learning in Augmented Internet of Things"
    },
    {
        "authors": [
            "Xu Yu",
            "Yan Lu",
            "Feng Jiang",
            "Qiang Hu",
            "Junwei Du",
            "Dunwei Gong"
        ],
        "published_in": "Published in: IEEE Transactions on Network and Service Management ( Early Access )",
        "date_of_publication": "16 August 2024",
        "doi": "10.1109/TNSM.2024.3444909",
        "publisher": "IEEE",
        "abstract": "The purpose of Intrusion Detection Systems (IDS) is to identify security issues in data transmitted by various devices and communication protocols. For domains with sparse data, such as the Internet of Things (IoT), cross-domain models are applied to solve the sparse problem by transfer knowledge from the source domain with rich data to the target domain. However, most of the cross-domain intrusion detection methods map different explicit features in the source and target domains to implicit features in a common implicit space, which weakens the interpretability of these methods. To enhance the interpretability of cross-domain models, we propose a Cross-Domain Intrusion Detection Method Based on Nonlinear Augmented Explicit Features (NAEF). Specifically, we augment the feature space of the source and target domains as the combination of shared features, source domain specific features and target domain specific features. Moreover, we model the nonlinear mapping relationship from shared features to special features in the source and target domains separately. Then, the original features in the source and target domains are mapped to uniform explicit features in the augmented space by migration of the nonlinear mapping relationship. Additionally, a classifier based on ensemble learning and attention mechanism balances the data distribution and selects important features to enhance detection performance. Our experimental results demonstrate the effectiveness of the proposed NAEF method on four public datasets.",
        "issn": {
            "Electronic ISSN": "1932-4537"
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Intrusion detection",
                "Internet of Things",
                "Semantics",
                "Data models",
                "Electronic mail",
                "Training"
            ],
            "Author Keywords": [
                "Intrusion Detection",
                "Augmented Explicit Features",
                "Transfer Learning",
                "Attention Mechanism",
                "Ensemble Learning"
            ]
        },
        "title": "A Cross-Domain Intrusion Detection Method Based on Nonlinear Augmented Explicit Features"
    },
    {
        "authors": [
            "Mingfeng Huang",
            "Zhetao Li",
            "Anfeng Liu",
            "Xinglin Zhang",
            "Zhemin Yang",
            "Min Yang"
        ],
        "published_in": "Published in: IEEE Transactions on Dependable and Secure Computing ( Early Access )",
        "date_of_publication": "08 July 2024",
        "doi": "10.1109/TDSC.2024.3424448",
        "publisher": "IEEE",
        "abstract": "As a collaborative and open network, billions of devices can be free to join the IoT-based data collection network for data perception and transmission. Along with this trend, more and more malicious attackers enter the network, they steal or tamper with data, and hinder data exchange and communication. To address these issues, we propose a Proactive Trust Evaluation System (PTES) for secure data collection by evaluating the trust of mobile data collectors. Specifically, PTES guarantees evaluation accuracy from trust evidence acquisition, trust evidence storage, and trust value calculation. First, PTES obtains trust evidence based on active detection of drones, feedbacks from interacted objects, and recommendations from trusted third parties. Then, these trust evidences are stored according to interaction time by adopting a sliding window mechanism. After that, credible, untrustworthy, and uncertain evidence sequences are extracted from the storage space, and assigned with positive, negative, and tendentious trust values, respectively. Consequently, the final normalized trust is obtained by combining the three trust values. Finally, extensive experiments conducted on a real-world dataset demonstrate PTES is superior to benchmark methods in terms of detection accuracy and profit.",
        "issn": {
            "Print ISSN": "1545-5971",
            "Electronic ISSN": "1941-0018"
        },
        "keywords": {
            "IEEE Keywords": [
                "Data models",
                "Computational modeling",
                "Accuracy",
                "Predictive models",
                "Cloud computing",
                "Bayes methods",
                "Analytical models"
            ],
            "Author Keywords": [
                "Internet of Things",
                "Trust mechanism",
                "Data security",
                "Sequence extraction",
                "Evaluation accuracy"
            ]
        },
        "title": "A Proactive Trust Evaluation System for Secure Data Collection Based on Sequence Extraction"
    },
    {
        "authors": [
            "Yutao Liu",
            "Xiaoning Zhang",
            "Zhihao Zeng",
            "Shui Yu"
        ],
        "published_in": "Published in: IEEE Transactions on Cognitive Communications and Networking ( Early Access )",
        "date_of_publication": "03 June 2024",
        "doi": "10.1109/TCCN.2024.3408416",
        "publisher": "IEEE",
        "abstract": "In Federated Learning (FL), each participating client needs to frequently compute the local gradient updates and communicate with the central parameter server, which causes high energy consumption on clients. Such energy consumption brings a significant challenge to battery-constrained clients (such as mobile devices, notebooks, IoT sensors, etc.) for edge networks. To tackle this challenge, in this study, we propose a method named FedEco to minimize energy consumption of all participating clients in FL. FedEco has the following features: i). FedEco achieves energy consumption minimization through hyperparameter1 adaptive tuning, which means allocating several kinds of optimized hyperparameters to the FL clients periodically. ii). The used hyperparameters include computing hyperparameters (i.e., training speed and training volume) and communication hyperparameters (i.e., transmission speed and transmission volume). iii). FedEco can theoretically guarantee FL convergence while satisfying the heterogeneous and dynamic edge environments. iv). FedEco finds the optimal computing and communication hyperparameters by decomposing the studied problem into several subproblems and solving them iteratively. Therefore, FedEco can achieve a good tradeoff between energy efficiency and model training convergence. We conduct extensive simulations to show that FedEco significantly saves energy consumption by up to 87.04% compared with the benchmark algorithms on average while guaranteeing the convergence of model training.",
        "issn": {
            "Electronic ISSN": "2332-7731"
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Convergence",
                "Energy consumption",
                "Energy efficiency",
                "Computational modeling",
                "Quantization (signal)",
                "Servers"
            ],
            "Author Keywords": [
                "Artificial intelligence",
                "Minimum energy control",
                "Data compression",
                "Neural networks",
                "Networks"
            ]
        },
        "title": "FedEco: Achieving Energy-Efficient Federated Learning by Hyperparameter Adaptive Tuning"
    },
    {
        "authors": [
            "Sicheng Zhang",
            "Longfei Li",
            "Zixin Li",
            "Haichao Zhang",
            "Guangzhen Si",
            "Yu Wang",
            "Guan Gui",
            "Yun Lin"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "05 September 2024",
        "doi": "10.1109/JIOT.2024.3454668",
        "publisher": "IEEE",
        "abstract": "Deep Learning (DL)-based Automatic Modulation Classification (AMC) is increasingly utilized in wireless applications, particularly within the Internet of Things (IoT) ecosystem. However, the open data collection for these systems can lead to vulnerabilities, as datasets are susceptible to malicious manipulations, potentially resulting in backdoor attacks. In this paper, We propose a novel Wavelet Domain Frequency Steganography (WDFS) backdoor attack method to demonstrate this security flaw, designed explicitly for misleading AMC. This method employs discrete wavelet transform and singular value decomposition to segment signals into distinct wavelet domain frequency components. We embed the backdoor trigger directly into these components, ensuring it is sample-specific and undetectable. Extensive testing shows that our WDFS method outperforms existing methods in terms of attack efficiency and stealth and successfully evades several advanced backdoor defense mechanisms, demonstrating its robustness. These findings highlight the urgent need for enhanced security measures in AMC systems within the AI domain.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Security",
                "Training",
                "Wavelet domain",
                "Modulation",
                "Discrete wavelet transforms",
                "Wireless communication"
            ],
            "Author Keywords": [
                "Automatic modulation classification",
                "deep learning",
                "backdoor attack",
                "wavelet domain",
                "frequency steganography"
            ]
        },
        "title": "Wavelet Domain Frequency Steganography Backdoor Attack for Misleading Automatic Modulation Classification"
    },
    {
        "authors": [
            "Tse-Chuan Hsu",
            "Yao-Hong Tsai",
            "William Cheng-Chung Chu"
        ],
        "published_in": "Published in: IEEE Transactions on Reliability ( Early Access )",
        "date_of_publication": "06 June 2024",
        "doi": "10.1109/TR.2024.3393424",
        "publisher": "IEEE",
        "abstract": "Deep learning is developing rapidly, and the emergence of many network architectures has brought significant breakthroughs to training recognition models. Due to the maturity of edge computing technology, we can perform regional image training through distributed nodes, which significantly improves the training model's accuracy while performing transfer learning to achieve better performance. In image processing technology, high-precision recognition of non-luminous images can currently be achieved by modeling, if we replace the visual recognition target with a glowing digital panel, the recognition rate cannot be the same as the static text recognition rate. This article uses Keras to build a convolutional neural networks deep learning model to identify glowing light-emitting diodes (LED) digits, incremental learning to complete transfer learning on edge computing nodes, and an integrated IoT architecture to achieve better recognition results. In the experiment, the verification results obtained from the distributed training nodes were successfully combined to model and retrain the nodes. The proposed distributed learning method can increase the accuracy from 70% to 89%. At the same time, the misclassified images can be retrained by integrating the transfer learning model with the distributed learning results, and the accuracy reaches more than 92%.",
        "issn": {
            "Print ISSN": "0018-9529",
            "Electronic ISSN": "1558-1721"
        },
        "keywords": {
            "IEEE Keywords": [
                "Image recognition",
                "Training",
                "Computational modeling",
                "Edge computing",
                "Data models",
                "Transfer learning",
                "Text recognition"
            ],
            "Author Keywords": [
                "Deep learning",
                "digit recognition",
                "incremental learning",
                "Keras",
                "transfer learning"
            ]
        },
        "title": "Enhancing Digit Recognition for Luminous Images in Edge Computing Through Transfer Learning With Robustness and Fault Tolerance"
    },
    {
        "authors": [
            "Anais Boumendil",
            "Walid Bechkit",
            "Karima Benatchba"
        ],
        "published_in": "Published in: IEEE Transactions on Neural Networks and Learning Systems ( Early Access )",
        "date_of_publication": "24 July 2024",
        "doi": "10.1109/TNNLS.2024.3430028",
        "publisher": "IEEE",
        "abstract": "Providing high-quality predictions is no longer the sole goal for neural networks. As we live in an increasingly interconnected world, these models need to match the constraints of resource-limited devices powering the Internet of Things (IoT) and embedded systems. Moreover, in the era of climate change, reducing the carbon footprint of neural networks is a critical step for green artificial intelligence, which is no longer an aspiration but a major need. Enhancing the energy efficiency of neural networks, in both training and inference phases, became a predominant research topic in the field. Training optimization has grown in interest recently but remains challenging, as it involves changes in the learning procedure that can impact the prediction quality significantly. This article presents a study on the most popular techniques aiming to reduce the energy consumption of neural networks’ training. We first propose a classification of the methods before discussing and comparing the different categories. In addition, we outline some energy measurement techniques. We discuss the limitations identified during our study as well as some interesting directions, such as neuromorphic and reservoir computing (RC).",
        "issn": {
            "Print ISSN": "2162-237X",
            "Electronic ISSN": "2162-2388"
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer architecture",
                "Optimization",
                "Neural networks",
                "Surveys",
                "Hardware",
                "Energy efficiency",
                "Climate change",
                "Deep learning",
                "Energy consumption",
                "Resource management",
                "Carbon footprint"
            ],
            "Author Keywords": [
                "Deep learning",
                "efficiency",
                "energy",
                "neural networks",
                "resource consumption"
            ]
        },
        "title": "On-Device Deep Learning: Survey on Techniques Improving Energy Efficiency of DNNs"
    },
    {
        "authors": [
            "Pan Zhang",
            "Lei Xu",
            "Lin Mei",
            "Chungen Xu"
        ],
        "published_in": "Published in: IEEE Transactions on Computers ( Early Access )",
        "date_of_publication": "08 October 2024",
        "doi": "10.1109/TC.2024.3475578",
        "publisher": "IEEE",
        "abstract": "In recent years, cross-device federated learning (FL), particularly in the context of Internet of Things (IoT) applications, has demonstrated its remarkable potential. Despite significant efforts, empirical evidence suggests that FL algorithms have yet to gain widespread practical adoption. The primary obstacle stems from the inherent bandwidth overhead associated with gradient exchanges between clients and the server, resulting in substantial delays, especially within communication networks. To deal with the problem, various solutions are proposed with the hope of finding a better balance between efficiency and accuracy. Following this goal, we focus on investigating how to design a lightweight FL algorithm that requires less communication cost while maintaining comparable accuracy. Specifically, we propose a Sketch-based FL algorithm that combines the incremental singular value decomposition (ISVD) method in a way that does not negatively affect accuracy much in the training process. Moreover, we also provide adaptive gradient error accumulation and error compensation mechanisms to mitigate accumulated gradient errors caused by sketch compression and improve the model accuracy. Our extensive experimentation with various datasets demonstrates the efficacy of our proposed approach. Specifically, our scheme achieves nearly a 93% reduction in communication cost during the training of multi-layer perceptron models (MLP) using the MNIST dataset.",
        "issn": {
            "Print ISSN": "0018-9340",
            "Electronic ISSN": "1557-9956"
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling",
                "Servers",
                "Matrix decomposition",
                "Adaptation models",
                "Accuracy",
                "Training",
                "Convergence",
                "Costs",
                "Performance evaluation",
                "Federated learning"
            ],
            "Author Keywords": [
                "Federated Learning",
                "Sketch",
                "SVD",
                "gradient error compensation",
                "gradient compression"
            ]
        },
        "title": "Sketch-based Adaptive Communication Optimization in Federated Learning"
    },
    {
        "authors": [
            "Yuan Gao",
            "Zhongya Zhang",
            "Zhiyong Zhang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "17 September 2024",
        "doi": "10.1109/JIOT.2024.3462412",
        "publisher": "IEEE",
        "abstract": "GIFT, as a lightweight block cipher algorithm, is mainly suitable for resource-constrained environments, such as the Internet of Things, to achieve efficient cryptographic communication. For the diffusion characteristics of round function of GIFT algorithm, this paper proposes two byte-based differential fault attack models. Theoretically, the first and second models require 53.44 and 12.42 byte faults to recover the master key. Experimental results show that the first and second models require about 79 and 16 byte faults, respectively, to recover the master key. The models proposed in this paper have made significant breakthroughs in terms of both the attack range and the number of required faults, which provide important guidance for algorithmic security research and the design of fault-tolerant mechanisms for IoT devices.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Cryptography",
                "Internet of Things",
                "Ciphers",
                "Computational modeling",
                "Clustering algorithms",
                "Technological innovation",
                "Manufacturing"
            ],
            "Author Keywords": [
                "Lightweight Cipher",
                "Block Cipher",
                "Differential Analysis",
                "Differential Fault Attack",
                "GIFT"
            ]
        },
        "title": "Differential Fault Attack of Lightweight Cipher GIFT Based on Byte Model"
    },
    {
        "authors": [
            "Kamran Ahmad Awan",
            "Ikram Ud Din",
            "Ahmad Almogren",
            "Joel J. P. C. Rodrigues"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "05 June 2024",
        "doi": "10.1109/TCE.2024.3409845",
        "publisher": "IEEE",
        "abstract": "Metaverse is an emerging research area where substantially virtualized realities and interactive digital spaces meet through augmented reality (AR), virtual reality (VR), and Internet of Things (IoT). This amalgamation confers a multifaceted platform for healthcare innovation, extending from remote surgeries to immersive patient education. Nevertheless, successfully deploying these technologies in healthcare necessitates overcoming demanding integration challenges. This paper aims to develop a strong system for integrating digital twins into the Metaverse while securing data management through leveraging blockchain technologies, web 3.0 features, and advanced consumer electronics, which allow for dynamic interactions. The main emphasis of this approach is to address the challenges related to data synchronization and security. It aims to protect healthcare data within the Metaverse and also navigate the aspects associated with using digital replicas. The proposed system uses blockchain and provides a procedure to secure data while ensuring cryptographic strength and quantum resilience. The simulations are performed to compare the proposed approach with the existing ones, where the obtained results show that our proposed MediTwin approach has significantly improved system efficiency by increasing data synchronization speed by 20% and resource utilization by 15%.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Medical services",
                "Metaverse",
                "Blockchains",
                "Digital twins",
                "Consumer electronics",
                "Semantic Web",
                "Security"
            ],
            "Author Keywords": [
                "Digital Twins",
                "Metaverse",
                "Web 3.0",
                "Patient-Centric Healthcare",
                "Blockchain",
                "Privacy",
                "Consumer Electronics"
            ]
        },
        "title": "MediTwin: A Web 3.0-Integrated Digital Twin for Secure Patient-Centric Healthcare in the Metaverse"
    },
    {
        "authors": [
            "Shradha Kishore",
            "Sonam Kumari Bharti",
            "Priyadarshi Anand",
            "Dishant Srivastav",
            "Shubham Sonali"
        ],
        "published_in": "Published in: IEEE Transactions on Industry Applications ( Early Access )",
        "date_of_publication": "10 September 2024",
        "doi": "10.1109/TIA.2024.3456753",
        "publisher": "IEEE",
        "abstract": "The development of precise, efficient, and cost-effective temperature prediction algorithms to maintain the ambient charging and operational environment of electric vehicles (EVs) has gained significance in recent times. Machine learning algorithms are widely used for prediction and decision-making processes due to the growing presence of optimized statistical models based on training datasets, that are far superior to conventional computational statistical methods. In this work, a temperature dataset has been generated in real-time from an IoT and temperature sensor-based hardware, clipped-on the casing of a vehicle, pre-possessed and fitted into optimized models so that future values of temperature can be accurately predicted using different supervised regression machine learning algorithms. The predicted temperatures have been compared with the actual recorded temperatures and a statistical error analysis has been done to compare the results based on the Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-Square (R2) and adjusted R2 Score. The advantage of this methodology is that it is independent of system parameters, and that any future values of critically high temperature may be predicted before they occur, so that drivers can be alerted and take corrective measures before actual damage takes place. This methodology may be applied to prevent incidents of fire in EV batteries, energy wastage due to high temperatures during charging or normal running, and to enhance passenger comfort.",
        "issn": {
            "Print ISSN": "0093-9994",
            "Electronic ISSN": "1939-9367"
        },
        "keywords": {
            "IEEE Keywords": [
                "Temperature sensors",
                "Temperature measurement",
                "Machine learning algorithms",
                "Predictive models",
                "Monitoring",
                "Batteries",
                "Prediction algorithms"
            ],
            "Author Keywords": [
                "Regression",
                "Machine Learning",
                "Temperature Prediction",
                "Random Forest",
                "R-Square"
            ]
        },
        "title": "Temperature Prediction for Electric Vehicles using Machine Learning Algorithms"
    },
    {
        "authors": [
            "Jie Hu",
            "Jingwen Cui",
            "Luping Xiang",
            "Kun Yang"
        ],
        "published_in": "Published in: IEEE Transactions on Communications ( Early Access )",
        "date_of_publication": "12 June 2024",
        "doi": "10.1109/TCOMM.2024.3413672",
        "publisher": "IEEE",
        "abstract": "In order to transmit data and transfer energy to the low-power Internet of Things (IoT) devices, integrated data and energy networking (IDEN) system may be harnessed. In this context, we propose a bitwise end-to-end design for polar coded IDEN systems, where the conventional encoding/decoding, modulation/demodulation, and energy harvesting (EH) modules are replaced by the neural networks (NNs). In this way, the entire system can be treated as an AutoEncoder (AE) and trained in an end-to-end manner. Hence achieving global optimization. Additionally, we improve the common NN-based belief propagation (BP) decoder by adding an extra hypernetwork, which generates the corresponding NN weights for the main network under different number of iterations, thus the adaptability of the receiver architecture can be further enhanced. Our numerical results demonstrate that our BP-based end-to-end design is superior to conventional BP-based counterparts in terms of both the BER and power transfer, but it is inferior to the successive cancellation list (SCL)-based conventional IDEN system, which may be due to the inherent performance gap between the BP and SCL decoders.",
        "issn": {
            "Print ISSN": "0090-6778",
            "Electronic ISSN": "1558-0857"
        },
        "keywords": {
            "IEEE Keywords": [
                "Polar codes",
                "Artificial neural networks",
                "Decoding",
                "Encoding",
                "Energy exchange",
                "Internet of Things",
                "Analytical models"
            ],
            "Author Keywords": [
                "Integrated data and energy networking (IDEN)",
                "wireless energy transfer (WET)",
                "polar code",
                "deep neural network (DNN)"
            ]
        },
        "title": "End-to-End Design of Polar Coded Integrated Data and Energy Networking"
    },
    {
        "authors": [
            "Chao Yang",
            "Hao Yu",
            "Qize Guo",
            "Tarik Taleb",
            "Jose Costa Requena",
            "Kari Tammi"
        ],
        "published_in": "Published in: IEEE Network ( Early Access )",
        "date_of_publication": "21 October 2024",
        "doi": "10.1109/MNET.2024.3483930",
        "publisher": "IEEE",
        "abstract": "Robotic teleoperation has seen widespread adoption across industries. Advances in technologies like the Internet of Things (IoT), Artificial Intelligence (AI), and Extended Reality (XR) are making teleoperation more flexible. The integration of visual, audio, tactile, and immersive interfaces enhances situational awareness, enabling teleoperators to interact effectively with complex remote environments and make informed decisions. However, challenges persist, particularly in environments characterized by constrained network resources. The limited bandwidth, delays, and intermittent communication can disrupt the teleoperator’s interaction with the robot. This study aims to comprehensively understand the scenarios of industrial ground robotic teleoperation and the intricacies of its network to effectively enhance teleoperation performance. Initially, we introduce the multimodal perception-enhanced robotic teleoperation, accompanied by an analysis of the Quality of Service (QoS) of all involved streams. Subsequently, we introduce a Deterministic Robotic Teleoperation (Det-RT) system, along with a deterministic traffic flow scheduling framework designed for real-time remote environment perception. Finally, we evaluate the proposed scheduling solution in a simulated environment to assess its performance within the Det-RT system. The results obtained demonstrate the capability of our solution to deliver high-quality teleoperation performance.",
        "issn": {
            "Print ISSN": "0890-8044",
            "Electronic ISSN": "1558-156X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Robots",
                "Service robots",
                "Streams",
                "Quality of service",
                "Teleoperators",
                "Robot kinematics",
                "Real-time systems",
                "Haptic interfaces",
                "Visualization",
                "Job shop scheduling"
            ],
            "Author Keywords": []
        },
        "title": "Deterministic Networking Empowered Robotic Teleoperation"
    },
    {
        "authors": [
            "Yufan Song",
            "Liquan Chen",
            "Jiaorui Shen",
            "Peng Zhang",
            "Qingyao Gu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "04 September 2024",
        "doi": "10.1109/JIOT.2024.3454291",
        "publisher": "IEEE",
        "abstract": "Physical Layer Key Generation (PLKG) is an information-theoretic security technique aimed at addressing key distribution challenges among resource-constrained legitimate users in the Internet of Things (IoT). However, in quasi-static or blocked environments, key generation faces significant limitations owing to its low entropy. To overcome this challenge, we propose leveraging a Simultaneously Transmitting and Reflecting (STAR) reconfigurable intelligent surface (RIS). In this study, we introduce a STAR-RIS-aided multiuser PLKG framework and formulate an optimization problem focused on minimizing the average bit disagreement ratio through optimal phase shift matrix adjustments. Initially, we derive closed-form solutions for optimal phase shifts, simplifying the problem to depend solely on amplitude coefficients. We then employ a Newton-Raphson algorithm under the energy-splitting (ES) protocol and a traversal method under the mode-switching (MS) and time-switching (TS) protocols to solve the optimization problem. Simulation results confirm the efficacy of our proposed schemes by comparison with existing benchmark schemes.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Protocols",
                "Encryption",
                "Wireless communication",
                "Reconfigurable intelligent surfaces",
                "Physical layer",
                "Internet of Things",
                "Relays"
            ],
            "Author Keywords": [
                "Physical layer key generation",
                "simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS)",
                "average bit disagreement ratio",
                "phase shift matrices"
            ]
        },
        "title": "Phase Shift Matrices Optimization in STAR-RIS-Aided Physical Layer Key Generation"
    },
    {
        "authors": [
            "Wen Deng",
            "Si Li",
            "Xiang Wang",
            "Zhitao Huang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "30 October 2024",
        "doi": "10.1109/JIOT.2024.3488118",
        "publisher": "IEEE",
        "abstract": "Automatic Modulation Classification (AMC) is crucial for ensuring secure and efficient operation of the Internet of Things (IoT). In this study, we focus on addressing the challenge of cross-domain modulation classification, which arises due to differences in data distribution between source and target domains. Current research focuses on leveraging multimodal information to improve performance, but struggles with mapping diverse domains into a common feature space for effective classification. Additionally, most existing domain-adversarial approaches measure the distribution differences between source and target domains using batch sample average differences, without considering the labels of the samples. To tackle these issues, we propose a progressive multisource partial domain adaptation modulation classification (PMSPDMC) method. PMSPDMC utilizes multiple sources comprising three sequence modalities and an auxiliary modality. It consists of two main components: feature space alignment and decision space alignment. In feature space alignment, we design a hybrid metric function to extract distinctive features that maintain tight intraclass distances and separable interclass distances, achieving fine-grained alignment between the source and target domains in the feature space. To achieve decision space alignment, a weighted fusion strategy and prediction consistency regularization are applied to the final classification prediction. This design of distribution difference measurement criteria effectively addresses the practical situation where the target domain has fewer modulation types than the source domain. The simulation results demonstrate that the proposed PMSPDMC exhibits performance gains in the cross-domain modulation classification tasks.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Modulation",
                "Feature extraction",
                "Internet of Things",
                "Training",
                "Adaptation models",
                "Phase modulation",
                "Accuracy",
                "Wireless communication",
                "Testing",
                "Probability distribution"
            ],
            "Author Keywords": [
                "Automatic modulation classification",
                "transfer learning",
                "domain adaptation",
                "multimodal information",
                "maximum mean discrepancy",
                "Internet of Things"
            ]
        },
        "title": "Cross-Domain Automatic Modulation Classification: A Multimodal Information-Based Progressive Unsupervised Domain Adaptation Network"
    },
    {
        "authors": [
            "Xiaohan Wang",
            "Yuanjun Laili",
            "Lin Zhang",
            "Yongkui Liu"
        ],
        "published_in": "Published in: IEEE Transactions on Automation Science and Engineering ( Early Access )",
        "date_of_publication": "04 March 2024",
        "doi": "10.1109/TASE.2024.3371250",
        "publisher": "IEEE",
        "abstract": "Cloud manufacturing (CMfg) converts the traditional manufacturing system into an Internet-of-things-enabled (IoT-enabled) manufacturing system, where both manufacturing and computational tasks must be scheduled among distributed and heterogeneous resources. Deep reinforcement learning (DRL) has recently become a promising idea for task scheduling in CMfg. However, existing DRL-based methods depend heavily on problem-specific reward engineering and struggle to represent hybrid decision variables. To this end, this paper proposed the sparse-reward deep reinforcement learning (SDRL) method to solve the hybrid task scheduling problem in CMfg. First, the hybrid task scheduling model in CMfg is constructed to minimize the makespan. We reformulate the studied problem as a partially observable Markov decision process (POMDP). Then, the objective hindsight experience replay (objective HER) mechanism is proposed to alleviate the sparse reward issue, through which the scheduling policy can be effectively trained without problem-specific reward engineering. The continuous action space is defined to represent hybrid decision variables, and the implicit action-selection mapping is utilized to alleviate the boundary effect. Numerical experiments validated the effectiveness and superiority of our method compared to eleven popular scheduling algorithms including evolutionary algorithms and DRL. Compared to mainstream DRL scheduling methods, the proposed SDRL outperforms the second-best one at most by $23.6\\%$ regarding generalization, and a scheduling solution can be generated in $0.5$ seconds. Note to Practitioners —With the intelligentization of the CMfg platform, hybrid tasks, including manufacturing and computational tasks, need to be scheduled simultaneously. However, this hybrid task scheduling problem is rarely considered by existing works. DRL exhibits many benefits in addressing scheduling problems, but the strong dependency on problem-specific reward engineering l...",
        "issn": {
            "Print ISSN": "1545-5955",
            "Electronic ISSN": "1558-3783"
        },
        "keywords": {
            "IEEE Keywords": [
                "Job shop scheduling",
                "Task analysis",
                "Processor scheduling",
                "Manufacturing",
                "Dynamic scheduling",
                "Computational modeling",
                "Collaboration"
            ],
            "Author Keywords": [
                "Scheduling",
                "cloud manufacturing",
                "deep reinforcement learning",
                "intelligent manufacturing",
                "machine learning"
            ]
        },
        "title": "Hybrid Task Scheduling in Cloud Manufacturing With Sparse-Reward Deep Reinforcement Learning"
    },
    {
        "authors": [
            "Geng Chen",
            "Lili Cheng",
            "Qingtian Zeng",
            "Fei Shen",
            "Yu-Dong Zhang"
        ],
        "published_in": "Published in: IEEE Transactions on Green Communications and Networking ( Early Access )",
        "date_of_publication": "01 August 2024",
        "doi": "10.1109/TGCN.2024.3436535",
        "publisher": "IEEE",
        "abstract": "With the rapid development of Internet of Things (IoT) technology, cooperative positioning is gradually becoming a key technology improving the localization performance without any infrastructure change. However, the game balance problem of positioning accuracy and resource consumption in resource-constrained scenarios remains challenging. To address these issues, we propose a cooperative positioning algorithm based on Competitive Game (CG) for optimization power management subject to the power budgets, and then adopt the Cooperative Willingness Allocation Rule (CWAR) to further improve positioning accuracy. Firstly, a clustering algorithm is applied to form a node cluster with the target node k as the cluster head for node selection and power allocation conveniently. Secondly, a new energy management strategy based on competitive game is proposed to minimize square position error bound of each agent individually with penalization by its power cost. The optimal response equilibrium point of Nash equilibrium is obtained by using the proposed CG algorithm to develop a solution for energy management games combining global information. Moreover, a fairness aware CWAR is proposed, which uses an improved Shapley value to proportionally distribute the cooperative willingness among the reference agents based on each node’s contribution for further expanding the location information of the agent nodes. The experimental results have shown that the proposed algorithm has an excellent performance in position accuracy and resource consumption. Compared with the Average, Random, Link Bargaining Equilibrium(LBE) and Price Allocation Rule (PAR) algorithms, the proposed algorithm improves the positioning accuracy by 38.50%, 49.00%,31.55% and 17.08%, respectively. Meanwhile, compared with the Exhaustive, Random, LBE and PAR algorithm, the proposed algorithm reduced resource consumption by 87.50%, 69.80%, 57.58% and 63.89% respectively.",
        "issn": {
            "Electronic ISSN": "2473-2400"
        },
        "keywords": {
            "IEEE Keywords": [
                "Location awareness",
                "Accuracy",
                "Games",
                "Resource management",
                "Clustering algorithms",
                "Optimization",
                "Green products"
            ],
            "Author Keywords": [
                "Cooperative Positioning",
                "Power Allocation",
                "Competitive Game",
                "CWAR",
                "Position Accuracy"
            ]
        },
        "title": "Willingness Allocation-Assisted Cooperative Localization Algorithm Based on Competitive Game for Resource-Constrained Environment"
    },
    {
        "authors": [
            "Chen Hou"
        ],
        "published_in": "Published in: IEEE Transactions on Automation Science and Engineering ( Early Access )",
        "date_of_publication": "02 August 2024",
        "doi": "10.1109/TASE.2024.3435880",
        "publisher": "IEEE",
        "abstract": "The paper considers the embedded system that can either compute tasks locally by itself or offload tasks to the edge server for remote computing during the running period (RP) and switch to the sleep mode to save energy once the RP ends, i.e., the idle period (IP) arrives. The tasks are stored in the cache, and the more tasks are computed during the RP, the less cache space will be occupied at the end of RP, while also leading to more energy consumption. Meanwhile, the sleep mode that the embedded system enters during the IP also influences the energy consumption. Therefore, how to make the optimal tradeoff between energy consumption and cache occupancy arises as an interesting issue. To address this issue, this paper first establishes an optimization-theoretical framework to formulate the energy consumption under the constraint of cache occupancy, then discovers the most energy-saving RP, computing mode (i.e., local or edge computing), and low-power mode. An algorithm based on our discovered theoretical results is proposed for the embedded system to minimize the energy consumption within the acceptable level of cache occupancy. Theoretical analysis and field experiments jointly verify its good performance. Note to Practitioners —This paper addresses the interesting tradeoff between energy consumption and cache occupancy in the embedded system that operates in the environments with limited available energy as well as cache space. It facilitates to improve the operation efficiency of the embedded systems in the area of Internet of Things (IoT) or Cyber-Physical Systems (CPS) that employs edge computing technology to empower embedded systems with more computing capability and sleep modes to guarantee embedded systems with more energy savings, in order to minimize the accumulative energy consumption, while maintaining the cache occupancy in terms of task data bits stored within an acceptable range. Experimental investigations show that the solution proposed here outpe...",
        "issn": {
            "Print ISSN": "1545-5955",
            "Electronic ISSN": "1558-3783"
        },
        "keywords": {
            "IEEE Keywords": [
                "Embedded systems",
                "Task analysis",
                "Energy consumption",
                "IP networks",
                "Edge computing",
                "Servers",
                "IEEE 802.16 Standard"
            ],
            "Author Keywords": [
                "Embedded system",
                "low-power mode",
                "computing mode",
                "energy consumption",
                "cache occupancy"
            ]
        },
        "title": "Optimization of Embedded System With Edge Computing and Sleep Modes for Balance Between Energy Consumption and Cache Occupancy"
    },
    {
        "authors": [
            "Shenhui Ma",
            "Jiahao Nie",
            "Siwei Guan",
            "Zhiwei He",
            "Mingyu Gao"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "14 August 2024",
        "doi": "10.1109/JIOT.2024.3443910",
        "publisher": "IEEE",
        "abstract": "Recent unsupervised framework-based anomaly detection methods in Internet of Things (IoT) have been proved successful by learning temporal pattern representations from normal time series data. However, such methods have long suffered from weakened normal-abnormal boundary incurred by discrimination-insufficient temporal pattern representations, due to: (1) normal time series often suffer from some unprocessed abnormal noise, interfering with model’s discriminative representations between normal and abnormal points. (2) Previous methods rarely focus on contextual semantic features of time series that are also crucial for constructing discriminative representations. In this paper, we propose MPFormer, a transformer-based multi-patch contrastive learning framework. This novel framework leverages contrastive learning to guide discriminative feature learning. It incorporates a data augmentation strategy to generate positive samples and maximizes the similarity between positive sample pairs, thereby capturing inherent representations of normal temporal patterns and enhancing robust discrimination ability to abnormal noise. To further facilitate discriminative representation modeling, we divide input time series into multiple patches, and design a transformer-based dual attention module to explore contextual semantic features in both inter-patch and intra-patch views. Benefiting from discriminative representations being modeled, MPFormer effectively strengthens the normal-abnormal boundary, thus improving detection accuracy. Experimental results on six widely adopted datasets demonstrate that our proposed MPFormer outperforms existing baseline methods.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Time series analysis",
                "Transformers",
                "Noise",
                "Anomaly detection",
                "Semantics",
                "Feature extraction",
                "Contrastive learning"
            ],
            "Author Keywords": [
                "Multivariate time series",
                "anomaly detection",
                "contrastive learing"
            ]
        },
        "title": "MPFormer: Multi-Patch Transformer for Multivariate Time Series Anomaly Detection With Contrastive Learning"
    },
    {
        "authors": [
            "Xue Fu",
            "Yu Wang",
            "Yun Lin",
            "Tomoaki Ohtsuki",
            "Guan Gui",
            "Hikmet Sari"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "11 September 2024",
        "doi": "10.1109/JIOT.2024.3457832",
        "publisher": "IEEE",
        "abstract": "Radiofrequency signal Identification (RSI) provides a critical security solution for device authentication in the Internet of Things (IoT), characterized by extensive interconnections and interactions among numerous entities. By analyzing received radiofrequency signals, device-specific features are extracted at the receiver and used for identification. In a dynamic and ever-changing communication environment, where some devices not visible during the training process may appear during testing, a robust RSI method must not only identify devices encountered during training but also reject those that were not. In this paper, we propose an open-set RSI method based on hypersphere manifold embedding. This approach leverages hypersphere projection for radiofrequency signal feature extraction on a hypersphere manifold, thereby avoiding the need to optimize intra-device variation in the radial direction. Additionally, we introduce an open-set identification approach based on generalized Pareto distribution, which does not rely on any radiofrequency signals from unknown devices. Extensive experimental results demonstrate that the proposed method achieves state-of-the-art identification performance.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Object recognition",
                "Internet of Things",
                "Vectors",
                "Training",
                "Radio frequency",
                "Manifolds"
            ],
            "Author Keywords": [
                "Radiofrequency Signal Identification (RSI)",
                "open-set",
                "hypersphere projection",
                "discriminative learning",
                "extreme value theory"
            ]
        },
        "title": "Toward Robust Open-Set Radiofrequency Signal Identification in Internet of Things Using Hypersphere Manifold Embedding"
    },
    {
        "authors": [
            "Rongqian Zhang",
            "Li Yin",
            "Yuyu Hao",
            "Hui Gao",
            "Mingxiong Zhao"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "16 October 2024",
        "doi": "10.1109/TCE.2024.3481635",
        "publisher": "IEEE",
        "abstract": "As the 6G network advances, the integration of sophisticated data mining techniques within the Consumer Internet of Things (CIoT) intensifies challenges in CPU temperature management and power efficiency. The growing use of consumer electronics and applications is also expected to drastically increase data traffic. Consequently, the substantial computational demands of these technologies risk causing CPU overheating, particularly given the limited capacities of Mobile Edge Computing (MEC) servers and IoT devices. Addressing these issues requires innovative resource allocation strategies that effectively manage CPU temperatures while maintaining robust system performance. This paper introduces the Thermal-aware Offloading and Resource Allocation Strategy (TORAS), specifically designed to minimize total execution latency for users across multi-server MEC networks by tackling a complex mixed-integer nonlinear programming problem. We streamline this problem into manageable subproblems: offloading ratio selection and resource allocation optimization, which are alternately optimized. Our approach includes deriving closed-form solutions for offloading ratio selection and transmission power allocation, and employing the primal-dual method to determine the optimal resource allocation strategies. This methodology not only addresses the thermal and operational challenges in MEC environments but also enhances latency performance, achieving improvements of 5.2% and 9.7% compared to other schemes, respectively.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Servers",
                "Resource management",
                "Internet of Things",
                "Delays",
                "Energy consumption",
                "Heating systems",
                "Consumer electronics",
                "6G mobile communication",
                "Central Processing Unit",
                "OFDM"
            ],
            "Author Keywords": [
                "Mobile Edge Computing (MEC)",
                "CPU temperature",
                "total execution latency",
                "resource allocation",
                "Consumer Internet of Things (CIoT)"
            ]
        },
        "title": "Multi-Server Assisted Task Offloading and Resource Allocation for Latency Minimization in Thermal-Aware MEC Networks"
    },
    {
        "authors": [
            "Lin Chen",
            "Yiwei Yang",
            "Li Yang",
            "Chao Xu",
            "Yinbin Miao",
            "Zhiquan Liu",
            "Ximeng Liu",
            "Kim-Kwang Raymond Choo"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/JIOT.2024.3489957",
        "publisher": "IEEE",
        "abstract": "With the rapid growth of encrypted image data outsourced to cloud servers, achieving data confidentiality and searchability in cloud-assisted Internet of Things (IoT) environments has become increasingly feasible. However, achieving high efficiency and strong security simultaneously over large-scale encrypted image datasets remains a challenge. To address this, we propose a novel efficient and secure content-based image retrieval scheme in Cloud-assisted Internet of Things. Specifically, our scheme leverages lattice-based fully homomorphic encryption and homomorphic comparison techniques, utilizing CKKS’s batch processing and Single Instruction Multiple Data capabilities. This approach significantly reduces the overhead of fully homomorphic computations, making the query process computational complexity independent of dataset size under certain conditions. Moreover, by integrating Private Information Retrieval technology, the scheme enhances privacy by hiding access patterns of image data. Formal security analysis demonstrates that our scheme achieves Indistinguishability against Chosen-Plaintext Attack (IND-CPA), and extensive experiments based on real datasets confirm that our scheme is both practical and efficient for real-world applications.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Image retrieval",
                "Nearest neighbor methods",
                "Indexes",
                "Homomorphic encryption",
                "Cloud computing",
                "Security",
                "Feature extraction",
                "Privacy",
                "Data privacy"
            ],
            "Author Keywords": [
                "Content-based image retrieval",
                "encrypted image data",
                "fully homomorphic encryption",
                "homomorphic comparison",
                "private information retrieval"
            ]
        },
        "title": "Efficient and Secure Content-Based Image Retrieval in Cloud-assisted Internet of Things"
    },
    {
        "authors": [
            "Nassim Sehad",
            "Lina Bariah",
            "Wassim Hamidouche",
            "Hamed Hellaoui",
            "Riku Jantti",
            "Merouane Debbah"
        ],
        "published_in": "Published in: IEEE Communications Magazine ( Early Access )",
        "date_of_publication": "23 October 2024",
        "doi": "10.1109/MCOM.001.2400199",
        "publisher": "IEEE",
        "abstract": "Over the past two decades, the Internet-of-things (IoT) has become a transformative concept, and as we approach 2030, a new paradigm known as the Internet of senses (IoS) is emerging. Unlike conventional virtual reality (VR), IoS seeks to provide multi-sensory experiences, acknowledging that in our physical reality, our perception extends far beyond just sight and sound; it encompasses a range of senses. This article explores the existing technologies driving immersive multi-sensory media, delving into their capabilities and potential applications. This exploration includes a comparative analysis between conventional immersive media streaming and a proposed use case that leverages semantic communication empowered by generative artificial intelligence (AI). The focal point of this analysis is the substantial reduction in bandwidth consumption by 99.93% in the proposed scheme. Through this comparison, we aim to underscore the practical applications of generative AI for immersive media. Concurrently addressing major challenges in this field, such as temporal synchronization of multiple media, ensuring high throughput, minimizing the end-to-end (E2E) latency, and robustness to low bandwidth while outlining future trajectories.",
        "issn": {
            "Print ISSN": "0163-6804",
            "Electronic ISSN": "1558-1896"
        },
        "keywords": {
            "IEEE Keywords": [
                "Streaming media",
                "Haptic interfaces",
                "Media",
                "Synchronization",
                "Visualization",
                "Internet",
                "Three-dimensional displays",
                "Servers",
                "Resists",
                "Generative AI"
            ],
            "Author Keywords": []
        },
        "title": "Generative AI for Immersive Communication: The Next Frontier in Internet-of-Senses Through 6G"
    },
    {
        "authors": [
            "Tingting Wang",
            "Jinyu Tian",
            "Kai Fang",
            "Thippa Reddy Gadekallu",
            "Wei Wang"
        ],
        "published_in": "Published in: IEEE Consumer Electronics Magazine ( Early Access )",
        "date_of_publication": "15 August 2024",
        "doi": "10.1109/MCE.2024.3444312",
        "publisher": "IEEE",
        "abstract": "The development of smart cities is being driven by consumer electronics-grade digital twin led by a robust combination of advanced Artificial Intelligence (AI) technologies. AI's capabilities for complex data analysis and pattern recognition provide a solid foundation for real-time data transfer and feedback in digital twin, enhancing simulation and analytics capabilities. The security and privacy of consumer electronics-grade digital twin are critical for smart cities, especially in the context of Internet of Things (IoT) security, as they simulate and control real-world critical infrastructure. The combination of sophisticated AI algorithms for network security and threat detection can effectively prevent potential network attacks and data breaches, ensuring the safety and reliability of digital twin systems. Smart transportation digital twin systems stand out as exemplary applications of AI technologies within the realm of digital twin. These systems simulate city traffic flows, autonomous vehicles, and urban planning, leading to improved traffic efficiency, reduced congestion and accidents, and promoting sustainable urban development. Considering the significance vehicle trajectory prediction holds in smart transportation digital twin systems, we propose a vehicle-to-vehicle interaction-based Dynamic Graph Neural Network (DGNN) model, which we experimentally analyzed for its performance in vehicle trajectory prediction.",
        "issn": {
            "Print ISSN": "2162-2248",
            "Electronic ISSN": "2162-2256"
        },
        "keywords": {
            "IEEE Keywords": [
                "Digital twins",
                "Artificial intelligence",
                "Security",
                "Medical services",
                "Real-time systems",
                "Smart cities",
                "Internet of Things"
            ],
            "Author Keywords": []
        },
        "title": "AI and Digital Twin for Consumer Electronics in Smart Cities"
    },
    {
        "authors": [
            "M. Javad Fakhimi",
            "Ozgur B. Akan"
        ],
        "published_in": "Published in: IEEE Transactions on Molecular, Biological, and Multi-Scale Communications ( Early Access )",
        "date_of_publication": "29 July 2024",
        "doi": "10.1109/TMBMC.2024.3434545",
        "publisher": "IEEE",
        "abstract": "Nanoantennas, operating at optical frequencies, are a transformative technology with broad applications in 6G wireless communication, IoT, smart cities, healthcare, and medical imaging. This paper explores their fundamental aspects, applications, and advancements, aiming for a comprehensive understanding of their potential in various applications. It begins by investigating macroscopic and microscopic Maxwell’s equations governing electromagnetic wave propagation at different scales. The study emphasizes the critical role of surface plasmon polariton wave propagation in enhancing light-matter interactions, contributing to high data rates, and enabling miniaturization. Additionally, it explores using two-dimensional materials like graphene for enhanced control in terahertz communication and sensing. The paper also introduces the employment of nanoantennas as the main building blocks of Nano-scale Radar (NR) systems for the first time in the literature. NRs, integrated with communication signals, promise accurate radar sensing for nanoparticles inside a nano-channel, making them a potential future application in integrated sensing and communication (ISAC) systems. These nano-scale radar systems detect and extract physical or electrical properties of nanoparticles through transmitting, receiving, and processing electromagnetic waves at ultra-high frequencies in the optical range. This task requires nanoantennas as transmitters/receivers/transceivers, sharing the same frequency band and hardware for high-performance sensing and resolution.",
        "issn": {
            "Electronic ISSN": "2332-7804"
        },
        "keywords": {
            "IEEE Keywords": [
                "Nanoantennas",
                "Microscopy",
                "Maxwell equations",
                "Antennas",
                "Nanobioscience",
                "Mathematical models",
                "Electromagnetics"
            ],
            "Author Keywords": [
                "Nanoantennas",
                "light-matter interaction",
                "Maxwell’s equations",
                "terahertz radiation",
                "ultrafast data transmission",
                "6G wireless communications",
                "biosensors",
                "photo-detection",
                "integrated sensing and communication (ISAC)"
            ]
        },
        "title": "Nanoantennas and Nanoradars: The Future of Integrated Sensing and Communication at the Nanoscale"
    },
    {
        "authors": [
            "Zijian Chen",
            "Hong Zhang",
            "Miao Wang",
            "Liqiang Wang",
            "Lei Zhang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "16 September 2024",
        "doi": "10.1109/JIOT.2024.3461709",
        "publisher": "IEEE",
        "abstract": "The rapid development of the Internet of Things (IoT) has propelled Mobile Edge Computing (MEC) into the forefront of both academia and industry. Nevertheless, the surge in urban activities driven by economic development is putting a strain on infrastructure like transportation and utilities. Increased demand for computing tasks and server failures from natural disasters can severely strain MEC in a specific region due to its reliance on static edge servers. To address these challenges, we introduce an MEC framework called Onboard Edge Computing (OBEC), which explores onboard servers to provide computational offloading services in mobile scenarios. To determine the end devices that each onboard server will serve, we propose the concept of “hunger value” to accurately measure the resource idleness of an onboard server. We also implement a Genetic Optimization-based Hunting-Predation Algorithm, an onboard server as a predator and a service end device as a prey, to minimize the overall “hunger value” of the whole system. Taking into account the power consumption of onboard servers and the satisfaction of end devices, we introduce a Stackelberg game to allow each onboard server to select the optimal serviced end devices and efficiently provide the required resources. Since this Stackelberg game lacks an analytical solution, we employ Gradient Descent to calculate the optimal offloading and resource allocation strategy. Finally, we conduct simulation experiments to demonstrate the superiority of the proposed OBEC framework over other state-of-art methods across various scenarios, underscoring its potential to foster synergistic interactions between servers and end devices.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Servers",
                "Resource management",
                "Games",
                "Edge computing",
                "Internet of Things",
                "Autonomous aerial vehicles",
                "Computational modeling"
            ],
            "Author Keywords": [
                "Mobile edge computing",
                "Stackelberg game",
                "Computation task offloading",
                "VEC",
                "Genetic algorithm"
            ]
        },
        "title": "Onboard Edge Computing: Optimizing Resource Allocation and Offloading in Mobile Scenarios"
    },
    {
        "authors": [
            "Zahra Pooranian",
            "Mohammad Shojafar",
            "Rahim Taheri",
            "Rahim Tafazolli"
        ],
        "published_in": "Published in: IEEE Consumer Electronics Magazine ( Early Access )",
        "date_of_publication": "16 November 2023",
        "doi": "10.1109/MCE.2023.3333559",
        "publisher": "IEEE",
        "abstract": "The forthcoming Fog storage system should provide end users with secured and faster access to cloud services and minimise storage capacity using data deduplication. This method stores a single copy of data and provides a link to the cloud/fog owners. In client-side data deduplication, the system can reduce network bandwidth levels by duplicate check. This solution fails to cover user privacy and optimise the latency of real-time communications. Motivated by this, this magazine paper develops PrivAcy-preServing data deduplication in Fog stOraGe system (PASFOG) as a data deduplication protocol implemented between cloud storage and users to mitigate brute-force and poison attacks. PASFOG is implemented in fog computing to reduce real-time delay and communication when performing duplicate checks. Also, we propose PrivAcypreServing data dedupliCatiOn in blockchaIN-based Fog stOraGe system (PASCOINFOG) utilised blockchain techniques to realise a reliable system. In PASCOINFOG, when users want to send chunks to the cloud/fog nodes, process the duplicate check and create a new block for the blockchain to reduce real-time latency/communication and protect the cloud from attackers. The proposed protocols can enhance user privacy and reduce real-time communication delay, crucial for consumer electronics applications such as cloud storage and IoT devices.",
        "issn": {
            "Print ISSN": "2162-2248",
            "Electronic ISSN": "2162-2256"
        },
        "keywords": {
            "IEEE Keywords": [
                "Cloud computing",
                "Blockchains",
                "Bandwidth",
                "Cryptography",
                "Protocols",
                "Servers",
                "Data privacy"
            ],
            "Author Keywords": []
        },
        "title": "PASCOINFOG/PASFOG: Privacy-preserving Data Deduplication Algorithms for Fog Storage Systems"
    },
    {
        "authors": [
            "Chen Hou"
        ],
        "published_in": "Published in: IEEE Transactions on Automation Science and Engineering ( Early Access )",
        "date_of_publication": "13 June 2024",
        "doi": "10.1109/TASE.2024.3412229",
        "publisher": "IEEE",
        "abstract": "For multiple smart sensors (MSSs) operating with limited energy, a smart sensor is said to be controllable if there exists proper external signal (ES) enabling it to report the sensing data, reflecting the physical world with tolerable sensing errors. In order to improve energy-efficiency while meeting the required sensing accuracy, the controllable smart sensors are expected to be activated while the uncontrollable ones are not. However, due to the exterior noises or interior interferences (ENII) of MSSs, whether a smart sensor is or not controllable is often uncertain, which may wrongly activate uncontrollable sensors or deactivate controllable ones, not only wasting energy but also deteriorating sensing errors. Therefore, addressing the problem of assuring MSSs with uncertain controllability to report tolerable sensing errors at the minimum cost of energy emerges as an interesting and challenging issue. To address this problem, this paper first formulates MSSs as the probabilistic multiple input multiple output (PMIMO) system, with all the ESs and sensing errors as the input and output, respectively. It then discloses the optimal activation probability for each smart sensor and reveals the optimal ES corresponding to the minimum energy for MSSs experiencing uncertain controllability to report the desired sensing errors. An algorithm is proposed based on our discovered results to implement such ES. The theoretical analysis and field experiments verify its performance. Note to Practitioners —This paper addresses the challenging issue of how to guarantee MSSs experiencing uncertain controllability to report tolerable sensing errors at the minimum cost of energy. Though the insightful discovery of the optimal probability that each smart sensor is activated, and further the optimal ES corresponding to the minimum energy for enabling MSSs experiencing uncertain controllability to report the desired sensing errors, this paper facilitates MSSs with uncertain controllabi...",
        "issn": {
            "Print ISSN": "1545-5955",
            "Electronic ISSN": "1558-3783"
        },
        "keywords": {
            "IEEE Keywords": [
                "Intelligent sensors",
                "Sensors",
                "Controllability",
                "Network topology",
                "Energy efficiency",
                "Accuracy",
                "Topology"
            ],
            "Author Keywords": [
                "Multiple smart sensors (MSSs)",
                "energy consumption",
                "set of sensing errors (SSE)",
                "uncertain controllability",
                "activation probability"
            ]
        },
        "title": "Energy-Efficient Control of Multiple Smart Sensors With Uncertain Controllability"
    },
    {
        "authors": [
            "Wentao Ai",
            "Hongji Xu",
            "Jianjun Li",
            "Xiaoman Li",
            "Xinya Li",
            "Yiran Li",
            "Shijie Li",
            "Zhikai Xu",
            "Yonghui Yu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "11 November 2024",
        "doi": "10.1109/JIOT.2024.3492721",
        "publisher": "IEEE",
        "abstract": "Due to the flourishing of Internet of Things (IoT) technology, radar-based human activity recognition (HAR) technology has made significant progress and has become an indispensable research area. Many radar systems use multiple feature maps and handle them directly in image format. However, the generation of multiple forms of feature maps requires heavy computational resources, which makes it impractical for real-world applications. Additionally, many networks fail to fully extract temporal information from the time-Doppler (TD) map during the feature extraction. Therefore, a time-weighted network based on strip pooling (TWN-SP) using the TD map is proposed in this paper. The TWN-SP consists of two time-weighted modules based on Fire (TWMs-F) and a feature fusion module based on temporal and channel attention (FFM-TCA). Due to the application of depthwise separable convolution (DSC) and placing the feature fusion step at the forefront, the proposed network has fewer parameters. Moreover, a radar dataset named RadSet is constructed, containing TD maps of six daily activities. To validate the performance of the TWN-SP, a ten-fold cross-validation (CV) on the public dataset named Radar848 and a leave-one-subject-out (LOSO) CV on the RadSet dataset are carried out, respectively. To further validate the generalization performance of TWN-SP, a comparative analysis was conducted on another public dataset, Ci4R. The TWN-SP achieves an accuracy of 99.28% on the RadSet dataset. Experimental results indicate that the TWN-SP surpasses the current leading networks in both performance and complexity.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Human activity recognition",
                "Radar",
                "Data mining",
                "Internet of Things",
                "Classification algorithms",
                "Radar antennas",
                "Doppler effect",
                "Convolutional neural networks",
                "Convolution"
            ],
            "Author Keywords": [
                "Deep learning (DL)",
                "frequency-modulated continuous wave (FMCW) radar",
                "human activity recognition (HAR)",
                "micro-Doppler (mD) signatures"
            ]
        },
        "title": "Radar-Based Human Activity Recognition Using Time-Weighted Network Based on Strip Pooling"
    },
    {
        "authors": [
            "Liqing Chen",
            "Shiguo Xu",
            "Hao Zhang",
            "Jian Weng"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "21 October 2024",
        "doi": "10.1109/JIOT.2024.3484227",
        "publisher": "IEEE",
        "abstract": "Users are inclined to outsource their data to the cloud due to the rapid growth of data in response to the growing use of cloud computing and the Internet of Things (IoT). One of the primary issues for users is the secure sharing of personal data. Access control and fine-grained sharing of encrypted data can be accomplished with attribute-based encryption (ABE). However, the computational cost required for ABE decryption hinders its widespread deployment on lightweight devices. For lightweight mobile devices, outsourcing computation is a highly attractive technical that outsources heavy decryption tasks to the cloud. The majority of current outsourced ABE schemes, however, only verify whether the cloud returns the correct decryption result, and lack a mechanism to prevent mobile devices from maliciously accusing the cloud of returning incorrect results, and overlook fairness between mobile devices and the cloud. Furthermore, the increasing number of encrypted files stored in the cloud will impede efficient query processing. Moreover, attribute revocation and privacy protection are also crucial. Therefore, this paper proposes Fair and Exculpable Attribute-Based Searchable Encryption with Revocation and Verifiable Outsourced Decryption (FE-ABSE-RV) Using Smart Contract, and constructs a concrete FE-ABSE-RV scheme. The scheme is capable of withstanding chosen plaintext attack and chosen keyword attack according to security analysis. Theoretical analysis and simulation experiments illustrate that the FE-ABSE-RV scheme is more expressive, efficient, and practical.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Cloud computing",
                "Encryption",
                "Mobile handsets",
                "Smart contracts",
                "Internet of Things",
                "Servers",
                "Cryptography",
                "Outsourcing",
                "Costs",
                "Data privacy"
            ],
            "Author Keywords": [
                "Attribute-based searchable encryption",
                "Verifiable outsourced decryption",
                "Attribute revocation",
                "Privacy protection",
                "Smart contract"
            ]
        },
        "title": "Fair and Exculpable Attribute-Based Searchable Encryption With Revocation and Verifiable Outsourced Decryption Using Smart Contract"
    },
    {
        "authors": [
            "Tao Hai",
            "Zakirul Alam Bhuiyan",
            "Mohamed Rahouti",
            "Jincheng Zhou",
            "Jian Shen"
        ],
        "published_in": "Published in: IEEE Network ( Early Access )",
        "date_of_publication": "13 May 2024",
        "doi": "10.1109/MNET.2024.3399751",
        "publisher": "IEEE",
        "abstract": "Emerging applications, such as Smart Grid (SG), UAV (drone)-based delivery (logistics) involve multiple networks, including 5G-powered intelligent UAVs (drones), ground intelligent Internet-connected vehicles (IIoCVs), and Edge-assisted Internet of Things (IoT). Data exchange in these cross-border network applications faces significant challenges due to cyberattacks and faults in the network. This leads to untrustworthy situations in network automation regarding secret data access, data sensitivity, wrong regional status, and false direction. In this paper, we propose bTrust design, a blockchain-based trustworthy data exchange framework to improve these situations in network automation. The framework development consists of five procedures regarding the data provider and data requester’s trustworthiness. We adopt cloud computing to cover different terrain regions with Edge-computing units (ECUs, deployed at the IIoCVs), where one of the ECUs functions as the Edge server covering each region. When a drone claims that there is an untrustworthy action, such as compromised or incorrect data transmission against the IIoCV, the cloud authenticates the claims by inspecting the blockchain. It penalizes the compromised IIoCV if the claim is correct. Otherwise, the cloud can punish the drone itself (e.g., by reducing the trustworthiness score). As a result, bTrust enhances cross-regional data exchange trustworthiness using the blockchain that depends on a reduced trust assumption.",
        "issn": {
            "Print ISSN": "0890-8044",
            "Electronic ISSN": "1558-156X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Drones",
                "Internet of Things",
                "Cloud computing",
                "Blockchains",
                "Smart grids",
                "Servers",
                "Monitoring"
            ],
            "Author Keywords": []
        },
        "title": "Blockchain-based Trustworthiness in Cross-Border Data Exchange in 5G-powered Intelligent Connected Vehicles"
    },
    {
        "authors": [
            "Muyan Yao",
            "Dan Tao",
            "Peng Qi",
            "Ruipeng Gao"
        ],
        "published_in": "Published in: IEEE Transactions on Automation Science and Engineering ( Early Access )",
        "date_of_publication": "05 November 2024",
        "doi": "10.1109/TASE.2024.3486688",
        "publisher": "IEEE",
        "abstract": "Industrial environments pose distinctive challenges for anomaly detection, primarily stemming from the complexities associated with high dimensionality and the dynamic nature of data patterns over time. These properties determine that the model’s proper convergence on unlabeled data is unpromising, consequently leading to less efficient discrimination of anomalies in previous anomaly detection (AD) works. To address this problem, we present AnoDual , a novel, meta-learning AD framework. From the perspective of data reconstruction, we introduce the multi-memory enhanced VAE reconstructor M2ER , which learns to extract the most salient patterns in unlabeled noisy data through a self-supervised manner. This design eases impacts from potential anomalous components during data reconstruction, and enhances the discernibility of anomalies. To address performance degradation caused by the numerical deviation based AD scheme in most existing works, we design a dual-source self-supervised discriminator DSD , which examines characteristics in the domain of representations. This model actively assesses discrepancies between data pairs and representation pairs in parallel, and conducts AD on a fine-grained scale. In this way, anomalies that used to be unnoticed due to a less prominent numerical deviation can be spotted. Besides, we propose a meta-learning powered training pipeline to enable model training even when no real label is available, which is common in the industry. Extensive experiments on five large-scale real-world industrial datasets suggest that AnoDual achieves an average F1-Score with a substantial increment of 3.39 %, outperforming the latest state-of-the-art baseline. Note to Practitioners —A generative model plus a numerical threshold based detection approach currently takes a significant share in both academia and the industry. However, the performance of this workflow is not promising in actual applications, with multiple factors contributing to this...",
        "issn": {
            "Print ISSN": "1545-5955",
            "Electronic ISSN": "1558-3783"
        },
        "keywords": {
            "IEEE Keywords": [
                "Data models",
                "Training",
                "Numerical models",
                "Noise measurement",
                "Metalearning",
                "Anomaly detection",
                "Machine learning algorithms",
                "Correlation",
                "Convergence",
                "Time series analysis"
            ],
            "Author Keywords": [
                "Anomaly detection",
                "meta-learning",
                "self-supervised learning",
                "industrial Internet of Things",
                "discrepancy analysis"
            ]
        },
        "title": "Rethinking Discrepancy Analysis: Anomaly Detection via Meta-Learning Powered Dual-Source Representation Differentiation"
    },
    {
        "authors": [
            "Mohamed Mohsen",
            "Hamada Rizk",
            "Hirozumi Yamaguchi",
            "Moustafa Youssef"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "21 August 2024",
        "doi": "10.1109/JIOT.2024.3446844",
        "publisher": "IEEE",
        "abstract": "Locating the persons moving through an environment without the necessity of them being equipped with special devices has become vital for many applications including security, IoT, healthcare, etc. Existing device-free indoor localization systems commonly rely on the utilization of Received Signal Strength Indicator (RSSI) and WiFi Channel State Information (CSI) techniques. However, the accuracy of RSSI is adversely affected by environmental factors like multi-path interference and fading. Additionally, the lack of standardization in CSI necessitates the use of specialized hardware and software. In this paper, we present, a deep learning-based multi-person device-free indoor localization system that addresses these challenges. leverages Time of Flight information acquired by the fine-time measurement protocol of IEEE 802.11-2016 standard. Specifically, the measured round trip time between the transmitter and receiver is influenced by the dynamic changes in the environment induced by human presence. effectively detects this anomalous behavior using a stacked denoising auto-encoder model, thereby estimating the user’s location. The system incorporates a probabilistic approach on top of the deep learning model to ensure seamless tracking of the users. The evaluation of in two realistic environments demonstrates its efficacy, achieving a median localization accuracy of 1.57 and 2.65 meters. This surpasses the performance of state-of-the-art techniques by 49% and 103% in the two testbeds.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Location awareness",
                "Wireless fidelity",
                "Accuracy",
                "Protocols",
                "Time measurement",
                "Probabilistic logic",
                "Robustness"
            ],
            "Author Keywords": [
                "Wi-Fi RTT",
                "Indoor Localization",
                "Deep Learning learning",
                "Device-free Passive Localization",
                "Fingerprinting"
            ]
        },
        "title": "TimeSense: Multi-Person Device-Free Indoor Localization via RTT"
    },
    {
        "authors": [
            "Chuanneng Sun",
            "Tingcong Jiang",
            "Dario Pompili"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "30 October 2024",
        "doi": "10.1109/JIOT.2024.3488565",
        "publisher": "IEEE",
        "abstract": "Federated Learning (FL) has been popular recently as a framework for training Machine Learning (ML) models in a distributed and privacy-preserving manner. Traditional FL frameworks often struggle with model and statistical heterogeneity among participating clients, impacting learning performance and practicality. To overcome these fundamental limitations, we introduce Fed2KD+, a novel FL framework that leverages a set of tiny unified models and Conditional Variational Auto-Encoders (CVAEs) to enable FL training for heterogeneous models between network clients. Using forward and backward distillation processes, Fed2KD+ allows a seamless exchange of knowledge, mitigating data and heterogeneity problems of the model. Moreover, we propose a cosine similarity penalty in the loss function of CVAE+ to enhance the generalizability of CVAE for non-IID scenarios, improving the adaptability and efficiency of the framework. Furthermore, our framework design incorporates a co-design with Radio Access Network (RAN) architecture, reducing the fronthaul traffic volume and improving scalability. Extensive evaluations of one image and two IoT datasets demonstrate the superiority of Fed2KD+ in achieving higher accuracy and faster convergence compared to existing methods, including FedAvg, FedMD, and FedGen. Furthermore, we also performed hardware profiling on the Raspberry Pi and NVIDIA Jetson Nano to quantify the additional resources required to train the unified and CVAE+ models.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Data models",
                "Training",
                "Internet of Things",
                "Servers",
                "Computational modeling",
                "Convergence",
                "Radio access networks",
                "Federated learning",
                "Privacy",
                "Distributed databases"
            ],
            "Author Keywords": [
                "federated learning",
                "knowledge distillation",
                "edge learning",
                "model heterogeneity"
            ]
        },
        "title": "Heterogeneous Federated Learning via Generative Model-Aided Knowledge Distillation in the Edge"
    },
    {
        "authors": [
            "Wenjing Gao",
            "Jia Yu",
            "Huaqun Wang"
        ],
        "published_in": "Published in: IEEE Transactions on Cloud Computing ( Early Access )",
        "date_of_publication": "08 August 2024",
        "doi": "10.1109/TCC.2024.3440656",
        "publisher": "IEEE",
        "abstract": "Linear regression is a classical statistical model with a wide range of applications. The function of linear regression is to predict the value of a dependent variable (the output) given an independent variable (the input). The training of a linear regression model is to find a linear relationship between the input and the output based on data samples. IoT applications usually require real-time data processing. Nonetheless, the existing schemes about privacy-preserving outsourcing of linear regression cannot fully meet the rapid response requirement for computation. To address this issue, we consider employing multiple edge servers to accomplish privacy-preserving parallel computation of linear regression. We propose two novel solutions based on edge servers in edge computing networks and construct two efficient schemes for linear regression. In the first scheme, we present a new blinding technique for data privacy protection. Two edge servers are employed to execute the encrypted linear regression task in parallel. To further enhance the efficiency, we design an adaptive parallel algorithm, which is adopted in the second scheme. Multiple edge servers are employed in the second scheme to achieve higher efficiency. We analyze the correctness, privacy, and verifiability of the proposed schemes. Finally, we assess the computational overhead of the proposed schemes and conduct experiments to validate the performance advantages of the proposed schemes.",
        "issn": {
            "Electronic ISSN": "2168-7161"
        },
        "keywords": {
            "IEEE Keywords": [
                "Servers",
                "Computational modeling",
                "Task analysis",
                "Outsourcing",
                "Linear regression",
                "Internet of Things",
                "Privacy"
            ],
            "Author Keywords": [
                "Edge computing",
                "privacy-preserving outsourcing computation",
                "parallel computing",
                "linear regression",
                "Internet of Things"
            ]
        },
        "title": "Enabling Privacy-Preserving Parallel Computation of Linear Regression in Edge Computing Networks"
    },
    {
        "authors": [
            "Abdulrahman Ahmad",
            "Ameena S. Al-Sumaiti",
            "Young-Ji Byon",
            "Khalifa Alhosani"
        ],
        "published_in": "Published in: IEEE Transactions on Intelligent Transportation Systems ( Early Access )",
        "date_of_publication": "06 November 2024",
        "doi": "10.1109/TITS.2024.3484771",
        "publisher": "IEEE",
        "abstract": "Travel-time reduction is a primary objective for managing connected emergency vehicles (CEVs) to save people’s lives or put out a fire. With the integrity of internet of things (IoT) and connected autonomous vehicles (CAVs), it has been a research challenge to find a safe, reliable, and optimal strategy that not only minimizes the CEV’s travel time but also lessens the undesirable side-effects on other road users. This article introduces multiple intelligent control strategies in one framework to boost the potential of CEVs traveling via multiple traffic intersections. The framework includes a path-planning mechanism adapting to sudden traffic delays, traffic signal preemption controller adapting to the urgency level associated with the emergency event, and a deep-learning model for CAVs to predict the time required for giving way to the CEV. All modules are implemented through a microscopic traffic simulation environment (PTV-VISSIM). This article holds significant implications for various scenarios involving CEVs and intelligent transportation systems (ITS). The path planning approach showcased notable improvements, reducing average path travel time by 9% when compared to existing benchmarks. The regression error for predicting the merging time of CAVs is minimized to be 0.4 second. Furthermore, the signal preemption controller demonstrated an important trade-off analysis between the level of intrusive preemption signal control and the undesired impacts on the traffic network. This finding enables traffic management authorities to make informed decisions regarding signal preemption strategies, considering both the travel time optimization for CEVs and the potential network-wide traffic impacts.",
        "issn": {
            "Print ISSN": "1524-9050",
            "Electronic ISSN": "1558-0016"
        },
        "keywords": {
            "IEEE Keywords": [
                "Real-time systems",
                "Merging",
                "Path planning",
                "Heuristic algorithms",
                "Vehicle dynamics",
                "Traffic control",
                "Roads",
                "Predictive models",
                "Discharges (electric)",
                "Delays"
            ],
            "Author Keywords": [
                "Intelligent transportation",
                "emergency vehicles",
                "CAVs",
                "signal preemption",
                "dynamic path planning"
            ]
        },
        "title": "Multiple Intelligent Control Strategies for Travel-Time Reduction of Connected Emergency Vehicles"
    },
    {
        "authors": [
            "Ke Chen",
            "Ramana Vinjamuri",
            "Honggang Wang",
            "Sai Praveen Kadiyala"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "28 August 2024",
        "doi": "10.1109/JIOT.2024.3450653",
        "publisher": "IEEE",
        "abstract": "Internet-of-Things (IoT) based solutions are gaining momentum in delivering efficient solutions in health care domain, reducing financial and physical burden on patients and improving ease of treatment for physicians. One such smart health care solution are Serious games. Serious games aid rehabilitation in various fields. For physical rehabilitation, personalization is important for improving training results. A scientific approach for difficulty level design can facilitate players to get effective rehabilitation. The automation of personalized difficulty level design helps the self-guided game-based rehabilitation approach, become simplified and efficient. AI is advancing the design of personalized serious game for rehabilitation through data-driven and individual-oriented methods. In this work, we present Generative AI based design of gamified training plan, especially difficulty level plan which could go beyond rule based solutions. We apply Generative Adversarial Networks (GANs) to address the problem arising from large sequential data and variable requirement. This helps to overcome the limitation of unrealistic long term practice session for a rehabilitation patient by simplifying the training time. When compared with the results from Long Short-Term Memory (LSTM) based approach, our GANs based approach gave a 4.5X less variation in difficulty level and 6.5X less loss which proved the efficacy of our proposed approach in generating accurate difficulty levels. When compared with existing literature our proposed work simultaneously performs better on various parameters namely faster convergence, minimum emphasis on past performance of players, low data requirement for training and demographic flexibility.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Serious games",
                "Internet of Things",
                "Training",
                "Long short term memory",
                "Generative AI",
                "Data models",
                "Accuracy"
            ],
            "Author Keywords": [
                "Serious Games",
                "Rehabilitation",
                "Difficulty Levels",
                "Generative AI",
                "Generative Adversarial Networks"
            ]
        },
        "title": "Generative AI Based Difficulty Level Design of Serious Games for Stroke Rehabilitation"
    },
    {
        "authors": [
            "Zhuoran Cai",
            "Chuan Wang",
            "Wenxuan Ma",
            "Xiangzhen Li",
            "Ruoyu Zhou"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "02 October 2024",
        "doi": "10.1109/JIOT.2024.3471770",
        "publisher": "IEEE",
        "abstract": "Automatic Modulation Classification (AMC) is essential in non-cooperative communication systems, since it enables the automatic recognition of signal modulation types. The recent incorporation of deep learning, particularly Graph Neural Networks (GNNs), has significantly improved the AMC accuracy. The GNNs increase the performance by decoding the relationships between nodes and edges, which represent the topological structure of data. In AMC, signal features or time points are modeled as nodes, and their interconnections represent the interactions between these features. This modeling allows the GNNs to thoroughly analyze signals and accurately identify complex modulations. However, the existing traditional methods for mapping IQ signal sequences into graphs exhibit high computational load and excessive processing time. To solve these problems, this paper proposes a lightweight model of high performance, referred to as PGNet, which combines efficient partial convolution (PConv) with graph sparse attention techniques. This combination minimizes the computational load and maximizes the strengths of the CNNs and GNNs. The results of the conducted experiment show that PGNet respectively achieves average accuracies of 62.8% and 64.1% on the RML2016.10a and RML2016.10b datasets, with only 16,315 parameters and an inference time of only 2 ms/sample. Due to its high efficiency and compact size, the proposed PGNet provides a substantial potential for deployment in low computing resource scenarios, such as IoT devices with limited resources.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Convolution",
                "Computational modeling",
                "Accuracy",
                "Convolutional neural networks",
                "Modulation",
                "Data mining",
                "Load modeling",
                "Computational efficiency",
                "Internet of Things"
            ],
            "Author Keywords": [
                "Automatic modulation classification",
                "deep learning",
                "graph sparse attention",
                "partial convolution"
            ]
        },
        "title": "Lightweight Automatic Modulation Classification Based on Efficient Convolution and Graph Sparse Attention in Low-Resource Scenarios"
    },
    {
        "authors": [
            "Jinfeng Jing",
            "Yaozong Yang",
            "Xiaokang Zhou",
            "Jiwei Huang",
            "Lianyong Qi",
            "Ying Chen"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "24 October 2024",
        "doi": "10.1109/TCE.2024.3485633",
        "publisher": "IEEE",
        "abstract": "The development of the Internet of Things (IoT) has made the traditional Consumer Electronics (CE) evolve into a next-generation CE with many intelligent application which generates a large number of computing-intensive tasks and is sensitive to latency. The combination of blockchain technology and multi-UAV-assisted edge computing is considered a promising technology that can process tasks near CE devices instead of sending data to remote data centers, and it has advantages in flexible deployment, differentiated services, and security assurance. This paper considers a Blockchain-enabled MEC scenario where CE devices can offload tasks to multiple-UAV-edge layers for decentralized cooperative computing, and utilize the reputation mechanism of blockchain to identify abnormal nodes in edge offloading. To solve the problem of computation offloading and resource allocation in this scenario, we jointly optimize the CPU cycle frequencies and offloading resources of UAVs and CE devices, and the decision of UAVs. Our goal is to minimize system energy consumption while ensuring the quality of service. The stochastic optimization technique is employed to transform the original stochastic optimization problem into a deterministic optimization problem, decomposing it into multiple sub-problems which can be solved in parallel. Additionally, we design a dynamic priority scoring strategy to facilitate cooperative processing among multiple UAVs and propose a Blockchain-Based Multi-UAV Cooperative Task Offloading (BMCTO) algorithm. Through theoretical analysis and simulation experiments, we demonstrate that the BMCTO algorithm can effectively reduce system energy consumption while maintaining system performance.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Blockchains",
                "Security",
                "Optimization",
                "Autonomous aerial vehicles",
                "Resource management",
                "Collaboration",
                "Edge computing",
                "Consumer electronics",
                "Servers",
                "Privacy"
            ],
            "Author Keywords": [
                "Multi-UAV Cooperation",
                "Resource Allocation",
                "Blockchain",
                "Multi-Access Edge Computing (MEC)",
                "Stochastic Optimization"
            ]
        },
        "title": "Multi-UAV Cooperative Task Offloading in Blockchain-Enabled MEC for Consumer Electronics"
    },
    {
        "authors": [
            "Seyed Mahmoud Sajjadi Mohammadabadi",
            "Syed Zawad",
            "Feng Yan",
            "Lei Yang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "28 October 2024",
        "doi": "10.1109/JIOT.2024.3487473",
        "publisher": "IEEE",
        "abstract": "Federated learning enables collaborative training of a model while keeping the training data decentralized and private. However, in IoT systems, inherent heterogeneity in processing power, communication bandwidth, and task size can significantly hinder the efficient training of large models. Such heterogeneity would render vast variations in the training time of clients, lengthening overall training and wasting resources of faster clients. To tackle these heterogeneity challenges, we propose Dynamic Tiering-based Federated Learning (DTFL), a novel system that leverages distributed optimization principles to improve edge learning performance. Based on clients’ resources, DTFL dynamically offloads part of the global model to the server, alleviating resource constraints on slower clients and speeding up training.By leveraging Split Learning, DTFL offloads different portions of the global model to clients in different tiers and enables each client to update the models in parallel via local-loss-based training. This helps reduce the computation and communication demand on resource-constrained devices, mitigating the straggler problem. DTFL introduces a dynamic tier scheduler that uses tier profiling to estimate the expected training time of each client based on their historical training time, communication speed, and dataset size. The dynamic tier scheduler assigns clients to suitable tiers to minimize the overall training time in each round. We theoretically prove the convergence properties of DTFL and validate its effectiveness by training large models (ResNet-56 and ResNet-110) across varying numbers of clients (from 10 to 200) using popular image datasets (CIFAR-10, CIFAR-100, CINIC-10, and HAM10000) under both IID and non-IID systems. DTFL seamlessly integrates various privacy measures without sacrificing performance. Extensive experimental results show that compared with state-of-the-art FL methods, DTFL can significantly reduce the training time by up to 80% while...",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Computational modeling",
                "Internet of Things",
                "Servers",
                "Federated learning",
                "Accuracy",
                "Dynamic scheduling",
                "Performance evaluation",
                "Optimization",
                "Load modeling"
            ],
            "Author Keywords": [
                "Edge computing",
                "federated learning",
                "heterogeneous devices",
                "split learning",
                "distributed optimization"
            ]
        },
        "title": "Speed Up Federated Learning in Heterogeneous Environments: A Dynamic Tiering Approach"
    },
    {
        "authors": [
            "Jiabao Wen",
            "Huiao Dai",
            "Jingyi He",
            "Lijiao Sun",
            "Liqing Gao"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "03 April 2024",
        "doi": "10.1109/JIOT.2024.3384476",
        "publisher": "IEEE",
        "abstract": "With the development of society and the economy, low-carbon and low-energy means of exploiting marine resources are receiving increasing attention. Autonomous path planning is a fundamental capability for IoT Autonomous Underwater Vehicle (AUV) to carry out ocean exploration tasks. Currently, the main issue lies in the numerous disturbances and uncertainties present in the marine environment during practical applications, which can significantly impact path planning, leading to high energy consumption and carbon emissions. To address this challenge, this paper presents a sustainable reinforcement learning algorithm for handling time-varying current disturbances to achieve low-carbon AUV path planning, which is delineated into three steps. Firstly, a three-dimensional time-varying current environment is established as the environmental framework for reinforcement learning, and the dynamic model of the AUV is formulated. Secondly, to enhance training efficiency and reduce AUV’s energy consumption, this paper puts forth the OCDRP (Ocean Current Disturbance Rejection PPO) algorithm, which incorporates tidal current information to enhance the AUV’s resilience to time-varying currents. Lastly, expectile regression methods are introduced to facilitate the algorithm’s convergence. Experimental results confirm the efficacy of the proposed algorithm and its adaptability to time-varying currents, making it an efficient, adaptable, and low-carbon sustainable path planning approach.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Oceans",
                "Heuristic algorithms",
                "Reinforcement learning",
                "Path planning",
                "Adaptation models",
                "Planning",
                "Energy consumption"
            ],
            "Author Keywords": [
                "Low-Carbon",
                "sustainable computing",
                "path planning",
                "autonomous underwater robot",
                "reinforcement learning",
                "time-varying ocean current",
                "disturbance rejection"
            ]
        },
        "title": "Intelligent Decision-Making Method for AUV Path Planning Against Ocean Current Disturbance Via Reinforcement Learning"
    },
    {
        "authors": [
            "Ying Zhang",
            "Qiang Li",
            "Hongli Liu",
            "Liu Yang",
            "Jian Yang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/JIOT.2024.3486068",
        "publisher": "IEEE",
        "abstract": "Radio Frequency Fingerprint Identification (RFFI) technology uniquely identifies emitters by analyzing unique distortions in the transmitted signal caused by non-ideal hardware. Recently, RFFI based on deep learning methods has gained popularity and is seen as a promising way to address the device authentication problem for Internet of Things (IoT) systems. However, in cross-receiver scenarios, where the RFFI model is trained over RF signals from some receivers but deployed at a new receiver, the alteration of receiver’s characteristics would lead to data distribution shift and cause significant performance degradation at the new receiver. To address this problem, we first perform a theoretical analysis of the cross-receiver generalization error bound and propose a sufficient condition, named Separable Condition (SC), to minimize the classification error probability on the new receiver. Guided by the SC, a Receiver-Independent Emitter Identification (RIEI) model is devised to decouple the received signals into emitter-related features and receiver-related features and only the emitter-related features are used for identification. Furthermore, by leveraging federated learning, we also develop a FedRIEI model to eliminate the need for centralized collection of raw data from multiple receivers. Experiments on two real-world datasets demonstrate the superiority of our proposed methods over some baseline methods.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Receivers",
                "Data models",
                "Fingerprint recognition",
                "Wireless communication",
                "Internet of Things",
                "Training",
                "Feature extraction",
                "RF signals",
                "Error probability",
                "Communication system security"
            ],
            "Author Keywords": [
                "Radio Frequency Fingerprint Identification",
                "domain generalization",
                "feature disentanglement",
                "Federated Learning"
            ]
        },
        "title": "Domain Generalization for Cross-Receiver Radio Frequency Fingerprint Identification"
    },
    {
        "authors": [
            "Zhao Hong",
            "Chew Ze Yong",
            "Kosasih Lucky",
            "Goh Jun Rong",
            "Wang Joheng"
        ],
        "published_in": "Published in: IEEE Transactions on Semiconductor Manufacturing ( Early Access )",
        "date_of_publication": "05 November 2024",
        "doi": "10.1109/TSM.2024.3483781",
        "publisher": "IEEE",
        "abstract": "The semiconductor industry faces increasing pressure to improve energy efficiency while maintaining competitiveness and sustainability. Apart from more conventional energy efficiency measures look at equipment modernization and process and design optimization, this paper explores the potential of data-driven approaches to address these challenges and optimize energy consumption across both the facility and manufacturing space of a semiconductor manufacture plant. By harnessing advanced analytics, machine learning algorithms, and IoT technologies, semiconductor manufacturers can gain real-time insights into energy usage patterns, and identify areas of opportunities that leads to the implementation of targeted interventions to optimize performance. The paper first looks into the challenges and measures of enabling and enhancing data visibility which is the foundation of the data-driven approach, then it examines case studies, best practices and various systematic approaches, demonstrating the transformative impact of data-driven energy efficiency measures which leads to operational efficiency, cost reduction, and environmental sustainability. Ultimately, this paper aims to provide a fresh angle into the energy efficiency study for peers in semiconductor industries to leverage in their journey towards a more sustainable and energy efficient future.",
        "issn": {
            "Print ISSN": "0894-6507",
            "Electronic ISSN": "1558-2345"
        },
        "keywords": {
            "IEEE Keywords": [
                "Energy consumption",
                "Energy efficiency",
                "Micrometers",
                "Semiconductor device measurement",
                "Meters",
                "Manufacturing processes",
                "Energy measurement",
                "Manufacturing",
                "Semiconductor device manufacture",
                "Monitoring"
            ],
            "Author Keywords": []
        },
        "title": "A Data-Driven Approach for Improving Energy Efficiency in a Semiconductor Manufacturing Plant"
    },
    {
        "authors": [
            "Yirui Wu",
            "Hao Cao",
            "Yong Lai",
            "Liang Zhao",
            "Xiaoheng Deng",
            "Shaohua Wan"
        ],
        "published_in": "Published in: IEEE Transactions on Network and Service Management ( Early Access )",
        "date_of_publication": "30 August 2024",
        "doi": "10.1109/TNSM.2024.3450993",
        "publisher": "IEEE",
        "abstract": "Digital twins (DT) and mobile networks have evolved forms of intelligence in Internet of Things (IoT). In this work, we consider a Digital Twin Mobile Network (DTMN) scenario with few multimedia samples. Facing challenges of knowledge extraction with few samples, stable interaction with dynamic changes of multimedia data, time and privacy saving in low-resource mobile network, we propose an edge computing and few-shot learning featured intelligent framework. Considering time-sensitive property of transmission and privacy risks of directly uploads in mobile network, we deploy edge computing to locally run networks for analysis, thus saving time to offload computing request and enhancing privacy by encrypting original data. Inspired by remarkable relationship representation of graphs, we build Graph Neural Network (GNN) in cloud to map physical mobile systems to virtual entities with DT, thus performing semantic inferences in cloud with few samples uploaded by edges. Occasionally, node features in GNN could converge to similar, non-discriminative embeddings, causing catastrophic unstable phenomena. An iterative reweight and drop structure (IRDS) is thus constructed in cloud, which nonetheless contributes stability with respect to edge uncertainty. As part of IRDS, a drop Edge&Node scheme is proposed to randomly remove certain nodes and edges, which not only enhances distinguished capability of graph neighbor patterns, but also offers data encryption with random strategy. We show one implementation case of image classification in social network, where experiments on public datasets show that our framework is effective with user-friendly advantages and significant intelligence.",
        "issn": {
            "Electronic ISSN": "1932-4537"
        },
        "keywords": {
            "IEEE Keywords": [
                "Digital twins",
                "Graph neural networks",
                "Edge computing",
                "Data privacy",
                "Cloud computing",
                "Training",
                "Iterative methods"
            ],
            "Author Keywords": [
                "Digital Twin Mobile Network",
                "New Paradigm for DTMNs",
                "Edge Computing enabled DTMN",
                "Edge Intelligence",
                "Few-shot Learning",
                "Graph Neural Network"
            ]
        },
        "title": "Edge Computing and Few-shot Learning Featured Intelligent Framework in Digital Twin empowered Mobile Networks"
    },
    {
        "authors": [
            "Siyun Xu",
            "Miao Zhang",
            "Tong Wang"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "22 August 2024",
        "doi": "10.1109/TCE.2024.3445458",
        "publisher": "IEEE",
        "abstract": "The global Blockchain networks are growing and demand for resources is also growing respectively. The systems are switching from traditional systems to advanced systems where there is a seamless connectivity with 6G communication channels and security of data due to decentralized nature of Blockchain environment. The resources play integral part in Blockchain networks such as computational resources, data storage resources, bandwidth, sensors and energy generation power resources. The forecasting of futuristic demand of resources is important for the smooth functioning of Blockchain networks. The advanced technologies like 6G networks and machine learning techniques, Internet of Things (IoT), Digital Twins, Cyber Physical systems and AI enabled tools are playing an important role in reshaping the Blockchain networks. This research work is utilizing deep learning and game theory to map the resource requirement and to evaluate the Blockchain systems to find the potential demand for resources for smooth functioning of Blockchain enabled systems. The sampling data has been collected from Blockchain nodes and parameter based migration methods are devised to improve the predictions of deep learning models. The resource needs of the software based Blockchain networks can be predicted where the future load can be predicted on Blockchain enabled networks. The trained model based on deep learning neural networks achieves multi-layer conversion combinations through nonlinear modules to make accurate predictions in Blockchain based systems for resource requirement. This article uses the migration theory, combined with the advantages of deep neural networks to produce accurate predictions. The forecasting prediction accuracy of the required futuristic resources on raw variables is attained at 85.87%. The proposed model helps to determine the futuristic need of the resources for smooth functioning of Blockchain systems as many applications nowadays are dependent upon the Blockcha...",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Blockchains",
                "Resource management",
                "Predictive models",
                "Deep learning",
                "Game theory",
                "6G mobile communication",
                "Finance"
            ],
            "Author Keywords": [
                "Resource management",
                "Blockchain",
                "Deep Learning (DL)",
                "Game theory",
                "Demand for resources"
            ]
        },
        "title": "Game Theory and Deep Learning for Predicting Demand for Future Resources Within Blockchain-Networks"
    },
    {
        "authors": [
            "Trong-Hung Nguyen",
            "Duc-Thuan Dam",
            "Phuc-Phan Duong",
            "Binh Kieu-Do-Nguyen",
            "Cong-Kha Pham",
            "Trong-Thuc Hoang"
        ],
        "published_in": "Published in: IEEE Transactions on Circuits and Systems I: Regular Papers ( Early Access )",
        "date_of_publication": "20 August 2024",
        "doi": "10.1109/TCSI.2024.3443238",
        "publisher": "IEEE",
        "abstract": "Quantum computing raises questions about the security of data encrypted using modern methods. Hence, the National Institute of Standards and Technology (NIST) has undertaken standardization of post-quantum cryptography (PQC) algorithms to defend against attacks from both classical and quantum computers. Following four rounds of evaluation, CRYSTALS-Kyber has been selected for standardization. In this paper, we present an efficient hardware architecture of CRYSTALS-Kyber for resource-constrained IoT devices. Firstly, we propose a compact hash module for CRYSTALS-Kyber. A single buffer is designed to perform padding, hashing, and holding data. Hence, using large FIFOs for data input/output is eliminated. Then, we propose a novel non-memory-based iterative number theoretic transform (NMI-NTT) architecture. Finally, the data flow between modules is optimized to improve parallelization and execution time. Implementation results on an Artix-7 FPGA show that our design consumes minimal hardware resources compared to the designs reported to date, corresponding to 5487 LUTs, 3426 FFs, 1548 SLICEs, 3.5 BRAMs, and 2 DSPs. Our design computes key generation, encapsulation, and decapsulation phases in 3.3/4.5/6.1 K-cycles for Kyber512, 5.6/7.1/9.2 K-cycles for Kyber768, and 8.5/10.1/12.9 K-cycles for Kyber1024, with 185MHz operating frequency. Our area-time-product (ATP) performance outperforms other designs.",
        "issn": {
            "Print ISSN": "1549-8328",
            "Electronic ISSN": "1558-0806"
        },
        "keywords": {
            "IEEE Keywords": [
                "Hardware",
                "Polynomials",
                "Iterative methods",
                "Computer architecture",
                "Internet of Things",
                "Standards",
                "Pulse width modulation"
            ],
            "Author Keywords": [
                "Post-quantum cryptography",
                "CRYSTALS-Kyber",
                "lightweight hardware architecture",
                "non-memory-based iterative NTT",
                "compact SHA-3"
            ]
        },
        "title": "Efficient Hardware Implementation of the Lightweight CRYSTALS-Kyber"
    },
    {
        "authors": [
            "Nour Badini",
            "Mona Jaber",
            "Mario Marchese",
            "Fabio Patrone"
        ],
        "published_in": "Published in: IEEE Transactions on Aerospace and Electronic Systems ( Early Access )",
        "date_of_publication": "29 July 2024",
        "doi": "10.1109/TAES.2024.3434771",
        "publisher": "IEEE",
        "abstract": "Multiple Low Earth Orbit (LEO) satellites have recently been launched in constellations to insure direct Internet access to users anywhere and at any time. Due to the high-speed mobility of LEO satellites, users undergo multiple handovers (HO)s during their service time, which has a negative impact on users' Quality of Service (QoS) if occurred in high frequency. Moreover, next-generation communication technologies are designed to support a wide spectrum of applications, including Artificial Intelligence, Virtual Reality, and Internet of Things (IoT). Thus, differentiating User Equipments (UEs) with different and varying Traffic-Profiles (TP) has become necessary due to each application's unique performance requirements. However, LEO satellites have limited onboard resources and the launched constellations ensure that each UE will be covered by more than one LEO satellite at any given moment, making it challenging to select the optimal satellite at any given time to assure the optimum QoS. Therefore, a satellite HO strategy has to effectively use the few available satellite resources and prevent network congestion while respecting the various resource requirements per TP. To address all the above requirements, we propose a user-centric Multi-Agent Deep Q-Network (MADQN) satellite HO strategy, that is the first in the state of the art to address the variety and diversity of UEs' performance requirements and generated traffic statistics. Our method showcases a significant achievement of approximately 60% reduction in HO rate and around 91% reduction in blocking rate compared to conventional single criterion approaches.",
        "issn": {
            "Print ISSN": "0018-9251",
            "Electronic ISSN": "1557-9603"
        },
        "keywords": {
            "IEEE Keywords": [
                "Satellites",
                "Low earth orbit satellites",
                "Satellite broadcasting",
                "Quality of service",
                "Handover",
                "Aerospace and electronic systems",
                "Optimization"
            ],
            "Author Keywords": [
                "Multi-Agent Reinforcement Learning",
                "Deep Neural Network",
                "Deep Q-Learning",
                "Distributed Satellite Handover",
                "Satellite-Terrestrial Integrated Network Simulator"
            ]
        },
        "title": "User Centric Satellite Handover for Multiple Traffic Profiles Using Deep Q-Learning"
    },
    {
        "authors": [
            "Chun-Cheng Lin",
            "Lei Shu",
            "Der-Jiunn Deng",
            "Tzu-Lei Yeh",
            "Yu-Hsiang Chen",
            "Hsin-Lung Hsieh"
        ],
        "published_in": "Published in: IEEE Cloud Computing ( Early Access )",
        "date_of_publication": "22 December 2017",
        "doi": "10.1109/MCC.2017.455160123",
        "publisher": "IEEE",
        "abstract": "Condition-based maintenance (CBM) is to collect a huge amount of production data stream continuously from sensors or IoT devices attached to machines to forecast the time when to maintain machines or replace components. However, as conditions of machines change dynamically with time owing to machine aging or malfunction, the concept of capturing the foresting pattern from the data stream could drift unpredictably so that it is hard to find a robust forecasting method with high precision. Therefore, this work proposes an ensemble learning method with multiple classifier types and diversity for CBM in manufacturing industries. Aside from manipulating data diversity from previous works, the proposed method includes the features of multiple classifier types, dynamic weight adjusting, and data-based adaption to concept drifts for offline learning models, to promote the precision of the forecasting model and precisely detect and adapt to concept drifts. With these features, the proposed method requires powerful computing resources to efficiently respond to practical CBM applications in manufacturing industries. Therefore, furthermore, the implementation of the proposed method based on the MapReduce framework is proposed to increase computational efficiency. Simulation shows that the proposed method can detect and adapt to all concept drifts with a high precision rate.",
        "issn": {
            "Electronic ISSN": "2325-6095"
        },
        "keywords": {
            "IEEE Keywords": [
                "Learning systems",
                "Adaptation models",
                "Training data",
                "Computational modeling",
                "Data models",
                "Training",
                "Bagging"
            ],
            "Author Keywords": [
                "concept drift",
                "ensemble learning",
                "MapReduce",
                "condition-based maintenance",
                "Industry 4.0"
            ]
        },
        "title": "A MapReduce-Based Ensemble Learning Method with Multiple Classifier Types and Diversity for Condition-based Maintenance with Concept Drifts"
    },
    {
        "authors": [
            "Sachin Gupta",
            "Ashish Kumar Tripathi",
            "Venu Parameswaran"
        ],
        "published_in": "Published in: IEEE Transactions on Intelligent Transportation Systems ( Early Access )",
        "date_of_publication": "18 October 2024",
        "doi": "10.1109/TITS.2024.3476132",
        "publisher": "IEEE",
        "abstract": "Autonomous 6G-enabled Vehicle Transportation System (VTS) is receiving significant attention from researchers to ensure robust and safe driving operations. 6G-supported communication technologies show remarkable advancements in several vehicular domains, including automated and accurate identification of lanes, vehicles, traffic signs, and obstacles within the vehicle’s proximity. Integrating IoT and AI technologies can leverage the extensive information gathered by Autonomous Vehicles (AVs) for precise vehicle detection. Despite the rapid advancements in object detection for completely visible objects from oncoming vehicles, detecting objects in low-visibility environments remains a challenging task, especially at night. This paper presents an efficient vehicle road cooperation method using automated real-time roadside object identification system by processing the captured video frames from LiDAR sensors. The developed method leverages the strength of the teacher-student-based distilled deep learning architecture for precisely identifying roadside objects. The teacher model utilizes the improved weighting factor to lower the false positives for better feature refinement. Meanwhile, the student model is equipped with convolutional attention and hierarchical feature fusion to capture balanced positional and semantic discriminative and multi-scale feature maps. Further, a gradient-based attention transfer mechanism has been utilized for significant knowledge transfer using attention maps from the teacher-to-student model to capture spatial feature information for night vision. Extensive experimental results demonstrate that the developed method overshadows the state-of-the-art object detection methods by achieving 41.45 $\\%$ , and 55.44 $\\%$ mAP and mAR, respectively. Additionally, the efficacy of the developed method has been validated by incorporating the different use cases in 6G-enabled VTS.",
        "issn": {
            "Print ISSN": "1524-9050",
            "Electronic ISSN": "1558-0016"
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "6G mobile communication",
                "Real-time systems",
                "Feature extraction",
                "Computational modeling",
                "Roads",
                "Cameras",
                "Accuracy",
                "Vehicle detection",
                "Object recognition"
            ],
            "Author Keywords": [
                "Deep learning",
                "Internet of Things",
                "6G-enabled vehicle transportation system",
                "knowledge distillation",
                "knowledge transfer"
            ]
        },
        "title": "Attention Transfer-Based Deep Distilled Architecture for 6G Driven-Smart Vehicle Transportation System"
    },
    {
        "authors": [
            "Longyu Zhou",
            "Supeng Leng",
            "Qing Wang",
            "Tony Q.S. Quek",
            "Mohsen Guizani"
        ],
        "published_in": "Published in: IEEE Communications Magazine ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/MCOM.001.2400207",
        "publisher": "IEEE",
        "abstract": "The advancement of 6G wireless communication technology and the rise of the Internet of things (IoT) has made digital twin (DT) a promising tool for unmanned aerial vehicles (UAVs)-based 6G applications such as intelligent logistics, management of smart cities, and autonomous driving. DT allows UAVs to imitate the status of physical entities in a virtual space for service provisions in the physical space. However, traditional DT solutions are challenging to accurately imitate and derive highly dynamic physical entities due to the limited computing resources of UAVs. To address this issue, we propose a cooperative DT framework to achieve a highly accurate and low-latency DT performance with a double-scale spatial DT manner. We first propose a UAV-based cooperative sensing algorithm to implement comprehensive data collection for building accurate small-scale spatial DT models. Then we propose an adaptive model parameter adjustment algorithm to improve the DT accuracy. Considering the high mobility of physical entities, we propose a model transfer algorithm to achieve a low-latency service provision. Finally, we demonstrate the effectiveness of our proposed framework using a case study of UAV-based multi-target tracking. The experiment results show that our solution achieves an accurate DT with a successful tracking ratio of up to 95 percent with a low system latency of under 1 second compared to traditional DT manners on average, respectively.",
        "issn": {
            "Print ISSN": "0163-6804",
            "Electronic ISSN": "1558-1896"
        },
        "keywords": {
            "IEEE Keywords": [
                "Accuracy",
                "Real-time systems",
                "Sensors",
                "Adaptation models",
                "6G mobile communication",
                "Digital twins",
                "Autonomous aerial vehicles",
                "Low latency communication",
                "Heuristic algorithms",
                "Data models"
            ],
            "Author Keywords": []
        },
        "title": "Cooperative Digital Twins for UAV-Based Scenarios"
    },
    {
        "authors": [
            "Partha Chakrabarti",
            "Neeraj Kumar Goyal"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "14 March 2024",
        "doi": "10.1109/TCE.2024.3377238",
        "publisher": "IEEE",
        "abstract": "IoT-enabled consumer electronics (CE) communication networks, which involve billions of connected consumer devices, can be aptly modeled as Multistate Flow Networks (MFN). In such networks, the edges (transmission links) and nodes (CE devices/ access points) are characterized by multi-valued capacity states. Evaluating the reliability of these intricate networks presents an NP-hard computational challenge as the network grows. In this study, we propose a novel algorithm founded on the Sum of Disjoint Products (SDP) concept to compute the probability of successfully transmitting d units of data from a source node to a destination node of the MFN. To validate the correctness and robustness of our approach, we illustrate the algorithm’s application using a benchmark network sourced from relevant literature. Additionally, we demonstrate the applicability and scalability of the proposed work through computational experiments, comparing it with two best-known existing MFN reliability evaluation methods. Our method surpasses both the methods by achieving a remarkable 84% and 70% reduction in the computations required to evaluate reliability.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Reliability",
                "Vectors",
                "Consumer electronics",
                "Computer network reliability",
                "Reliability theory",
                "Internet of Things",
                "Capacity planning"
            ],
            "Author Keywords": [
                "Network reliability",
                "consumer electronics",
                "multistate flow networks",
                "sum of disjoint products",
                "minimal paths"
            ]
        },
        "title": "A Novel Sum-of-Disjoint-Products Technique for Reliability Evaluation of Multistate Consumer Electronics Communication Flow Network"
    },
    {
        "authors": [
            "Leonardo Balocchi",
            "Michele Vitelli",
            "Simone Contardi",
            "Iacopo Nannipieri",
            "Paolo Bruschi",
            "Stefania Bonafoni",
            "Luca Roselli"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "04 November 2024",
        "doi": "10.1109/JSEN.2024.3487249",
        "publisher": "IEEE",
        "abstract": "This paper presents a one data wire communication protocol, developed by Sensichips s.r.l., for multi-sensor readout applications with advantages in transfer efficiency and readout speed over currently used protocols. The protocol has been named Sensibus, it is a multi-drop single master multiple slaves type asynchronous protocol, and takes some features of standard serial data buses, improving the addressing and data transfer efficiency aspects. It allows multiple real-time writings, readings and it supports beam-forming applications through sensor level programmatic delays. In the case of sensor arrays, i.e. ultrasonic sensing, the ability to perform beam-forming and beam-stearing provides a better detection in a preferential spatial direction through precise and accurately programmable delays. The Sensibus was then implemented on a cable and used for gas measurements with distributed sensors to demonstrate its actual effectiveness. This work opens the door to new applications for distributed sensing, enabling fast measurements for real-time tasks: the simplicity of the protocol makes it easily implementable and low cost, ideal for what is required by IoT standards.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Protocols",
                "Sensors",
                "Main-secondary",
                "Sensor arrays",
                "Internet of Things",
                "Registers",
                "Transmission line measurements",
                "Object recognition",
                "Writing",
                "Wire"
            ],
            "Author Keywords": [
                "Distributed Sensors",
                "Low-Cost Protocols",
                "Sensiplus",
                "Sensibus",
                "Serial Data Protocols I²C",
                "SPI"
            ]
        },
        "title": "New One Data Wire Serial Bus for Distributed Sensor Arrays"
    },
    {
        "authors": [
            "Wenfeng Wu",
            "Luping Xiang",
            "Qiang Liu",
            "Kun Yang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "30 October 2024",
        "doi": "10.1109/JIOT.2024.3477314",
        "publisher": "IEEE",
        "abstract": "In the wake of the swift evolution of technologies such as the Internet of Things (IoT), the global data landscape is undergoing an exponential surge, propelling DNA storage into the spotlight as a prospective medium for contemporary cloud storage applications. This paper introduces a Semantic Artificial Intelligence-enhanced DNA storage (SemAI-DNA) paradigm, distinguishing itself from prevalent deep learning-based methodologies through two key modifications: 1) embedding a semantic extraction module at the encoding terminus, facilitating the meticulous encoding and storage of nuanced semantic information; 2) conceiving a forethoughtful multi-reads filtering model at the decoding terminus, leveraging the inherent multi-copy propensity of DNA molecules to bolster system fault tolerance, coupled with a strategically optimized decoder’s architectural framework. Numerical results demonstrate the SemAI-DNA’s efficacy, attaining 2.61 dB Peak Signal-to-Noise Ratio (PSNR) gain and 0.13 improvement in Structural Similarity Index (SSIM) over conventional deep learning-based approaches.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "DNA",
                "Semantics",
                "Internet of Things",
                "Decoding",
                "Channel coding",
                "Image coding",
                "Sequential analysis",
                "Data mining",
                "Image reconstruction",
                "Fault tolerant systems"
            ],
            "Author Keywords": [
                "DNA storage",
                "deep learning",
                "large model",
                "multi-reads",
                "Internet of Things"
            ]
        },
        "title": "SemAI: Semantic Artificial Intelligence-enhanced DNA Storage for Internet-of-Things"
    },
    {
        "authors": [
            "Haitao Zhao",
            "Zhipeng Kong",
            "Yunxiang He",
            "Biyao Ding",
            "Hao Huang",
            "Yiyang Ni",
            "Guan Gui",
            "Hikmet Sari",
            "Fumiyuki Adachi"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "20 September 2024",
        "doi": "10.1109/JIOT.2024.3465219",
        "publisher": "IEEE",
        "abstract": "In this paper, a novel multi-user Multiple-Input Multiple-Output (MIMO) communication system for Internet of things (IoT) is proposed, where the Aerial Reconfigurable Intelligent Surface (ARIS) and Non-Orthogonal Multiple Access (NOMA) are used as the sum rate enhancement pathway. The Base Station (BS) has multiple antennas that transmit superimposed signals to multiple users. The passive ARIS serves as a flexible transmit relay to reduce path loss and improve channel gains. Users are divided into several groups based on their channel status, each sharing a Radio Frequency (RF) chain. To maximize the sum rate of all users, the placement of ARIS, the passive/active beamforming design and the power allocation among users are jointly optimized. As the joint optimization for user grouping, passive/active beamforming and power distribution is formulated as a mixed-integer non-linear program (MINLP) which is non-convex and coupled and hence, obtaining an optimal solution is challenging. In this paper, the problem is decoupled into three subproblems and solved alternately efficiently. The numerical results demonstrate that the suggested MIMO-ARIS-NOMA system can achieve higher sum rate performance than traditional schemes.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "NOMA",
                "Optimization",
                "Reconfigurable intelligent surfaces",
                "Internet of Things",
                "Autonomous aerial vehicles",
                "Radio frequency",
                "Telecommunications"
            ],
            "Author Keywords": [
                "Air reconfigurable intelligent surface",
                "non-orthogonal multiple access",
                "user grouping",
                "beamforming design",
                "power allocation"
            ]
        },
        "title": "A Joint Optimization Framework for Sum-Rate Maximization in Air Reconfigurable Intelligent Surface Assisted MIMO-NOMA Systems"
    },
    {
        "authors": [
            "Weihan Zhang",
            "Shaohua Wu",
            "Siqi Meng",
            "Jinghang He",
            "Qinyu Zhang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "18 September 2024",
        "doi": "10.1109/JIOT.2024.3463652",
        "publisher": "IEEE",
        "abstract": "Deep joint source-channel coding (DeepJSCC) has emerged as a novel technology in semantic communication, coinciding with the increasing demand for edge devices in the Internet of Things (IoT). Consequently, the deployment of DeepJSCC on edge devices has become a crucial research direction. However, DeepJSCC faces challenges related to channel fading. Moreover, implementing DeepJSCC on edge devices poses challenges due to the constrained computational resources as well as the compatibility issue between DeepJSCC and digital systems. In this paper, we devote to engineering the DeepJSCC system deployed on edge devices. First, we propose a method named DeepJSCC with Ensemble learning (DeepJSCC-ES) to resist the channel fading. Then, we present a pruning algorithm called the DeepJSCC SNR-Adaptive Pruning method (DJSAP) to make the DeepJSCC network lightweight, reducing the computational demands of on edge nodes. Further, we propose a method called the Simulated Fixed-Point Quantization training based on Soft Quantization function (SFPQSQ) to tackle the compatibility issue between DeepJSCC and digital systems. Finally, we deploy the whole DeepJSCC system on edge devices and conduct experiments to test the DeepJSCC system. The results of simulations show that the proposed DeepJSCC-ES system outperforms the baseline DeepJSCC, particularly excelling in low SNR conditions. Furthermore, the parameter size of the pruned model using DJSAP is compressed by 93.37% while the average structural similarity index metric (SSIM) decreases only by 0.92% compared with the baseline DeepJSCC. Additionally, the SFPQSQ works better than ordinary quantization methods in tackling the compatibility issue between DeepJSCC and digital systems. The experiment results also show that our proposed system can serve as a feasible solution for practical deployment on edge devices.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Quantization (signal)",
                "Semantics",
                "Rician channels",
                "Mathematical models",
                "Internet of Things",
                "Training",
                "Image reconstruction"
            ],
            "Author Keywords": [
                "deep joint source-channel coding",
                "edge devices",
                "Internet of Things",
                "the Rician fading channel",
                "lightweight model",
                "quantization"
            ]
        },
        "title": "Engineering a Lightweight Deep Joint Source-Channel Coding Based Semantic Communication System"
    },
    {
        "authors": [
            "Lin-Cong Shu",
            "Shu-Lin Sha",
            "Si-Han Yan",
            "Shan Li",
            "Xue-Qiang Ji",
            "Jia-Han Zhang",
            "Ming-Ming Jiang",
            "Wei-Hua Tang",
            "Zeng Liu"
        ],
        "published_in": "Published in: IEEE Transactions on Electron Devices ( Early Access )",
        "date_of_publication": "23 October 2024",
        "doi": "10.1109/TED.2024.3478171",
        "publisher": "IEEE",
        "abstract": "With the incorporation of indium (In), the surface energy band of (In\n0.25\nGa\n0.75\n)\n2\nO\n3\n(InGaO) bends upward, leading to mitigating electron loss. The thin MgO layer optimizes optical performance and improves resonance quality factor by reducing losses. Here, Ag nanoparticles (AgNPs) coated by a 10-nm thick MgO layer were fabricated on\n3\nInGaO films. By localized surface plasmon resonance (LSPR) effect of AgNPs, the enhanced optical properties of MgO, and high- k dielectric passivation, a high-performance solar-blind photodetector was achieved. This photodetector demonstrated high responsivity ( R ), detectivity (\nD\n∗\n), and external quantum efficiency (EQE) of 2438.8 mA/W, 2.4\n×\n10\n14\nJones, and 1193.6%, respectively, significantly improving the detection performance of the InGaO thin films fabricated by plasma enhanced chemical vapor deposition (PECVD). In addition, the InGaO@AgNPs/MgO is integrated into optical communication systems as a signal receiver and implements ASCII communication with minimal power consumption and weak light intensity. This work presents a straightforward and efficient method for significantly enhancing the performance of detectors, facilitating their applications in the fields of the Internet of Things (IoT) and optical sensing.",
        "issn": {
            "Print ISSN": "0018-9383",
            "Electronic ISSN": "1557-9646"
        },
        "keywords": {
            "IEEE Keywords": [
                "Optical sensors",
                "Optical films",
                "Optical device fabrication",
                "Absorption",
                "Optical imaging",
                "Photodetectors",
                "Photonic band gap",
                "Surface plasmons",
                "Nanoparticles",
                "Optical diffraction"
            ],
            "Author Keywords": [
                "Ag-nanoparticles",
                "indium (In)-doped Ga $_{\\text{2}}$ O $_{\\text{3}}$",
                "MgO",
                "solar-blind sensing",
                "surface plasmon"
            ]
        },
        "title": "Synergistic Contribution of Surface Plasmon and Passivation to High-Performance Solar-Blind UV Optical-Communication Sensor"
    },
    {
        "authors": [
            "Sen Fu",
            "Zhengjie Yang",
            "Chuang Hu",
            "Wei Bao"
        ],
        "published_in": "Published in: IEEE Transactions on Big Data ( Early Access )",
        "date_of_publication": "20 May 2024",
        "doi": "10.1109/TBDATA.2024.3403387",
        "publisher": "IEEE",
        "abstract": "In this paper, we propose pFedMo, a personalized federated learning algorithm with contrastive momentum. In pFedMo, we design a score function to personalize worker models by distilling knowledge from the aggregator's representation model so as to address the non-i.i.d. issue. To accelerate the convergence, we leverage the momentum acceleration on both the worker side and the aggregator side. However, the typical momentum without personalization does not suit well for the worker models with personalization, influencing convergence performance. To address this, we develop a personalized/contrastive momentum method for efficient momentum acceleration. We provide mathematical proof for the convergence of pFedMo on non-i.i.d. data. Extensive experiments based on real-world datasets and IoT system are conducted, verifying that pFedMo outperforms existing mainstream benchmarks, and achieves up to 35.90% accuracy increase and 3.64x training time speedup under a wide range of settings.",
        "issn": {
            "Electronic ISSN": "2332-7790"
        },
        "keywords": {
            "IEEE Keywords": [
                "Adaptation models",
                "Convergence",
                "Training",
                "Data models",
                "Computational modeling",
                "Federated learning",
                "Servers"
            ],
            "Author Keywords": [
                "Federated learning",
                "Edge computing",
                "Momentum",
                "Contrastive Learning"
            ]
        },
        "title": "Personalized Federated Learning with Contrastive Momentum"
    },
    {
        "authors": [
            "Mohammad Abuyaghi",
            "Samir Si-Mohammed",
            "George Shaker",
            "Catherine Rosenberg"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "29 October 2024",
        "doi": "10.1109/JIOT.2024.3487822",
        "publisher": "IEEE",
        "abstract": "As 5G networks proliferate globally, the need for accurate, reliable, and scalable positioning solutions has become increasingly critical across industries such as IoT, healthcare, and autonomous systems. This paper comprehensively reviews current and emerging positioning techniques within 5G, exploring the advancements enabled by sidelink communication, Reconfigurable Intelligent Surfaces (RIS), machine learning, and massive MIMO. We examine the evolution of 5G positioning as defined by key 3GPP releases, and provide a comparative analysis of the techniques in terms of accuracy, cost, and robustness. The review also highlights key challenges, including non-line-of-sight (NLOS) environments, real-time data processing, and security concerns, which must be addressed for widespread adoption. Finally, we discuss future directions for 5G-Advanced and 6G positioning technologies, offering insights into potential improvements and the ongoing evolution of the field.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "5G mobile communication",
                "3GPP",
                "Accuracy",
                "Surveys",
                "Sidelink",
                "Internet of Things",
                "6G mobile communication",
                "Reconfigurable intelligent surfaces",
                "Real-time systems",
                "Reviews"
            ],
            "Author Keywords": [
                "5G",
                "Positioning",
                "RIS",
                "Machine Learning",
                "Massive MIMO",
                "Beamforming",
                "Hybrid Techniques",
                "Internet of Things"
            ]
        },
        "title": "Positioning in 5G Networks: Emerging Techniques, Use Cases, and Challenges"
    },
    {
        "authors": [
            "Shaohui Yao",
            "Jingyu Cong",
            "Du Li",
            "Zhenmiao Deng"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "26 August 2024",
        "doi": "10.1109/JIOT.2024.3449408",
        "publisher": "IEEE",
        "abstract": "Traditional vital sign monitoring devices typically involve direct contact with the skin using electrodes, making them unsuitable for daily vital sign monitoring due to the discomfort and skin damage. Remote detection technology provides an effective solution to these issues. This article introduces an efficient and robust algorithm for estimating vital signs using frequency-modulated continuous-wave (FMCW) radar. The integration of this method with emerging technologies such as the Internet of Things (IoT), enables long-term and contactless vital sign monitoring, which facilitates a new model of self-management for chronic diseases and their prevention. While the breathing estimation accuracy is typically constrained by noise, heart rate estimation is primarily hindered by strong interference from the breathing signal and its higher-order harmonics. A maximum likelihood estimatior based on the Newton’s method is derived and proposed in this paper to accurately assess the breathing and heartbeat frequencies by enhancing the precision of vital sign parameter estimation. The proposed algorithm is validated utilizing a 77 GHz FMCW radar and compared with a reliable reference sensor. Experimental results from eight subjects demonstrate that the proposed method enhances the estimation accuracy, outperforming both conventional spectral estimation and other methods. Specifically, the root mean square error between the reference sensor measurements and the estimations is lower than 1 bpm for breathing rates and 1.5 bpm for heart rates. Additionally, the Bland-Altman plots demonstrate a high level of agreement between these estimations and the reference measurements.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Monitoring",
                "Medical services",
                "Radar",
                "Heart beat",
                "Cloud computing",
                "Maximum likelihood estimation",
                "Internet of Things"
            ],
            "Author Keywords": [
                "Vital sign monitoring",
                "Internet of Things",
                "FMCW radar",
                "maximum likelihood estimation",
                "Newton’s method"
            ]
        },
        "title": "Non-Contact Vital Sign Monitoring With FMCW Radar via Maximum Likelihood Estimation"
    },
    {
        "authors": [
            "Ming Yan",
            "Chien Aun Chan",
            "André F. Gygax",
            "Chunguo Li",
            "Ampalavanapillai Nirmalathas",
            "Chih-Lin I"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "20 August 2024",
        "doi": "10.1109/JIOT.2024.3446664",
        "publisher": "IEEE",
        "abstract": "Unmanned aerial vehicle (UAV)-assisted multiaccess edge computing (MEC) networks can effectively broaden the application scope of the Internet of Things (IoT) in complex scenarios such as maritime operations, military communications, and emergency commands. However, uncertain factors, such as weather changes and temporary airspace control, pose great challenges to UAV flight safety. Obstacles resulting from these uncertain factors may intersect with UAVs with preplanned flight paths, leading to accidents. Therefore, generating the optimal flight trajectory to avoid these obstacles is key in the successful operation of this fuzzy system. In this paper, we present a heuristic trajectory generation scheme for complex offshore environments that can generate optimal trajectories according to complex terrain conditions and avoid uncertain obstacles. First, we build a complex terrain model based on a three-dimensional (3D) offshore environment to simulate the conditions in UAV-assisted MEC networks. Second, we propose a network performance optimization objective function that is based on UAV characteristics. Third, we improve the existing ant colony optimization (ACO) algorithm by introducing chaotic mapping, polarizing the pheromone recording rule and implementing a simulated annealing screening mechanism to efficiently generate trajectories. Finally, we design an efficient obstacle avoidance algorithm for different combinations of obstacle regions. The simulation results show that our proposed trajectory generation scheme can efficiently avoid obstacles and significantly improve the total trajectory loss rate compared with that of baseline schemes.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Task analysis",
                "Autonomous aerial vehicles",
                "Heuristic algorithms",
                "Optimization",
                "Trajectory optimization",
                "Internet of Things",
                "Trajectory planning"
            ],
            "Author Keywords": [
                "Unmanned aerial vehicles (UAVs)",
                "trajectory generation",
                "trajectory optimization",
                "multiaccess edge computing (MEC)"
            ]
        },
        "title": "Efficient Generation of Optimal UAV Trajectories With Uncertain Obstacle Avoidance in MEC Networks"
    },
    {
        "authors": [
            "Han Zhu",
            "Jiamiao Zhan",
            "Chan-Tong Lam",
            "Bidong Chen",
            "Benjamin K. Ng"
        ],
        "published_in": "Published in: IEEE Transactions on Cognitive Communications and Networking ( Early Access )",
        "date_of_publication": "10 September 2024",
        "doi": "10.1109/TCCN.2024.3457532",
        "publisher": "IEEE",
        "abstract": "Ambient backscatter communication (AmBC) is emerging as a promising energy-saving and spectrum-efficient passive Internet of Things (IoT) technology that can be used for battery-less communication devices due to its low power consumption and cost constraints. However, in AmBC systems, recovering tag information at the reader is a challenging problem due to the difficulty in obtaining relevant channel state information (CSI). In this paper, we propose a variational Bayesian inference and machine learning (VBI-ML) based blind signal detection method, which can automatically recover tag information in AmBC systems. Firstly, two known labels are transmitted from the tag to the reader before valid data is transmitted, thus eliminating the need for CSI estimation. Secondly, we use the VBI approach with the Gaussian mixture model to obtain the real constellation information, which does not require a priori signal modulation technology to recover the tag information at the reader automatically. Using real constellation information, the signal detection problem is converted into a clustering problem. Finally, we cluster all the received signals using an improved expectation maximization algorithm in ML to learn the parameters in labeled and unlabeled signals and recover the signals. Thorough simulation results demonstrate that our proposed method performs similarly to the optimal detector with perfect CSI and outperforms traditional constellation learning methods. More critically, ML algorithms can mitigate direct link interference and simplify the number of estimated parameters.",
        "issn": {
            "Electronic ISSN": "2332-7731"
        },
        "keywords": {
            "IEEE Keywords": [
                "Radio frequency",
                "Signal detection",
                "Detectors",
                "Backscatter",
                "Clustering algorithms",
                "Internet of Things",
                "Modulation"
            ],
            "Author Keywords": [
                "Ambient backscatter communication",
                "blind signal detection",
                "expectation maximization algorithm",
                "Gaussian mixture model",
                "Internet of Things",
                "variational Bayesian inference"
            ]
        },
        "title": "Machine Learning Based Blind Signal Detection for Ambient Backscatter Communication Systems"
    },
    {
        "authors": [
            "Dingyi Zeng",
            "Yichen Xiao",
            "Wanlong Liu",
            "Huilin Du",
            "Enqi Zhang",
            "Dehao Zhang",
            "Yuchen Wang",
            "Malu Zhang",
            "Wenyu Chen"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "25 September 2024",
        "doi": "10.1109/JIOT.2024.3467284",
        "publisher": "IEEE",
        "abstract": "With the development of informatization of IOT devices, non-terrestrial networks (NTN) are becoming more and more important. NTN, including air and space networks, face challenges such as high computational complexity, bandwidth requirements, and memory constraints. An intelligent automatic modulation classification (AMC) mechanism based on neural networks plays a pivotal role in enhancing spectrum efficiency, throughput, and link reliability. Past work in AMC has evolved from likelihood-based and feature-based methods to traditional machine learning techniques and, more recently, to deep neural networks (DNNs). However, existing DNN architectures pose challenges for NTN due to high computational complexity, bandwidth requirements, and memory consumption. Addressing this problems, we proposes a spiking transformer-based model for automatic modulation classification, exploiting temporal dynamics for enhanced performance. Biologically inspired spiking neural networks enable us to exploit the sparse and binarized activation properties of spiking neurons, allowing us to build AMC models with high energy efficiency and high availability that can be used in NTN systems. Furthermore, we introduce a weight binarization method to reduce the model size, which also further reduces the bandwidth and memory requirements of AMC in NTN edge deployment. Experimental results demonstrate the superiority of our approach over state-of-the-art methods, with the binarized model achieving comparable accuracy at a fraction of the size.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Modulation",
                "Neurons",
                "Computational modeling",
                "Biological neural networks",
                "Biological system modeling",
                "Memory management",
                "Transformers"
            ],
            "Author Keywords": [
                "Non-terrestrial Networks",
                "Automatic Modulation Classification",
                "Spiking Neural Networks",
                "Binary Quantization"
            ]
        },
        "title": "Efficient Automatic Modulation Classification in Non-Terrestrial Networks With SNN-Based Transformer"
    },
    {
        "authors": [
            "Prakhar Consul",
            "Ishan Budhiraja",
            "Deepak Garg"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "04 October 2024",
        "doi": "10.1109/TCE.2024.3473971",
        "publisher": "IEEE",
        "abstract": "The computationally intensive tasks generated by AIoTD face challenges due to its limitations in battery power and computing capabilities. These devices typically operate in resource-constrained environments where energy efficiency (EE), computational efficiency, and real-time responsiveness are critical factors. To address these challenges, MEC is a promising solution as it allows tasks to be transferred from IoT devices to MEC for processing. By utilizing UAVs in MEC systems, computing and storage resources are brought closer to end devices, particularly in remote areas. This study investigates the TO issue within a AIoT-enabled UAV-assisted MEC system empowered by DT technologies. The DT concept is employed to provide environmental information and facilitate data exchange for agents linked to AIoT devices. The TO problem is structured to optimize EE and distribute workloads effectively among edge servers. Subsequently, the problem is reformulated as a MDP and an energy-efficient TO strategy, termed FedRL-EETO (FedRL based), is proposed. Moreover, a DT-enabled FedRL-EETO approach is implemented to enhance privacy protection and train decentralized RA strategies for communication networks, further improving network efficiency. Experimental evaluations are conducted to represents the benefits of the FedRL-EETO algorithm in enhancing EE and workload distribution, demonstrating its superior performance in comparison to existing method.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Servers",
                "Optimization",
                "Artificial intelligence",
                "Sensors",
                "Vehicle dynamics",
                "Training",
                "Quality of service",
                "Metaverse",
                "Sensor systems"
            ],
            "Author Keywords": [
                "Mobile edge computing",
                "Resource allocation",
                "Task offloading",
                "Unmanned aerial vehicle",
                "Internet of Things",
                "Federated reinforcement learning",
                "Digital twin"
            ]
        },
        "title": "Task Offloading in AIoT-Enabled UAV-Assisted MEC Network: A Digital Twin-Empowered Approach With FedRL"
    },
    {
        "authors": [
            "Kathiroli Raja",
            "Kottilingam Kottursamy",
            "Vishal Ravichandran",
            "Sivaganesh Balaganesh",
            "Kapal Dev",
            "Lewis Nkenyereye",
            "Gunasekaran Raja"
        ],
        "published_in": "Published in: IEEE Transactions on Vehicular Technology ( Early Access )",
        "date_of_publication": "17 April 2024",
        "doi": "10.1109/TVT.2024.3390226",
        "publisher": "IEEE",
        "abstract": "Unmanned Aerial Vehicles (UAVs) are widely used for commercial transportation and data collection in many applications. Recently, UAVs have been used as flying relays to support terrestrial cellular networks for enhancing coverage and connectivity. Multiple UAVs serve as data collectors at a common altitude in a UAV-aided Internet of Things (IoT) network. Data collected is processed by Federated Learning (FL) before being sent to central servers to protect users’ privacy and reduce communication costs. Batteries in UAVs are tiny and efficient only for a short duration, making FL challenging to execute for many iterations. Therefore, a Multi-UAV Energy Efficient Coverage Deployment algorithm based on a Spatial Adaptive Play (MUECD-SAP) is proposed in this article. MUECD uses a modified Particle Swarm Optimization (PSO) to optimize the accurate location of deployed UAVs based on the Signal-toInterference Ratio (SINR) to increase the data collection rate. Also, to make the FL run for multiple iterations, SAP dynamically allocates resources using Deep Deterministic Policy Gradient (DDPG) to optimize the energy consumed and the link latency between the user and the UAV system. The proposed MUECD algorithm outperforms all the state-of-the-art algorithms by achieving an improved data rate and balanced SINR value. The proposed SAP resource allocation strategy has improved the FL execution by 66.67% and attains a very low latency and energy consumed compared to other resource allocation techniques.",
        "issn": {
            "Print ISSN": "0018-9545",
            "Electronic ISSN": "1939-9359"
        },
        "keywords": {
            "IEEE Keywords": [
                "Autonomous aerial vehicles",
                "Internet of Things",
                "Resource management",
                "6G mobile communication",
                "Data collection",
                "Signal to noise ratio",
                "Servers"
            ],
            "Author Keywords": [
                "Unmanned Aerial Vehicles",
                "Coverage Optimization",
                "Resource Allocation",
                "Internet of Things",
                "Federated Learning",
                "Deep Reinforcement Learning"
            ]
        },
        "title": "An Efficient 6G Federated Learning-enabled Energy-Efficient Scheme for UAV Deployment"
    },
    {
        "authors": [
            "Miao Yu",
            "Hao Zhang",
            "Jun Ma",
            "Xiaoxia Duan",
            "Shuling Kang",
            "Jiaqi Li"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "01 July 2024",
        "doi": "10.1109/JIOT.2024.3420949",
        "publisher": "IEEE",
        "abstract": "With the acceleration of globalization and urbanization processes, cold chain logistics (CCL) for agricultural products plays a pivotal role in modern agricultural supply chains. This study aims to explore methods to enhance the efficiency and quality of agricultural product CCL supervision through the use of Internet of Things (IoT) technology. Addressing challenges such as real-time data acquisition difficulties, information asymmetry, and data security concerns in the agricultural CCL sector, this research develops an IoTbased agricultural CCL supervision system. This system innovatively integrates three components: a real-time data collection module, a temperature and humidity regulation module, and a logistics information privacy protection module. The latter employs an anonymous authentication based on blockchain to bolster the security and confidentiality of logistics information. The system model is simulated and evaluated by Matlab. Results demonstrate that the proposed system exhibits optimal computational efficiency, maintaining a computational overhead of approximately 0.10 seconds, which is lower than other algorithms exceeding 0.13 seconds. In terms of operational efficiency, with 250 consensus nodes, the consensus throughput remains consistently above 90 bps, while CPU utilization does not exceed 20%. Regarding data transmission security, the average delivery rate of the model exceeds 80.48%, and the average leakage rate remains below 5.72% for a data volume of 10Mb, underscoring robust network data security transmission performance. Therefore, the system model developed in this study significantly enhances the real-time accuracy and security of agricultural product CCL supervision, presenting a pioneering technical solution essential for advancing the efficiency and quality of the supply chain.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Agricultural products",
                "Internet of Things",
                "Blockchains",
                "Supply chains",
                "Real-time systems",
                "Monitoring",
                "Data privacy"
            ],
            "Author Keywords": [
                "Internet of Things technology",
                "cold chain logistics supervision",
                "real-time monitoring",
                "blockchain",
                "agricultural products"
            ]
        },
        "title": "Cold Chain Logistics Supervision of Agricultural Products Supported Using Internet of Things Technology"
    },
    {
        "authors": [
            "Jie Fang",
            "Xiongwei Wu",
            "Mengyun Xu",
            "Chunping Li",
            "Xuesong Wu",
            "Li Li"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "16 September 2024",
        "doi": "10.1109/JIOT.2024.3452422",
        "publisher": "IEEE",
        "abstract": "The widespread deployment of the Internet of Things (IoT) allows for the utilization of mobile signaling data (MSD) in cellular networks to perceive the underlying traffic states on road networks. Nevertheless, the application of MSD for traffic flow prediction can be impeded by the limited positioning accuracy of MSD and stringent privacy policies. To surmount these obstacles, we propose a traffic flow prediction method that integrates an estimation process using a macroscopic fundamental function (M) with a positional factor (P), combined with the sequence-to-sequence (S2S) neural networks model (MPS2S). First, we estimate traffic flow using a macroscopic fundamental function (M). In this approach, we introduce a positional factor (P) that fuses road inflows and outflows to capture the relationship between cellular network and road network. Next, we employ a S2S neural network model, which considers the spatial and temporal aggregation information to infer future traffic flows. Specifically, an encoder estimates traffic parameters from MSD, a decoder infers multi-step traffic flows, and a coefficients estimation method introduces the macroscopic fundamental function into the neural network. To validate the prediction performance of our proposed model, we collect video data from roadside cameras to measure ground truth values. In addition, we analyze the impact of the positional factor on our estimation method and the impact of spatial and temporal aggregation on our prediction model in real-world freeway scenarios. The results indicate that MSD can link the potential traffic information, and our method contributes to better traffic flow prediction.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Estimation",
                "Roads",
                "Cellular networks",
                "Neural networks",
                "Accuracy",
                "Long short term memory",
                "Trajectory"
            ],
            "Author Keywords": [
                "Mobile signaling data",
                "macroscopic fundamental function",
                "traffic flow prediction",
                "neural networks",
                "cellular networks"
            ]
        },
        "title": "A Macroscopic Fundamental Function Aided Neural Networks For Traffic Flows Prediction From Mobile Signaling Data"
    },
    {
        "authors": [
            "Bing Huang",
            "Xiuqing Ren",
            "Bin Zhou",
            "Zhen Zhang",
            "Xiaotao Zhou",
            "Jianmin Miao"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "16 October 2024",
        "doi": "10.1109/JIOT.2024.3478784",
        "publisher": "IEEE",
        "abstract": "The development of unmanned surface vehicles (USVs) technologies considerably promote the application of Internet of Things (IoT) in marine activities. However, frequently changed geometrical configurations and potential network congestion severely threaten system performance. Regarding this, this paper proposes an anti-competition communication mechanism based affine formation maneuver control (AFMC) scheme for the internet of USVs. Firstly, an auxiliary system is introduced to construct two-layer based formation maneuver structure, i.e., localization layer and tracking layer. In the localization layer, an intermittent interleaved period event-triggered mechanism (IIPETM) is proposed such that each individual could enjoy interleaved communication periods. Then, by establishing a backstepping-based controller in the tracking layer, formation members will sail along a trajectory generated by the localization layer, thus forming the desired geometrical configuration. The proposed method can significantly reduce network throughput and improve communication efficiency even under intermittent network connectivity. Finally, theoretical analysis and simulations are performed to validate the feasibility of the proposed method.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Throughput",
                "Location awareness",
                "Vectors",
                "Trajectory",
                "Stress",
                "Wireless communication",
                "Vehicle dynamics",
                "Reliability",
                "Event detection"
            ],
            "Author Keywords": [
                "Internet of unmanned surface vehicles",
                "affine formation maneuver",
                "anti-competition communication mechanism",
                "intermittent communication"
            ]
        },
        "title": "An Intermittent Anti-Competition Communication Mechanism based Formation Maneuvers for the Internet of Unmanned Surface Vehicles"
    },
    {
        "authors": [
            "Trever Schirmer",
            "Joel Scheuner",
            "Tobias Pfandzelter",
            "David Bermbach"
        ],
        "published_in": "Published in: IEEE Transactions on Cloud Computing ( Early Access )",
        "date_of_publication": "28 August 2024",
        "doi": "10.1109/TCC.2024.3451108",
        "publisher": "IEEE",
        "abstract": "The Function-as-a-Service (FaaS) execution model increases developer productivity by removing operational concerns such as managing hardware or software runtimes. Developers, however, still need to partition their applications into FaaS functions, which is error-prone and complex: Encapsulating only the smallest logical unit of an application as a FaaS function maximizes flexibility and reusability. Yet, it also leads to invocation overheads, additional cold starts, and may increase cost due to double billing during synchronous invocations. Conversely, deploying an entire application as a single FaaS function avoids these overheads but decreases flexibility. In this paper we present FUSIONIZE, a framework that automates optimizing for this trade-off by automatically fusing application code into an optimized multi-function composition. Developers only need to write fine-grained application code following the serverless model, while FUSIONIZE automatically fuses different parts of the application into FaaS functions, manages their interactions, and configures the underlying infrastructure. At runtime, it monitors application performance and adapts it to minimize request-response latency and costs. Real-world use cases show that FUSIONIZE can improve the deployment artifacts of the application, reducing both median request-response latency and cost of an example IoT application by more than 35%.",
        "issn": {
            "Electronic ISSN": "2168-7161"
        },
        "keywords": {
            "IEEE Keywords": [
                "Task analysis",
                "Costs",
                "Cloud computing",
                "Monitoring",
                "Runtime",
                "Optimization",
                "Load modeling"
            ],
            "Author Keywords": [
                "serverless computing",
                "FaaS",
                "function fusion",
                "cloud orchestration"
            ]
        },
        "title": "FUSIONIZE++: Improving Serverless Application Performance Using Dynamic Task Inlining and Infrastructure Optimization"
    },
    {
        "authors": [
            "Sherenaz Al-Haj Baddar",
            "Alessandro Languasco",
            "Mauro Migliardi"
        ],
        "published_in": "Published in: IEEE Transactions on Pattern Analysis and Machine Intelligence ( Early Access )",
        "date_of_publication": "31 October 2024",
        "doi": "10.1109/TPAMI.2024.3489645",
        "publisher": "IEEE",
        "abstract": "Modeling count data using suitable statistical distributions has been instrumental for analyzing the patterns it conveys. However, failing to address critical aspects, like overdispersion, jeopardizes the effectiveness of such an analysis. In this paper, overdispersed count data is modeled using the Dirichlet Multinomial (DM) distribution by maximizing its likelihood using a fixed-point iteration algorithm. This is achieved by estimating the DM distribution parameters while comparing the recent Languasco-Migliardi (LM), and the Yu-Shaw (YS) procedures, which address the well-known computational difficulties of evaluating its log-likelihood. Experiments were conducted using multiple datasets from different domains spanning polls, images, and IoT network traffic. They all showed the superiority of the LM procedure as it succeeded at estimating the DM parameters at the designated level of accuracy in all experiments, while the YS procedure failed to produce sufficiently accurate results (or any results at all) in several experiments. Moreover, the LM procedure achieved a speedup that ranged from 2-fold to 20-fold over YS.",
        "issn": {
            "Print ISSN": "0162-8828",
            "Electronic ISSN": "1939-3539"
        },
        "keywords": {
            "IEEE Keywords": [
                "Accuracy",
                "Data models",
                "Computational modeling",
                "Polynomials",
                "Convergence",
                "Instruments",
                "Vectors",
                "Telecommunication traffic",
                "Statistical distributions",
                "Reliability"
            ],
            "Author Keywords": [
                "Dirichlet multinomial distribution",
                "fixed-point iteration",
                "log-likelihood function",
                "overdispersed data",
                "Pattern analysis"
            ]
        },
        "title": "Efficient Analysis of Overdispersed Data Using an Accurate Computation of the Dirichlet Multinomial Distribution"
    },
    {
        "authors": [
            "Jia-Meng Yao",
            "Qiong Li",
            "Hao-Kun Mao",
            "Ahmed A. Abd El-Latif"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "24 September 2024",
        "doi": "10.1109/TCE.2024.3467010",
        "publisher": "IEEE",
        "abstract": "The development of emerging consumer technologies such as the Internet of Things (IoT), artificial intelligence (AI), and cloud computing brings convenience but also raises critical concerns about securing communication data. Quantum computing threatens the security of classical public key cryptography, making post-quantum data protection essential. Quantum Key Distribution (QKD) networks offer a solution, but the dependence on trusted relays raises security concerns. The existing QKD networks mainly consist of two types: measurement-device-dependent protocol based QKD networks which have a high dependence on trusted relays, and measurement-device-independent protocol based QKD networks which have a weak dependence on trusted relays but communication capability is limited. This paper proposes a Multi-Protocol Collaborative (MPC) networking cell combining both types of QKD protocol, which can reduce the dependence on trusted relays while maintaining a high communication capacity, and consequently has an enhanced practicality. An optimal topology method is also designed to further enhance the performance of the networks. Results show that communication capacity is improved 37 times compared to measurement-device-independent protocol based QKD networks. In addition, 23% reduction in the dependence on trusted relays compared to measurement-device-dependent protocol based QKD networks. By overcoming the trade-off between trust and capacity, this work significantly advances the practicalization of QKD networks in consumer electronics.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Protocols",
                "Relays",
                "Consumer electronics",
                "Security",
                "Encryption",
                "Collaboration",
                "Internet of Things"
            ],
            "Author Keywords": [
                "Quantum Key Distribution (QKD)",
                "QKD networks",
                "multi-protocol collaboration",
                "mathematical model",
                "topology optimization"
            ]
        },
        "title": "A Practical Multi-Protocol Collaborative Quantum Key Distribution Networking Scheme for Consumer Electronics Devices"
    },
    {
        "authors": [
            "Mengyi Fu",
            "Pan Wang",
            "Minyao Liu",
            "Ze Zhang",
            "Xiaokang Zhou"
        ],
        "published_in": "Published in: IEEE Transactions on Vehicular Technology ( Early Access )",
        "date_of_publication": "17 May 2024",
        "doi": "10.1109/TVT.2024.3402366",
        "publisher": "IEEE",
        "abstract": "The traditional vehicular ad hoc network (VANET) gradually evolved into the internet of vehicles (IoV), which has also become a potential target for attacks and faces security challenges in an open network environment. Intrusion detection systems (IDS) based on machine learning (ML) and deep learning (DL) are introduced to mitigate security threats. However, existing ML/DL-based IDS suffer from challenges in IoV environments. First, due to the limitations of ML/DLbased methods, classification performance is unsatisfactory when they extract only unidirectional contextual features or spatial characteristics. Second, existing research on in-vehicle network IDS often limits validation and testing to a static dataset of a single vehicle model. This approach may not adequately address diverse potential attacks in a dynamic environment. Third, few studies of hybrid IDS can simultaneously implement in-vehicle and extra-vehicle network intrusion detection. Large language models (LLM) have shown outstanding applications in fields such as natural language processing (NLP) and computer vision (CV). In particular, bidirectional encoder representations from transformers (BERT) obtain new state-of-the-art results on eleven famous NLP tasks. Consequently, this paper introduces a hybrid network IDS in IoV utilising LLM, denoted as IoV-BERT-IDS. This framework encompasses four modules: semantic extractor (SE), input embedding, IoV-BERT-IDS pre-training, and IoVBERT- IDS fine-tuning. To conform to the BERT model, the semantic extractor is introduced to transform traffic data devoid of apparent semantics into contextual semantics, comprising bidirectional and unidirectional SE. Through SE, controller area network (CAN) data is transformed into a CAN byte sentence (CBS), while extra-vehicle network traffic data is transformed into a traffic byte sentence (TBS). Additionally, two pre-training tasks, the masked byte word model (MBWM) and next byte sentence prediction (NBSP) are proposed t...",
        "issn": {
            "Print ISSN": "0018-9545",
            "Electronic ISSN": "1939-9359"
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Encoding",
                "Bidirectional control",
                "Task analysis",
                "Natural language processing",
                "Computational modeling",
                "Telecommunication traffic"
            ],
            "Author Keywords": [
                "BERT",
                "pre-training model",
                "intrusion detection system",
                "internet of vehicle",
                "large language model"
            ]
        },
        "title": "IoV-BERT-IDS: Hybrid Network Intrusion Detection System in IoV Using Large Language Models"
    },
    {
        "authors": [
            "Mohammed Amin Almaiah",
            "Aitizaz Ali",
            "Tayseer Alkhdour",
            "Abdalwali Lutfi"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "28 October 2024",
        "doi": "10.1109/ACCESS.2024.3487217",
        "publisher": "IEEE",
        "abstract": "Several cloud-based applications are used for digital healthcare data allocation, but such an approach relies on third-party software such as the cloud. Existing models mainly utilize blockchain as a data storage tool rather than a security platform. Biomedical and monitoring devices generate massive amounts of data, and the existing approach overloads the blockchain with IoT data. This research proposes blockchain as a unique method for securing patient-related data access and integrating homomorphic encryption with an end-to-end privacy-protecting system. In this research, we propose a blockchain-based architecture for identifying security threats in personal medical devices to address the existing issues related to healthcare devices. The proposed framework uses certificate authority to assign an access control token in order to access a particular session. A certificate authority is the nodes based on the reputation within the blockchain network elected through consensus protocol. Proposed framework uses dual certificate authorities, which leads to more reliability and security if one certificate authority is down. Moreover, the existing algorithm overburden the medical devices which are resource constraint such as power oriented and such approaches leads to storage and communication cost overhead. By minimizing latency, security, and data ownership, the proposed framework outperforms the existing centralized system, by comparing the framework and evaluating its performance with the benchmark models.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Blockchains",
                "Security",
                "Medical services",
                "Wireless sensor networks",
                "Intelligent sensors",
                "Transportation",
                "Smart contracts",
                "Privacy",
                "Data models"
            ],
            "Author Keywords": [
                "Authentication",
                "Blockchain",
                "Smart contracts",
                "Latency",
                "Optimization",
                "Security",
                "Healthcare"
            ]
        },
        "title": "Security Risk and Breach Detection Approach Based Blockchain for Medical Applications"
    },
    {
        "authors": [
            "Qun Tu",
            "K. Sakthidasan Sankaran",
            "V. Nagarajan"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "11 June 2024",
        "doi": "10.1109/TCE.2024.3412588",
        "publisher": "IEEE",
        "abstract": "Split learning (SL) is a privacy-focused method for training deep learning models that prevents clients and servers from sharing raw data. There are a few downsides to supervised learning, despite its numerous advantages. The need for large client-side computing resources and the possibility of data privacy compromise are two major drawbacks. Binary Sequential Split Vanilla networks are suggested in this research as a way to safely and efficiently transfer data. In addition, the proposed fix may lessen the impact of accuracy while reducing privacy breaches caused by SL damaged data. Here, each client is tasked with training a subset of the model, called a \"cut layer,\" that is smaller than the whole. The results of these cut layers are then sent to a central server, which continues the training process without having direct access to the initial client data. As an additional measure to supplement the present model and fortify privacy protection, it is also proposed including the bullet fish algorithm. This research tries to evaluate several benchmark models in relation to experimental findings by using the MUsculoskeletal RAdiographs (MURA) datasets. Lightweight applications in the IoT and Consumer Electronics sectors, particularly those involving mobile healthcare and other areas with high privacy protection concerns, may benefit from the suggested models, according to our research.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Privacy",
                "Servers",
                "Consumer electronics",
                "Deep learning",
                "Artificial neural networks",
                "Training data"
            ],
            "Author Keywords": [
                "Split learning",
                "Smart Health care",
                "Privacy preservation",
                "Bullet fish algorithm",
                "Binary Sequential split Vanilla network",
                "Consumer Electronics"
            ]
        },
        "title": "Secure Smart Health Care Data Transfer in Consumer Electronics Using Split Learning Based Binary Sequential Split Vanilla Network"
    },
    {
        "authors": [
            "Mehdi Shirichian",
            "Gholamreza Moradi",
            "Reza Sarraf Shirazi",
            "Mohammad Javad Emadi"
        ],
        "published_in": "Published in: IEEE Transactions on Vehicular Technology ( Early Access )",
        "date_of_publication": "13 August 2024",
        "doi": "10.1109/TVT.2024.3442545",
        "publisher": "IEEE",
        "abstract": "One of the main challenges to develop the Internet of Things (IoT), mobile communication and unmanned aerial vehicle (UAV) devices is the limitation of the power supply. As a promising solution, simultaneous wireless information and power transfer (SWIPT) concept has been introduced while the performance highly depends on modeling the nonlinear characteristics of the receiver. In this article, a novel nonlinear model is presented for the DC output power of the rectifier as a function of the input power and the operating frequency. The proposed model is named Entropy model since the entropy of the input power and the operating frequency of the rectifier serve as indices of the rectifier's DC output power. Also, it can be used to compare the shape of various rectifiers' conversion efficiency. In addition, the proposed model is applicable to different operating frequency bands and works for a wide range of input power. The similarity of the proposed model with the measured data is more than 98%. To maximize the total received power over the frequency band by utilizing the proposed model, we solve a non-concave optimization problem subject to minimum required signal-to-interference-plusnoise ratios (SINRs) for three different receiver structures, i.e. power splitting, time switching and separated receivers. For all receivers, the non-concave optimization problem is transformed into a tractable form. Moreover, the ratio of the power splitter in the PS is optimized during solving resource allocation problems considering its effect on the total received power and SINR. The results demonstrate that resource allocation can significantly enhance the performance by use of the proposed nonlinear model rather than the conventional linear and nonlinear ones.",
        "issn": {
            "Print ISSN": "0018-9545",
            "Electronic ISSN": "1939-9359"
        },
        "keywords": {
            "IEEE Keywords": [
                "Receivers",
                "Rectifiers",
                "Power generation",
                "OFDM",
                "Antennas",
                "Transmitters",
                "Optimization"
            ],
            "Author Keywords": [
                "Conversion efficiency",
                "nonlinear model",
                "optimization",
                "SWIPT"
            ]
        },
        "title": "A Novel Nonlinear Power Receiver Model for Resource Allocation in MIMO SWIPT Based on Input Power and Operating Frequency of Rectifier's Entropy"
    },
    {
        "authors": [
            "Hengzhou Ye",
            "Jiaming Li",
            "Qiu Lu"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "11 November 2024",
        "doi": "10.1109/ACCESS.2024.3494775",
        "publisher": "IEEE",
        "abstract": "Multi-access Edge Computing (MEC) is an emerging and promising computing paradigm that distributes computational resources closer to users at the network edge, effectively reducing both computation and communication latency. In practical Internet of Things (IoT) systems, many applications consist of interdependent subtasks. Therefore, determining how to offload these tasks while maintaining their dependencies to minimize latency becomes a challenging problem. Particularly in a dynamic environment with multiple users and MEC servers. Most existing studies rely on heuristic approaches, which lack adaptability in dynamic MEC environments, while machine learning-based methods often overlook task dependencies. Unlike previous work, our research focuses on the problem of offloading dependent tasks in multi-user, multi-MEC server scenarios. In this article, we first model the dependent task offloading problem as a Markov Decision Process (MDP). Then we propose a deep reinforcement learning (DRL)-based framework called GDDTO, with the aim of reducing task completion time. Specifically, this framework employs a Graph Convolutional Network (GCN) to extract task dependencies and dynamic MEC environment features, combined with a Double Deep Q-learning Network (DDQN) model and an optimized experience replay mechanism to select and evaluate task offloading strategies. Finally, comparative experiments demonstrate that this method significantly reduces task completion latency across various scenarios, proving its effectiveness.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Servers",
                "Heuristic algorithms",
                "Feature extraction",
                "Dynamic scheduling",
                "Computational modeling",
                "Quality of service",
                "Convolutional neural networks",
                "Approximation algorithms",
                "Resource management",
                "Multi-access edge computing"
            ],
            "Author Keywords": [
                "MEC",
                "Dependent task offloading",
                "GCN",
                "DRL"
            ]
        },
        "title": "Deep Reinforcement Learning For Dependent Task Offloading In Multi-access Edge Computing"
    },
    {
        "authors": [
            "Van Lic Tran",
            "Souebou Bouro",
            "Manh Thao Nguyen",
            "Fabien Ferrero"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "13 September 2024",
        "doi": "10.1109/JIOT.2024.3459874",
        "publisher": "IEEE",
        "abstract": "Wireless Sensor Network (WSN) position is required in many applications and the Global Positioning System (GPS) is not compatible with battery life management issues. In this article, we propose an autonomous, low-cost and energy-efficient localization solution based on LoRa wireless network. Our design uses a low-power relay system to extend the coverage of a typical LoR-WAN gateway to improve the accuracy of Received Signal Strength Indicator (RSSI) based approaches. We then apply Artificial Intelligence (AI) techniques, mainly regression algorithms, to obtain accurate location estimates of the target node. The proposed ML-MTL (Machine Learning-Multilateration) algorithm-based localization approach better meets the demands of IoT (Internet of Things) applications and could achieve accuracy comparable to more intricate and energy intensive methods like Angle of Arrival (AoA), Time of Arrivals (ToA), and Time Difference of Arrive (TDoA).",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Relays",
                "Wireless sensor networks",
                "Location awareness",
                "LoRaWAN",
                "Uplink",
                "Logic gates",
                "Internet of Things"
            ],
            "Author Keywords": [
                "Localization",
                "Wireless Sensor Networks",
                "LoRa/Lorawan",
                "Low Power Consumption",
                "Artificial Intelligence"
            ]
        },
        "title": "A Novel Localization Technique in LoRa Based Low-Power Relay Using Machine Learning"
    },
    {
        "authors": [
            "Shuaifan Xia",
            "Qingwei Jiang",
            "Wen Fang",
            "Qingwen Liu",
            "Shengli Zhou",
            "Mingqing Liu",
            "Mingliang Xiong"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "30 August 2024",
        "doi": "10.1109/JIOT.2024.3452121",
        "publisher": "IEEE",
        "abstract": "The rapid expansion of the Internet of Things (IoT) necessitates robust solutions for charging and communicating with a multitude of devices, making simultaneous wireless information and power transfer (SWIPT) technology increasingly vital. However, existing methods can hardly provide high charging power, great channel capacity, and flexible mobility at the same time. This manuscript introduces a millimeter wave resonant beam system for SWIPT (mmRB-SWIPT), leveraging retro-directive antenna arrays to enable automatic beam alignment and enhanced transmission efficiency without additional controls. A dual-frequency design allows the system to operate in a frequency division duplex mode, thereby resolving the echo interference issues encountered in prior resonant beam systems. Analytical models are developed to evaluate the system’s viability and performance, with numerical analysis indicating the capability to transmit watt-level power and achieve 4.8bps/Hz of spectral efficiency in indoor settings.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Simultaneous wireless information and power transfer",
                "Resonance",
                "Laser beams",
                "Millimeter wave communication",
                "Echo interference",
                "Downlink"
            ],
            "Author Keywords": [
                "Resonant beam system",
                "retro-directive antenna",
                "Simultaneous wireless information and power transfer",
                "Millimeter wave"
            ]
        },
        "title": "Millimeter-Wave Resonant Beam SWIPT"
    },
    {
        "authors": [
            "Dongxiao Zhao",
            "Dawei Zhang",
            "Qingqi Pei",
            "Lei Liu",
            "Peixin Yue"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "09 September 2024",
        "doi": "10.1109/JIOT.2024.3455425",
        "publisher": "IEEE",
        "abstract": "The traditional data security systems have the problems such as poor adaptability, technical barriers, and closed interfaces, which cannot meet the development requirements of Beyond 5G (B5G) and the Internet of Things (IoT). In this paper, we design a novel deployment network framework for adaptive security by using blockchain technology in the Software Defined Network (SDN)-enabled Mobile Edge Computing (MEC) system. The blockchain is deployed on multiple SDN servers, ensuring decision consistency and data security. The distributed SDN controllers can schedule and combine atomic security functions (ASFs) from the security resource pool of the MEC system to provide comprehensive security services. Furthermore, we developed a Multi-Agent Deep Deterministic Policy Gradient (MADDPG) scheduling optimization algorithm to enhance the utility of our model while optimizing latency and energy cost. Simulations indicate that the algorithm successfully maximizes the overall utility of the MEC system, while adhering to the constraints of latency and energy cost.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Security",
                "Blockchains",
                "Network security",
                "Firewalls (computing)",
                "Resource management",
                "Optimization",
                "Costs"
            ],
            "Author Keywords": [
                "Blockchain",
                "MEC",
                "SDN",
                "security policy deployment",
                "resource allocation"
            ]
        },
        "title": "Blockchain-Based Security Deployment and Resource Allocation in SDN-Enabled MEC System"
    },
    {
        "authors": [
            "Qi Wu",
            "Zengke Li",
            "Yanlong Liu",
            "Kefan Shao"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "11 October 2024",
        "doi": "10.1109/JIOT.2024.3478329",
        "publisher": "IEEE",
        "abstract": "The development of the Internet of Things (IoT) relies on completing the ubiquitous positioning system, but the pedestrian-oriented indoor location-based services (POILBS) have been an unresolved problem. One of the leading solutions for POILBS involves using smartphones to sense information and employing magnetic field matching (MFM) and pedestrian dead reckoning (PDR). However, this technique has several issues, such as a lack of robust orientation observation, the influence of equipment diversity difference, and the limitation of Kalman filtering (KF) based on constant noise, making the technique face difficulties in achieving high-precision positioning. This study performs experimental research to enhance the adaptive controllability of the positioning system in dynamic application scenes to address these issues, exploring more available information with limited information sources. We proposed four improved methods: MFM improved by integrated weights, adaptive KF enhanced by noise indicator, orientation correction-based KF, and recalculation trajectory recalculated by fitted orientation. The proposed method is conducted in four tailored tests using two public and one personal datasets, and compared to six other research methods to verify its performance. The results show that the root mean square error (RMSE) of three major proposed methods is decreased by 14.66%, 25.18%, and 31.54% compared to that of constant model method, respectively, proving that the proposed methods have better positioning performance and stronger robustness.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Fingerprint recognition",
                "Accuracy",
                "Noise",
                "Pedestrians",
                "Internet of Things",
                "Magnetic separation",
                "Sensors",
                "Magnetometers",
                "Magnetic field measurement",
                "Trajectory"
            ],
            "Author Keywords": [
                "Adaptive Kalman filter",
                "indoor positioning",
                "integrated weight",
                "orientation correction",
                "trajectory recalculation"
            ]
        },
        "title": "Motion Data and Multi-Scene Feature Enhanced Magnetic/Inertia Positioning Method for Pedestrian"
    },
    {
        "authors": [
            "G. Oudi Ghadim",
            "M. Dakhilalian",
            "P. Rastegari",
            "F. Hendessi",
            "W. Susilo"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "30 October 2024",
        "doi": "10.1109/ACCESS.2024.3488570",
        "publisher": "IEEE",
        "abstract": "The Internet of Vehicles (IoV), a subset of the Internet of Things (IoT) within the transportation sector, enhances driving safety and comfort by utilizing intelligent transportation systems to facilitate communication among vehicles and other entities. In this context, communication extends beyond vehicles and roadside units (RSUs), supporting interactions between vehicles, mobile devices, sensors, and infrastructure. Ensuring the security of vehicular communications is essential in the IoV paradigm. Recently, Limbasiya et al. proposed secure and efficient communication protocols for IoV, claiming their protocols are resilient against various known attacks. However, our paper identifies significant vulnerabilities in Limbasiya et al.’s protocols, including susceptibility to identity guessing, impersonation, password guessing, and man-in-the-middle attacks. Furthermore, these protocols lack confidentiality, unlinkability, and the ability for the IoV server to trace malicious vehicle users when necessary. To address these drawbacks, we introduce improved protocols for secure communications within the IoV framework. Afterwards, we assess the security of our proposals through both informal and formal analyses, employing the RoR model, BAN logic, the ProVerif, Scyther, and Tamarin tools.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Protocols",
                "Security",
                "Elliptic curve cryptography",
                "Authentication",
                "Cryptography",
                "Vehicular ad hoc networks",
                "Privacy",
                "Libraries",
                "Impersonation attacks",
                "Computational efficiency",
                "Authentication",
                "Cryptography"
            ],
            "Author Keywords": [
                "Authentication",
                "Cryptanalysis",
                "IoV",
                "Message communication",
                "Privacy",
                "Security"
            ]
        },
        "title": "On Secure and Energy-Efficient Message Communication Protocols for Internet of Vehicles: Cryptanalysis and Improvements"
    },
    {
        "authors": [
            "Samira Hosseini",
            "Abdulsalam Yassine",
            "M. Shamim Hossain"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "30 August 2024",
        "doi": "10.1109/JIOT.2024.3446863",
        "publisher": "IEEE",
        "abstract": "Artificial Intelligence of Vehicles (AIV) is poised to revolutionize transportation by promoting low-carbon alternatives such as Electric Vehicles (EVs). However, the deployment of Fixed Charging Stations (FCSs) lags behind the growing demand, particularly in rural areas, causing range anxiety among potential EV owners. This paper proposes a smart transportation solution within the Artificial Intelligence of Things (AIoT) framework to establish a sustainable, low-carbon system. AIoT systems enable real-time data acquisition and analysis through extensive embedded IoT EV sensors and communication networks for pattern recognition and decision making on the cloud. The proposed solution integrates sensor information from Vehicle-to-Vehicle (V2V) charging, smart Home Charging Stations (HCS), and Mobile Charging Services (MCS), coordinated by cloud-fog nodes in geographically distributed zones. This paper employs the Hungarian matching algorithm for optimal decision-making of matching EVs with charging services. Our approach incorporates AIV and AIoT technologies to enhance decision making by using an ensemble-based Machine Learning (ML) model for precise EV range estimation. The comprehensive details and specifications of these proposed models are elaborated in this paper.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Costs",
                "Internet of Things",
                "Decision making",
                "Smart transportation",
                "Energy exchange",
                "Charging stations",
                "Batteries"
            ],
            "Author Keywords": [
                "Internet of Electric Vehicle",
                "Machine Learning",
                "Low-carbon Sustainable Transportation",
                "Charging Service",
                "Estimation"
            ]
        },
        "title": "Optimizing Electric Vehicle Charging through an Artificial Intelligence Mechanism for Smart Transportation"
    },
    {
        "authors": [
            "Chen He",
            "Huixu Luan",
            "Z. Jane Wang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "27 September 2024",
        "doi": "10.1109/JIOT.2024.3467371",
        "publisher": "IEEE",
        "abstract": "Backscatter communications is receiving increasing attention in internet of things (IoT), while one major challenge for backscatter communications is that the circuit complexity for high performance tag is also high. In this paper, for dual-antenna backscatter tag, we propose a zero-padding space-time block code (ZPSTBC) that can simultaneously improve the tag performance and reduce the complexity of the tag circuit. An interesting thing is that padding zero into space-time block code (STBC) is not beneficial in conventional MIMO channels, but may lead to performance improvement in MIMO backscatter channels, from perspectives of error rate and energy harvesting, and also reduce the circuit complexity of the tag. This is due to characteristic of the MIMO structures and tag circuit of backscatter communications. We provide rigorous mathematical analysis and numerical simulations to illustrate the performance improvement and the tag complexity reduction caused by the proposed ZPSTBC.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Backscatter",
                "Antennas",
                "Energy harvesting",
                "Complexity theory",
                "Codes",
                "Transmitting antennas",
                "Symbols",
                "Internet of Things",
                "Encoding",
                "Block codes"
            ],
            "Author Keywords": [
                "backscatter communications",
                "space-time block code (STBC)",
                "energy harvesting",
                "backscatter tag circuit"
            ]
        },
        "title": "Zero-Padding Space-Time Block Code for Dual-Antenna Backscatter Tag"
    },
    {
        "authors": [
            "Zhedong Wang",
            "Chao Qian",
            "Yasir Saifullah",
            "Yuan Fang",
            "Er-Ping Li",
            "Hongsheng Chen"
        ],
        "published_in": "Published in: IEEE Transactions on Antennas and Propagation ( Early Access )",
        "date_of_publication": "25 October 2024",
        "doi": "10.1109/TAP.2024.3484014",
        "publisher": "IEEE",
        "abstract": "On-demand manipulation of electromagnetic (EM) far-fields has greatly accelerated the extension of intelligent metasurfaces in diverse applications. An important prerequisite for most of the effective utilizations boils down to the precise and expeditious determination of far-field. However, the existence of one-to-many mapping between the far-field space and metasurface space renders the inverse-design neural network hard to converge and ill-posed, especially in the case of phaseless far-field. In the work, we introduce an invertible neural network (INN) based generation-elimination framework. By introducing extra trainable latent variables in inverse training, the model can largely mitigate the impact by the phaseless far-field, which is the chief culprit of the one-to-many issue. The INN also provides a powerful stochastically generative capability based on trained latent space. An elimination network is cascaded to INN, where groups of nominated patterns from INN will be screen and eliminate the inferior metasurface patterns. Both simulated and experimental results demonstrate that the far-field match degrees are improved by up to 50%, compared with the case of general deep neural network (DNN), meanwhile retaining a competitive process cycle within 1 ms. This cost-effective and rapid scheme can be deployed on metasurfaces with larger scale to significantly improve signal strength and the quality of wireless communication for Internet of Things (IoT) devices.",
        "issn": {
            "Print ISSN": "0018-926X",
            "Electronic ISSN": "1558-2221"
        },
        "keywords": {
            "IEEE Keywords": [
                "Metasurfaces",
                "Training",
                "Deep learning",
                "Vectors",
                "Photonics",
                "Numerical models",
                "Jacobian matrices",
                "Internet of Things",
                "Couplings",
                "Computer architecture"
            ],
            "Author Keywords": [
                "inverse design",
                "intelligent metasurfaces",
                "invertible neural network"
            ]
        },
        "title": "Robust Large-scale Metasurface Inverse Design Using Phaseless Data with Invertible Neural Network"
    },
    {
        "authors": [
            "Venkatesan C",
            "Jeevanantham S",
            "Rebekka B"
        ],
        "published_in": "Published in: IEEE Transactions on Network and Service Management ( Early Access )",
        "date_of_publication": "06 November 2024",
        "doi": "10.1109/TNSM.2024.3492234",
        "publisher": "IEEE",
        "abstract": "With the surging magnification of IoT applications, the low latency and security are the major challenges. To counteract these challenges, the integration of blockchain in edge network is mandated to offer reduced network overhead, transparency, decentralization and fault tolerance. The respective network nodes are capable of collaborating effectively through blockchain consensus mechanism. However, the major bottleneck is the complexity of consensus mechanism. Hence, to perform efficient realization of blockchain in edge network, we propose an exclusive idea of integrating Physical Layer Authentication (PLA) with consensus to put forth PoPLA consensus mechanism. Also, we propose a BLF approach to deliver byzantine tolerance with reduced communication complexity. The experimental evaluation of PoPLA is performed on LoRa WAN. The PoPLA reduces the mining time by 76.25%, 42.13% and 30.05% in comparison with PoW, PBFT and RAFT respectively. The BLF enables PoPLA to outperform PBFT by reducing communication complexity from O(N2) to O(N) while increasing BFT level from 33% to 50% faulty nodes. The cryptanalysis showcases that the PoPLA is resistant to security attacks in blockchain network at edge.",
        "issn": {
            "Electronic ISSN": "1932-4537"
        },
        "keywords": {
            "IEEE Keywords": [
                "Consensus protocol",
                "Fault tolerant systems",
                "Fault tolerance",
                "Peer-to-peer computing",
                "Internet of Things",
                "Physical layer",
                "Authentication",
                "Hardware",
                "Filtering",
                "Costs"
            ],
            "Author Keywords": [
                "Blockchain",
                "Consensus mechanism",
                "Physical Layer Authentication",
                "Edge computing"
            ]
        },
        "title": "A lightweight Physical Layer Authentication-Based Blockchain Consensus Mechanism for Edge Networks"
    },
    {
        "authors": [
            "Khalid Haseeb",
            "Amjad Rehman",
            "Tanzila Saba",
            "Huihui Wang",
            "Fahad F. Alruwaili"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "18 September 2024",
        "doi": "10.1109/JIOT.2024.3462982",
        "publisher": "IEEE",
        "abstract": "By exploring the future network, smart technologies promote the development of cutting-edge industrial applications. Internet of Things (IoT) systems use sensing approaches to acquire data and control real-time processing and complex tasks. Several techniques have been proposed for coping with environmental behavior in industrial management and reducing the response in crucial circumstances. However, due to the unique and limited constraints of the industrial environment, managing data routing and sustainable development are recent research concerns. In addition, security is essential for industrial communication systems due to the probability of unauthorized access, thus trust level must be improved. The framework addresses real-world challenges in industrial networks by incorporating a lightweight data verification algorithm designed for green communication, reducing energy consumption while maintaining data integrity. Moreover, a lightweight data verification algorithm for green communication is also developed. Firstly, predictive computing is implemented using Ant Colony Optimization (ACO) based on real-time requirements and selects the dynamic and communication channels for data transmission across the industrial platform. Second, mobile sinks offer more authentic techniques for verifying sensor data and delivering it securely to cloud servers. The framework was evaluated and validated in a simulation-based environment, revealing a considerable improvement in terms of network throughput, packet drop ratio, connectivity ratio, and network overhead over the existing approaches.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Routing",
                "Reliability",
                "Artificial intelligence",
                "Wireless sensor networks",
                "Real-time systems",
                "Wireless communication"
            ],
            "Author Keywords": [
                "complex networks",
                "technological development",
                "ant colony optimization",
                "information security",
                "sustainable applications"
            ]
        },
        "title": "Empowering Real-Time Data Optimizing Framework Using Artificial Intelligence of Things for Sustainable Computing"
    },
    {
        "authors": [
            "Bo Shen",
            "Qian Ma",
            "Gang Yang",
            "Ru Wang",
            "Wen Ji"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "25 October 2024",
        "doi": "10.1109/JIOT.2024.3486351",
        "publisher": "IEEE",
        "abstract": "Artificial Intelligence of Things (AIoT) aims to build a self-learning, self-adaptive, and self-evolving IoT ecosystem, which has facilitated many promising intelligent services. Data is an important foundational element for many applications. Establishing a well-designed trading mechanism to collect the necessary data from various sources is essential to realize the vision of AIoT. In the paper we investigate the data trading incentive mechanism between multi-providers and multi-buyers for AIoT. To address the two-sided dilemma, we develop a joint optimization game to maximize the payoff of all market participants. A two-layer Stackelberg evolutionary game theoretic framework is developed to divide the optimization problem into two sub-problems: one for data pricing by providers and the other for purchasing decisions by buyers. The sub-problem of optimal data pricing for providers is modeled as a non-cooperative game. Providers utilize the game’s equilibrium solution to dynamically modify their pricing strategies in response to a changing competitive environment and demanding. Because buyers have limited information, their behaviors are modeled via evolutionary game. By encouraging data providers to take the buyers’ evolutionary dynamics into account has the potential to overcome the myopia behaviors. The equilibrium solution is obtained via replicator dynamics. Extensive experiments demonstrate the efficacy and efficiency of the proposed hierarchical interaction framework. Overall, our results show the proposed Stackelberg evolutionary game framework establishes a desired data market and achieves higher long-term revenue for both sides of participants in the market. The hierarchical framework can effectively prompts data trading in the market.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Pricing",
                "Games",
                "Artificial intelligence",
                "Data models",
                "Computational modeling",
                "Wireless communication",
                "Base stations",
                "Real-time systems",
                "Optimization"
            ],
            "Author Keywords": [
                "AIoT",
                "evolutionary stable strategy",
                "hierarchical framework",
                "data market",
                "Stackelberg evolutionary game"
            ]
        },
        "title": "A Stackelberg Evolutionary Game Theoretic Framework for Dynamical Data Trading in Artificial Intelligence of Things (AIoT)"
    },
    {
        "authors": [
            "Wentao Dong",
            "Kaiqi Sheng",
            "Bo Huang",
            "Kun Xiong",
            "Kun Liu",
            "Xiao Cheng"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "26 September 2024",
        "doi": "10.1109/JSEN.2024.3464633",
        "publisher": "IEEE",
        "abstract": "Flexible and stretchable self-powered sensors are widely applied to Internet of the Things (IoT), human motion monitoring, and human robot interaction (HRI), and it reveals more and more attentions due to the flexibility and self-powered function. The paper focuses on the stretchable triboelectric nanogenerator (TENG) sensor for human gesture monitoring and HRI application based on conductive ionic gels and machine learning. Stretchable self-powered TENG sensor is fabricated by solution method based on Polyacrylamide (PAAM)/NaCl conductive hydrogel and silicone rubber. Self-powered triboelectric nanogenerator sensor array (TENG-SA) is applied to gesture detection at different parts (wrist, elbow, knee and neck) of human body successfully without external power supply. Conductive PAAM/NaCl hydrogel could be stretched up to 307%, which satisfies the deformation of the human joints. Intelligent sensing system is designed and developed to control the motion of two-wheeled robot through TENG-SA and long and short time memory (LSTM) neural network via Bluetooth. Experiments have demonstrated that the neck motion signals are collected and recognized by intelligent TENG-SA system with LSTM neural network during the neck rotation process, which is applied to control the motion of two-wheeled robot successfully. HRI based on self-powered TENG sensor will promote the interaction among humans and machines/robots to improve the disabled for rehabilitation, medical monitoring and human-robot cooperation.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Hydrogels",
                "Rubber",
                "Robot sensing systems",
                "Monitoring",
                "Neck",
                "Human-robot interaction",
                "Electrodes",
                "Sensor arrays",
                "Long short term memory"
            ],
            "Author Keywords": [
                "Triboelectric nanogenerator sensor array (TENG-SA)",
                "self-powered sensor",
                "conductive ionic gels",
                "LSTM neural network",
                "human robot interaction"
            ]
        },
        "title": "Stretchable self-powered TENG sensor array for human robot interaction based on conductive ionic gels and LSTM neural network"
    },
    {
        "authors": [
            "Xiaoyue Ji",
            "Zhekang Dong",
            "Liyan Zhu",
            "Chenhao Hu",
            "Chun Sing Lai"
        ],
        "published_in": "Published in: IEEE Journal of Biomedical and Health Informatics ( Early Access )",
        "date_of_publication": "23 April 2024",
        "doi": "10.1109/JBHI.2024.3392648",
        "publisher": "IEEE",
        "abstract": "Human activity recognition has played a crucial role in healthcare information systems due to the fast adoption of artificial intelligence (AI) and the internet of thing (IoT). Most of the existing methods are still limited by computational energy, transmission latency, and computing speed. To address these challenges, we develop an efficient human activity recognition in-memory computing architecture for healthcare monitoring. Specifically, a mechanism-oriented model of Ag/a-Carbon/Ag memristor is designed, serving as the core circuit component of the proposed in-memory computing system. Then, one-transistor-two-memristor (1T2M) crossbar array is proposed to perform high-efficiency multiply-accumulate (MAC) operation and high-density memory in the proposed scheme. To facilitate understanding of the proposed efficient human activity recognition in-memory computing design, self-attention ConvLSTM module, multi-head convolutional attention module, and recognition module are proposed. Furthermore, the proposed system is applied to perform human activity recognition, which contains eleven different human activities, including five different postural falls, and six basic daily activities. The experimental results show that the proposed system has advantages in recognition performance (≥ 0.20% accuracy, ≥ 1.10% F1-score) and time consumption (approximately 8∼10 times speed up) compared to existing methods, indicating an advancement in smart healthcare applications.",
        "issn": {
            "Print ISSN": "2168-2194",
            "Electronic ISSN": "2168-2208"
        },
        "keywords": {
            "IEEE Keywords": [
                "Memristors",
                "In-memory computing",
                "Human activity recognition",
                "Computer architecture",
                "Medical services",
                "Bioinformatics",
                "Monitoring"
            ],
            "Author Keywords": [
                "Human activity recognition",
                "in-memory computing",
                "memristor",
                "healthcare monitoring"
            ]
        },
        "title": "An Efficient Human Activity Recognition In-Memory Computing Architecture Development for Healthcare Monitoring"
    },
    {
        "authors": [
            "Zhuofan Liao",
            "Xiyu Han",
            "Xiaoyong Tang",
            "Chaochao Feng"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "20 September 2024",
        "doi": "10.1109/JIOT.2024.3464641",
        "publisher": "IEEE",
        "abstract": "Multi-access Edge Computing (MEC) is extensively utilized within the Internet of Things (IoT), wherein end-users pay services to meet the latency demands of their respective tasks. The pricing is impacted not solely by the quantity of data offloaded by the user but also associated with the leased computing and communication resources. Nevertheless, prevailing pricing strategies seldom account for the personalized resource requisites during user offloading. In this paper, we present an adaptive pricing-oriented approach for concomitant task offloading and resource allocation, considering hybrid resources, comprising two key components. Firstly, we propose a differential pricing framework for communication and computation resources, where the unit price will be influenced by the proportion of resources rented by users. Subsequently, we design a two-stage Stackelberg game model: employing convex optimization theory to mitigate problem intricacies and employing gradient descent to ascertain the potentially optimal price, thus achieving a balance between minimizing user expenses and maximizing server profitability. Simulation outcomes demonstrate that our approach slashes user costs by 23.3% and enhances average server revenue by 65.6% compared to a flat pricing model with a high user request rate (five user-initiated requests per 100 ms). This maintains server occupancy within 60% to 80%, thereby alleviating user queuing and refining user Quality of Experience (QoE).",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Pricing",
                "Resource management",
                "Servers",
                "Costs",
                "Internet of Things",
                "Optimization",
                "Games"
            ],
            "Author Keywords": [
                "Multiple-access Edge Computing (MEC)",
                "Stackelberg game",
                "offloading decision",
                "pricing",
                "computational offloading",
                "resources allocation"
            ]
        },
        "title": "An Adaptable Pricing-Based Resource Allocation Scheme Considering Users Offloading Needs in Edge Computing"
    },
    {
        "authors": [
            "Weilong Ding",
            "Tianpu Zhang",
            "Honghao Gao",
            "Qi Yu",
            "Jianwu Wang",
            "Zhuofeng Zhao"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "18 June 2024",
        "doi": "10.1109/TCE.2024.3415615",
        "publisher": "IEEE",
        "abstract": "The proliferation of Internet of Things (IoT) and the adoption of Edge Computing paradigms have led to a substantial increase in data volume. Data imbalance is inevitable in distributed environment composed of edge nodes, due to inherent diversity in processing capabilities, network connectivity, and geographical distribution. In specific highway domain, such edge derived data makes predictive precision of crucial traffic flow hard to guarantee. First, data imbalance would deteriorate the predictive performance at certain locations. Second, various spatial dependencies existed in highway network have not been sufficiently explored. To address these challenges in highway domain, we propose a novel method for daily traffic flow prediction on imbalanced data. On the one hand, a pre-processing strategy is presented to normalize traffic flow data in long-tail distribution. The data is collected from highway electronics through edge nodes. On the other hand, a multi-graph convolution model is designed to capture spatio-temporal features in distinct physical, statistical and latent perspectives. Incorporating external characteristics like meteorological and calendar features, prediction can be achieved precisely. Through extensive experiments and case studies conducted on a Chinese provincial highway, our method demonstrates a significant improvement of predictive accuracy than baselines and outstanding effects in a practical project.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Road transportation",
                "Data models",
                "Topology",
                "Accuracy",
                "Network topology",
                "Internet of Things",
                "Deep learning"
            ],
            "Author Keywords": [
                "Traffic flow prediction",
                "Data imbalance",
                "Spatio-temporal features",
                "Multi-graph fusion",
                "Graph convolutional network"
            ]
        },
        "title": "Multi-Graph Spatio-Temporal Convolution for Traffic Flow Prediction Focusing on Edge Derived Imbalanced Data From Highway Electronics"
    },
    {
        "authors": [
            "Jianwei Liu",
            "Xinyue Fang",
            "Yike Chen",
            "Jiantao Yuan",
            "Guanding Yu",
            "Jinsong Han"
        ],
        "published_in": "Published in: IEEE Transactions on Mobile Computing ( Early Access )",
        "date_of_publication": "21 October 2024",
        "doi": "10.1109/TMC.2024.3483550",
        "publisher": "IEEE",
        "abstract": "For safety guard and crime prevention, video surveillance systems have been pervasively deployed in many security-critical scenarios, such as the residence, retail stores, and banks. However, these systems could be infiltrated by the adversary and the video streams would be modified or replaced, i.e., under the video forgery attack. The prevalence of Internet of Things (IoT) devices and the emergence of Deepfake- like techniques severely emphasize the vulnerability of video surveillance systems under such attacks. To secure existing surveillance systems, in this paper we propose a vision-WiFi cross-modal video forgery detection system, namely WiSil . Leveraging a theoretical model based on the principle of signal propagation, WiSil constructs wave front information of the object in the monitoring area from WiFi signals. With a well-designed deep learning network, WiSil further recovers silhouettes from the wave front information. Based on a Siamese network-based semantic feature extractor, WiSil can eventually determine whether a frame is manipulated by comparing the semantic feature vectors extracted from the video's silhouette with those extracted from the WiFi's silhouette. We enhance the basic version of WiSil  [1] by developing a model compression method and a forgery trace localization method. Extensive experiments show that WiSil achieves 95%+ accuracy in detecting tampered frames.",
        "issn": {
            "Print ISSN": "1536-1233",
            "Electronic ISSN": "1558-0660"
        },
        "keywords": {
            "IEEE Keywords": [
                "Wireless fidelity",
                "Forgery",
                "Streaming media",
                "Feature extraction",
                "Real-time systems",
                "Cameras",
                "Video surveillance",
                "Forensics",
                "Data preprocessing",
                "Accuracy"
            ],
            "Author Keywords": [
                "Deep learning",
                "video forgery detection",
                "WiFi sensing"
            ]
        },
        "title": "Real-Time Video Forgery Detection Via Vision-WiFi Silhouette Correspondence"
    },
    {
        "authors": [
            "Bin Qian",
            "Yubo Xuan",
            "Di Wu",
            "Zhenyu Wen",
            "Renyu Yang",
            "Shibo He",
            "Jiming Chen",
            "Rajiv Ranjan"
        ],
        "published_in": "Published in: IEEE Network ( Early Access )",
        "date_of_publication": "09 May 2024",
        "doi": "10.1109/MNET.2024.3398724",
        "publisher": "IEEE",
        "abstract": "Streaming video analytics focuses on the real-time analysis of streaming video data from multiple resources, such as security cameras, and IoT devices with video capabilities. It involves applications of various techniques to extract valuable information from live video streams. Edge computing and cloud computing facilitate video stream analytics by utilizing computation resources across both ends, enabling both high accuracy and low latency. However, video streaming behaviours are dynamic and constantly evolving across the edge and the cloud. The network conditions, computing resources, and video content can change rapidly, making it crucial to continuously adjust the analytics methods to provide accurate results. Previous works both based on deep neural networks (DNNs) or heuristic algorithms learn a suitable deployment plan for streaming video analytics applications from historical data or synthetic data and therefore are not able to capture the dynamics. Hence, we propose reinforcement learning-based methods that can adapt to ongoing changes in video streaming behaviours. To ensure the scalability of video analytics in distributed environments, we implement OSMOTICGATE2, a distributed streaming video analytics system that features optimized processing pipelines and multi-agent RL-based controllers for fast adapting the system configurations across the edge and the cloud. Experiments on a real testbed show that our method outperforms baselines, assuring real-time video analysis and high accuracy in dynamic and distributed environments.",
        "issn": {
            "Print ISSN": "0890-8044",
            "Electronic ISSN": "1558-156X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Streaming media",
                "Visual analytics",
                "Cloud computing",
                "Servers",
                "Streams",
                "Real-time systems",
                "Delays"
            ],
            "Author Keywords": [
                "real-time video analytics",
                "distributed system",
                "multiagent deep reinforcement learning",
                "edge cloud collaboration"
            ]
        },
        "title": "Edge-Cloud Collaborative Streaming Video Analytics with Multi-agent Deep Reinforcement Learning"
    },
    {
        "authors": [
            "Sreeram Sivadasan",
            "Nagarajan Govindan",
            "Amal Dev Parakkat"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "21 October 2024",
        "doi": "10.1109/ACCESS.2024.3483894",
        "publisher": "IEEE",
        "abstract": "Wireless Sensor Networks (WSNs) are critical for various applications ranging from environment monitoring to industrial monitoring. The varying and continuously growing interest in this field demands an understanding of the sensor node distribution to ensure robustness and to improve resource utilization for data processing and decision making. In this paper, we focus on reconstructing the boundaries of a wireless sensor network, which also has a lot of applications in IoT and Robotics. As these sensor node locations can be considered as a set of points in the 2D plane, boundary detection of a WSN can be related to classical shape reconstruction problem in Computational Geometry. In this paper, we extend a simple and generic strategy for hole detection to a geometric solution for boundary/shape reconstruction. Furthermore, we introduce a simple and controllable heuristic algorithm to patch the coverage holes identified by our boundary reconstruction algorithm. Not only does this study improve the reliability of WSNs, but it also provides a useful tool for the extensive domain of computational geometry and shape analysis. Our different experiments show that the proposed reconstruction algorithm outperforms the existing state-of-the-art methods, and hole patching gives a simple and controllable solution for mobile node placement.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape measurement",
                "Wireless sensor networks",
                "Sensors",
                "Reconstruction algorithms",
                "Surveys",
                "Object recognition",
                "Mobile nodes",
                "Surveillance",
                "Smart cities",
                "Security"
            ],
            "Author Keywords": [
                "Shape reconstruction",
                "Delaunay Triangulation",
                "Coverage Hole Detection",
                "Dynamic Node Placement",
                "Coverage Optimization"
            ]
        },
        "title": "Boundary Reconstruction for Wireless Sensor Networks"
    },
    {
        "authors": [
            "Athanasios Koukosias",
            "Christos Anagnostopoulos",
            "Kostas Kolomvatsos"
        ],
        "published_in": "Published in: IEEE Transactions on Knowledge and Data Engineering ( Early Access )",
        "date_of_publication": "23 October 2024",
        "doi": "10.1109/TKDE.2024.3485531",
        "publisher": "IEEE",
        "abstract": "Context-aware data selectivity in Edge Computing (EC) requires nodes to efficiently manage the data collected from Internet of Things (IoT) devices, e.g., sensors, for supporting real-time and data-driven pervasive analytics. Data selectivity at the network edge copes with the challenge of deciding which data should be kept at the edge for future analytics tasks under limited computational and storage resources. Our challenge is to efficiently learn the access patterns of data-driven tasks (analytics) and predict which data are relevant, thus, being stored in nodes' local datasets. Task patterns directly indicate which data need to be accessed and processed to support end-users' applications. We introduce a task workload-aware mechanism which adopts one-class classification to learn and predict the relevant data requested by past tasks. The inherent uncertainty in learning task patterns, identifying inliers and eliminating outliers is handled by introducing a lightweight fuzzy inference estimator that dynamically adapts nodes' local data filters ensuring accurate data relevance prediction. We analytically describe our mechanism and comprehensively evaluate and compare against baselines and approaches found in the literature showcasing its applicability in pervasive EC.",
        "issn": {
            "Print ISSN": "1041-4347",
            "Electronic ISSN": "1558-2191"
        },
        "keywords": {
            "IEEE Keywords": [
                "Filters",
                "Peer-to-peer computing",
                "Sensors",
                "Distributed databases",
                "Internet of Things",
                "Data models",
                "Uncertainty",
                "Real-time systems",
                "Predictive models",
                "Edge computing"
            ],
            "Author Keywords": [
                "Edge Computing",
                "Data Filter",
                "Data Selectivity",
                "One-Class Support Vector Machines",
                "Fuzzy Inference"
            ]
        },
        "title": "Task-Aware Data Selectivity in Pervasive Edge Computing Environments"
    },
    {
        "authors": [
            "Namory Fofana",
            "Asma Ben Letaifa",
            "Abderrezak Rachedi"
        ],
        "published_in": "Published in: IEEE Transactions on Vehicular Technology ( Early Access )",
        "date_of_publication": "15 July 2024",
        "doi": "10.1109/TVT.2024.3427814",
        "publisher": "IEEE",
        "abstract": "The proliferation of connected vehicles and the Internet of Things (IoT) has made access to high-quality services increasingly viable. However, the growing number of vehicular applications poses challenges for embedded systems that need to perform tasks efficiently despite network fluctuations. To solve this problem, we have developed a Vehicular Edge Computing (VEC) system with a task offloading algorithm adapted to the Internet of Vehicles (IoV). Our solution uses a four-stage Stackelberg game and a reinforcement learning model. The approach consists of analyzing the vehicle state in real time to determine computational requirements and cost functions, implementing communication methods and cost functions, and using multi-agent reinforcement learning to design an experiencebased offloading strategy. Through simulations, our algorithm demonstrates a balanced optimization of time, expense, work vehicle utility and service vehicle utility, while improving the probability of task success under various constraints.",
        "issn": {
            "Print ISSN": "0018-9545",
            "Electronic ISSN": "1939-9359"
        },
        "keywords": {
            "IEEE Keywords": [
                "Task analysis",
                "Games",
                "Servers",
                "Resource management",
                "Game theory",
                "Costs",
                "Computational modeling"
            ],
            "Author Keywords": [
                "Deep Reinforcement Learning",
                "Game theory",
                "Compute Offloading",
                "IoV",
                "VEC",
                "MEC",
                "Multi-agent scenarios"
            ]
        },
        "title": "Intelligent Task Offloading in Vehicular Networks: a Deep Reinforcement Learning Perspective"
    },
    {
        "authors": [
            "Lantu Guo",
            "Chao Liu",
            "Yuchao Liu",
            "Yun Lin",
            "Guan Gui"
        ],
        "published_in": "Published in: IEEE Transactions on Cognitive Communications and Networking ( Early Access )",
        "date_of_publication": "03 June 2024",
        "doi": "10.1109/TCCN.2024.3408417",
        "publisher": "IEEE",
        "abstract": "Specific emitter identification (SEI) based on unavoidable hardware impairments of transmitters has emerged as a potential technology for physical layer authentication of Internet of Things (IoT) devices. The integration with deep learning has significantly accelerated the advancement of SEI in recent years, showcasing its strong application prospects. However, most SEI works are designed for close-set scenarios, leading to misclassification of unknown transmitters outside the close-set, which poses a serious security risk to the authentication system. To address this issue, a SEI framework with anomaly detection capabilities is essential. In this paper, we propose a novel open-set SEI framework that leverages the Auxiliary Classifier Generative Adversarial Network (ACGAN) to generate outlier samples, introduces OpenMax to obtain calibrated activation vectors (AV), and applies a threshold on the category confidence. Through the proposed triple anomaly detection condition, our SEI framework is endowed with enhanced capabilities for identifying anomalous transmitters. We evaluate the framework using 10 known and 6 unknown WiFi transmitters and achieve a high F1-Score of 0.911 and a low False Positive Rate (FPR) of 0.063 in an anomaly detection task.",
        "issn": {
            "Electronic ISSN": "2332-7731"
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Anomaly detection",
                "Authentication",
                "Training",
                "Generative adversarial networks",
                "Probability",
                "Vectors"
            ],
            "Author Keywords": [
                "Specific emitter identification (SEI)",
                "open-set recognition",
                "physical layer authentication",
                "auxiliary classifier generative adversarial network",
                "OpenMax"
            ]
        },
        "title": "Towards Open-Set Specific Emitter Identification Using Auxiliary Classifier Generative Adversarial Network and OpenMax"
    },
    {
        "authors": [
            "Li Jihao",
            "Li Hewu",
            "Lai Zeqi",
            "Wang Xiaomo"
        ],
        "published_in": "Published in: China Communications ( Early Access )",
        "date_of_publication": "10 April 2024",
        "doi": "10.23919/JCC.ea.2021-0151.202401",
        "publisher": "IEEE",
        "abstract": "Emerging long-range industrial IoT applications (e.g., remote patient monitoring) have increasingly higher requirements for global deterministic delay. Although many existing methods have built deterministic networks in small-scale networks through centralized computing and resource reservation, they cannot be applied on a global scale. The emerging mega-constellations enable new opportunities for realizing deterministic delay globally. As one constellation (e.g., Starlink) might be managed by a single operator (e.g., SpaceX), packets can be routed within deterministic number of hops. Moreover, the path diversity brought by the highly symmetrical network structure in mega-constellations can help to construct a congestion free network by routing. This paper leverages these unique characteristics of mega-constellations to avoid the traditional network congestion caused by multiple inputs and single output, and to determine the routing hops, and thus realizing a global deterministic network (DetSpace). The model based on the 2D Markov chain theoretically verifies the correctness of DetSpace. The effectiveness of DetSpace in different traffic load conditions is also verified by extensive simulations.",
        "issn": {},
        "keywords": {
            "IEEE Keywords": [
                "Delays",
                "Satellites",
                "Orbits",
                "Routing",
                "Propagation delay",
                "Topology",
                "Telecommunication traffic"
            ],
            "Author Keywords": [
                "congestion free",
                "deterministic network",
                "mega-constellations",
                "path diversity"
            ]
        },
        "title": "DetSpace: Distributed congestion free routing for global deterministic network in mega-constellations"
    },
    {
        "authors": [
            "Liangbo Xie",
            "Yukun Zhang",
            "Changming Zhao",
            "Chenlin Zhang",
            "Zhou Mu",
            "Xiaolong Yang"
        ],
        "published_in": "Published in: IEEE Network ( Early Access )",
        "date_of_publication": "29 July 2024",
        "doi": "10.1109/MNET.2024.3435087",
        "publisher": "IEEE",
        "abstract": "With the development of the Internet of Things (IoTs), there is an increasing demand for Location Based Services (LBS) for various applications. Traditional indoor and outdoor positioning systems, such as global positioning system (GPS) and Wi-Fi, have mature solutions for line-of-sight (LOS) environments. However, performances of existing positioning techniques in multipath environments may experience significant degradation due to the lack of LOS path information and the inaccuracies in parameter estimation caused by the presence of non-line-of-sight (NLOS) influences. In this article, we provide an extensive overview of the positioning techniques for the outdoor and indoor environments. We first introduce the mainstream techniques of current positioning system and further give an insight for NLOS positioning techniques. Then a virtual station-based single-station NLOS positioning algorithm is proposed to improve the positioning accuracy and relax the constraints of existing NLOS positioning solutions. Finally, we investigate technologies that have great potential for positioning system in the sixth generation (6G) communication system.",
        "issn": {
            "Print ISSN": "0890-8044",
            "Electronic ISSN": "1558-156X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Accuracy",
                "Global navigation satellite system",
                "Wireless fidelity",
                "Satellites",
                "Radiofrequency identification",
                "Global Positioning System",
                "Bluetooth"
            ],
            "Author Keywords": []
        },
        "title": "Positioning under Multipath Environments in Wireless Network: Survey, Design and Opportunities"
    },
    {
        "authors": [
            "Ruizhi Liu",
            "Ke Wu"
        ],
        "published_in": "Published in: IEEE Antennas and Propagation Magazine ( Early Access )",
        "date_of_publication": "08 February 2024",
        "doi": "10.1109/MAP.2024.3355843",
        "publisher": "IEEE",
        "abstract": "Antenna and array technologies have been in constant evolution for more than 100 years for various purposes in wireless connectivity among distant devices and/or systems by radiating electromagnetic waves, usually operating in a far-field manner. With the proliferation of the Internet of Things (IoT) that started more than two decades ago, wireless devices are often required to transfer, share, read, and store their information over a short distance in a secure and efficient way, which may not fall into the category of far-field applications. However, the conventional theories and techniques for far-field applications cannot generally be applied directly to the development of short-range communication because the nature of the electromagnetic fields in the near-field (NF) region is quite different from those in the far-field region. Therefore, the NF theories were developed to accurately describe the phenomenon of waves in antenna NF regions and then yield novel and practical applications. This article encompasses the recently developed principles and applications of NF techniques for multifunction wireless connectivity, including NF coupling, imaging, and signal processing.",
        "issn": {
            "Print ISSN": "1045-9243",
            "Electronic ISSN": "1558-4143"
        },
        "keywords": {
            "IEEE Keywords": [
                "Noise measurement",
                "Coils",
                "Couplings",
                "Electromagnetic scattering",
                "Antennas",
                "Behavioral sciences",
                "Transmitting antennas"
            ],
            "Author Keywords": []
        },
        "title": "Near-Field Technologies for Multifunction Wireless Connectivity: Recent principles and applications"
    },
    {
        "authors": [
            "Xinxing Ren",
            "Chun Sing Lai",
            "Zekun Guo",
            "Gareth Taylor"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "16 October 2024",
        "doi": "10.1109/TCE.2024.3482101",
        "publisher": "IEEE",
        "abstract": "Consumer electronics such as advanced GPS, vehicular sensors, inertial measurement units (IMUs), and wireless modules integrate vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) within internet of thing (IoT), enabling connected autonomous electric vehicles (CAEVs) to optimize energy optimization through eco-driving. In scenarios with traffic light intersections and partial wireless charging lanes (WCL), an eco-driving algorithm must consider net and gross energy consumption, safety, and traffic efficiency. We introduced a deep reinforcement learning (DRL) based eco-driving control approach, employing a twin-delayed deep deterministic policy gradient (TD3) agent for real-time acceleration planning. This approach uses reward functions for acceleration, velocity, safety, and efficiency, incorporating a dynamic velocity range model which not only enables the vehicle to smoothly pass the signalized intersections but also uses partial WCL efficiently and time-adaptively while ensuring traffic efficiency in diverse traffic scenarios. Tested in Simulation of Urban Mobility (SUMO) across various intersections with partial WCL, our method significantly lowered net and gross energy consumption by up to 44.01% and 17.19%, respectively, compared to conventional driving, while adhering to traffic and safety norms.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Batteries",
                "Inductive charging",
                "Optical wavelength conversion",
                "Safety",
                "Roads",
                "Consumer electronics",
                "Energy consumption",
                "Wireless communication",
                "Vehicle dynamics",
                "Real-time systems"
            ],
            "Author Keywords": [
                "Consumer electronics",
                "vehicle-to-vehicle communications",
                "vehicle-to-infrastructure communication",
                "connected autonomous electric vehicles",
                "autonomous electric vehicles",
                "eco-driving",
                "wireless charging lane",
                "deep reinforcement learning"
            ]
        },
        "title": "Eco-Driving With Partial Wireless Charging Lane at Signalized Intersection: A Reinforcement Learning Approach"
    },
    {
        "authors": [
            "Weihong Xu",
            "Saransh Gupta",
            "Justin Morris",
            "Xincheng Shen",
            "Mohsen Imani",
            "Baris Aksanli",
            "Tajana Rosing"
        ],
        "published_in": "Published in: IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems ( Early Access )",
        "date_of_publication": "29 July 2024",
        "doi": "10.1109/TCAD.2024.3435679",
        "publisher": "IEEE",
        "abstract": "The Internet of Things (IoT) has led to the emergence of big data. Processing this data, specially in learning algorithms, poses a challenge for current embedded computing systems. Brain-inspired hyperdimensional (HD) computing reduces several complex learning operations to simpler bitwise and arithmetic operations. However, it requires the use of large dimensional vectors, hypervectors, further increasing the amount of data to be processed. Processing in-memory (PIM) enables in-place computation which reduces data movement, a major latency bottleneck in conventional systems. In this paper, we propose, an in-memory HD computing architecture that performs HD classification in memory. To the best of authors’ knowledge, is the first ReRAM PIM architecture to implement the complete HD computing-based classification pipeline including encoding, training, re-training, and inference for non-binary data. We also propose a novel distance metric that is PIM-friendly and provides similar application accuracy as the more complex baseline metric. Our proposed architecture is enabled in PIM by fast and energy-efficient in-memory logic operations. We exploit the voltage threshold-based memristors to enable single cycle operations. We also increase the amount of in-memory parallelism in our design by segmenting bitlines using switches. Our evaluation shows that for all applications tested using HD, provides on average 434× (2170×) speedup and consumes 4114× (26019×) less energy as compared to the CPU while running end-to-end HD training (inference). also achieves at least 2.2% higher classification accuracy than the existing PIM-based HD designs.",
        "issn": {
            "Print ISSN": "0278-0070",
            "Electronic ISSN": "1937-4151"
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Logic",
                "Vectors",
                "Memristors",
                "Accuracy",
                "Indexes",
                "Encoding"
            ],
            "Author Keywords": [
                "Processing in-memory",
                "non-volatile memories",
                "hyperdimensional computing",
                "machine learning",
                "classification",
                "memristors",
                "RRAM"
            ]
        },
        "title": "Tri-HD: Energy-Efficient On-Chip Learning With In-Memory Hyperdimensional Computing"
    },
    {
        "authors": [
            "Weizheng Wang",
            "Xingxing Gong",
            "Xiangqi Wang",
            "Shuo Cai",
            "Peng Liu",
            "Neal N. Xiong"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "04 September 2024",
        "doi": "10.1109/JIOT.2024.3454483",
        "publisher": "IEEE",
        "abstract": "The data privacy and security of Internet-of-Things (IoT) applications are progressively crucial and cryptography is frequently used to ensure security. Cryptographic hardware implementation is commonly adopted for high throughput and low computational resources. The crypto circuits have to be strictly tested to guarantee the correctness of data. Scan-based design-for-testability widely employed in the chip industry improves the controllability and observability of circuits. However, it facilitates illegal users to steal the internal data for cracking cryptographic keys. Many researchers have recently suggested effective defense technologies against scan-based attacks, but each technology has its own negative aspects. In this paper, we propose an effective Design-For-Testability Authorization Framework (DAF) based on obfuscation mechanisms. The authentication module is embedded to verify users’ keys, and it can empower each chip distinctive key to minimize the loss of key divulgence. If the test authorization key is accurate, the typical scan operation can be carried out. Conversely, if the key is inaccurate, the inserted obfuscation module comes into play to scramble the scan data. Additionally, a random number generation circuit is used to increase the uncertainty of data obfuscation, effectively preventing attackers from inferring sensitive information. Simulation results show that this design has a low overhead, and theoretical analysis demonstrates that the design has high security with no impact on the testability of the chip.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Cryptography",
                "Encryption",
                "Security",
                "Circuits",
                "Internet of Things",
                "Hardware",
                "Discrete Fourier transforms"
            ],
            "Author Keywords": [
                "DFT",
                "Scan chain",
                "scan-based attacks",
                "obfuscation"
            ]
        },
        "title": "DAF: An Effective Design-For-Testability Authorization Framework Based on Obfuscation Mechanisms for Defending Complex Attacks"
    },
    {
        "authors": [
            "Yuan Gao",
            "Ziyue Lin",
            "Maoguo Gong",
            "Yuanqiao Zhang",
            "Yihong Zhang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "02 September 2024",
        "doi": "10.1109/JIOT.2024.3452717",
        "publisher": "IEEE",
        "abstract": "The exponential growth of the Internet of Things (IoT), driven by the increasing number of connected devices and sensors, is profoundly transforming many industries. This proliferation has been enabled by advances in storage and processing capabilities, facilitating the implementation of deep model. However, concerns surrounding privacy and substantial bandwidth required for data transmission arise from the centralized processing of data collected from numerous distributed devices. To address these issues, federated learning (FL) has emerged as a viable solution with a decentralized strategy. It ensures data privacy by allowing devices to locally train models while keeping their data localized, thus reducing centralization needs and concerns related to privacy and bandwidth. To mitigate the data heterogeneity problem in FL scenarios, we propose federated pattern extraction clustering (FedPEC) to cluster clients with similar data distributions. Compare to existing client clustering methods, FedPEC requires no additional data transmission and ensures great flexibility, scalability and privacy. Furthermore, we discuss the discrepancy between representation and data distribution in existing methods from the perspective of pattern representation. Based on this, we propose a variant of FedPEC as a solution, which has achieved excellent performance on multiple FL datasets.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Data models",
                "Computational modeling",
                "Internet of Things",
                "Federated learning",
                "Data privacy",
                "Servers",
                "Training"
            ],
            "Author Keywords": [
                "Pattern representation",
                "client clustering",
                "distributed learning",
                "federated learning"
            ]
        },
        "title": "Enhancing Federated Learning With Pattern-Based Client Clustering"
    },
    {
        "authors": [
            "Mohamed Shahawy",
            "Elhadj Benkhelifa",
            "David White"
        ],
        "published_in": "Published in: IEEE Transactions on Neural Networks and Learning Systems ( Early Access )",
        "date_of_publication": "24 September 2024",
        "doi": "10.1109/TNNLS.2024.3453973",
        "publisher": "IEEE",
        "abstract": "Despite the significant advances achieved in deep learning, the deep neural networks’ (DNNs) design approach remains notoriously tedious, depending primarily on intuition, experience, and trial and error. This human-dependent process is often time-consuming and prone to errors. Furthermore, the models are generally bound to their training contexts, with no considerations to their surrounding environments. Continual adaptiveness and automation of neural networks is of paramount importance to several domains where model accessibility is limited after deployment (e.g., IoT devices, self-driving vehicles, etc.). Additionally, even accessible models require frequent maintenance postdeployment to overcome issues such as data/concept drift, which can be cumbersome and restrictive. By leveraging and combining approaches from neural architecture search (NAS) and continual learning (CL), more robust and adaptive agents can be developed. This study conducts the first extensive review on the intersection between NAS and CL, formalizing the prospective paradigm and outlining research directions for lifelong autonomous DNNs.",
        "issn": {
            "Print ISSN": "2162-237X",
            "Electronic ISSN": "2162-2388"
        },
        "keywords": {
            "IEEE Keywords": [
                "Adaptation models",
                "Reviews",
                "Data models",
                "Computational modeling",
                "Training",
                "Computer architecture",
                "Topology"
            ],
            "Author Keywords": [
                "Adaptive neural networks",
                "AutoML",
                "continual learning (CL)",
                "continual neural architecture search (NAS)",
                "incremental learning",
                "lifelong learning",
                "NAS"
            ]
        },
        "title": "Exploring the Intersection Between Neural Architecture Search and Continual Learning"
    },
    {
        "authors": [
            "Feipeng Guo",
            "Zifan Wang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "06 August 2024",
        "doi": "10.1109/JIOT.2024.3439527",
        "publisher": "IEEE",
        "abstract": "In the era of Internet of Things (IoT), intelligent recommendation systems are crucial components for users to locate the items they require. Existing recommendation systems overlook the diversity of user behaviors and rely solely on utilizing a singular form of user-item interaction data. Multi-Behavior Recommendation (MBR) works to solve this problem by utilizing multi-typed user behaviors to mine the heterogeneous relations between users and items to improve recommendation accuracy. Nevertheless, there are still challenges to be overcome, including capture of differences and commonalities between different types of behaviors, learning of users’ personalized behavioral patterns, consideration of semantic knowledge, and building of users’ trust in algorithms. In light of the aforementioned considerations, we propose a Knowledge-enhanced Explainable Multi-Behavior Recommendation model (KEMB-Rec) with graph contrastive learning, comprising two modules. The first is the user behavior-aware module, which mines user’s behavior pattern using the user behavior hyper meta-graphs and captures the differences and commonalities between different behaviors through graph contrastive learning. The second is the semantic knowledge-aware module, which is based on single behavior interaction graphs to mine the semantic relational knowledge, and makes full use of it to represent users and items. Then, we design contrastive learning task and recommendation task, and the two tasks are optimized jointly. At the same time, effective recommendation explanations are provided by mining paths and semantics between users and items as a way to enhance user trust and satisfaction. The proposed KEMB-Rec is evaluated in real-world datasets, with results indicating that KEMB-Rec outperforms various baselines.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Contrastive learning",
                "Semantics",
                "Task analysis",
                "Internet of Things",
                "Data mining",
                "Recommender systems",
                "Accuracy"
            ],
            "Author Keywords": [
                "Multi-behavior recommendation",
                "Explanation",
                "Graph contrastive learning",
                "Semantic knowledge",
                "Graph neural network"
            ]
        },
        "title": "KEMB-Rec: Knowledge-Enhanced Explainable Multi-Behavior Recommendation With Graph Contrastive Learning"
    },
    {
        "authors": [
            "Haoyu Zhan",
            "Lisheng Fan",
            "Chao Li",
            "Xianfu Lei",
            "Feng Li"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "27 August 2024",
        "doi": "10.1109/JIOT.2024.3450477",
        "publisher": "IEEE",
        "abstract": "The development of Internet-of-Thing (IoT) networks causes an increasing demand for high-quality video streaming, which results in the burden of traditional mobile cloud computing (MCC) networks, such as high energy consumption and dissatisfaction with the user’s quality of experience (QoE). To address this issue, mobile edge computing (MEC) networks have recently been widely employed for high-quality video streaming scenarios under time-varying wireless channels. However, in MEC networks, limited resources deteriorate the video transmission rate and the quality of videos. Therefore, in this paper, we investigate a MEC network with cloud-edge integration for adaptive bitrate (ABR) video streaming, where the edge server (ES) performs cloud-edge selection to decide whether the requested video should be directly obtained from the cloud server (CS) or through a transcoding process. We first design edge caching and transcoding processes to enhance resource utilization and reduce computational consumption through bitrate adaptation strategy and cloud-edge selection decision. Moreover, we utilize the energy efficiency by jointly considering the user’s QoE and energy consumption to measure the network’s performance, and then formulate the optimization objective to maximize energy efficiency. In further, we employ a deep deterministic policy gradient (DDPG)-based scheme to solve the optimization problem by applying video quality adaptation technique, allocating computational resources and transmit power. Finally, simulation results demonstrate that the proposed scheme accomplishes superior energy efficiency compared to the competing schemes at least 41.1%.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Streaming media",
                "Bit rate",
                "Quality of experience",
                "Cloud computing",
                "Transcoding",
                "Servers",
                "Resource management"
            ],
            "Author Keywords": [
                "Mobile edge computing",
                "adaptive video streaming",
                "resource allocation",
                "deep reinforcement learning"
            ]
        },
        "title": "Cloud-Edge Learning for Adaptive Video Streaming in B5G Internet-of-Thing Systems"
    },
    {
        "authors": [
            "Sabine Sint",
            "Alexandra Mazak-Huemer",
            "Martin Eisenberg",
            "Daniel Waghubinger",
            "Manuel Wimmer"
        ],
        "published_in": "Published in: IEEE Transactions on Automation Science and Engineering ( Early Access )",
        "date_of_publication": "12 April 2024",
        "doi": "10.1109/TASE.2024.3386313",
        "publisher": "IEEE",
        "abstract": "For continuously checking and updating the virtual representation of a real system during operation, the continuous sensing and interpretation of raw sensor data is a must. The challenge is to bundle sensor value streams (e.g., from IoT networks) and aggregate them to a higher logical state level to enable process-oriented viewpoints and to handle uncertainties about sensor measurements and state realization precision. To address these uncertainties, so-called “tolerance ranges” must be defined in which logical states are detected during operation with acceptable deviations. Specifying such tolerance ranges manually is a time-consuming, error-prone task and often not feasible due to the huge associated value search space. To tackle this challenge, the problem is turned into an optimization problem in this paper. For this purpose, we present a framework based on meta-heuristic search that enables the automatic configuration of tolerance ranges based on available execution traces of multiple sensor value streams. An exploratory study evaluates the approach. For this purpose, we implemented a lab-sized demonstrator of a five-axis grip arm robot, which we continuously monitored during operation in a simulated environment. The evaluation shows the advantage of using meta-heuristic optimizers such as Harmony Search or Genetic Algorithm to identify stable tolerance ranges automatically for state detection at runtime. Note to Practitioners —Monitoring sensor values streams is nowadays a frequently employed technique in many automation domains. However, combining and mapping single value streams to higher-level state-based representations such as state machines or other design-time related models is a major challenge due to measurement and realization precision uncertainties. Thus, simply mapping monitored raw data to these design descriptions can lead to falsely identified or missed states. To improve this situation, we present an approach that provides a mechanism to cont...",
        "issn": {
            "Print ISSN": "1545-5955",
            "Electronic ISSN": "1558-3783"
        },
        "keywords": {
            "IEEE Keywords": [
                "Robot sensing systems",
                "Runtime",
                "Streams",
                "Uncertainty",
                "Monitoring",
                "Metaheuristics",
                "Grippers"
            ],
            "Author Keywords": [
                "Uncertainties",
                "meta-heuristic search",
                "harmony search",
                "genetic algorithm",
                "runtime monitoring",
                "model-driven engineering"
            ]
        },
        "title": "Automatic Optimization of Tolerance Ranges for Model-Driven Runtime State Identification"
    },
    {
        "authors": [
            "Shuangshuang Liu",
            "Zhi Wang",
            "Saru Kumari",
            "Jianhui Lv",
            "Chien-Ming Chen"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "19 August 2024",
        "doi": "10.1109/JIOT.2024.3445375",
        "publisher": "IEEE",
        "abstract": "In the rapidly evolving field of smart healthcare, integrating modern information technologies such as IoT, big data, and AI has significantly enhanced the quality, efficiency, and accessibility of medical services. However, this technological advancement also brings substantial security challenges, particularly regarding protecting electronic medical records (EMRs) from phishing attacks. This paper presents a provably secure anti-phishing scheme to safeguard EMRs in smart healthcare systems. By utilizing Authentication and Key Agreement technology, our proposed scheme ensures mutual authentication between users and servers, leveraging elliptic curve encryption, symmetric encryption, hash functions, and XOR operations to secure session keys and verify the legitimacy of medical records. Our scheme addresses critical security challenges, including phishing attacks, stolen mobile device attacks, offline password guessing attacks, replay attacks, and temporary information leakage. The Real-Oracle-Random (ROR) model validates the correctness and security of our scheme, confirming its robustness against common cybersecurity threats. Performance evaluations demonstrate that our scheme provides enhanced security and offers lower time and communication costs compared to existing methods. This makes it a highly efficient and practical solution for safeguarding patient data in smart healthcare environments, ultimately contributing to the reliability and trustworthiness of smart healthcare systems.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Phishing",
                "Medical services",
                "Authentication",
                "Security",
                "Electronic medical records",
                "Servers",
                "Electronic mail"
            ],
            "Author Keywords": [
                "Smart Healthcare",
                "Electronic Medical Records",
                "Phishing Attacks",
                "Authentication and Key Agreement"
            ]
        },
        "title": "Provably Secure Anti-Phishing Scheme for Medical Information in Smart Healthcare"
    },
    {
        "authors": [
            "Yang Li",
            "Okyay Kaynak",
            "Li Jia",
            "Chun Liu",
            "Yulong Wang",
            "Enrico Zio"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "02 August 2024",
        "doi": "10.1109/JIOT.2024.3437660",
        "publisher": "IEEE",
        "abstract": "To achieve fault diagnosis and prognosis, obtaining adequate and reliable life-cycle data is essential. However, this poses a challenge in current high-reliable Internet of Things (IoT) systems. Fortunately, accelerated degradation testing can be employed to overcome this hurdle. Nevertheless, a dependable testing and measuring technique is required to construct an accurate model for accelerated degradation testing. This testing method plays a vital role in evaluating fault diagnosis, prognosis, lifetime, and maintenance decisions for reliable products under operational stress. To ensure effective testing, it is crucial to utilize appropriate models that account for the individual heterogeneity of products. However, the commonly used single stochastic models in accelerated degradation testing overlook the impact of this condition in real-world applications, resulting in mis-specification problem. To address this limitation, we propose a novel mixed stochastic process model that integrates multi-Wiener processes and dynamic weights. In addition, we leverage interval analysis to analyze system lifetime, considering the limited data size. The estimation of unknown parameters in our mixed model is achieved using the Metropolis-Hastings algorithm. By analyzing stress relaxation data from electrical connectors, we demonstrate the superior accuracy of our mixed model over conventional single stochastic models in accelerated degradation testing.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Degradation",
                "Testing",
                "Reliability",
                "Stress",
                "Life estimation",
                "Analytical models",
                "Internet of Things"
            ],
            "Author Keywords": [
                "Accelerated degradation testing",
                "fault diagnosis",
                "fault prognosis",
                "lifetime",
                "mixed stochastic model"
            ]
        },
        "title": "A Generalized Testing Model for Interval Lifetime Analysis Based on Mixed Wiener Accelerated Degradation Process"
    },
    {
        "authors": [
            "Dingwen Chi",
            "Jun Tao",
            "Haotian Wang",
            "Yifan Xu"
        ],
        "published_in": "Published in: IEEE Transactions on Computational Social Systems ( Early Access )",
        "date_of_publication": "17 October 2024",
        "doi": "10.1109/TCSS.2024.3473297",
        "publisher": "IEEE",
        "abstract": "Utilizing mobile crowdsensing (MCS) for data collection and analysis has become a prominent paradigm in the Internet of Things (IoTs). However, the existing research predominantly focuses on platform-user interactions, often neglecting the potential for user collaboration, which is crucial for improving data quality and task efficiency. In practical applications, mobile users tend to cooperate with familiar individuals based on their preferences in sensing tasks. To tackle this issue, we introduce a novel MCS model that integrates user cooperation, significantly enhancing the system’s overall effectiveness. Specifically, users’ capabilities and costs are synthesized and managed through a cooperation degree matrix. Additionally, cooperation is updated based on historical behaviors and user preferences. To incentivize user participation, currencies are employed for recruitment. Within this framework, we investigate the maximum collaborative user selection (MCUS) problem, which is dedicated to the problem of maximizing the amount of recruitment under user cooperation. The MCUS problem is proved to be an NP-hard problem and thus intractable. To address this, we propose the minimum weighted cost replacement (MWCR) algorithm. Experimental results demonstrate that the MWCR algorithm exhibits low complexity and high efficiency across various scales, making it an excellent solution for collaborative crowd recruitment.",
        "issn": {
            "Electronic ISSN": "2329-924X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Costs",
                "Collaboration",
                "Recruitment",
                "Mobile computing",
                "Sensors",
                "Optimization",
                "Resource management",
                "Heuristic algorithms",
                "Crowdsensing",
                "Quality of service"
            ],
            "Author Keywords": [
                "Collaboration",
                "mobile crowdsensing (MCS)",
                "replacement algorithm",
                "user recruitment"
            ]
        },
        "title": "Tradeoff Between Capacity and Cost: Maximizing User Recruitment Through Collaboration in Mobile Crowdsensing"
    },
    {
        "authors": [
            "Mohammed Hassan Husain",
            "Mahmood Ahmadi",
            "Farhad Mardukhi"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "01 October 2024",
        "doi": "10.1109/ACCESS.2024.3471068",
        "publisher": "IEEE",
        "abstract": "The increase in general-purpose vehicular applications has driven the demand for robust low-latency communication and processing capabilities in Internet of Things (IoT) networks, especially in devices with limited resources. For this purpose, the establishment of Vehicle Fog Computing (VFC) is recommended. The processing, storage and computing capacity of intelligent vehicles in parking lots can be used for important applications as fog nodes. In this paper, the Workflow Scheduling and Resource Allocation (WSRA) as a multi-objective optimization problem is formulated. The GWO-MFO algorithm, incorporating both Gray Wolf Optimization (GWO) and Moth-flame Optimization (MFO), is intended to optimize the reduction of latency and energy consumption, while still meeting deadline and reliability requirements. GWO-MFO searches the problem search space more powerfully and obtains the best solutions. Minimization of objective function in GWO-MFO, using Processor Merging (PrM) and Dynamic Voltage Frequency Scaling (DVFS) techniques, reduces static and dynamic energy consumption, respectively. The effectiveness of the proposed algorithm was checked on three workflows of 6, 14, and 15 tasks based on the evaluation metric of energy consumption and latency. According to the experiments, the proposed method achieves better performance (latency and energy consumption) than the compared methods.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Resource management",
                "Energy consumption",
                "Internet of Things",
                "Edge computing",
                "Heuristic algorithms",
                "Cloud computing",
                "Vehicle dynamics",
                "Optimization",
                "Linear programming",
                "Computer architecture",
                "Workflow management software",
                "Scheduling",
                "Low latency communication",
                "Vehicular ad hoc networks"
            ],
            "Author Keywords": [
                "VFC",
                "GWO",
                "MFO",
                "DVFS",
                "Energy Consumption",
                "Workflow Scheduling and Resource Allocation"
            ]
        },
        "title": "A GWO-MFO-based Resource Allocation in Vehicular Fog Computing with Latency Constraints and Energy Reduction"
    },
    {
        "authors": [
            "Sifan Long",
            "Fengxiao Tang",
            "Yangfan Li",
            "Tiao Tan",
            "Zhengjie Jin",
            "Ming Zhao",
            "Nei Kato"
        ],
        "published_in": "Published in: IEEE Network ( Early Access )",
        "date_of_publication": "30 September 2024",
        "doi": "10.1109/MNET.2024.3470774",
        "publisher": "IEEE",
        "abstract": "The sixth generation mobile communication standard (6G) can promote the development of Industrial Internet and Internet of Things (IoT). To achieve comprehensive intelligent development of the network and provide customers with higher quality personalized services. This paper proposes a network performance optimization and intelligent operation network architecture based on Large Language Model (LLM), aiming to build a comprehensive intelligent 6G network system. The Large Language Model, with more parameters and stronger learning ability, can more accurately capture patterns and features in data, which can achieve more accurate content output and high intelligence and provide strong support for related research such as network data security, privacy protection, and health assessment. This paper also presents the design framework of a network health assessment system based on LLM and focuses on its potential application scenarios, through the case of network health management system, it is fully demonstrated that the 6G intelligent network system based on LLM has important practical significance for the comprehensive realization of intelligence.",
        "issn": {
            "Print ISSN": "0890-8044",
            "Electronic ISSN": "1558-156X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Large language models",
                "6G mobile communication",
                "Optimization",
                "Monitoring",
                "Data models",
                "Transformers",
                "Fault diagnosis",
                "Training",
                "Protection",
                "Long short term memory"
            ],
            "Author Keywords": [
                "6G",
                "Large Language Model",
                "Network health assessment",
                "Network performance optimization"
            ]
        },
        "title": "6G comprehensive intelligence: network operations and optimization based on Large Language Models"
    },
    {
        "authors": [
            "Yiheng He",
            "Dengpan Ye",
            "Long Tang",
            "Ziyi Liu",
            "Chuanxi Chen"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "30 September 2024",
        "doi": "10.1109/JIOT.2024.3470872",
        "publisher": "IEEE",
        "abstract": "The proliferation of Internet of Things (IoT) devices equipped with cameras, such as those in electric vehicles, has increased the collection of personal image data. However, the potential misuse of Cross-View Geo-Localization (CVGL) models, which can infer precise locations from ground view images, has been overlooked and seriously threatens individual location privacy. In this paper, we introduce AdvLUT, a novel semantic-based adversarial 3D LookUp Tables (3DLUT) privacy protection framework designed to safeguard geographic location privacy against CVGL models. The AdvLUT employs a geographic feature encoder to extract semantic features rich in geographic information from the ground view input. These features then guide a specialized adversarial 3DLUT generator in producing a 3DLUT that alters the color properties of the input image, thereby obstructing accurate location inference. Furthermore, AdvLUT is designed with a generative architecture that enables rapid image processing within milliseconds, eliminating the need for the corresponding satellite image or CVGL model. Experimental results on multiple benchmark datasets and CVGL models demonstrate that our method achieves up to a 65.48% reduction in R@1 localization accuracy, with performance further improving to 69.25% after JPEG compression.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Privacy",
                "Protection",
                "Perturbation methods",
                "Satellite images",
                "Internet of Things",
                "Feature extraction",
                "Optimization",
                "Circuit faults",
                "Colored noise"
            ],
            "Author Keywords": [
                "Cross-View Geo-Localization",
                "Adversarial Attack",
                "Geographic Location Privacy",
                "3D LookUp Tables"
            ]
        },
        "title": "AdvLUT: Cloaking Geographic Location With Semantic-Based Adversarial 3D LookUp Tables"
    },
    {
        "authors": [
            "Wen Deng",
            "Xiang Wang",
            "Zhitao Huang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "01 August 2024",
        "doi": "10.1109/JIOT.2024.3436683",
        "publisher": "IEEE",
        "abstract": "Spectrum sensing (SS) is essential in the cognitive radio-enabled Internet of Things (CR-IoT) to enable spectrum resource allocation. In this study, we propose a novel semantic segmentation-based deep SS scheme (SSDSS) for co-channel signals. Through the extraction of the interleaved signal-wise differential feature (SWDF) sequence, interference among the target signals is efficiently alleviated.Our approach formulates the SS task as a sequence semantic segmentation problem using the interleaved SWDF sequence, which encodes symbol information from latent signals. By constructing three semantic segmentation networks, we successfully extract the SWDF sequence for each latent signal by classifying sequence elements based on their semantics. This aspect enables us to detect the activity time of different target signals within the mixed signal. Notably, our method achieves co-channel signals detection with multiple targets, all accomplished within a single step and a single network. Numerical results confirm strong detection performance against co-channel signals with varying power ratios. Additionally, the SWDF sequence demonstrates robustness against element loss and noise. Furthermore, the offline-trained SSDSS performs well in diverse open-set tests. Moreover, this research highlights the superiority of self-attention mechanisms and recurrent neural networks over convolutional neural networks for this sequence modeling task.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Feature extraction",
                "Semantic segmentation",
                "Spectrogram",
                "Internet of Things",
                "Task analysis",
                "Symbols"
            ],
            "Author Keywords": [
                "Spectrum sensing",
                "semantic segmentation",
                "cochannel signal",
                "cognitive radio-enabled Internet of Things",
                "neural network"
            ]
        },
        "title": "Semantic Segmentation-Based Deep Spectrum Sensing for Cochannel Signals"
    },
    {
        "authors": [
            "Yanxia Liang",
            "Songlin Zhao",
            "Xin Liu",
            "Hua He",
            "Xiaofan Zhao",
            "Huan Wang"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "04 October 2024",
        "doi": "10.1109/JSEN.2024.3458425",
        "publisher": "IEEE",
        "abstract": "Due to characteristics of small size, passive operation, easy deployment, real-time monitoring and data collection, wireless sensor networks are widely used in the field of the Internet of Things (IoT). However, sensor networks face challenges related to limited energy and short lifespan. In order to improve network energy efficiency and extend the lifecycle of WSN, we propose a multi factor competitive clustering routing algorithm based on K-Means and weighted inter standard correlation, denoted as MC-CRITIC-KM. Firstly, in the clustering stage, the K-Means algorithm is used to cluster the sensor network nodes. Secondly, in the selection of the cluster head stage, considering factors such as residual energy, distance, density, and load rate, as well as the objectivity of the WSN environment, the CRITIC weighting method is used to weight the factors. The objectivity of the CRITIC standard eliminates the subjective impact of adjusting the weights of each factor. Finally, four factors and their respective weights are designed to change in each round of data collection to adapt to the high dynamic changes of the sensor network in subsequent rounds. The simulation results show that this scheme is superior to LEACH, KMeans-LEACH, EEUC, IACA routing protocols in terms of network node energy efficiency, and improves the energy efficiency and lifecycle of the entire system.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Wireless sensor networks",
                "Clustering algorithms",
                "Sensors",
                "Heuristic algorithms",
                "Routing",
                "Energy efficiency",
                "Data communication",
                "Base stations",
                "Wireless communication",
                "Voting"
            ],
            "Author Keywords": [
                "Large-Scale Wireless Sensor Network(WSN)",
                "CRITIC Weighting Method",
                "Cluster Head Election",
                "Clustering"
            ]
        },
        "title": "A Balanced Energy-Efficient Clustering Strategy for WSNs"
    },
    {
        "authors": [
            "Rundong Jiang",
            "Jun Hu",
            "He Huang",
            "Chudi Zhang",
            "Lei Wang",
            "Shiyou Xu"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "14 June 2024",
        "doi": "10.1109/JSEN.2024.3411696",
        "publisher": "IEEE",
        "abstract": "Radio frequency (RF) fingerprint (RFF) refers to a unique hardware-related feature of RF devices, characterized by non-reproducible physical characteristics. This feature makes RFF suitable for equipment identity authentication in wireless communication, particularly in the Internet of Things (IoT). Therefore, we propose a more generalized and intuitive definition of the RFF, namely the differences between the received and ideal RF signals. The ideal RF signal is defined as having the same input information source, encoding, and modulation schemes as the received signal, but without any influences from hardware devices or wireless transmission channels. Our approach begins by analyzing the effects of transmission channels, such as attenuation, dispersion, multi-path propagation, and noise, treating them as a series of processes. We represent both the received and ideal signals in a designed multi-domain space.We then develop a Siamese-based neural network to extract features from the multi-domain representations of paired signals simultaneously. A difference block is used to eliminate common features and retain the expected generalized RFF (GRFF). Identification is achieved by treating each device as a specific class, utilizing a softmax classifier. Besides, we introduce a two-stage strategy involving nearest neighbor and particle swarm optimization to discriminate the out-of-distributionsignals. Experiments are carried out using a public ADS-B signal dataset, and the trained network achieves a classification accuracy of 99.13% on a test dataset with 7442 samples from 58 classes in total and out-of-distributiondetection accuracy of 95.45% under a class confidence 0.96 on 5560 samples from additional 206 classes.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Wireless communication",
                "Authentication",
                "Accuracy",
                "RF signals",
                "Hardware",
                "Fingerprint recognition"
            ],
            "Author Keywords": [
                "Neural Network",
                "out-of-distributionDetection",
                "Particle Swarm Optimization",
                "Siamese Structure",
                "RF Fingerprint"
            ]
        },
        "title": "A Generalized Radio Frequency Fingerprint Based Wireless Device Identification Using Siamese-Based Neural Network"
    },
    {
        "authors": [
            "Lumin Liu",
            "Jun Zhang",
            "Shenghui Song",
            "Khaled B. Letaief"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "20 August 2024",
        "doi": "10.1109/JIOT.2024.3446751",
        "publisher": "IEEE",
        "abstract": "Federated learning (FL) is a promising paradigm for privacy-preserving deep learning using data distributed on IoT devices. Traditional model sharing-based methods, e.g., federated averaging (FedAvg), suffer from high communication overhead and difficulty in accommodating heterogeneous model architectures. Federated Distillation (FD) is a recently proposed alternative to enable communication-efficient and robust FL, as well as heterogeneous client models. However, there is a lack of theoretical understanding of FD-based methods, and their design guidelines remain elusive. This paper presents a generic meta-algorithm for FD, generalizing most existing FD training algorithms. By studying a linear classification problem, we show that, with sufficient distillation samples, the training performance of the meta-algorithm is the same as the vanilla FedAvg. To guide the algorithm design and improve communication efficiency, we further investigate the binary classification problem with a Gaussian Mixture Model, which shows that more distillation data and sampling data with higher confidence improve the training performance. Furthermore, we propose an effective distillation data sampling technique to improve the performance of the FD-meta algorithm, which also reduces communication overhead. Simulations on benchmark datasets validate the theoretical findings and demonstrate that our proposed algorithm effectively reduces the communication overhead while achieving a satisfactory performance.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Data models",
                "Internet of Things",
                "Servers",
                "Costs",
                "Convergence",
                "Computer architecture"
            ],
            "Author Keywords": [
                "Federated learning",
                "federated distillation",
                "communication efficiency"
            ]
        },
        "title": "Communication-Efficient Federated Distillation: Theoretical Analysis and Performance Enhancement"
    },
    {
        "authors": [
            "Jibo Shi",
            "Bin Ge",
            "Hang Jiang",
            "Ruichang Yang",
            "Guangzhen Si",
            "Yu Wang",
            "Guan Gui",
            "Yun Lin"
        ],
        "published_in": "Published in: IEEE Transactions on Cognitive Communications and Networking ( Early Access )",
        "date_of_publication": "20 May 2024",
        "doi": "10.1109/TCCN.2024.3403229",
        "publisher": "IEEE",
        "abstract": "As communication technology evolves, specific emitter identification (SEI) gains significance in areas like wireless network security and IoT device identification. While big data and deep learning have spurred centralized identification methods, they often falter in low-resource, real-world settings due to data dispersion and heterogeneity, as well as limited computational power. Addressing these challenges, this paper presents feature-imitation federated learning (FIFL), a novel SEI approach for resource-constrained environments. FIFL utilizes a global classifier, refined through Kullback-Leibler divergence, to manage feature prediction alignment. Simulation results on actual data demonstrate FIFLs effectiveness in overcoming global model drift, ensuring accuracy and reliability even amidst diverse, distributed data in resource-limited settings.",
        "issn": {
            "Electronic ISSN": "2332-7731"
        },
        "keywords": {
            "IEEE Keywords": [
                "Data models",
                "Federated learning",
                "Training",
                "Data privacy",
                "Feature extraction",
                "Distributed databases",
                "Adaptation models"
            ],
            "Author Keywords": [
                "Low-resource learning",
                "federated learning",
                "feature imitation",
                "specific emitter identification"
            ]
        },
        "title": "Feature-Imitation Federated Learning: An Efficient Approach for Specific Emitter Identification in Low-Resource Environments"
    },
    {
        "authors": [
            "Quy Vu Khanh",
            "Abdellah Chehri",
            "Anh Dang Van",
            "Quy Nguyen Minh"
        ],
        "published_in": "Published in: IEEE Transactions on Emerging Topics in Computing ( Early Access )",
        "date_of_publication": "10 October 2024",
        "doi": "10.1109/TETC.2024.3473911",
        "publisher": "IEEE",
        "abstract": "Across all periods of human history, the importance attributed to health has remained a fundamental and significant facet. This statement holds greater validity within the present context. The pressing demand for healthcare solutions with real-time capabilities, affordability, and high precision is crucial in medical research and technology progress. In recent times, there has been a significant advancement in emerging technologies such as AI, IoT, blockchain, and edge computing. These breakthrough developments have led to the creation of various intelligent applications. Smart healthcare applications can be realized by combining robust AI detection and prediction capabilities with edge computing architecture, which offers low computing costs and latency. In this paper, we begin by conducting a literature review of AI-assisted EC-based smart healthcare applications from the past three years. Our goal is to identify gaps and barriers in this field. We propose a smart healthcare architecture model that integrates AI technology into the edge. Finally, we summarize the challenges and research directions associated with the proposed model.",
        "issn": {
            "Electronic ISSN": "2168-6750"
        },
        "keywords": {
            "IEEE Keywords": [
                "Artificial intelligence",
                "Medical services",
                "Internet of Things",
                "Cloud computing",
                "Computer architecture",
                "Servers",
                "Real-time systems",
                "Medical diagnostic imaging",
                "Prediction algorithms",
                "Computational modeling"
            ],
            "Author Keywords": [
                "AI-IoHT",
                "artificial intelligence",
                "edge computing",
                "federated learning",
                "smart healthcare"
            ]
        },
        "title": "Federated Learning Approach for Collaborative and Secure Smart Healthcare Applications"
    },
    {
        "authors": [
            "Zhiwei Zhou",
            "Li Pan",
            "Shijun Liu"
        ],
        "published_in": "Published in: IEEE Transactions on Network Science and Engineering ( Early Access )",
        "date_of_publication": "07 November 2024",
        "doi": "10.1109/TNSE.2024.3494542",
        "publisher": "IEEE",
        "abstract": "With the proliferation of mobile phones, IoT devices, and the rising demand for computational resources, computation offloading has emerged as a promising technique for improving performance, and optimizing resource usage. It involves transferring computational tasks from local devices to edge servers. However, reducing latency and device energy consumption remains a challenge in current research. In this paper, we propose a potential game-theoretic approach to optimize computation offloading in edge computing environments. We consider heterogeneous edge servers, where each server may have different computational capabilities. By formulating the problem as a potential game, we have end devices acting as players deciding whether to execute tasks locally or on edge servers. Our framework includes utility functions capturing the latencyenergy consumption trade-off. Through a detailed analysis, we introduce an innovative algorithm for potential games aiming at achieving Nash equilibrium. This algorithm demonstrates exceptional convergence properties, ensuring reliable convergence even in complex scenarios. Extensive experiments validate the convergence of our algorithm and demonstrate its better performance compared to other benchmark algorithms in terms of latency and energy consumption.",
        "issn": {
            "Electronic ISSN": "2327-4697"
        },
        "keywords": {
            "IEEE Keywords": [
                "Servers",
                "Games",
                "Energy consumption",
                "Computational modeling",
                "Computational efficiency",
                "Clocks",
                "Resource management",
                "Heuristic algorithms",
                "Nash equilibrium",
                "Convergence"
            ],
            "Author Keywords": [
                "Computation offloading",
                "Edge computing",
                "Potential game",
                "Heterogeneous edge servers"
            ]
        },
        "title": "Potential Game-Based Computation Offloading in Edge Computing With Heterogeneous Edge Servers"
    },
    {
        "authors": [
            "Hui Yin",
            "Yin Zhu",
            "Hua Deng",
            "Lu Ou",
            "Zheng Qin",
            "Keqin Li"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "30 September 2024",
        "doi": "10.1109/JIOT.2024.3470891",
        "publisher": "IEEE",
        "abstract": "The deep integration of Internet of Things (IoT) and cloud computing promotes a wide deployment of Body Area Networks (BAN) for smart health services. The data security raises new challenges when patients’ Health Records (HRs) are uploaded into the cloud server by BAN. The Attribute-Based Encryption (ABE) primitive is a potential option to ensure HRs security, which provides the data confidentiality guarantee and fine-grained access control simultaneously via cryptographic means. However, most ABE schemes are unsuitable to be deployed in smart health application as access policies associated with encrypted HRs reveal patient’s privacies. Though the recently proposed ABE with partially hidden access policy based on composite order can alleviate the privacy leakage by only disclosing the attribute names and concealing the practical attribute values, the exposed attribute names still leak individual privacies. In this paper, we put forward a privacy-enhanced and efficient ABE construction with fully hidden access policy over prime order group based on the prominent ABE construction due to Bethencourt et al.. Our scheme hides the sensitive attributes in the access structure by several non-trivial designs without compromising the correctness and security. Moreover, our scheme’s performance is far superior to the attribute partially hidden schemes. Extensive experiments demonstrate the conclusion.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Heart rate",
                "Privacy",
                "Cryptography",
                "Cloud computing",
                "Servers",
                "Internet of Things",
                "Encryption",
                "Smart healthcare",
                "Myocardium",
                "Logic gates"
            ],
            "Author Keywords": [
                "Attribute-based encryption",
                "Access control",
                "Cloud computing",
                "Internet of things",
                "Privacy protection",
                "Smart health"
            ]
        },
        "title": "Privacy-Preservation Enhanced and Efficient Attribute-Based Access Control for Smart Health in Cloud-Assisted Internet of Things"
    },
    {
        "authors": [
            "Jia Xu",
            "Xiao Liu",
            "Azadeh Ghari Neiat",
            "Liju Chu",
            "Xuejun Li",
            "Yun Yang"
        ],
        "published_in": "Published in: IEEE Transactions on Services Computing ( Early Access )",
        "date_of_publication": "28 August 2024",
        "doi": "10.1109/TSC.2024.3451243",
        "publisher": "IEEE",
        "abstract": "With the widespread use of Internet of Things (IoT) technology, an enormous number of end devices that request various kinds of cloud services have been connected to the Internet. Multi-access edge computing (MEC) can reduce the service response time by selecting the required edge computing resources closer to the end device. However, MEC-based smart systems require heterogeneous and diverse services support. Taking the unmanned aerial vehicle (UAV) last-mile delivery system as an example, there are two types of services required: delivery and computational services. The edge services in MEC environments are distributed and limited. Inefficient service selection plans will affect the quality of services of such smart systems. Therefore, how to design a suitable service selection strategy is a crucial issue for MEC-based smart systems. To address this issue, we propose a service selection framework and a holistic and hybrid service selection (HHSS denoted as $H^{2}S^{2}$ ) strategy for MEC-based UAV last-mile delivery systems in real-world UAV last-mile delivery scenarios. This framework considers three important characteristics of UAV delivery systems: diverse service requirements, service volatility, and service mobility. The $H^{2}S^{2}$ strategy focuses on selecting the optimal delivery and computational services and provides an integrated approach with a static service selection algorithm and a dynamic service reselection algorithm. The $H^{2}S^{2}$ strategy determines the optimal delivery and computational service selection plans with the lowest UAV energy consumption and shortest service response time. We evaluate the effectiveness and efficiency of the $H^{2}S^{2}$ strategy through ablation studies and comparisons with various representative strategies. The experimental results show that the $H^{2}S^{2}$ strategy improves the effectiveness and efficiency of the UAV delivery system by significantly reducing the UAVś energy consumption and the service ...",
        "issn": {
            "Electronic ISSN": "1939-1374"
        },
        "keywords": {
            "IEEE Keywords": [
                "Autonomous aerial vehicles",
                "Task analysis",
                "Logistics",
                "Cloud computing",
                "Optimization",
                "Computational modeling",
                "Time factors"
            ],
            "Author Keywords": [
                "Multi-access Edge Computing",
                "Service Selection",
                "Smart Logistics Systems",
                "UAV Delivery"
            ]
        },
        "title": "A Holistic and Hybrid Service Selection Strategy for MEC-based UAV Last-mile Delivery Systems"
    },
    {
        "authors": [
            "Hao Lin",
            "Jinze Du"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "08 October 2024",
        "doi": "10.1109/JIOT.2024.3476248",
        "publisher": "IEEE",
        "abstract": "Delegated Proof of Stake (DPoS) is a widely utilized consensus protocol in blockchain-based Internet of Things (IoT) systems. We propose a heuristic algorithm-based accounting rights allocation method, also referred to as the witness election method, which aims to address the challenges in DPoS. The challenges associated with selected witness nodes that do not reflecting majority stakeholder preferences and susceptibility to manipulation of the vote. This method employs Kendall’s Rank Correlation as the fitness function to optimize the arrangement of the top-k producers, thereby maximizing stakeholder preferences. We propose a novel heuristic algorithm, termed SP-DEWOA, which combines the differential evolution algorithm and whale optimization with piecewise chaotic mapping to maximize permutation similarity, i.e., stakeholder preferences. To further improve the efficiency of SP-DEWOA, we parallelize SP-DEWOA based on the Spark-based parallelization design. Experimental results demonstrate that the witness nodes selected through SP-DEWOA are consistent with the preferences of the majority of stakeholders. Furthermore, SP-DEWOA has been proven to have high scalability and resilience against vote manipulation.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Stakeholders",
                "Voting",
                "Internet of Things",
                "Consensus protocol",
                "Heuristic algorithms",
                "Proof of stake",
                "Whale optimization algorithms",
                "Security",
                "Optimization",
                "Support vector machines"
            ],
            "Author Keywords": [
                "Delegate Proof of Stake",
                "Kendall’s Rank Correlation",
                "whale optimization",
                "Picewise chaotic mapping"
            ]
        },
        "title": "SP-DEWOA: An Evolutionary Distributed Witness Node Election Method for Delegated Proof of Stake"
    },
    {
        "authors": [
            "Jiahuan Hu",
            "Pan Li",
            "Jiahao Feng",
            "Feng Zhou",
            "Ding Yi",
            "Sunil Bisnath"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "09 October 2024",
        "doi": "10.1109/JIOT.2024.3476973",
        "publisher": "IEEE",
        "abstract": "The ubiquity of smartphones catalyzes myriad smartphone-based Internet of Things (IoT) applications, amongst which smartphone positioning which utilizes global navigation satellite system (GNSS) observations to provide spatial information plays a crucial role. However, noisy smartphone GNSS measurements prevent decimeter-level positioning performance in user environments. Recovering the integer property of GNSS carrier phase measurement ambiguities shows great potential in achieving high-accuracy positioning solutions. However, inaccurate ambiguity estimates and short signal wavelengths are the main barriers to successful ambiguity resolution (AR). Therefore, a partial AR with wide-lane (WL) ambiguities and an automatic ambiguity hold strategy is proposed. Simulated results show that, for single-epoch WL AR, even with one WL ambiguity correctly fixed, the positioning solution can be improved by 3 cm. With actual static and kinematic datasets, the proposed algorithm is evaluated and validated. Static results show an improvement of 83% in horizontal position when the WL AR approach is applied, and positioning accuracies can reach 6.8, 2.9, and 11.5 cm in the E, N, and U direction components, respectively. For kinematic data collected in highly variable realistic driving environments, the time series of positioning errors of WL AR solutions exhibit less variation than float solutions. And with fixed WL ambiguities, solutions can be improved to varying degrees, ranging from several centimeters to up to 8 decimeters depending on the environment. The largest improvement of 8 dm is observed for 95th percentile horizontal positioning errors under a suburban environment.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Phase measurement",
                "Internet of Things",
                "Global navigation satellite system",
                "Noise measurement",
                "Kinematics",
                "Antennas",
                "Satellites",
                "Global Positioning System",
                "Frequency measurement",
                "Accuracy"
            ],
            "Author Keywords": [
                "Ambiguity resolution",
                "Global navigation satellite systems",
                "Real-time kinematic",
                "Realistic environment",
                "Smartphone positioning",
                "Wide-lane"
            ]
        },
        "title": "Enhancing Smartphone Relative Positioning With Partial Wide-Lane Ambiguity Resolution: Path to Real-time, Decimeter-Level Positioning in User Environments"
    },
    {
        "authors": [
            "Xun Zhou",
            "Songbai Liu",
            "A. K. Qin",
            "Kay Chen Tan"
        ],
        "published_in": "Published in: IEEE Transactions on Emerging Topics in Computational Intelligence ( Early Access )",
        "date_of_publication": "29 July 2024",
        "doi": "10.1109/TETCI.2024.3427763",
        "publisher": "IEEE",
        "abstract": "The recent proliferation of edge computing has led to the deployment of deep neural networks (DNNs) on edge devices like smartphones and IoT devices to serve end users. However, developing the most suitable DNN model for each on-device task is nontrivial, due to data governance of these tasks and data heterogeneity across them. Existing approaches tackle this issue by learning task-specific models on the device, but this requires substantial computational resources, exacerbating the computational and energy demands on edge devices. This research strives to enhance the deployment efficiency of advanced models on edge devices, with a specific focus on reducing the on-device learning cost. In pursuit of this goal, we propose a category-specific but task-agnostic evolutionary neural architecture search (CSTA-ENAS) method. This method can utilize the available datasets from multiple other tasks in the same category as on-device tasks to design a transferable architecture on the server. Then, this architecture only requires light on-device fine-tuning to satisfactorily solve all different on-device tasks, significantly reducing the on-device learning time and related energy consumption. To improve the search efficiency of our method, a supernet-based partial training strategy is proposed to reduce the evaluation cost for candidate architectures. To showcase the effectiveness of CSTA-ENAS, we build transferable DNN models and evaluate their accuracies on a set of new image classification tasks. Our models demonstrate competitive performance compared to most of the existing task-specific models and transferable models while requiring fewer on-device computational resources.",
        "issn": {
            "Electronic ISSN": "2471-285X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Task analysis",
                "Computer architecture",
                "Data models",
                "Computational modeling",
                "Training",
                "Performance evaluation",
                "Costs"
            ],
            "Author Keywords": [
                "Evolutionary algorithm",
                "neural architecture search",
                "transferable architecture"
            ]
        },
        "title": "Evolutionary Neural Architecture Search for Transferable Networks"
    },
    {
        "authors": [
            "Shimaa A. Mohamed",
            "Sameh Sorour",
            "Sara A. Elsayed",
            "Hossam S. Hassanein"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "21 October 2024",
        "doi": "10.1109/JIOT.2024.3484410",
        "publisher": "IEEE",
        "abstract": "Mobile Edge Computing (MEC) is a promising paradigm for IoT applications requiring synchronized user experiences. However, sustaining scalable and reliable MEC services is challenging when computational resources are overloaded, especially as MEC Service Providers (SPs) must minimize operational costs to maximize profits while offering competitively priced services. This paper proposes the Cooperative Multi-Provider Market (CMPM) scheme, the first to cooperatively enhance service scalability and reliability while addressing the profit-pricing dilemma in a multi-provider market. CMPM enables overloaded Home Service Providers (HSPs) to leverage underutilized computational resources from reliable Foreign Service Providers (FSPs) via reputation-based service replication, meeting the stringent Quality of Service (QoS) requirements for real-time applications involving user groups. CMPM resolves the pricing dilemma by applying a game-theoretic approach, allowing FSPs to dynamically optimize revenue and adjust prices when HSPs cannot meet user demand. We formulate the resource allocation and pricing problem as a Stackelberg game, establish the existence of the equilibrium, and develop a distributed algorithm to reach it. Extensive evaluations show that CMPM significantly reduces unit prices, attracts more HSPs, and better manages high-density user loads compared to state-of-the-art schemes that overlook service provider reputation and social welfare. CMPM also achieves up to 84% higher FSP revenue, a 67% improvement in scalability, and a 70% higher task success rate compared to baseline schemes.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Resource management",
                "Quality of service",
                "Reliability",
                "Scalability",
                "Pricing",
                "Servers",
                "Costs",
                "Synchronization",
                "Internet of Things",
                "Games"
            ],
            "Author Keywords": [
                "Mobile Edge Computing (MEC)",
                "Service Providers",
                "Service Replication",
                "Reputation",
                "Stackelberg Game",
                "Multi-Service Provider Market"
            ]
        },
        "title": "Profitable and Scalable MEC: Reputation-Based Service Replication via Stackelberg Game"
    },
    {
        "authors": [
            "Daniel G. Costa"
        ],
        "published_in": "Published in: IEEE Technology and Society Magazine ( Early Access )",
        "date_of_publication": "08 November 2024",
        "doi": "10.1109/MTS.2024.3487775",
        "publisher": "IEEE",
        "abstract": "Smart cities have gained significant attention and promotion due to the numerous benefits that they are expected to offer in enhancing urban living conditions. These cities, driven by advancements in Internet of Things (IoT), artificial intelligence (AI), and data science (DS) technologies, are increasingly being recognized as crucial tools to achieve sustainable goals [1]. The United Nations highlights the importance of smart cities in the pursuit of its Agenda 2030 stated in its Sustainable Development Goal 11. Nevertheless, while major cities around the world are already committing significant public resources to support such “transformation” [2], we still need to unravel the urban fabric of spatial inequalities that have permeated our cities for centuries.",
        "issn": {
            "Print ISSN": "0278-0097",
            "Electronic ISSN": "1937-416X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Smart cities",
                "Emergency services",
                "Resilience",
                "Terrorism",
                "Floods",
                "Disasters",
                "Artificial intelligence",
                "Smart phones",
                "Resource management",
                "Population density"
            ],
            "Author Keywords": []
        },
        "title": "Resilient Smart Cities for Everyone: Overcoming Inequalities in Emergency Solutions"
    },
    {
        "authors": [
            "Shennan Huang",
            "Lantu Guo",
            "Xue Fu",
            "Yang Peng",
            "Yongan Guo",
            "Yu Wang",
            "Qianyun Zhang",
            "Guan Gui",
            "Hikmet Sari"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "22 May 2024",
        "doi": "10.1109/JIOT.2024.3404042",
        "publisher": "IEEE",
        "abstract": "Specific Emitter Identification (SEI) is pivotal for ensuring the security of the Internet of Things (IoT). Traditional deep learning-based SEI techniques often falter in real-world applications, particularly when distinguishing between legitimate and rogue devices amid noisy conditions and low Signal-to-Noise Ratios (SNR). To surmount these challenges, we propose a novel open-set SEI (OS-SEI) strategy that utilizes a Metric-enhanced Denoising Auto-encoder (MeDAE) architecture. This advanced framework incorporates a deep residual shrinkage network, significantly augmenting the denoising autoencoder’s capability, thereby bolstering its resilience against noisy environments. Further, the integration of discriminative metrics, such as center loss, markedly enhances feature discrimination, resulting in heightened accuracy of device identification. Our comprehensive experimental assessments, conducted on an Automatic Dependent Surveillance-Broadcast (ADS-B) dataset, underscore the superiority of our proposed OS-SEI method over existing models. The findings confirm our approach’s enhanced robustness to noise and its superior accuracy in device identification within open-set scenarios.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Training",
                "Feature extraction",
                "Performance evaluation",
                "Noise reduction",
                "Wireless communication",
                "Signal to noise ratio"
            ],
            "Author Keywords": [
                "Specific emitter identification",
                "deep learning",
                "deep metric learning",
                "denoising autoencoder",
                "open-set",
                "noise robustness"
            ]
        },
        "title": "Open-Set Specific Emitter Identification Leveraging Enhanced Metric Denoising Auto-Encoders"
    },
    {
        "authors": [
            "Yu-Zhen Janice Chen",
            "Daniel S. Menasché",
            "Don Towsley"
        ],
        "published_in": "Published in: IEEE Transactions on Network and Service Management ( Early Access )",
        "date_of_publication": "11 October 2024",
        "doi": "10.1109/TNSM.2024.3468997",
        "publisher": "IEEE",
        "abstract": "Effective resource allocation in sensor networks, IoT systems, and distributed computing is essential for applications such as environmental monitoring, surveillance, and smart infrastructure. Sensors or agents must optimize their resource allocation to maximize the accuracy of parameter estimation. In this work, we consider a group of sensors or agents, each sampling from a different variable of a multivariate Gaussian distribution and having a different estimation objective. We formulate a sensor or agent’s data collection and collaboration policy design problem as a Fisher information maximization (or Cramer-Rao bound minimization) problem. This formulation captures a novel trade-off in energy use, between locally collecting univariate samples and collaborating to produce multivariate samples. When knowledge of the correlation between variables is available, we analytically identify two cases: (1) where the optimal data collection policy entails investing resources to transfer information for collaborative sampling, and (2) where knowledge of the correlation between samples cannot enhance estimation efficiency. When knowledge of certain correlations is unavailable, but collaboration remains potentially beneficial, we propose novel approaches that apply multi-armed bandit algorithms to learn the optimal data collection and collaboration policy in our sequential distributed parameter estimation problem. We illustrate the effectiveness of the proposed algorithms, DOUBLE-F, DOUBLE-Z, UCB-F, UCB-Z, through simulation.",
        "issn": {
            "Electronic ISSN": "1932-4537"
        },
        "keywords": {
            "IEEE Keywords": [
                "Collaboration",
                "Estimation",
                "Data collection",
                "Correlation",
                "Distributed databases",
                "Parameter estimation",
                "Optimization",
                "Vectors",
                "Wireless sensor networks",
                "Resource management"
            ],
            "Author Keywords": [
                "Distributed Parameter Estimation",
                "Sequential Estimation",
                "Sensor Selection",
                "Vertically Partitioned Data",
                "Fisher Information",
                "Multi-Armed Bandit (MAB)",
                "Kalman Filter"
            ]
        },
        "title": "On Collaboration in Distributed Parameter Estimation With Resource Constraints"
    },
    {
        "authors": [
            "Mahmoud Wagih",
            "Andrew Bainbridge",
            "Bashayer Alsulami",
            "Jeff Kettle"
        ],
        "published_in": "Published in: IEEE Journal of Microwaves ( Early Access )",
        "date_of_publication": "02 October 2024",
        "doi": "10.1109/JMW.2024.3455575",
        "publisher": "IEEE",
        "abstract": "Information Communication and Technology (ICT) accounts for an increasing share of global Green House Gas (GHG) emissions. Wireless circuits and systems are indispensable in across all ICT sectors, from cellular networks through satellite communications, to the Internet of Things (IoT). While Life Cycle Assessment (LCA) is an industry-standard methodology for assessing the environmental impact of systems, there has been no comprehensive LCA study focusing on RF systems. We present the first comparative LCA specific to RF and microwave applications, and a design-for-sustainability guide covering mainstream RF applications. With Integrated Circuits (ICs) having the largest environmental impact, the trends in RFICs and MMICs are summarized and evaluated from an environmental perspective. Moving to mmWave frequencies, beyond 20 GHz, results in a shift to over 50% smaller node size above 28 GHz, but with minimal reduction in the core chip area, which inevitably increases the environmental footprint of 5G/6G mmWave systems. We then review active and passive microwave circuits focusing on phased array elements and filters, both distributed and lumped. A bespoke model for RF PCBs is also presented to model the surface finish and transmission line or antenna area accurately. Our LCA indicates that design choices such as the CMOS process, PCB material and surface finish, can have a large environmental impact at the manufacturing stage. We highlight the importance of low-loss components, comparing microstrip and LC filters, where a higher end-to-end RF system efficiency directly translates to a lower global warming potential (GWP), reducing Scope 2 GHG emissions.",
        "issn": {
            "Electronic ISSN": "2692-8388"
        },
        "keywords": {
            "IEEE Keywords": [
                "Life cycle assessment",
                "Microwave communication",
                "Electromagnetic heating",
                "Microwave integrated circuits",
                "Microwave circuits",
                "Microwave FET integrated circuits",
                "Radio frequency",
                "Wireless communication",
                "Three-dimensional printing",
                "Climate change"
            ],
            "Author Keywords": [
                "Additive manufacturing",
                "life-cycle assessment (LCA) microstrip lines",
                "microwaves in climate change",
                "printed circuit boards",
                "radio frequency (RF)",
                "sustainable electronics",
                "transmission lines"
            ]
        },
        "title": "Environmental Life-Cycle Assessment (LCA) of Wireless RF Systems: A Comparative Sustainability Analysis and a Microwave Engineers' Guide to LCA"
    },
    {
        "authors": [
            "Zhenyu Wang",
            "Yingdong Yang",
            "Fucheng Wu"
        ],
        "published_in": "Published in: IEEE Transactions on Industrial Informatics ( Early Access )",
        "date_of_publication": "27 August 2024",
        "doi": "10.1109/TII.2024.3441653",
        "publisher": "IEEE",
        "abstract": "Air quality monitor is important for both the environment and human health. In recent years, many computer vision-based air quality monitoring methods have been proposed and have achieved good accuracy in specific scenarios. However, current deep-learning-based models' deployment on mobile devices is impeded due to their excessively large size. To build a lightweight model, we investigate the potential of dilated convolutional kernels, which can effectively increase the receptive field and acquire multiscale information while maintaining model size. Eventually, we propose a novel method termed multiscale dilated network. The network is tailored to the characteristics of the air quality dataset by rearranging the dilate rate of the dilated convolutional kernels to focus on the global and semantically higher level features, thus improving the detection efficiency and accuracy of the model. Our proposed multiscale dilated block also includes a multiscale fusion mechanism that enables the model to extract air quality information at multiple scales simultaneously. Compared to the current state-of-the-art models, the multiscale dilated network integrates three different scales of dilated kernels, which leads to a reduction of 86.7% of parameters and 88.5% of floating point operations and achieves a top-1 classification accuracy of 94.2%. With the proposed model enhancements, we envision a future where mobile phones and IoT edge devices can function as mobile air quality monitoring stations, thereby creating a dynamic and distributed air quality map for air quality protection.",
        "issn": {
            "Print ISSN": "1551-3203",
            "Electronic ISSN": "1941-0050"
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Atmospheric modeling",
                "Air quality",
                "Monitoring",
                "Task analysis",
                "Computational modeling",
                "Accuracy"
            ],
            "Author Keywords": [
                "Air quality",
                "deep learning",
                "dilated convolutional kernel",
                "lightweight",
                "mobile phones"
            ]
        },
        "title": "A Lightweight Air Quality Monitoring Method Based on Multiscale Dilated Convolutional Neural Network"
    },
    {
        "authors": [
            "Jinghai Wang",
            "Shanlin Xiao",
            "Jilong Luo",
            "Bo Li",
            "Lingfeng Zhou",
            "Zhiyi Yu"
        ],
        "published_in": "Published in: IEEE Transactions on Very Large Scale Integration (VLSI) Systems ( Early Access )",
        "date_of_publication": "01 October 2024",
        "doi": "10.1109/TVLSI.2024.3464870",
        "publisher": "IEEE",
        "abstract": "Asynchronous circuits with low power and robustness are revived in emerging applications such as the Internet of Things (IoT) and neuromorphic chips, thanks to clock-less and event-driven mechanisms. However, the lack of mature computer-aided design (CAD) tools for designing large-scale asynchronous circuits results in low design efficiency and high cost. This article proposes an end-to-end bundled-data (BD) asynchronous circuit design flow, which can facilitate building asynchronous circuits, even if the designer has little or no asynchronous circuit foundation. Three features that enable this are: 1) a lightweight circuit converter developed in Python can convert circuits from synchronous descriptions to corresponding asynchronous ones at register transfer level (RTL). Desynchronization flow helps designers maintain a “synchronization mentality” to construct asynchronous circuits; 2) a synchronization-like verification method is proposed for asynchronous circuits so that it can be functionally verified before synthesis. Avoids the risk of rework after logic defects are discovered during the synthesis and implementation, as asynchronous circuits often cannot be simulated until gate-level (GL) netlist generation; and 3) the whole implementation flow from RTL to graphic data system (GDS) is based on commercial electronic design automation (EDA) tools. Similar to the design flow of synchronous circuits, it helps designers implement asynchronous circuits with “synchronization habits.” Furthermore, to validate this methodology, two asynchronous processors were, respectively, implemented and evaluated in the TSMC 28-nm CMOS process. Compared to their synchronous counterparts, the general-purpose asynchronous RISC-V processor achieves 20.5% power savings. And the domain-specific asynchronous spiking neural network (SNN) accelerator achieves 58.46% power savings and 2.41 $\\times$ energy efficiency improvement at 70% input spike sparsity.",
        "issn": {
            "Print ISSN": "1063-8210",
            "Electronic ISSN": "1557-9999"
        },
        "keywords": {
            "IEEE Keywords": [
                "Asynchronous circuits",
                "Circuits",
                "Timing",
                "Program processors",
                "Clocks",
                "Protocols",
                "Optical signal processing",
                "Encoding",
                "Logic gates",
                "Internet of Things"
            ],
            "Author Keywords": [
                "Asynchronous circuits",
                "design flow",
                "low power",
                "RISC-V",
                "spiking neural networks (SNNs)"
            ]
        },
        "title": "An End-to-End Bundled-Data Asynchronous Circuits Design Flow: From RTL to GDS"
    },
    {
        "authors": [
            "Zhengchuan Chen",
            "Kang Lang",
            "Nikolaos Pappas",
            "Howard H. Yang",
            "Min Wang",
            "Zhong Tian",
            "Tony Q. S. Quek"
        ],
        "published_in": "Published in: IEEE Transactions on Communications ( Early Access )",
        "date_of_publication": "04 June 2024",
        "doi": "10.1109/TCOMM.2024.3409194",
        "publisher": "IEEE",
        "abstract": "Timely status updating is the premise of emerging interaction-based applications in the Internet of Things (IoT). Using redundant devices to update the status of interest is a promising method to improve the timeliness of information. However, parallel status updating leads to out-of-order arrivals at the monitor, significantly challenging timeliness analysis. This work studies the Age of Information (AoI) of a multi-queue status update system where multiple devices monitor the same physical process. Specifically, two systems are considered: the Basic System , which only has type-1 devices that are ad hoc devices located close to the source, and the Hybrid System , which contains additional type-2 devices that are infrastructure-based devices located in fixed points compared to the Basic System . Using the Stochastic Hybrid Systems (SHS) framework, a mathematical model that combines discrete and continuous dynamics, we derive the expressions of the average AoI of the considered two systems in closed form. Numerical results verify the accuracy of the analysis. It is shown that when the number and parameters of the type-1 devices/type-2 devices are fixed, the logarithm of average AoI will linearly decrease with the logarithm of the total arrival rate of type-2 devices or that of the number of type-1 devices under specific condition. It has also been demonstrated that the proposed systems can significantly outperform the FCFS M/M/ N status update system.",
        "issn": {
            "Print ISSN": "0090-6778",
            "Electronic ISSN": "1558-0857"
        },
        "keywords": {
            "IEEE Keywords": [
                "Monitoring",
                "Performance evaluation",
                "Measurement",
                "Servers",
                "Internet of Things",
                "Surgery",
                "Queueing analysis"
            ],
            "Author Keywords": [
                "Parallel status updating systems",
                "Age of Information",
                "Stochastic Hybrid Systems"
            ]
        },
        "title": "Timeliness of Status Update System: The Effect of Parallel Transmission Using Heterogeneous Updating Devices"
    },
    {
        "authors": [
            "Alaa Tharwat",
            "Wolfram Schenck"
        ],
        "published_in": "Published in: IEEE Transactions on Neural Networks and Learning Systems ( Early Access )",
        "date_of_publication": "26 January 2024",
        "doi": "10.1109/TNNLS.2024.3352279",
        "publisher": "IEEE",
        "abstract": "Recently, the massive growth of IoT devices and Internet data, which are widely used in many applications, including industry and healthcare, has dramatically increased the amount of free unlabeled data collected. However, this unlabeled data is useless if we want to learn supervised machine learning models. The expensive and time-consuming cost of labeling makes the problem even more challenging. Here, the active learning (AL) technique provides a solution by labeling small but highly informative and representative data, which guarantees a high degree of generalizability over space and improves classification performance with data we have never seen before. The task is more difficult when the active learner has no predefined knowledge, such as initial training data, and when the obtained data is incomplete (i.e., contains missing values). In previous studies, the missing data should first be imputed. Then, the active learner selects from the available unlabeled data, regardless of whether the points were originally observed or imputed. However, selecting inaccurate imputed data points would negatively affect the active learner and prevent it from selecting informative and/or representative points, thus reducing the overall classification performance of the prediction models. This motivated us to introduce a novel query selection strategy that accounts for imputation uncertainty when querying new points. For this purpose, we first introduce a novel multiple imputation method that considers feature importance in selecting the most promising feature groups for missing values estimation. This multiple imputation method provides the ability to quantify the imputation uncertainty of each imputed data point. Furthermore, in each of the two phases of the proposed active learner (exploration and exploitation), imputation uncertainty is taken into account to reduce the probability of selecting points with high imputation uncertainty. We tested the effectiveness of the propos...",
        "issn": {
            "Print ISSN": "2162-237X",
            "Electronic ISSN": "2162-2388"
        },
        "keywords": {
            "IEEE Keywords": [
                "Uncertainty",
                "Labeling",
                "Data models",
                "Training data",
                "Costs",
                "Predictive models",
                "Search problems"
            ],
            "Author Keywords": [
                "Active learning (AL)",
                "imputation uncertainty",
                "missing data",
                "multiple imputation"
            ]
        },
        "title": "Active Learning for Handling Missing Data"
    },
    {
        "authors": [
            "JiHyun Park",
            "Sieun Choi",
            "TaeYoung Kim",
            "ChanMin Lee",
            "SuKyoung Lee"
        ],
        "published_in": "Published in: IEEE Transactions on Intelligent Transportation Systems ( Early Access )",
        "date_of_publication": "02 September 2024",
        "doi": "10.1109/TITS.2024.3449034",
        "publisher": "IEEE",
        "abstract": "With the advancements in artificial intelligence technology, unmanned aerial vehicles (UAVs) are increasingly being utilized for various smart applications, such as surveillance systems. However, because of their limited computing resources and battery capacity, it is necessary to offload computationally intensive tasks to ground infrastructure, such as edge servers and vehicles. This approach faces challenges, especially in densely populated cities where edge servers may process tasks slower because they receive requests not only from UAVs but also from a number of Internet of Things (IoT) devices. Additionally, in the case of private vehicles, their highly dynamic and unpredictable mobility, coupled with self-interested tendencies may result in a reluctance to share computing resources without incentives. Addressing these limitations, this paper proposes a UAV task offloading scheme utilizing public buses pursuing public service objectives. An optimization problem is formulated to minimize the UAV’s system cost, including energy consumption and task completion delay, and an algorithm based on the successive convex approximation method is introduced. Public bus information and a map of Seoul are utilized in the simulation to ensure the real-world applicability of the proposed method. The simulation results indicate that our method not only reduces the system cost compared with that of other benchmark schemes but also notably improves the task completion rate.",
        "issn": {
            "Print ISSN": "1524-9050",
            "Electronic ISSN": "1558-0016"
        },
        "keywords": {
            "IEEE Keywords": [
                "Task analysis",
                "Autonomous aerial vehicles",
                "Servers",
                "Smart cities",
                "Internet of Things",
                "Costs",
                "Real-time systems"
            ],
            "Author Keywords": [
                "Unmanned aerial vehicles",
                "task offloading",
                "public bus",
                "system cost",
                "task completion rate",
                "successive convex approximation"
            ]
        },
        "title": "Public Bus-Assisted Task Offloading for UAVs"
    },
    {
        "authors": [
            "Yufei Zhang",
            "Hongwei Wang",
            "Weiming Shen",
            "Gongzhuang Peng"
        ],
        "published_in": "Published in: IEEE Transactions on Automation Science and Engineering ( Early Access )",
        "date_of_publication": "11 September 2023",
        "doi": "10.1109/TASE.2023.3307588",
        "publisher": "IEEE",
        "abstract": "Surface defect is a crucial factor affecting the product quality of steel products. Current studies mainly focus on defect recognition and classification using machine vision-based algorithms, which lack the trace of potential causes and the reuse of experiential knowledge. To address this issue, we construct a knowledge graph for steel surface defects by fusing the multi-source and heterogeneous industrial data, including process parameters, chemical compositions, defect images, operation logs and empirical knowledge. A policy-based reinforcement learning approach is developed to solve the path reasoning problem over the industrial knowledge graph in defect detection and diagnosis. The approach employs two agents to explore the path efficiently from opposite directions, utilizes an integrated reward function that comprehensively considers the path direction, path length and entity distance to perform action selection, and adopts the path sharing mechanism and the prior knowledge to update selection policy. Experimental comparisons with the state-of-the-art knowledge reasoning algorithms on two benchmark datasets, NELL-995 and FB15K-237, validate the performance and merits of the proposed method. The effectiveness of the proposed method is also evaluated on a practical steel surface defect dataset, and the results show that our approach performs well in knowledge reasoning on the surface defect graph. Note to Practitioners —The surface quality of products has become a widely concerned focus in manufacturing industries. With the development of industrial IoT and Cyber-physical system technologies, more and more industrial data has been collected, and machine learning-based algorithms have been developed and applied to the recognition of detect defects. However, the algorithms do not take full advantage of the multi-source and heterogeneous defect-related data. On the other hand, it is also difficult to accumulate, inherit and reuse the experts’ knowledge of solving ...",
        "issn": {
            "Print ISSN": "1545-5955",
            "Electronic ISSN": "1558-3783"
        },
        "keywords": {
            "IEEE Keywords": [
                "Steel",
                "Knowledge graphs",
                "Cognition",
                "Surface treatment",
                "Production",
                "Feature extraction",
                "Classification algorithms"
            ],
            "Author Keywords": [
                "Steel surface defect",
                "knowledge graph",
                "reinforcement learning",
                "prior knowledge"
            ]
        },
        "title": "DuAK: Reinforcement Learning-Based Knowledge Graph Reasoning for Steel Surface Defect Detection"
    },
    {
        "authors": [
            "Rong Wang",
            "Xu Zhuang",
            "Xiaogang Zhu",
            "Ali Kashif Bashir",
            "Maryam M. Al Dabel",
            "Keping Yu"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "11 June 2024",
        "doi": "10.1109/TCE.2024.3412678",
        "publisher": "IEEE",
        "abstract": "The Consumer Internet of Things (CIoT) integrates the advantage of Internet of Things (IoT) technologies to provide convenience in consumers’ daily lives. With the rapid development of the CIoT, data collected from consumer smart devices has increased exponentially. In the CIoT, web pages, as internet information carriers, offer spammers opportunities to conduct security attacks, which could harm the CIoT systems. Inspired by this challenge, this paper introduces an intelligent feature extraction method, page2vec, and a new classification algorithm, RFiRF, to detect web spam in the CIoT. page2vec is based on a score propagation model, which calculates each page’s goodness and badness scores through the links of a web graph. It is observed that different scoring functions can produce different web page features. Based on this observation, 20 to 30 scoring functions are designed and incorporated into page2vec. This way, page2vec can automatically extract web page features from a web graph. Unfortunately, page2vec also brings a high dimensionality problem when constructing binary classifiers for spam detection. To address this problem, a new classification algorithm, called Random Forest in Random Forest (RFiRF), is proposed. RFiRF replaces a random forest’s meta-classifier (decision tree) with a random forest. It divides the sparse data space into dense sub-spaces by randomly sampling training instances and features. Experiments on two benchmark datasets show that 1) page2vec features are much more predictive than the raw link-based and node2vec features, and 2) RFiRF outperforms some classification algorithms in most cases when facing the high dimensionality problem. We hope this paper can give peers valuable insights into the CIoT security.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Random forests",
                "Internet of Things",
                "Classification algorithms",
                "Web pages",
                "Unsolicited e-mail",
                "Task analysis"
            ],
            "Author Keywords": [
                "Consumer Internet of Things security",
                "graph mining",
                "web spam detection",
                "feature extraction"
            ]
        },
        "title": "Intelligent Web Spam Detection in the Consumer Internet of Things"
    },
    {
        "authors": [
            "Xinping Rao",
            "Le Qin",
            "Yugen Yi",
            "Jin Liu",
            "Gang Lei",
            "Yuanlong Cao"
        ],
        "published_in": "Published in: IEEE Transactions on Network and Service Management ( Early Access )",
        "date_of_publication": "27 September 2024",
        "doi": "10.1109/TNSM.2024.3469374",
        "publisher": "IEEE",
        "abstract": "In recent years, indoor localization has attracted a lot of interest and has become one of the key topics of Internet of Things (IoT) research, presenting a wide range of application scenarios. With the advantages of ubiquitous universal Wi-Fi platforms and the \"unconscious collaborative sensing\" in the monitored target, Channel State Information (CSI)-based device-free passive indoor fingerprinting localization has become a popular research topic. However, most existing studies have encountered the difficult issues of high deployment labor costs and degradation of localization accuracy due to fingerprint variations in real-world dynamic environments. In this paper, we propose BSWCLoc, a device-free passive fingerprint localization scheme based on the beyond-sharing-weights approach. BSWCLoc uses the calibrated CSI phases, which are more sensitive to the target location, as localization features and performs feature processing from a two-dimensional perspective to ultimately obtain rich fingerprint information. This allows BSWLoc to achieve satisfactory accuracy with only one communication link, significantly reducing deployment consumption. In addition, a beyond-sharing-weights (BSW) method for domain adaptation is developed in BSWCLoc to address the problem of changing CSI in dynamic environments, which results in reduced localization performance. The BSW method proposes a dual-flow structure, where one flow runs in the source domain and the other in the target domain, with correlated but not shared weights in the adaptation layer. BSWCLoc greatly exceeds the state-of-the-art in terms of positioning accuracy and robustness, according to an extensive study in the dynamic indoor environment over 6 days.",
        "issn": {
            "Electronic ISSN": "1932-4537"
        },
        "keywords": {
            "IEEE Keywords": [
                "Location awareness",
                "Fingerprint recognition",
                "Accuracy",
                "Feature extraction",
                "Training",
                "Adaptation models",
                "Databases",
                "Wireless fidelity",
                "Bayes methods",
                "Support vector machines"
            ],
            "Author Keywords": [
                "Channel State Information (CSI)",
                "Convolutional Neural Network (CNN)",
                "Domain Adaptation",
                "Indoor Device-free Passive Localization"
            ]
        },
        "title": "A Novel Adaptive Device-Free Passive Indoor Fingerprinting Localization Under Dynamic Environment"
    },
    {
        "authors": [
            "Meng Deng",
            "Zhou Huan",
            "Jiang Kai",
            "Zheng Hantong",
            "Cao Yue",
            "Chen Peng"
        ],
        "published_in": "Published in: China Communications ( Early Access )",
        "date_of_publication": "09 April 2024",
        "doi": "10.23919/JCC.ea.2022-0591.202401",
        "publisher": "IEEE",
        "abstract": "Edge caching has emerged as a promising application paradigm in 5G networks, and by building edge networks to cache content, it can alleviate the traffic load brought about by the rapid growth of Internet of Things (IoT) services and applications. Due to the limitations of Edge Servers (ESs) and a large number of user demands, how to make the decision and utilize the resources of ESs are significant. In this paper, we aim to minimize the total system energy consumption in a heterogeneous network and formulate the content caching optimization problem as a Mixed Integer Non-Linear Programming (MINLP). To address the optimization problem, a Deep Q-Network (DQN)-based method is proposed to improve the overall performance of the system and reduce the backhaul traffic load. In addition, the DQN-based method can effectively solve the limitation of traditional reinforcement learning (RL) in complex scenarios. Simulation results show that the proposed DQN-based method can greatly outperform other benchmark methods, and significantly improve the cache hit rate and reduce the total system energy consumption in different scenarios.",
        "issn": {},
        "keywords": {
            "IEEE Keywords": [
                "Delays",
                "Optimization",
                "Energy consumption",
                "Deep reinforcement learning",
                "Costs",
                "Telecommunication traffic",
                "Servers"
            ],
            "Author Keywords": [
                "deep reinforcement learning",
                "edge caching",
                "energy consumption",
                "markov decision process"
            ]
        },
        "title": "Deep reinforcement learning for energy-efficient edge caching in mobile edge networks"
    },
    {
        "authors": [
            "Bomin Mao",
            "Yingying Wu",
            "Jiajia Liu",
            "Hongzhi Guo",
            "Jiadai Wang",
            "Nei Kato"
        ],
        "published_in": "Published in: IEEE Transactions on Cognitive Communications and Networking ( Early Access )",
        "date_of_publication": "04 September 2024",
        "doi": "10.1109/TCCN.2024.3454256",
        "publisher": "IEEE",
        "abstract": "Non-Orthogonal Multiple Access (NOMA) based Federated Learning (FL) can achieve the massive connectivity of Internet of Thing (IoT) devices, high transmission rate, and pervasive intelligence in 6G networks. However, the stochastic channels and frequent model parameter updates may incur degraded transmission rate and diminished FL performance, while privacy leakage may happen if Eavesdroppers (Eves) intercept the FL training process. To address the above issues, we exploit Intelligent Reflecting Surface (IRS) to reconfigure wireless signal propagation for secure transmission and fast convergence of NOMA-based FL. In this article, a Deep Reinforcement Learning (DRL) based approach is proposed to jointly optimize the transmission power of edge devices and IRS phase shift to maximize the minimum secrecy rate in the model parameter uploading process. Numerical results validate the efficiency of our proposed algorithm and demonstrate that IRS can improve the secrecy rate.",
        "issn": {
            "Electronic ISSN": "2332-7731"
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "NOMA",
                "Wireless communication",
                "Security",
                "Communication system security",
                "6G mobile communication",
                "Privacy"
            ],
            "Author Keywords": [
                "Federated Learning (FL)",
                "Non-Orthogonal Multiple Access (NOMA)",
                "Intelligent Reflecting Surface (IRS)",
                "secrecy rate"
            ]
        },
        "title": "Optimizing Secrecy Rate for Federated Learning Model Aggregation With Intelligent Reflecting Surface Towards 6G Ubiquitous Intelligence"
    },
    {
        "authors": [
            "Zhiyuan Li",
            "Xia Zhang"
        ],
        "published_in": "Published in: IEEE Journal of Biomedical and Health Informatics ( Early Access )",
        "date_of_publication": "20 August 2024",
        "doi": "10.1109/JBHI.2024.3434420",
        "publisher": "IEEE",
        "abstract": "In today's era of rapidly advancing technologies, such as sensors and the Internet of Things (IoT), and the increasing focus on promoting healthy lifestyles, smart wearable devices play a crucial role in real-time detection and diagnosis of physical health conditions. Through analyzing the multi-featured time series data captured by these devices with multiple sensors, we can uncover hidden diseases and provide timely treatment. Therefore, it is imperative to study an anomaly detection model with robust feature learning and anomaly diagnosis capabilities. To address this need, this paper proposes an enhanced autoencoder-based anomaly detection model for time series data obtained from wearable medical devices. Initially, the model utilizes a convolutional neural network to learn the correlations between multiple features. Subsequently, a long and short-term memory network is employed to capture the sequence correlations, and an multi-head attention mechanism is used to mitigate the performance degradation caused by increasing the sequence length. The residual loss is also used to effectively mitigate the vanishing gradient problem. Finally, the model is evaluated using two widely recognized public datasets: the HeartDisease dataset, which contains information on patients with heart conditions, and the MIMIC dataset, a comprehensive database of de-identified health data related to critical care. The experimental results demonstrate that our model can achieve an accuracy of 95.37% and 95.56% on the two datasets, respectively. Compared to the best performing baseline methods, our model improves 8.6% and 12.3% on the two datasets, respectively. Overall, our model enables efficient analysis of sequential data, effectively captures long-term dependencies, and significantly improves the success rate of early health diagnosis for individuals.",
        "issn": {
            "Print ISSN": "2168-2194",
            "Electronic ISSN": "2168-2208"
        },
        "keywords": {
            "IEEE Keywords": [
                "Time series analysis",
                "Anomaly detection",
                "Data models",
                "Biomedical monitoring",
                "Feature extraction",
                "Correlation",
                "Computational modeling"
            ],
            "Author Keywords": [
                "Early health diagnosis",
                "Time series analysis",
                "Anomaly detection",
                "Autoencoder"
            ]
        },
        "title": "An Enhanced Autoencoder-based Anomaly Detection Model for time series Data from Wearable Medical Devices"
    },
    {
        "authors": [
            "Ahmadreza Abazari",
            "Mohammad Mahdi Soleymani",
            "Saba Marandi",
            "Mohsen Ghafouri",
            "Chadi Assi",
            "Danial Jafarigiv",
            "Ribal Atallah"
        ],
        "published_in": "Published in: IEEE Reliability Magazine ( Early Access )",
        "date_of_publication": "20 September 2024",
        "doi": "10.1109/MRL.2024.3451429",
        "publisher": "IEEE",
        "abstract": "The electrification of private and public transportation systems, e.g., through the use of electric vehicles (EVs), is a promising step toward achieving a sustainable future and reducing the negative environmental impacts of traditional transportation methods. As of 2021, there were approximately 7.2 million EVs worldwide, with a record 3.2 million EVs sold during the COVID-19 pandemic in 2020. Furthermore, global sales of electric cars have kept rising strongly in 2022, with 2 million sold in the first quarter. In addition to Level 1 and Level 2 chargers, EV ecosystems now include Level 3 chargers (dc fast chargers) with 40–350-kW power ratings. Large-scale deployment of EVs provides power grid operators with several opportunities, such as bidirectional energy transfers and frequency and voltage ancillary services, which can improve the reliability and efficiency of power grids. To fully realize these advantages for EV ecosystems and power grids, information and communication technologies (ICTs) in the EV infrastructure and smart power grids must be widely employed. On this basis, the EV supply equipment (EVSE) network relies on various Internet-of-Things (IoT) devices and smartphone applications to facilitate the charging process for EV users. However, the widespread deployment of cyber devices and information technologies makes these systems vulnerable to EV-based cyberattacks, including data manipulation and malware injections.",
        "issn": {
            "Electronic ISSN": "2641-8819"
        },
        "keywords": {
            "IEEE Keywords": [
                "Power grids",
                "Ecosystems",
                "Microprogramming",
                "Charging stations",
                "Reliability",
                "Power system dynamics",
                "Vectors"
            ],
            "Author Keywords": []
        },
        "title": "Electric Vehicle-Based Load-Altering Attacks and Their Impacts on Power Grids Operations"
    },
    {
        "authors": [
            "Ling Li",
            "Yan Zhang",
            "Lin Yuan",
            "Shuang Li",
            "Xinbo Gao"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "06 November 2024",
        "doi": "10.1109/JIOT.2024.3492716",
        "publisher": "IEEE",
        "abstract": "Face Super-Resolution (FSR) is critical for bolstering intelligent security in Internet of Things (IoT) systems. Recent deep learning-driven FSR algorithms have attained remarkable progress. However, they always require separate model training and optimization for each scaling factor or input resolution, leading to inefficiency and impracticality. To overcome these limitations, we propose SAFNet, an innovative framework tailored for scale-adaptive FSR with arbitrary input resolution. SAFNet integrates scale information into representation learning to enable adaptive feature extraction and introduces dual-embedding attention to boost adaptive feature reconstruction. It leverages facial self-similarity and spatial-frequency collaboration to achieve precise scale-aware SR representations. This is attained through three key modules: the Scale Adaption Guidance Unit (SAGU), the Scale-aware Non-Local Self-similarity (SNLS) module, and the Spatial-Frequency Interactive Modulation (SFIM) module. SAGU imports scaling factors using frequency encoding, SNLS exploits self-similarity to enrich feature representations, and SFIM incorporates spatial and frequency information to predict target pixel values adaptively. Comprehensive evaluations across four benchmark datasets reveal that SAFNet outperforms the second-best compared state-of-the-art (SOTA) method by about 0.2dB/0.007 in PSNR/SSIM (×4 on CelebA) with reduced 18.68%/42.64% computational complexity/time cost. This demonstrates SAFNet’s effectiveness and superiority, showcasing its potential as a promising solution for scale and input resolution adaptation challenges in FSR. The code will be available at https://github.com/ICVIPLab/SAFNethttps://github.com/ICVIPLab/SAFNet.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Faces",
                "Internet of Things",
                "Spatial resolution",
                "Adaptation models",
                "Superresolution",
                "Image reconstruction",
                "Face recognition",
                "Encoding",
                "Data mining"
            ],
            "Author Keywords": [
                "Face super-resolution (FSR)",
                "Scale adaptation",
                "Self-similarity",
                "Frequency analysis",
                "Complementarity"
            ]
        },
        "title": "See as You Desire: Scale-adaptive Face Super-Resolution for Varying Low Resolutions"
    },
    {
        "authors": [
            "Muhammad Asaad Cheema",
            "Vinay Chakravarthi Gogineni",
            "Pierluigi Salvo Rossi",
            "Stefan Werner"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "14 August 2024",
        "doi": "10.1109/JIOT.2024.3443467",
        "publisher": "IEEE",
        "abstract": "Distributed and collaborative machine learning over emerging IoT networks is complicated by resource constraints, device and data heterogeneity, and the need for personalized models that cater to the individual needs of each network device. This complexity becomes even more pronounced when new devices are added to a system that must rapidly adapt to personalized models. Along these lines, we propose a networked federated meta-learning (NF-ML) algorithm that utilizes meta-learning and underlying shared structures across the network to enable fast and personalized model adaptation of newly added network devices. The NF-ML algorithm learns two sets of model parameters for each device in a distributed manner, with devices communicating only with their immediate neighbors. One set of parameters is personalized for the device-specific task, whereas the other is a generic parameter set learned via peer-to-peer communication. The performance of the proposed NF-ML algorithm was validated using both synthetic and real-world data, and the results show that it adapts to new tasks in just a few epochs, using as little as 10% of the available data, significantly outperforming traditional federated learning methods.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Servers",
                "Adaptation models",
                "Internet of Things",
                "Training",
                "Metalearning",
                "Task analysis",
                "Data models"
            ],
            "Author Keywords": [
                "generic parameters",
                "meta-learning",
                "graph federated learning",
                "distributed"
            ]
        },
        "title": "Networked Federated Meta-Learning Over Extending Graphs"
    },
    {
        "authors": [
            "Eslam Mounier",
            "Mohamed Elhabiby",
            "Michael Korenberg",
            "Aboelmagd Noureldin"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "06 November 2024",
        "doi": "10.1109/JIOT.2024.3492913",
        "publisher": "IEEE",
        "abstract": "Accurate and reliable positioning is essential for Vehicular iot applications, such as autonomous and connected vehicles, to ensure their effective and safe operation. This calls for innovative methods that leverage various sensors and systems to fulfill such demands across diverse environmental and operational conditions. This paper presents a multi-sensor positioning and navigation system that leverages cost-effective commercial-grade sensors for gnss-challenging urban and indoor environments. The system integrates the vehicle’s onboard motion sensor measurements with 3D point clouds from lidar registered to high-accuracy 3D digital maps for sustained decimeter-level positioning accuracy. Key contributions include accurate lidar scan georeferencing with motion compensation, efficient map-to-map registration, and an effective decentralized fusion. Road test experiments on a professional land vehicle setup equipped with a multisensory navigation instrument were performed in downtown and covered parking garage environments with accurate 3D gdb available. Results from several road test trajectories demonstrate robust high-precision positioning performance with an average RMSE of 20 cm horizontally and 13 cm vertically, as well as position errors of less than 50 cm for 97% of the time and less than 30 cm for 90.7% of the time. The proposed system is a practical option for the positioning and navigation of self-driving cars and has the potential for cooperative mapping and updating 3D city maps.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Laser radar",
                "Three-dimensional displays",
                "Accuracy",
                "Sensors",
                "Navigation",
                "Global navigation satellite system",
                "Internet of Things",
                "Point cloud compression",
                "Urban areas",
                "Pose estimation"
            ],
            "Author Keywords": [
                "Multi-sensor fusion",
                "LiDAR",
                "inertial sensors",
                "positioning",
                "autonomous driving",
                "3D maps",
                "geodatabase (GDB)"
            ]
        },
        "title": "LiDAR-Based Multi-Sensor Fusion with 3D Digital Maps for High-Precision Positioning"
    },
    {
        "authors": [
            "Xuli Gao",
            "Liqin Shi",
            "Yinghui Ye",
            "Gan Zheng",
            "Guangyue Lu"
        ],
        "published_in": "Published in: IEEE Transactions on Vehicular Technology ( Early Access )",
        "date_of_publication": "01 October 2024",
        "doi": "10.1109/TVT.2024.3472093",
        "publisher": "IEEE",
        "abstract": "In this paper, we investigate a short-packet backscatter communication (BackCom) network, where multiple Internet of Things (IoT) nodes (INs) subsequently backscatter their short-packet data to the information receiver via BackCom. In order to improve the reliability of the investigated network, we formulate an optimization problem to minimize the maximum error probability (EP) among all INs by jointly optimizing the transmit power (TP) of the dedicated energy source (ES), the short-packet blocklength and power reflection coefficient (PRC) at each IN. The problem is formulated as a mixed-integer non-linear programming problem. To tackle it, we first derive the close-form expressions for both the optimal TP at the ES and the optimal PRC at each IN, resulting in two subproblems where the optimal TP and PRC are utilized. Then the optimal solution to the original problem (OP) corresponds to that of the subproblem with the lower EP at the worst IN. After replacing integer variables with continuous ones, the convexities of the two subproblems are demonstrated. Consequently, the solution to the OP can be achieved by addressing the two subproblems via convex tools and utilizing an integer conversion method. Simulation results show the superiority of the proposed scheme in terms of the EP by comparing with several benchmark schemes.",
        "issn": {
            "Print ISSN": "0018-9545",
            "Electronic ISSN": "1939-9359"
        },
        "keywords": {
            "IEEE Keywords": [
                "Reliability",
                "Optimization",
                "Backscatter",
                "Resource management",
                "Signal to noise ratio",
                "Receivers",
                "Internet of Things",
                "Channel estimation",
                "Autonomous aerial vehicles",
                "Throughput"
            ],
            "Author Keywords": [
                "Backscatter communications",
                "short-packet communications",
                "resource allocation",
                "reliability"
            ]
        },
        "title": "Reliability-Oriented Resource Allocation in Short-Packet Backscatter Communications"
    },
    {
        "authors": [
            "Mohit Kumar Saxena",
            "Kapal Dev",
            "Sudhir Kumar"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "18 July 2024",
        "doi": "10.1109/TCE.2024.3427725",
        "publisher": "IEEE",
        "abstract": "Consumer Electronics (CE) devices in healthcare, smart homes, entertainment, etc., generate a substantial volume of computing tasks in Consumer IoT (CIoT) networks. However, CE devices typically have limited computational resources to ensure the minimum latency and energy consumption in task execution. Thus, CE devices can offload their computational tasks to fog nodes, which are the computational resources between CE devices and the centralized cloud. Hence, CE devices can efficiently overcome the dependency on far-located cloud servers. However, the resource limitation of fog nodes imposes new challenges in resource allocation in fog networks. The computational tasks do not always need the total available computational resources of the fog nodes. Hence, in this work, we propose an optimization strategy that dynamically adjusts the fog nodes’ Central Processing Unit (CPU) frequency with variations in the task arrival rate, data size of the task, and CPU word size. An optimization problem is formulated using the proximal operator and the Lagrange multiplier. The numerical results show that the proposed algorithm outperforms the other strategies by 36% at least.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Task analysis",
                "Energy consumption",
                "Servers",
                "Cloud computing",
                "Resource management",
                "Power demand",
                "Computational modeling"
            ],
            "Author Keywords": [
                "Consumer Electronics",
                "Fog Networks",
                "Internet of Things",
                "Frequency Scaling",
                "Resource Allocation",
                "Latency"
            ]
        },
        "title": "Dynamic CPU Frequency Scaling for Efficient Resource Allocation in Heterogeneous Fog Networks for CIoT Applications"
    },
    {
        "authors": [
            "Zemin Wang",
            "Yongtao Ma",
            "Xiuyan Liang",
            "Yicheng Chu",
            "Haibo Zhao",
            "Kai Huang",
            "Kaihua Liu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "12 September 2024",
        "doi": "10.1109/JIOT.2024.3454995",
        "publisher": "IEEE",
        "abstract": "With the rapid development of the Internet of Things (IoT), an increasing number of industrial demands have become urgent. Radio frequency identification (RFID) system plays a crucial role in addressing these challenges. It has now become a significant direction for the development of industrial automatic identification and data collection technology. However, the existing three-dimensional (3D) high-precision absolute localization methods have issues that need to be resolved. For example, the antenna sampling position needs to be used as known prior information, and the timestamp information of the samples is not fully utilized. Additionally, relying on multiple antennas or reference tags as auxiliary measures reduces the system flexibility. To overcome these challenges, we propose a single-antenna, multi-target, and 3D localization method based on an attention mechanism and neural network regression model inspired by the synthetic aperture (SA) method. This method suggests using a single antenna moving uniformly on a slider for continuous motion and sampling. By utilizing sample phase and time-domain information, it achieves 3D localization of cargo boxes in intelligent warehousing environments. In comparison with the current state-of-the-art localization solutions, our method doesn’t give the use of reference tags and utilizes a reduced number of antennas. The experimental results prove the feasibility of using only mobile antennas without reference tags to perform 3D multi-target positioning tasks. The combined dimensional average positioning error of 3D indoor positioning achieves a high-precision positioning, reaching 2.04cm and exhibits good robustness.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Location awareness",
                "Antennas",
                "Three-dimensional displays",
                "Solid modeling",
                "RFID tags",
                "Internet of Things",
                "Aperture antennas"
            ],
            "Author Keywords": [
                "Localization",
                "radio frequency identification(RFID)",
                "synthetic aperture (SA)",
                "neural network",
                "single-antenna"
            ]
        },
        "title": "3D Localization of RFID Tags Using SA Single-Antenna Based on Time Series Regression Model"
    },
    {
        "authors": [
            "Mohammad Ali",
            "Ximeng Liu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "11 September 2024",
        "doi": "10.1109/JIOT.2024.3457988",
        "publisher": "IEEE",
        "abstract": "Ensuring data integrity and confidentiality is critical in communication systems. Traditional methods, such as separate encryption and signature schemes, often lead to inefficiencies. While signcryption methods address both needs simultaneously, they are inadequate for data outsourcing systems like cloud computing due to the requirement for original files during data verification. Conversely, remote data integrity checking (RDIC) methods eliminate the need for original files but lack confidentiality. This paper fills the gap in cloud security by introducing remote signcryption (RSC) and presenting the first concrete RSC scheme. We demonstrate the application of this novel approach in cloud-assisted Internet of Things (IoT) networks. Our work includes defining the security for an RSC scheme and proving its security under the Bilinear Diffie-Hellman (BDH) hardness assumption. Implementing our proposed scheme on real data from trending YouTube video statistics, we found that RSC significantly outperforms existing methods, achieving 100 times faster speed and reducing communication costs by half compared to RDIC methods. Notably, even with 5MB of outsourced data, RSC keeps the communication cost under 8KB during data recovery, a substantial improvement over classical signcryption methods that require over 5MB.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Cloud computing",
                "Encryption",
                "Costs",
                "Internet of Things",
                "Security",
                "Data integrity",
                "Public key"
            ],
            "Author Keywords": [
                "Data integrity",
                "data confidentiality",
                "remote data integrity checking",
                "signcryption",
                "remote signcryption",
                "provable security"
            ]
        },
        "title": "A Novel Framework in Cloud Security: Remote Signcryption"
    },
    {
        "authors": [
            "Shao-I Chu",
            "Syuan-An Ke"
        ],
        "published_in": "Published in: IEEE Transactions on Emerging Topics in Computing ( Early Access )",
        "date_of_publication": "23 October 2024",
        "doi": "10.1109/TETC.2024.3482324",
        "publisher": "IEEE",
        "abstract": "Post-quantum cryptography (PQC) has recently gained intensive attention as the existing public-key cryptosystems are vulnerable to quantum attacks. The ring-learning-with-errors (RLWE)-based PQC is one promising type of the lattice-based schemes. A light variant, called binary RLWE (BRLWE), was developed with applications to Internet-of-Things (IoT) and edge computing. However, deploying the number theoretic transform (NTT) is not beneficial to the parameter settings of the BRLWE-based scheme. This paper presents three high-speed architectures of decryption for the BRLWE-based scheme with low area-time complexity. The first one is modified and corrected from the low-latency design of the previous work. The second and third ones utilize the multiplexer-based design for multiplication and innovatively exploit the property of the skew-circulant matrix to reduce the computational latency. Moreover, the third one applies the Karatsuba algorithm to reduce the number of multiplications. However, the results demonstrate that it is not in favor of the design since the multiplication is involved in an integer and a binary number, not both integers. Let the lengths of the secret and public keys be\nn\nand\nn\nlog\n2\nq\nbits. The synthesized results reveal that the second and third architectures are superior to the lookup table (LUT)-based and linear- feedback shift register (LFSR)-based designs in the previous works in terms of area-time complexity. The FPGA implementation results indicate the second design outperforms the Karatsuba and Toeplitz matrix vector product (TMVP)-initiated accelerators in the literatures by reductions of 62.4% and 51.7% in area-time complexity for the case of\n(n,q)=(256,256)\n. As\n(n,q)=(512,256)\n, the improvements are 44.3% and 28.3%. The third architecture is also superior to these high-speed designs. The proposed implementations are efficient in area-time complexity and are suitable for high-performance applications.",
        "issn": {
            "Electronic ISSN": "2168-6750"
        },
        "keywords": {
            "IEEE Keywords": [
                "Polynomials",
                "Computer architecture",
                "Hardware",
                "Encryption",
                "Vectors",
                "Internet of Things",
                "NIST",
                "Multiplexing",
                "Field programmable gate arrays",
                "Transforms"
            ],
            "Author Keywords": [
                "Binary ring-learning-with-errors",
                "finite field arithmetic",
                "hardware design",
                "post-quantum cryptography"
            ]
        },
        "title": "Area-Time Efficient Hardware Implementation for Binary Ring-LWE Based Post-Quantum Cryptography"
    },
    {
        "authors": [
            "Haoqian Li",
            "Honghan Ye",
            "Jing-Ru C. Cheng",
            "Kaibo Liu"
        ],
        "published_in": "Published in: IEEE Transactions on Automation Science and Engineering ( Early Access )",
        "date_of_publication": "14 June 2024",
        "doi": "10.1109/TASE.2024.3411770",
        "publisher": "IEEE",
        "abstract": "With the rapid advances in Internet of Things (IoT) technology and computational infrastructure, heterogeneous data streams are becoming common in various manufacturing applications. Meanwhile, the resource constraints often restrict the full observability of data streams due to limited budget for deploying or turning on every sensor at a time, as well as limited transmission and processing time for collecting high frequency information from all data streams, which poses significant challenges for multivariate statistical process control (SPC) and quality improvement. In this article, diverging from conventional heuristic approaches, we propose a new algorithm based on Q-learning to online monitor and quickly detect mean shifts occurring to heterogeneous data streams in the context of limited resources, where only a subset of observations is available at each acquisition time. In particular, we integrate Q-learning with a nonparametric cumulative sum (CUSUM) procedure to effectively detect a wide range of possible mean shifts when data streams follow arbitrary distributions. Both simulations and a case study are thoroughly conducted to evaluate the performance and demonstrate the superiority of the proposed method. Note to Practitioners —This paper is motivated by the practical issue of online process monitoring and anomaly detection with resource limitations. In particular, we can only select a subset of data streams to monitor at each time epoch, and the challenges are to dynamically choose which ones to observe and when to raise an alarm. Unlike the existing methodologies which are heuristic and only consider short-term rewards from the dynamic sampling, this paper proposes a novel monitoring and sampling strategy that allows the practitioners to cost-effectively monitor heterogeneous data streams by considering the long-term rewards. Three main steps are involved in the proposed method: (i) construct a local nonparametric monitoring statistic for each data stre...",
        "issn": {
            "Print ISSN": "1545-5955",
            "Electronic ISSN": "1558-3783"
        },
        "keywords": {
            "IEEE Keywords": [
                "Streams",
                "Monitoring",
                "Q-learning",
                "Process control",
                "Layout",
                "Data acquisition",
                "Process monitoring"
            ],
            "Author Keywords": [
                "Data categorization",
                "heterogeneity",
                "mean shift detection",
                "partial observations",
                "reinforcement learning"
            ]
        },
        "title": "Online Monitoring of Heterogeneous Partially Observable Data Streams Based on Q-Learning"
    },
    {
        "authors": [
            "Masahiro Nakagawa",
            "Haruka Minami",
            "Takafumi Fukatan",
            "Takeshi Seki",
            "Rie Hayash",
            "Takeshi Kuwahara"
        ],
        "published_in": "Published in: IEICE Transactions on Communications ( Early Access )",
        "date_of_publication": "16 September 2024",
        "doi": "10.23919/transcom.2024CEI0011",
        "publisher": "IEICE",
        "abstract": "IoT and AI-related services and applications are becoming more common and it is expected that cyber-physical systems will enrich our daily lives in the near future. In such a situation, a huge amount of data is exchanged throughout the world in a real time fashion, which will increasingly push the need for optical network evolution. Considering these trends, intensive efforts on research and development have recently been dedicated to realize All-Photonics Network (APN) that is a key element of Innovative Optical and Wireless Network (IOWN). For materializing APN vision, one of the fundamental challenges is to expand network capacity in a highly cost- and energy-efficient manner. Meanwhile, many detailed studies have greatly improved the performance of multi-band optical networks. Thanks to such efforts, multi-band networking has been becoming feasible as a promising capacity-scaling solution, which can be a key enabler for APN. This paper reviews the recent advances in the multiband optical network from various aspects. Specifically, we take a quick look at the studies enabling multi-band transmission in terms of devices design of transceiver and amplifier, quality-of-transmission estimation, and launch power optimization. Then, we focus on progress in optical switch technology, optical node configuration, path provisioning, and network analysis, aiming to deploy multi-band networks nationwide. We also refer to several emerging technologies, which include our efforts related to wavelength-selective band switching technology. This paper gives an opportunity to understand the trends and potential of multi-band optical networking.",
        "issn": {
            "Electronic ISSN": "1745-1345",
            "Print ISSN": "0916-8516"
        },
        "keywords": {
            "IEEE Keywords": [
                "Optical amplifiers",
                "Optical switches",
                "Transceivers",
                "Stimulated emission",
                "Optical filters",
                "Nonlinear optics",
                "Optical fiber amplifiers"
            ],
            "Author Keywords": [
                "all-photonics network",
                "ultra-high capacity",
                "multi-band optical networking",
                "band switching"
            ]
        },
        "title": "Multi-band optical networking: Recent advances toward IOWN all-photonics network"
    },
    {
        "authors": [
            "Chun-I Fan",
            "Chien-I Lai",
            "Darshan Vishwasrao Medhane"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "28 August 2024",
        "doi": "10.1109/JIOT.2024.3450959",
        "publisher": "IEEE",
        "abstract": "The Industrial Internet of Things (IIoT) is widely used in smart factories, enabling smart manufacturing and improving productivity. Although the application of IIoT has significantly altered a number of industries, increased connectivity has also given rise to security concerns. For example, information is collected from different domains in smart industrial environments and transmitted through public IoT channels. However, this phase can lead to communication security and privacy leakage issues when applied to resource-constrained smart industrial devices. This work proposes a novel collaborative authentication and key exchange protocol based on physically unclonable functions (PUF) to enable safe and efficient communication amongst smart industrial devices in an industrial internet of things context. The proposed protocol uses PUF to enhance the security of smart industrial devices. Additionally, to mitigate difficult situations such as device loss, the proposed protocol combines an elliptic curve Diffie-Hellman key exchange scheme to achieve forward security and collaborative authentication between the domain server and smart industrial devices during the key exchange phase. We analyze and provide security proofs for the proposed protocol. Furthermore, performance analysis is conducted to evaluate the computational costs of the protocol. The results of this work contribute to the development of secure and reliable authentication and key exchange protocols within the context of IIoT, promoting the adoption of IIoT technology in smart factories while reducing potential security threats.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Security",
                "Protocols",
                "Industrial Internet of Things",
                "Authentication",
                "Physical unclonable function",
                "Collaboration",
                "Servers"
            ],
            "Author Keywords": [
                "Industrial Internet of Things (IIoT)",
                "Device-to-Device Communication",
                "Authentication",
                "Key Exchange",
                "Physically Unclonable Function (PUF)",
                "Forward Security"
            ]
        },
        "title": "CAKE-PUF: A Collaborative Authentication and Key Exchange Protocol Based on Physically Unclonable Functions for Industrial Internet of Things"
    },
    {
        "authors": [
            "Arsham Mostaani",
            "Thang X. Vu",
            "Hamed Habibi",
            "Symeon Chatzinotas",
            "Björn Ottersten"
        ],
        "published_in": "Published in: IEEE Transactions on Communications ( Early Access )",
        "date_of_publication": "24 June 2024",
        "doi": "10.1109/TCOMM.2024.3416898",
        "publisher": "IEEE",
        "abstract": "With countless promising applications in various domains such as IoT and Industry 4.0, task-oriented communication design (TOCD) is getting accelerated attention from the research community. This paper presents a novel approach for designing scalable task-oriented quantization and communications in cooperative multi-agent systems (MAS). The proposed approach utilizes the TOCD framework and the value of information (VoI) concept to enable efficient communication of quantized observations among agents while maximizing the average return performance of the MAS, a parameter that quantifies the MAS’s task effectiveness. The computational complexity of learning the VoI, however, grows exponentially with the number of agents. Thus, we propose a three-step framework: (i) learning the VoI (using reinforcement learning (RL)) for a two-agent system, (ii) designing the quantization policy for an N-agent MAS using the learned VoI for a range of bit-budgets and, (iii) learning the agents’ control policies using RL while following the designed quantization policies in the earlier step. Our analytical results show the applicability of the proposed framework under a wide range of problems. Numerical results show striking improvements in reducing the computational complexity of obtaining VoI needed for the TOCD in a MAS problem without compromising the average return performance of the MAS.",
        "issn": {
            "Print ISSN": "0090-6778",
            "Electronic ISSN": "1558-0857"
        },
        "keywords": {
            "IEEE Keywords": [
                "Task analysis",
                "Quantization (signal)",
                "Training",
                "Costs",
                "Time complexity",
                "Multi-agent systems",
                "Reinforcement learning"
            ],
            "Author Keywords": [
                "Task-oriented data compression",
                "communication for machine learning",
                "joint communication and control",
                "multiagent systems",
                "reinforcement learning"
            ]
        },
        "title": "Task-Oriented Communication Design at Scale"
    },
    {
        "authors": [
            "Di Wu",
            "Zunliang Wang",
            "Huijiang Pan",
            "Haipeng Yao",
            "Tianle Mai",
            "Song Guo"
        ],
        "published_in": "Published in: IEEE Transactions on Services Computing ( Early Access )",
        "date_of_publication": "18 September 2024",
        "doi": "10.1109/TSC.2024.3463475",
        "publisher": "IEEE",
        "abstract": "In recent years, the rapid growth of Internet of Things (IoT) devices and applications has posed significant challenges for existing Mobile Edge Computing (MEC) architectures. The inherent latency uncertainties in MEC architectures make it difficult to support latency-sensitive applications such as autonomous vehicles. Additionally, the increasing number of connected devices has led to substantial challenges in terms of limited throughput for MEC servers. With the recent advancements in programmable network hardware, such as SmartNICs and programmable switches, the Network-based Computing (NBC) paradigm has gained widespread attention. Leveraging line-rate processing capabilities, NBC offers a promising solution for high throughput and low latency processing. This paper aims to explore the potential benefits and challenges of incorporating NBC into existing MEC architectures. The feasibility of our proposed architecture is evaluated using two use cases, Linear Quadratic Regulator (LQR) control and Complex Event Processing (CEP), demonstrating significant improvements in latency performance.",
        "issn": {
            "Electronic ISSN": "1939-1374"
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer architecture",
                "Internet of Things",
                "Hardware",
                "Throughput",
                "Low latency communication",
                "Servers",
                "Protocols"
            ],
            "Author Keywords": [
                "In-Network computing",
                "Internet of Things",
                "mobile edge computing",
                "p4",
                "programmable data plane"
            ]
        },
        "title": "In-Network Computing Empowered Mobile Edge Offloading Architecture for Internet of Things"
    },
    {
        "authors": [
            "Mingyu Zhang",
            "Yahui Li",
            "Han Ren",
            "Mengcheng Wang",
            "Zhiyuan Hu",
            "Yan Wang",
            "Yunna Sun",
            "Zhuoqing Yang",
            "Guifu Ding"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "27 June 2024",
        "doi": "10.1109/JSEN.2024.3416340",
        "publisher": "IEEE",
        "abstract": "Near-zero consumption inertial switch has been widely used in various fields, as automobiles, wearable devices, and the Internet of Things (IoT). Here, an inertial switch with excellent contact performance is proposed, which is designed by the inspiration of the intrinsic dynamic characteristics of inertial switch and fabricated by surface micromachining process. The inertial switch combines the lightweight movable electrode and asymmetric springs design, and the dynamic response characteristics are investigated by finite element simulation. The result indicates the movable electrode performs the rotary swing during contact-separate motion, and the contact time is positively related with the contact area between the movable electrode and fixed electrode. In experiment, four kinds of asymmetric inertial switches are tested, and the measured thresholds are essentially comparable to the simulated thresholds, with a maximum error less than 7%. Meanwhile, the contact time of the inertial switch is significantly prolonged without obvious signal bounce after contact, and the contact resistance remains stable after 50 shocks, demonstrating the reliability and durability of the asymmetric inertial switch. Therefore, the proposed asymmetric inertial switch can serve as an effective candidate for near-zero consumption applications in the future.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Electrodes",
                "Switches",
                "Contacts",
                "Springs",
                "Force",
                "Sensors",
                "Mathematical models"
            ],
            "Author Keywords": [
                "MEMS inertial switch",
                "lightweight movable electrode",
                "asymmetric springs",
                "rotary swing",
                "spiral separation"
            ]
        },
        "title": "An inertial switch with prolonged contact time achieved by dynamically asymmetric design"
    },
    {
        "authors": [
            "Zhao Tong",
            "Jiaxin Deng",
            "Jing Mei",
            "Yuanyang Zhang",
            "Keqin Li"
        ],
        "published_in": "Published in: IEEE Transactions on Services Computing ( Early Access )",
        "date_of_publication": "11 October 2024",
        "doi": "10.1109/TSC.2024.3478841",
        "publisher": "IEEE",
        "abstract": "The widespread adoption of the Internet of Things (IoT) has increased demand for task processing via mobile edge computing (MEC). In this study, we designed a directed acyclic graph (DAG) task offloading workflow in MEC. Traditional task offloading often does not simultaneously take into account task upload delay and task communication delay, failing to accurately reflect real-world issues. The constraints between task execution delay, upload delay and communication delay were introduced to model system response time and energy consumption for optimization. To satisfy task dependencies, the edge rank_u sorting (ERS) algorithm is used to generate specific offloading queues. A federated deep q-network (FDQN) algorithm addresses the offloading issue. It is different from the traditional approach of uploading task information data to the edge and facing data privacy risks. FDQN deploies the model locally and only collects model parameters for aggregation to update the local model. The algorithm improves the performance and stability of the model while protecting user privacy. To automatically tune hyperparameters for multiple devices, we used the tree of parzen estimators (TPE) algorithm, and named the whole process federated DQN with automated hyperparameter optimization (FDAHO). Experimental results show that FDAHO outperforms other algorithms in scenarios of different task number, task types, and user numbers, with consideration of benchmarks.",
        "issn": {
            "Electronic ISSN": "1939-1374"
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling",
                "Internet of Things",
                "Heuristic algorithms",
                "Optimization",
                "Servers",
                "Delays",
                "Cloud computing",
                "Training",
                "Processor scheduling",
                "Privacy"
            ],
            "Author Keywords": [
                "FDAHO",
                "mobile edge computing",
                "multi-objective optimization",
                "task offloading"
            ]
        },
        "title": "Multi-Objective DAG Task Offloading in MEC Environment Based on Federated DQN With Automated Hyperparameter Optimization"
    },
    {
        "authors": [
            "Zhendong Song",
            "Menglin Xie",
            "Jinda Luo",
            "Tao Gong",
            "Wei Chen"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "03 October 2024",
        "doi": "10.1109/JIOT.2024.3472669",
        "publisher": "IEEE",
        "abstract": "The integration of Artificial Intelligence (AI) and the Internet of Things (IoT) has given rise to Artificial Intelligence of Things (AIoT) ecosystems, which, while revolutionizing various domains, face challenges in energy efficiency and environmental sustainability. This paper introduces LSCEA-AIoT, a low-carbon sustainable computing framework for energy-efficient acquisition and allocation in AIoT. The framework addresses two critical aspects: energy-efficient heterogeneous data acquisition and low-carbon task offloading optimization. We develop a multi-source model for data acquisition that considers acquisition time, load balancing, and energy consumption, coupled with an adaptive sensor node deployment strategy for optimized channel allocation. The task offloading component formulates a joint optimization problem, balancing local and edge computing models to minimize ecosystem costs and carbon emissions. We propose a carbon-aware multi-channel exploration offloading decision algorithm based on a Monte Carlo tree search to obtain near-optimal solutions. Extensive experiments compare LSCEA-AIoT with state-of-the-art methods across various metrics. Results demonstrate that LSCEA-AIoT significantly outperforms existing approaches, achieving lower data acquisition errors, reduced energy consumption, extended network lifetimes, and increased data acquisition volumes. LSCEA-AIoT exhibits superior performance in task offloading scenarios in normalized rewards, system costs, and adaptability to diverse network configurations. These findings validate LSCEA-AIoT’s effectiveness in achieving low-carbon, sustainable computing for AIoT ecosystems.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Data acquisition",
                "Ecosystems",
                "Energy efficiency",
                "Artificial intelligence",
                "Energy consumption",
                "Resource management",
                "Green computing",
                "Carbon dioxide",
                "Edge computing"
            ],
            "Author Keywords": [
                "Artificial Intelligence of Things",
                "carbon-aware framework",
                "energy-efficient data acquisition",
                "task offloading"
            ]
        },
        "title": "A Carbon-Aware Framework for Energy-Efficient Data Acquisition and Task Offloading in Sustainable AIoT Ecosystems"
    },
    {
        "authors": [
            "Aikun Xu",
            "Zhigang Hu",
            "Xi Li",
            "Rongti Tian",
            "Xinyu Zhang",
            "Bolei Chen",
            "Hui Xiao",
            "Hao Zheng",
            "Xianting Feng",
            "Meiguang Zheng",
            "Ping Zhong",
            "Keqin Li"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "15 August 2024",
        "doi": "10.1109/JIOT.2024.3443866",
        "publisher": "IEEE",
        "abstract": "In recent years, since edge computing has improved the performance of transportation systems, research on edge computing-enabled transportation systems has received widespread attention. However, most previous studies overlooked that task requests in transportation systems are unevenly distributed in time and space, which easily causes the overloading of edge servers, resulting in high response latency. To this end, we present a novel task offloading scheme based on Graph Neural Network (GNN) and Deep Reinforcement Learning (DRL) in Edge computing-enabled Transportation systems (TransEdge). Specifically, we first propose an adaptive node placement algorithm to assign IoT sensors to appropriate edge servers, thereby minimizing transmission latency. Then, an improved DRL scheme based on GNN is designed to capture the spatial features between sensors, aiming to improve the accuracy of task offloading decisions. Finally, we introduce a task forwarding strategy based on the greedy algorithm to achieve collaborative task offloading between different edge servers and overcome the system instability caused by a sudden surge in task requests. We conduct extensive experiments on two real-world traffic datasets. The results show that TransEdge reduces the response latency by at least 3.7% compared to four baselines while achieving a success rate of 99%.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Task analysis",
                "Servers",
                "Transportation",
                "Edge computing",
                "Computational modeling",
                "Collaboration",
                "Graph neural networks"
            ],
            "Author Keywords": [
                "deep reinforcement learning",
                "edge computing",
                "graph neural network",
                "task offloading",
                "transportation systems"
            ]
        },
        "title": "TransEdge: Task Offloading with GNN and DRL in Edge Computing-Enabled Transportation Systems"
    },
    {
        "authors": [
            "Yadong Zhang",
            "Peng Wang",
            "Qubeijian Wang",
            "Haibin Zhang",
            "Lexi Xu",
            "Wen Sun",
            "Bin Wang"
        ],
        "published_in": "Published in: IEEE Transactions on Green Communications and Networking ( Early Access )",
        "date_of_publication": "13 August 2024",
        "doi": "10.1109/TGCN.2024.3442910",
        "publisher": "IEEE",
        "abstract": "Wireless Computing Power Networks (WCPN), guided by green principles, aim to provide efficient, flexible, and environmentally friendly computing services for Internet of Things (IoT) applications by seamlessly coordinating computational and networking resources across diverse nodes. The integration of Digital Twin (DT) technology is crucial for achieving these objectives. However, different DT association strategies play a crucial role in enhancing the capabilities of WCPN. In this paper, recognizing the long-term and dynamic nature of DT deployment in real-world scenarios, we utilize evolutionary game theory to model the association and transfer of DTs, aiming for continuous adaptive adjustments and optimizations in their deployment. Specifically, we propose an evolutionary game-based algorithm for DT association as a complement to the independent decision-making process in DT deployment. Moreover, in light of the inherent limitations of the evolutionary game selection mechanism and the lack of self-learning ability, we introduce a deep Q-network (DQN) based evolutionary game approach that ensures adaptive DT association and transfer by considering factors such as DT synchronization delay, model consistency, and migration costs. Numerical results demonstrate that our proposed algorithms outperform the benchmarks in terms of average user utility and convergence speed.",
        "issn": {
            "Electronic ISSN": "2473-2400"
        },
        "keywords": {
            "IEEE Keywords": [
                "Games",
                "Computational modeling",
                "Wireless communication",
                "Internet of Things",
                "Adaptation models",
                "Servers",
                "Real-time systems"
            ],
            "Author Keywords": [
                "Wireless computing power networks",
                "Digital twin",
                "Evolutionary game",
                "Deep reinforcement learning"
            ]
        },
        "title": "Evolutionary Game-Based Adaptive DT Association and Transfer for Wireless Computing Power Networks"
    },
    {
        "authors": [
            "Jing Yang",
            "JiaLin Lu",
            "Xu Zhou",
            "ShaoBo Li",
            "ChuanYue Xiong",
            "Jianjun Hu"
        ],
        "published_in": "Published in: IEEE Transactions on Green Communications and Networking ( Early Access )",
        "date_of_publication": "04 June 2024",
        "doi": "10.1109/TGCN.2024.3409390",
        "publisher": "IEEE",
        "abstract": "Due to the rapid development of the IoT and data-driven applications, low-latency task scheduling methods that quickly respond to user tasks has become a significant challenge for edge servers. However, the existing task scheduling strategies do not overcome the impact of factors such as task characteristics, resource availability, and network conditions on delays. Meanwhile, the cross-regional maldistribution of edge servers is obvious, and the edge servers are either idle or overloaded. To address these issues, we propose a low-latency edge scheduling strategy based on the Hard Attention Mechanism and Advantage Actor-Critic (HA-A2C). The core element of this method is the adoption of a hard attention mechanism, which reduces computing complexity and increases efficiency. Effective attention allocation during the resource allocation process further reduces job completion time. Additionally, the deep reinforcement learning method is employed to enhance task dynamic scheduling capabilities, thereby reducing scheduling delays. The HA-A2C approach reduces task latency by approximately 40% compared to the DQN method. Consequently, the intelligent allocation of task resources achieved by integrating the hard attention technique significantly reduces task scheduling time in edge environments.",
        "issn": {
            "Electronic ISSN": "2473-2400"
        },
        "keywords": {
            "IEEE Keywords": [
                "Task analysis",
                "Processor scheduling",
                "Servers",
                "Optimal scheduling",
                "Resource management",
                "Dynamic scheduling",
                "Delays"
            ],
            "Author Keywords": [
                "Edge computing",
                "hard attention",
                "resource scheduling",
                "deep reinforcement learning",
                "delay optimization"
            ]
        },
        "title": "HA-A2C: Hard Attention and Advantage Actor-Critic for Addressing Latency Optimization in Edge Computing"
    },
    {
        "authors": [
            "Tingnan Bao",
            "Aisha Syed",
            "William Sean Kennedy",
            "Melike Erol-Kantarci"
        ],
        "published_in": "Published in: IEEE Transactions on Network and Service Management ( Early Access )",
        "date_of_publication": "29 October 2024",
        "doi": "10.1109/TNSM.2024.3486288",
        "publisher": "IEEE",
        "abstract": "The integration of unmanned aerial vehicles (UAVs) with mobile edge computing (MEC) and Internet of Things (IoT) technology is crucial for efficient resource management and sustainable agricultural productivity in smart frams. This paper addresses the critical need for optimizing task offloading in secure UAV-assisted smart farm networks, aiming to reduce total delay and energy consumption while maintaining robust security in data communications. We propose a multi-agent deep reinforcement learning (DRL)-based approach using a deep double Q-network (DDQN) with an action mask (AM), designed to manage task offloading dynamically and efficiently. Simulation results demonstrate the superior performance of our method in managing task offloading, highlighting significant improvements in operational efficiency, such as reduced delay and energy consumption. This aligns with the goal of developing sustainable and energy-efficient solutions for next-generation network infrastructures, making our approach an advanced solution for performance and sustainability in smart farming applications.",
        "issn": {
            "Electronic ISSN": "1932-4537"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Autonomous aerial vehicles",
                "Smart agriculture",
                "Servers",
                "Resource management",
                "Energy consumption",
                "Delays",
                "Sustainable development",
                "Real-time systems",
                "Productivity"
            ],
            "Author Keywords": [
                "Task Offloading",
                "Unmanned Aerial Vehicles (UAVs)",
                "Physical Layer Security",
                "Energy Consumption",
                "Deep Reinforcement Learning (DRL)"
            ]
        },
        "title": "Sustainable Task Offloading in Secure UAV-Assisted Smart Farm Networks: A Multi-Agent DRL With Action Mask Approach"
    },
    {
        "authors": [
            "Yihao Liu",
            "Jinyan Jiang",
            "Jumin Zhao",
            "Jiliang Wang"
        ],
        "published_in": "Published in: IEEE Transactions on Mobile Computing ( Early Access )",
        "date_of_publication": "14 October 2024",
        "doi": "10.1109/TMC.2024.3480137",
        "publisher": "IEEE",
        "abstract": "Backscatter sensing has emerged as a significant technology within the Internet of Things (IoT), prompting extensive research interest. This paper presents LoMu, the first long-range multi-target backscatter sensing system designed for low-cost tags operating under ambient LoRa. LoMu introduces an orthogonal sensing model that processes backscatter signals from multiple tags to extract motion information. The design addresses several practical challenges, including near-far interference among multiple tags, phase offsets from unsynchronized transceivers, and phase errors due to frequency drift in low-cost tags. To overcome these issues, we propose a conjugate-based energy concentration method to extract high-quality signals and a Hamming-window-based method to mitigate the near-far problem. Additionally, we exploit the relationship between excitation and backscatter signals to synchronize the transmitter (TX) and receiver (RX) and combine double sidebands of backscatter signals to eliminate tag frequency drift. Furthermore, a novel joint estimation algorithm is introduced to exploit both amplitude and phase information in target signals, enhancing frequency sensing results and robustness. Our implementation and extensive experiments demonstrate that LoMu can accurately sense up to 35 tags simultaneously and achieve an average frequency sensing error of 0.5% at a range of 400 meters, which is\n4×\nthe range of the state-of-the-art.",
        "issn": {
            "Print ISSN": "1536-1233",
            "Electronic ISSN": "1558-0660"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Backscatter",
                "LoRa",
                "Chirp",
                "Frequency shift keying",
                "Receivers",
                "Transmitters",
                "Meters",
                "Synchronization",
                "Signal to noise ratio"
            ],
            "Author Keywords": [
                "Backscatter",
                "LoRa",
                "long-range sensing",
                "multi-target sensing"
            ]
        },
        "title": "Enable Practical Long-Range Multi-Target Backscatter Sensing"
    },
    {
        "authors": [
            "Muhammad Usman",
            "Sarah Basharat",
            "Syed Ali Hassan",
            "Haris Pervaiz",
            "Zhiguo Ding",
            "Haejoon Jung"
        ],
        "published_in": "Published in: IEEE Transactions on Vehicular Technology ( Early Access )",
        "date_of_publication": "08 August 2024",
        "doi": "10.1109/TVT.2024.3440396",
        "publisher": "IEEE",
        "abstract": "Backscatter communication (BackCom), which allows the passive wireless sensors to transmit via modulation and reflection of an incident radio-frequency (RF) signal, is a promising solution for the sustained operation of Internet-of-Things (IoT) nodes. Moreover, to address the inherent coverage and data rate limitations of contemporary BackCom systems, reconfigurable intelligent surfaces (RISs) have come to the forefront for the adaptive configuration of wireless environments. Meanwhile, BackCom systems can incorporate non-orthogonal multiple access (NOMA) to achieve higher spectral efficiency for massive connectivity. Hence, we consider a NOMA-enabled bistatic BackCom system, where multiple backscatter nodes (BSNs) are grouped by the NOMA principle and served with the assistance of an RIS. Using the method of moments (MoM), we analyze and obtain the bit error rate (BER) expressions of a $2$ -BSN NOMA cluster under imperfect successive interference cancellation (SIC). The validity of the derived expressions is corroborated through extensive numerical simulations. Furthermore, the performance of the proposed system is evaluated in comparison with the conventional NOMA-BackCom system. Our results reveal the superior performance of the RIS-assisted NOMA system, and demonstrate the influence of the number of RIS elements and the choice of reflection coefficients on the BER performance of the proposed system.",
        "issn": {
            "Print ISSN": "0018-9545",
            "Electronic ISSN": "1939-9359"
        },
        "keywords": {
            "IEEE Keywords": [
                "Backscatter",
                "NOMA",
                "Wireless communication",
                "Training",
                "Radio frequency",
                "Internet of Things",
                "Bit error rate"
            ],
            "Author Keywords": [
                "Backscatter communication",
                "non-orthogonal multiple access",
                "reconfigurable intelligent surface",
                "bit error rate",
                "method of moments"
            ]
        },
        "title": "Performance Analysis of NOMA-Enabled RIS-Assisted Backscatter Communication"
    },
    {
        "authors": [
            "Felipe Mogollon",
            "Zaloa Fernandez",
            "Angel Martin",
            "Juan Diego Ortega",
            "Gorka Velez"
        ],
        "published_in": "Published in: IEEE Transactions on Intelligent Vehicles ( Early Access )",
        "date_of_publication": "28 October 2024",
        "doi": "10.1109/TIV.2024.3486926",
        "publisher": "IEEE",
        "abstract": "Vehicles are sophisticated machines equipped with sensors that provide real-time data for onboard driving assistance systems. Due to the wide variety of traffic, road, and weather conditions, continuous system enhancements are essential. Connectivity allows vehicles to transmit previously unknown data, expanding datasets and accelerating the development of new data models. This enables faster identification and integration of novel data, improving system reliability and reducing time to market. Data Spaces aim to create a data-driven, interconnected, and innovative data economy, where edge and cloud infrastructures support a virtualised IoT platform that connects data sources and development servers. This paper proposes an edge-cloud data platform to connect car data producers with multiple and heterogeneous services, addressing key challenges in Data Spaces, such as data sovereignty, governance, interoperability, and privacy. The paper also evaluates the data platform's performance limits for text, image, and video data workloads, examines the impact of connectivity technologies, and assesses latencies. The results show that latencies drop to 33ms with 5G connectivity when pipelining data to consuming applications hosted at the edge, compared to around 77ms when crossing both edge and cloud processing infrastructures. The results offer guidance on the necessary processing assets to avoid bottlenecks in car data platforms.",
        "issn": {
            "Electronic ISSN": "2379-8904",
            "Print ISSN": "2379-8858"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Cloud computing",
                "Data models",
                "Sensors",
                "Automobiles",
                "Safety",
                "Distributed databases",
                "Automotive engineering",
                "Training",
                "Protocols"
            ],
            "Author Keywords": [
                "5G",
                "CCAM",
                "Dataspace",
                "MEC",
                "vehicular communications"
            ]
        },
        "title": "Data streaming platform for crowd-sourced vehicle dataset generation"
    },
    {
        "authors": [
            "Shuqi Qiao",
            "Yizhe Shao",
            "Guodong Kang",
            "Jian Zhao",
            "Pingyue Yue",
            "Rui Zhang",
            "Sheng Ke",
            "Shuai Wang"
        ],
        "published_in": "Published in: IEEE Transactions on Aerospace and Electronic Systems ( Early Access )",
        "date_of_publication": "23 October 2024",
        "doi": "10.1109/TAES.2024.3485024",
        "publisher": "IEEE",
        "abstract": "Internet of Remote Things (IoRT), functioning as a supplementary element to terrestrial mobile Internet of Things (IoT) networks, significantly enhances communication for user devices in remote areas with the support of Low-Earth Orbit (LEO) satellites, providing considerable convenience. However, the open nature of satellite-to-ground channels and the predictability of satellite orbits make LEO satellite communications vulnerable to disruptive broadband malicious jamming from external sources. To address this challenge, we propose a broadband jamming suppression methodology that leverages the spatial diversity characteristics of multiple satellites. Moreover, in conditions of unequal power of desired and jamming signals, we derive the theoretical combined Signal-to-Jamming-plus-Noise-Ratio (SJNR). To achieve the maximum combined SJNR, we propose a code-assisted iterative parameter estimation algorithm that is tailored for multi-satellite operations. This algorithm employs Log-Likelihood Ratio (LLR) information feedback from the decoder to refine the estimation of combining weight coefficients, thereby boosting the broadband jamming suppression capacity of the IoRT system. Simulation results demonstrate that Maximum Ratio Combination (MRC) based on the proposed algorithm exhibits minimal combined SJNR loss and outperforms the classical time/frequency-domain anti-jamming algorithm based on Selective Combination (SC), Equal Gain Combination (EGC), and SC itself in Bit Error Rate (BER) performance.",
        "issn": {
            "Print ISSN": "0018-9251",
            "Electronic ISSN": "1557-9603"
        },
        "keywords": {
            "IEEE Keywords": [
                "Jamming",
                "Satellites",
                "Diversity reception",
                "Aerospace and electronic systems",
                "Broadband communication",
                "Earth",
                "Collaboration",
                "Channel estimation",
                "Performance evaluation",
                "Internet of Things"
            ],
            "Author Keywords": []
        },
        "title": "Code-Assisted Broadband Jamming Suppression in Multi-Satellite Collaborative Secure Communication"
    },
    {
        "authors": [
            "Weixing Tan",
            "Yusen Wang",
            "Lei Liu",
            "Xiaoding Wang",
            "Tong Ding"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "22 October 2024",
        "doi": "10.1109/JIOT.2024.3484230",
        "publisher": "IEEE",
        "abstract": "The introduction of semantic communication offers an effective solution for achieving efficient and reliable information transmission in the Social Internet of Things (SIoT). SIoT combines social networks with the Internet of Things (IoT) to create a “social network of smart objects”, utilizing analytical and statistical models to provide efficient and scalable services. However, ensuring high-quality and reliable data transmission within the SIoT remains a significant challenge. Semantic communication methods can effectively address this issue. Semantic communication represents an advanced paradigm aimed at achieving reliable transmission through semantic-level data compression. In this paper, we propose a semantic communication framework based on adaptive federated deep learning. This framework combines source-channel joint coding with channel bandwidth adaptation techniques to enhance transmission efficiency and promote natural and effective information exchange. Specifically, deep reinforcement learning is employed to manage dynamic bandwidth allocation, enabling the selection of optimal bandwidth under varying signal-to-noise ratios and data conditions, thereby improving transmission quality and bandwidth utilization. Additionally, we introduce a training method based on federated learning to enhance the model’s generalization ability under different channel conditions. Simulation results demonstrate that our proposed method outperforms traditional models, exhibiting excellent adaptability to low signal-to-noise ratios and low bandwidth environments, as well as higher stability. This positions our method as a valuable approach for ensuring reliable data communication in the SIoT.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Adaptation models",
                "Social Internet of Things",
                "Bandwidth",
                "Data models",
                "Signal to noise ratio",
                "Reliability",
                "Federated learning",
                "Decoding",
                "Training",
                "Deep reinforcement learning"
            ],
            "Author Keywords": [
                "social internet of things",
                "semantic communication",
                "deep learning",
                "deep reinforcement learning",
                "federated learning"
            ]
        },
        "title": "Adaptive Federated Deep Learning-based Semantic Communication in the Social Internet of Things"
    },
    {
        "authors": [
            "Muhammad Ali Naeem",
            "Ikram Ud Din",
            "Yahui Meng",
            "Ahmad Almogren",
            "Joel J. P. C. Rodrigues"
        ],
        "published_in": "Published in: IEEE Communications Surveys & Tutorials ( Early Access )",
        "date_of_publication": "07 November 2024",
        "doi": "10.1109/COMST.2024.3493626",
        "publisher": "IEEE",
        "abstract": "The substantial growth in data volume has led to considerable technological obstacles on the Internet. In order to address the high volume of Internet traffic, the research community has investigated the improvement of Internet architecture by implementing centrality-based caching, which could involve collaborative efforts. Different centrality-based caching strategies have been put forward that allow for different data distribution. These include betweenness centrality, degree centrality, and closeness centrality. Caching provides several advantages in terms of reducing latency, improving scalability, and enhancing data manageability. In addition, this study provides an overview of cache management algorithms based on centrality in the context of Information Centric Networking (ICN), Named Data Networking (NDN), and Internet of Things (IoT). It highlights the advantages and disadvantages of these algorithms and evaluates their performance in a network simulation environment, specifically in terms of cache hit ratio, data retrieval latency, and average hop count. Ultimately, we aim to pinpoint and deliberate on possible research directions for future studies concerning various aspects of centrality-based caching in communication systems.",
        "issn": {
            "Electronic ISSN": "1553-877X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Surveys",
                "Internet",
                "Measurement",
                "Edge computing",
                "Tutorials",
                "Industrial Internet of Things",
                "Smart cities",
                "Real-time systems",
                "Electronic mail"
            ],
            "Author Keywords": [
                "Information-Centric Networking",
                "Named Data Networking",
                "Caching",
                "Internet of Things"
            ]
        },
        "title": "Centrality-Based On-Path Caching Strategies in NDN-Based Internet of Things: A Survey"
    },
    {
        "authors": [
            "Jing Wang",
            "Yi Yang",
            "Zhen Li",
            "Sijun Du",
            "Xinling Yue",
            "Xun Liu",
            "Jun Han",
            "Xiaoyang Zeng",
            "Zhiyuan Chen"
        ],
        "published_in": "Published in: IEEE Journal of Solid-State Circuits ( Early Access )",
        "date_of_publication": "23 September 2024",
        "doi": "10.1109/JSSC.2024.3452113",
        "publisher": "IEEE",
        "abstract": "This article proposes a novel collaborative-flip synchronized switch harvesting on capacitors (CF-SSHCs) rectifier and multioutput synchronous dc–dc converters with shared capacitors. Compared to the traditional SSHC, our CF-SSHC rectifier can increase the number of flipping phases, potentially enhancing the flipping efficiency and output power under specific conditions where $C_{\\text{FLY}}$ is close to $C_P$ . The synchronous dc–dc converters reuse the flying capacitors to achieve a high maximum output power improving rate (MOPIR) over a limited input power range and provide multiple outputs. This work achieves an advanced number of flipping phases in capacitor-based rectifier interface technology and explores multiple-input multiple-output configurations, evaluating the system’s performance under periodic and shock conditions for the first time. The system’s adaptability to various piezoelectric transducer (PT) array configurations is validated, highlighting its potential for Internet of Things (IoT) networks. The design is fabricated in standard 0.18- $\\mu$ m CMOS. Measurement results demonstrate that the voltage flipping efficiency of up to 83% is achieved. Compared with full-bridge rectifier (FBR), the MOPIR can be increased to 5.06 $\\times$ and 4.78 $\\times$ under off-resonance and on-resonance excitation, respectively. It can also achieve a 2.14 $\\times$ power enhancement under shock excitation. Additionally, when the input power $P_{\\text{IN\\_FBR}}$ is in the range of 1.42–28.4 $\\mu$ W, the MOPIR of the proposed system is always greater than 4.",
        "issn": {
            "Print ISSN": "0018-9200",
            "Electronic ISSN": "1558-173X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Capacitors",
                "Rectifiers",
                "Voltage",
                "Switches",
                "Power generation",
                "Energy harvesting",
                "Synchronization"
            ],
            "Author Keywords": [
                "Maximum power point tracking (MPPT)",
                "multiinput",
                "multioutput",
                "piezoelectric",
                "piezoelectric energy harvesting (PEH)",
                "rectifiers",
                "shared capacitors",
                "shock excitation",
                "synchronized switch harvesting on capacitors (SSHCs)"
            ]
        },
        "title": "Enhancing Efficiency in Piezoelectric Energy Harvesting: Collaborative-Flip Synchronized Switch Harvesting on Capacitors Rectifier and Multioutput DC–DC Converters Utilizing Shared Capacitors"
    },
    {
        "authors": [
            "Xiaoyu Zhang",
            "Yichao Wang",
            "Xiting Peng",
            "Mianxiong Dong",
            "Kaoru Ota",
            "Lexi Xu"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "26 August 2024",
        "doi": "10.1109/TCE.2024.3443336",
        "publisher": "IEEE",
        "abstract": "The rise of the Internet of Things (IoT) and the Internet of Vehicles (IoV) has accelerated the realization of smart cities, where cameras as interconnected consumer electronics (CE) are deployed across cities to capture target images. The widespread deployment of monitoring equipment has prompted us to focus on the target re-identification (Re-ID) issue. One major challenge about this issue is that the identified targets are often obscured by different obstacles, which leads to bad performance. In practical applications, the occluded Re-ID task is very significant to complete. Previous approaches have focused on improving the occluded Re-ID performance but have neglected the lightweight problem, which makes the model difficult to deploy in the real world. Therefore, this paper proposes a lightweight framework that ensures occluded Re-ID performance and deploys at the edge to solve the problem of long transmission time and high latency caused by wireless and cloud technology in CE. This framework tackles occluded target Re-ID issues by integrating omni-scale features with human keypoint estimation and multi-head attention mechanism (OHMA). To solve the vehicle Re-ID problem, we use the cutout method to simulate an occlusion scene due to the lack of occluded vehicle data. Then, The multi-head attention mechanism combines with the omni-scale network (OSNet) to learn vehicles subtle features. To deal with occluded pedestrians, human keypoint estimation focuses on non-occluded areas of pedestrian images by paying attention to visible information about the human body. The generated heatmaps fuse omni-scale feature maps to explore better feature representations. In addition, the HUAWEI Atlas 200I DK A2 is used to simulate real edge devices and evaluate the experiments on both public and real-world private datasets. The results demonstrate that our framework improves the occluded Re-ID performance while ensuring lightweight. Compared with the previous methods, OHMA display...",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Pedestrians",
                "Consumer electronics",
                "Computational modeling",
                "Noise",
                "Cloud computing",
                "Cameras",
                "Attention mechanisms"
            ],
            "Author Keywords": [
                "Internet of Things",
                "target re-identification",
                "occlusion scene",
                "attention mechanism",
                "human keypoint estimation"
            ]
        },
        "title": "OHMA: An Edge-Based Lightweight Occluded Target Re-Identification Framework for Exploring Abundant Feature Expression"
    },
    {
        "authors": [
            "Ke Xu",
            "Youyun Xu",
            "Xiaoming Wang",
            "Xianbin Wang"
        ],
        "published_in": "Published in: IEEE Transactions on Vehicular Technology ( Early Access )",
        "date_of_publication": "25 September 2024",
        "doi": "10.1109/TVT.2024.3467255",
        "publisher": "IEEE",
        "abstract": "Supporting massive access from Internet of Things (IoT) devices plays a pivotal role in the design of 6G networks. Nonetheless, concurrent massive access to the network deteriorates the quality of 6G communications. To overcome the challenge of massive access, our focus shifts to optimizing the age-critical frameless ALOHA (ACFA) random access protocol. The conventional ACFA suffers from high latency and unreliability when massive devices attempt to access the network. Consequently, we introduce an adaptive algorithm to address the transmission issues. This paper optimizes the random access channel (RACH) procedure by maximizing a long-term multi-objective function, which consists of the average age of information (AoI), normalized throughput, traffic load and the average number of successfully accessed machine -type communication devices. To achieve the optimal objective in ACFA, we apply deep reinforcement learning (DRL) algorithms. In our algorithms, agents take action in both distributed and centralized manners. In the distributed approach, each device learns the choice of the access probability and the slot, guided by feedback from the base station (BS). Simultaneously, in the centralized approach, the BS restricts a specific number of devices from accessing the network and dynamically adjusts the frame length based on the transmission results of devices. Our simulation results demonstrate that the proposed scheme surpasses benchmark schemes and exhibits significant potential to minimize AoI performance.",
        "issn": {
            "Print ISSN": "0018-9545",
            "Electronic ISSN": "1939-9359"
        },
        "keywords": {
            "IEEE Keywords": [
                "6G mobile communication",
                "Access protocols",
                "Throughput",
                "Internet of Things",
                "Performance evaluation",
                "Training",
                "Minimization"
            ],
            "Author Keywords": [
                "Age of information",
                "massive access",
                "deep reinforcement learning",
                "machine -type communication",
                "frameless ALOHA"
            ]
        },
        "title": "A Hybrid Collaborative Learning for Age of Information Minimization in Massive Access"
    },
    {
        "authors": [
            "Bohang Wang",
            "Yunyang Zhang",
            "Rui Xu",
            "Siqi Jiang",
            "Aijun Liu",
            "Guoru Ding",
            "Xiaohu Liang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "23 August 2024",
        "doi": "10.1109/JIOT.2024.3448411",
        "publisher": "IEEE",
        "abstract": "This work investigates the problem of finite blocklength covert communications with relay assistance in Internet of Things (IoT) to extend the communications range. We reconstruct the framework for analyzing covert communications under decoded and forwarded protocols based on Willie’s optimal detection method. The analytic expression of Kullback-Leibler (KL) divergence is derived, the upper bound of KL divergence is solved by using the convexity of KL divergence, and the strict covertness constraint of the system is obtained. Meanwhile, to maximize the effective throughput, a covert communication parameter configuration scheme is proposed. Theoretical analysis and simulation results indicate that the compromised relationship of transmit power between Alice and relay nodes, and a reasonable power allocation scheme can enhance the effective throughput of the system.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Relays",
                "Internet of Things",
                "Decoding",
                "Throughput",
                "Security",
                "Wireless sensor networks",
                "Transmitters"
            ],
            "Author Keywords": [
                "Covert communications",
                "finite blocklength",
                "relay-assisted communications"
            ]
        },
        "title": "Relay-Assisted Finite Blocklength Covert Communications for Internet of Things"
    },
    {
        "authors": [
            "Hexiong Yao",
            "Yuexin Ma",
            "Peijing Li",
            "Chunlei Zhai",
            "Jiangbo Song",
            "Mingjun Ouyang",
            "Zhiqiang Dai",
            "Xiangwei Zhu"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "28 August 2024",
        "doi": "10.1109/JIOT.2024.3450967",
        "publisher": "IEEE",
        "abstract": "Visual-inertial odometry has played an important role in the field of the IoT, providing a variety of devices and systems with high-precision and reliable positioning and navigation capabilities. In particular, indoor environments have become an important scenario for its application. However, the lack of robustness of point-based VIO systems in low-textured man-made environments often leads to failure. Based on this issue, this paper proposes an innovative monocular visual-inertial odometry approach to fully utilize the available information in man-made environments. In the front end, the IMU measurement model is defined by the pre-integration method. In image data, first, the line features undergo a uniformization process, which reduces the redundant features and improves the accuracy of line feature matching. Then, the vanishing points in the image are detected using the Manhattan world assumption, and the structural line features are classified as either parallel or perpendicular to gravity based on vanishing points. In the back end, a novel residual term is defined for structural line features and gravity, deriving the corresponding Jacobian. This approach effectively addresses the issue of structural line degeneracy and continuously optimizes gravity, while also increasing the utilization of structural lines. A sliding window nonlinear optimization method is employed to minimize the sum of residuals. We tested the proposed system and the state-of-the-art VIO systems on both public datasets and our collected dataset to validate the effectiveness of the proposed system.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Gravity",
                "Accuracy",
                "Internet of Things",
                "Sensors",
                "Odometry",
                "Robustness"
            ],
            "Author Keywords": [
                "Visual-inertial odometry",
                "Manhattan world assumption",
                "Structural lines degeneracy",
                "Gravity constraints"
            ]
        },
        "title": "SG-VIO: Monocular Visual-Inertial Odometry With Tightly Coupled Structural Lines and Gravity to Avoid Degeneracy"
    },
    {
        "authors": [
            "Zhihong Deng",
            "Chunming Tang",
            "Taotao Li",
            "Debiao He"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "28 August 2024",
        "doi": "10.1109/JIOT.2024.3451303",
        "publisher": "IEEE",
        "abstract": "The emergence of 5G/6G networks has sparked new potentials for Internet of Things (IoT) scenarios such as Vehicle Ad Hoc Networks (VANETs), inspiring numerous scholars to leverage Blockchain-based Internet of Vehicles (BIoV) solutions to address prevailing issues in VANETs. However, the dynamic and decentralized nature of VANETs presents significant challenges in terms of security and privacy, hindering data providers from engaging in the data-sharing process. Furthermore, the reliability of edge nodes and system architecture in the BIoV paradigm faces several challenges, including limited consensus participation, high resource consumption, poor scalability, and centralization. To mitigate these challenges, we propose RTSP, a robust and trusted service protocol for VANETs, based on a distributed ledger technology. RTSP advocates a novel three-tier BIoV architecture suitable for any permissioned BIoV application scenario. To improve the quality of data-sharing, we design a decentralized reputation mechanism. This mechanism mitigates the performance bottleneck induced by consensus transactions by measuring the historical behavior of RSUs. Empirical evidence from simulation experiments and security performance analyses substantiates RTSP’s capabilities. It can hinder the number of faulty RSUs from increasing while simultaneously improving data-sharing efficiency, simplifying communication complexity, and enhancing system scalability and consensus stability.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Scalability",
                "Protocols",
                "Reliability",
                "Internet of Things",
                "Collaboration",
                "Computer architecture",
                "Consensus protocol"
            ],
            "Author Keywords": [
                "Blockchain",
                "VANETs",
                "Reputation",
                "Consensus",
                "Scalability",
                "Reliability"
            ]
        },
        "title": "A Distributed Ledger-A ssisted Robust and Trusted Service Protocol for VANETs"
    },
    {
        "authors": [
            "Syed Muhammad Awais",
            "Wu Yucheng",
            "Khalid Mahmood",
            "Mohammed J. F. Alenazi",
            "Ali Kashif Bashir",
            "Ashok Kumar Das",
            "Pascal Lorenz"
        ],
        "published_in": "Published in: IEEE Transactions on Intelligent Transportation Systems ( Early Access )",
        "date_of_publication": "10 September 2024",
        "doi": "10.1109/TITS.2024.3452928",
        "publisher": "IEEE",
        "abstract": "The increase in popularity of vehicles encourages the development of smart cities. With this advancement, vehicular ad-hoc networks, or VANETs, are now frequently utilized for inter-vehicular communication to gather data regarding traffic congestion, vehicle location, speed, and road conditions. Such a public network is open to various security risks. Overall, protecting personal information on VANET is a vital responsibility. The integration of fog computing and VANETs has gained significant importance in recent years, driven by advancements in cloud computing, Internet of Things (IoT) technologies, and intelligent transportation systems. However, ensuring secure communication in fog-based VANETs remains a major challenge. To overcome this challenge, we introduce a novel authenticated key agreement protocol that achieves mutual authentication, generates a secure session key for secret communication, and provides privacy protection without the use of bilinear pairing. We rigorously prove the security of our proposed protocol, which is designed specifically for fog-based VANETs, and has been shown to meet their stringent security requirements. Moreover, we performed formal and informal analysis that shows our proposed protocol is highly efficient,our protocol’s computational and communication overhead are lower than those of other relevant protocols by 45.570% and 29.432%, respectively. Finally we use NS-3 simulation to prove that our proposed algorithm is a practical and scalable solution for secure communication in fog-based VANETs.",
        "issn": {
            "Print ISSN": "1524-9050",
            "Electronic ISSN": "1558-0016"
        },
        "keywords": {
            "IEEE Keywords": [
                "Protocols",
                "Authentication",
                "Security",
                "Edge computing",
                "Cryptography",
                "Real-time systems",
                "Vehicular ad hoc networks"
            ],
            "Author Keywords": [
                "Fog computing",
                "authentication",
                "key agreement",
                "privacy",
                "VANET",
                "security"
            ]
        },
        "title": "Provably Secure and Lightweight Authentication and Key Agreement Protocol for Fog-Based Vehicular Ad-Hoc Networks"
    },
    {
        "authors": [
            "Wenbo Yang",
            "Hao Wang",
            "Zhi Li",
            "Ziyu Niu",
            "Lei Wu",
            "Xiaochao Wei",
            "Ye Su",
            "Willy Susilo"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "16 September 2024",
        "doi": "10.1109/JIOT.2024.3461410",
        "publisher": "IEEE",
        "abstract": "We propose a privacy-preserving machine learning scheme based on the cloud-edge-end architecture to address issues like weak computing power of Internet of Things (IoT) terminals, poor communication quality, and heavy cloud server burdens in traditional frameworks. Edge servers aggregate and forward terminal data, relieving terminals of heavy communication tasks and undertaking part of the computing tasks, which reduces the burden on cloud servers and improves system response speed. For privacy protection, we flexibly use homomorphic encryption and secret sharing techniques, and dynamically add differential privacy noise to resist member inference attacks. Task allocation is coordinated between different layers to optimize computing overhead. Shallow model training is performed on edge servers using homomorphic encryption, while deep model training is conducted on cloud servers using secret sharing. To achieve the conversion from homomorphic ciphertext to secret sharing shares, we design a distributed decryption protocol. Experimental results show our scheme reduces computation overhead by 20%-30% compared to existing privacy-preserving machine learning schemes based on the cloud-edge-end framework, while maintaining privacy protection throughout all stages.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Servers",
                "Cloud computing",
                "Machine learning",
                "Internet of Things",
                "Training",
                "Homomorphic encryption",
                "Computational modeling"
            ],
            "Author Keywords": [
                "privacy-preserving machine learning",
                "cloud-edge-end",
                "fully homomorphic encryption",
                "secret sharing",
                "differential privacy"
            ]
        },
        "title": "Privacy-Preserving Machine Learning in Cloud-Edge-End Collaborative Environments"
    },
    {
        "authors": [
            "Zouhir Bellal",
            "Laaziz Lahlou",
            "Nadjia Kara",
            "Ibtissam El Khayat"
        ],
        "published_in": "Published in: IEEE Transactions on Green Communications and Networking ( Early Access )",
        "date_of_publication": "01 July 2024",
        "doi": "10.1109/TGCN.2024.3420957",
        "publisher": "IEEE",
        "abstract": "Edge computing-based microservices (ECM) are pivotal infrastructure components for latency-critical applications such as Virtual Reality/Augmented Reality (VR/AR) and the Internet of Things (IoT). ECM involves strategically deploying microservices at the networks edge to fulfill the low latency needs of modern applications. However, achieving efficient resource and energy consumption while meeting the latency requirement in the ECM environment remains challenging. Dynamic Voltage and Frequency Scaling (DVFS) is a common technique to address this issue. It adjusts the CPU frequency and voltage to balance energy cost and performance. However, selecting the optimal CPU frequency depends on the nature of the microservice workload (e.g., CPU-bound, memory-bound, or mixed). Moreover, various microservices with different latency requirement can be deployed on the same edge node. This makes the DVFS application extremely challenging, particularly for a chip-wide DVFS implementation for which CPU cores operate at the same frequency and voltage. To this end, we propose GAS, enerGy Aware microServices edge computing framework, which enables CPU frequency scaling to meet diverse microservice latency requirement with the minimum energy cost. Our evaluation indicates that our CPU scaling policy decreases energy consumption by 5% to 23% compared to Linux governors while maintaining latency requirement and significantly contributing to sustainable edge computing.",
        "issn": {
            "Electronic ISSN": "2473-2400"
        },
        "keywords": {
            "IEEE Keywords": [
                "Microservice architectures",
                "Edge computing",
                "Energy consumption",
                "Energy efficiency",
                "Task analysis",
                "Frequency diversity",
                "Time-frequency analysis"
            ],
            "Author Keywords": [
                "Edge Computing",
                "Microservice",
                "DVFS",
                "Energyefficient",
                "Container Autoscaling"
            ]
        },
        "title": "GAS: DVFS-Driven Energy Efficiency Approach For Latency-Guaranteed Edge Computing Microservices"
    },
    {
        "authors": [
            "Yuan Yuan",
            "Bin Yang",
            "Wei Su",
            "Haoru Li",
            "Chang Wang",
            "Qi Liu",
            "Tarik Taleb"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "30 September 2024",
        "doi": "10.1109/JIOT.2024.3470847",
        "publisher": "IEEE",
        "abstract": "Wireless edge networks can provide edge services to support various time-critical Internet of Things (IoT) applications, like autonomous vehicles, where cache content updates are significant to maintaining information freshness quantified as information of age (AoI). However, frequent content updates result in high energy consumption at the edge nodes. This paper investigates the cache content updates in wireless edge networks, aiming to ensure information freshness and low energy consumption. To this end, we propose a rainbow deep reinforcement learning-based cache content update scheme (RB-DRN). In the RB-DRN scheme, we first establish a Markov Decision Process (MDP) to characterize the process of cache update. By fully taking advantage of R-Learning empowered Rainbow DQN, we then make optimal strategy to obtain the minimum long-term average overhead associated with energy consumption and information freshness. Extensive simulation results are presented to validate our proposed RB-DRN scheme and also to illustrate that our RB-DRN scheme outperforms the benchmark scheme in terms of information freshness and energy consumption.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Vehicle dynamics",
                "Energy consumption",
                "Internet of Things",
                "Servers",
                "Dynamic scheduling",
                "Monitoring",
                "Heuristic algorithms",
                "Wireless sensor networks",
                "Wireless communication"
            ],
            "Author Keywords": [
                "Wireless Edge Networks",
                "Internet of Things",
                "Age of Information",
                "Cache Update",
                "Rainbow DQN"
            ]
        },
        "title": "AoI and Energy-Driven Dynamic Cache Updates for Wireless Edge Networks"
    },
    {
        "authors": [
            "Ling-Feng Shi",
            "Rui Zeng",
            "Feng Jing",
            "Yifan Shi"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "16 August 2024",
        "doi": "10.1109/JIOT.2024.3445134",
        "publisher": "IEEE",
        "abstract": "People’s need for positioning and navigational data has grown over time as communication technology has advanced, becoming an essential and significant aspect of daily life. As a result of the Global Navigation Satellite Systems (GNSS) increasing maturity, satellite positioning technology can now achieve the highest centimeter-level positioning accuracy, effectively meeting the majority of outdoor positioning requirements. However, indoor positioning technology emerged as a solution for locations where satellite signals are obstructed and cannot normally provide positioning information. Researchers have focused on the Bluetooth positioning algorithm among them because it is a wireless positioning method. There are two types of traditional Bluetooth Received Signal Strength Indicator (RSSI) positioning methods based on the Internet of Things (IoT): trilateral and multilateral. However, because environmental interference causes the Bluetooth signal’s RSSI value to fluctuate greatly, using either of the above methods will result in significant errors and a decline in positioning accuracy when buildings, objects, or people are in the way. To address this issue, this paper proposes preprocessing the data prior to applying the least squares method. At the same time, based on processing outcomes, the weighted least squares method is utilized to determine Bluetooth RSSI positioning, resulting in a notable improvement over the accuracy of the traditional RSSI Bluetooth positioning.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Bluetooth",
                "Wireless communication",
                "Accuracy",
                "Mathematical models",
                "Internet of Things",
                "Global navigation satellite system",
                "Satellites"
            ],
            "Author Keywords": [
                "indoor positioning",
                "received Signal Strength Indicator",
                "least squares method",
                "Bluetooth"
            ]
        },
        "title": "Weighted Least Squares Bluetooth Localization Method Based on Data Preprocessing"
    },
    {
        "authors": [
            "Saike Zhu",
            "Cimang Lu",
            "Xiang Qiu",
            "Shifan Gao",
            "Xiang Ding",
            "Youngseo Kim",
            "Yi Zhao"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "19 August 2024",
        "doi": "10.1109/TCE.2024.3445341",
        "publisher": "IEEE",
        "abstract": "The Internet of Things (IoT) has proliferated ubiquitous information exchange between the physical and cyber worlds through consumer electronics, with a focus on moving computing power to edge terminals. Computing-in-memory (CIM) technology has emerged as a competitive candidate for edge computing because of its low power consumption and high performance. In order to achieve accurate inference for neural network models, it is crucial to comprehend the source of errors in the CIM-based analog computing paradigm. In this work, we analyzed the impact of random noises and output stabling times on the Programmable Linear Random Access Memory (PLRAM)-based CIM chip. Experimental results show that the impact of random noise is negligible. The output stabling time can be treated as RC delay, which is related to the weight distribution. We proposed a weight reordering strategy to achieve better performance without sacrificing computation accuracy. Experiments with a commercial 11-keyword speech recognition model show a 74.4% runtime reduction while maintaining a 95.6% classification accuracy.",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Accuracy",
                "Delays",
                "Noise",
                "Speech recognition",
                "In-memory computing",
                "Consumer electronics",
                "Computer architecture"
            ],
            "Author Keywords": [
                "Compute-In-Memory (CIM)",
                "Memristor",
                "Flash memory",
                "Resistance-Capacitance Delay (RC Delay)"
            ]
        },
        "title": "Mitigating RC-Delay Induced Accuracy Loss in Analog In-Memory Computing: A Non-Compromising Approach"
    },
    {
        "authors": [
            "Martin Lefebvre",
            "David Bol"
        ],
        "published_in": "Published in: IEEE Journal of Solid-State Circuits ( Early Access )",
        "date_of_publication": "12 June 2024",
        "doi": "10.1109/JSSC.2024.3406423",
        "publisher": "IEEE",
        "abstract": "In recent years, the development of the Internet of Things (IoT) has prompted the search for nA-range current references that are simultaneously constrained to a small area and robust to process, voltage and temperature variations. Yet, such references have remained elusive, as existing architectures fail to reach a low temperature coefficient (TC) while minimizing silicon area. In this work, we propose a nA-range constant-with-temperature (CWT) peaking current reference, in which a resistor is biased by the threshold voltage difference between two transistors in weak inversion. This bias voltage is lower than in conventional architectures to cut down the silicon area occupied by the resistor and is obtained by forward body biasing one of the two transistors with an ultra-low-power voltage reference so as to reduce its threshold voltage. In addition, the proposed reference includes a circuit to suppress the leakage of parasitic diodes at high temperature, and two simple trimming mechanisms for the reference current and its TC. As the proposed design relies on the body effect, it has been validated in both 0.11- \\mu m bulk and 22-nm fully-depleted silicon-on-insulator, to demonstrate feasibility across different technology types. In post-layout simulation, the 0.11- \\mu m design generates a 5-nA current with a 65-ppm/ ^\\circ C TC and a 2.84 \\% /V line sensitivity (LS), while in measurement, the 22-nm design achieves a 1.5-nA current with an 89-ppm/ ^\\circ C TC and a 0.51 \\% /V LS. As a result of the low resistor bias voltage, the proposed references occupy a silicon area of 0.00954 mm ^2 in 0.11 \\mu m (respectively, 0.00214 mm ^2 in 22 nm) at least 1.8 \\times (respectively, 8.2 \\times ) smaller than fabricated nA-range CWT references, but with a TC improved by 6.1 \\times (respectively, 4.4 \\times ).",
        "issn": {
            "Print ISSN": "0018-9200",
            "Electronic ISSN": "1558-173X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Threshold voltage",
                "Continuous wavelet transforms",
                "Resistors",
                "Transistors",
                "Silicon",
                "Internet of Things",
                "Logic gates"
            ],
            "Author Keywords": [
                "Constant-with-temperature (CWT)",
                "forward body biasing (FBB)",
                "peaking current reference (PCR)",
                "temperature coefficient (TC)",
                "temperature-independent"
            ]
        },
        "title": "A nA-Range Area-Efficient Sub-100-ppm/ ^\\circ C Peaking Current Reference Using Forward Body Biasing in 0.11- \\mu m Bulk and 22-nm FD-SOI"
    },
    {
        "authors": [
            "Nikitha Kannan",
            "Navneet Gupta"
        ],
        "published_in": "Published in: IEEE Journal on Flexible Electronics ( Early Access )",
        "date_of_publication": "02 September 2024",
        "doi": "10.1109/JFLEX.2024.3453779",
        "publisher": "IEEE",
        "abstract": "The rise of the Internet of Everything has spurred the need for flexible and stretchable electronic devices, particularly in biomedical applications. Monocrystalline silicon, a key material in the semiconductor industry, must be adapted to meet these demands. This article explores various thinning techniques to fabricate flexible silicon wafers, methods for transferring silicon to flexible substrates, and the importance of enhancing silicon’s stretchability. Furthermore, it discusses the impact of flexible silicon on sectors such as biomedical sensing, electronics, and power systems, highlighting the role of the Internet of Things (IoT) platform in interconnecting devices. Finally, the article examines current progress and future prospects in flexible silicon technology, paving the way for further advancements in this rapidly evolving field.",
        "issn": {
            "Electronic ISSN": "2768-167X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Silicon",
                "Substrates",
                "Photovoltaic cells",
                "Fabrication",
                "Internet of Things",
                "Flexible electronics",
                "Chemicals"
            ],
            "Author Keywords": [
                "Flexible Electronics",
                "Flexible silicon",
                "Ultra-thin chips fabrication",
                "progression"
            ]
        },
        "title": "Flexible Silicon: Status, Opportunities, and Challenges"
    },
    {
        "authors": [
            "Hemant Ghayvat",
            "Muhammad Awais",
            "Rebakah Geddam",
            "Mohammad Zuhair",
            "Muhammad Ahmed Khan",
            "Marcelo Milard",
            "Lewis Nkenyereye",
            "Kapal Dev"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "10 June 2024",
        "doi": "10.1109/JIOT.2024.3411798",
        "publisher": "IEEE",
        "abstract": "Artificial Intelligence of Medical Things (AIoMT) requires storing, preprocessing, monitoring, and analytics of large-scale sensor data fusion in the cloud. However, migrating to the cloud possesses intrinsic issues of cost, performance constraints, and sustainable computing. This research explores the potential of AIoMT in crafting intelligent models for daily activity patterns and predicting unusual occurrences. It delves into power-efficient and sustainable computing tailored for IoT sensors, methods, and systems geared towards crafting digitally enhanced smart homes for the elderly. Fusion data is collected from heterogenous sensors to track daily patterns and processed for anomaly detection and alert generation. The AIoMT model has employed the Time and Energy Minimization Scheduler (TEMS) algorithm, which considers energy consumption, processing duration, data transmission expenses, and standby device power consumption. This enables local computing in IoMT systems, mobile edge servers, and cloud controllers, promoting sustainability in healthcare. To optimize execution time and cost-effectiveness, task scheduling options include local Internet of Things devices, cloud infrastructure, and Multi-access Edge Computing (MEC). This approach could benefit digitally enhanced communities significantly, promoting Low-Carbon, Power-Efficient, Sustainable Computing (LCPESC). The LCPESC AIoMT approach demonstrates precision close to a 95% confidence level. Further, the proposed model is extended beyond individual households to encompass digitally augmented communities.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Hidden Markov models",
                "Computational modeling",
                "Green computing",
                "Cloud computing",
                "Task analysis",
                "Internet of Things",
                "Biological system modeling"
            ],
            "Author Keywords": [
                "Smart Home",
                "augmented communities",
                "Behavioral pattern generation",
                "Activities of Daily Living (ADL)",
                "Sensor Data Fusion",
                "Sustainable Computing",
                "Artificial Intelligence of Medical Things (AIoMT)",
                "Multi-access Edge Computing (MEC)",
                "Cloud controller (CC) Server"
            ]
        },
        "title": "Digitally Enhanced Home to the Village: AIoMT-Enabled Multi-Source Data Fusion and Power-Efficient Sustainable Computing"
    },
    {
        "authors": [
            "Wen Zhang",
            "Rui Li",
            "Pei Quan",
            "Jiang Chang",
            "Yongsheng Bai",
            "Bojun Su"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "20 August 2024",
        "doi": "10.1109/JIOT.2024.3445965",
        "publisher": "IEEE",
        "abstract": "Deep variational residual auto-encoder (ResNet-VAE) has shown promising outcomes in missing imputation of wastewater quality data. Nevertheless, with its large storage size and computation overhead, it is of great difficulty to deploy wastewater treatment plant (WWTP) sensors for real-time miss-ing imputation. To address this problem, we propose a novel approach called lightweight-ResNet-VAE to compress classical ResNet-VAE by network pruning, weight quantization, and rela-tive indexing in the paper. Firstly, we develop a three-step net-work pruning method to sparsify the weight matrices by remov-ing insignificant weights to reduce the time cost of model infer-ence. Secondly, we develop weight quantization and use eight shared weights to compress the size of each weight from 32-bit to 3-bit. Finally, the relative indexing is adopted to further com-press the size of the classical ResNet-VAE by compressed sparse row (CSR), which greatly accelerates the model computation and saves storage size. Experiments on the Beipai Iot influent quality dataset demonstrate that lightweight-ResNet-VAE compresses the size of the classical ResNet-VAE from 301.88KB to 26.24KB with a compression rate of 11.50 times, and outperforms the baseline methods in terms of computation acceleration, storage saving and energy consumption with only a slight decrease on accuracy as 2.74% in MAPE of missing imputation for wastewater quality data due to pruning less significant weights and quantizing the remaining weights.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Imputation",
                "Computational modeling",
                "Sensors",
                "Mathematical models",
                "Wastewater",
                "Quantization (signal)",
                "Indexing"
            ],
            "Author Keywords": [
                "lightweight ResNet-VAE",
                "model compression",
                "missing imputation",
                "wastewater quality"
            ]
        },
        "title": "Lightweight Deep Learning for Missing Data Imputation in Wastewater Treatment With Variational Residual Auto-Encoder"
    },
    {
        "authors": [
            "Sugandha Yadav",
            "Poornima Mittal",
            "Shubham Negi"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "30 October 2024",
        "doi": "10.1109/JSEN.2024.3485702",
        "publisher": "IEEE",
        "abstract": "The utilization of fluorescence detection for various bio-medical applications such as oximeter sensor, cancer detection, monitor of protein-DNA interactions, heart stroke detection etc. has been on rise which was firstly introduced by Heyduk and Lee in 1990. The integration of organic light emitting diode and organic photo diode is well suited for these kinds of health-care detections. In this paper, a methodology is proposed for covid-19 detection which consists of a highly flexible blue OLED, an OPD and human saliva sample. An in-depth investigation is being performed on both the OLED and OPD devices to make these devices suitable for the detection of SARS-COV-2 virus inside human saliva sample. The presented methodology is based on the fluorescence detection. Herein, the OLED emits excitation wavelength of 470 nm and OPD produces two different currents 63.5 mA and 37.2 mA corresponding to the emission wavelengths of 490 nm and 525 nm, correspondingly. It is concluded here that if the OPD produces 63.5 mA current, the person is infected by covid-19 and if it produces 37.2 mA current, the person is healthy. This paper also compares the proposed methodology with some other researcher’s work. The proposed OLED-OPD integration may be utilized for other sensing application such as environmental monitoring, multispectral sensing, optical sensing and IoT devices etc.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Organic light emitting diodes",
                "COVID-19",
                "Sensors",
                "Fluorescence",
                "Anodes",
                "Detectors",
                "Geometry",
                "Cathodes",
                "Pulse oximeter",
                "Mathematical models"
            ],
            "Author Keywords": [
                "Sensor",
                "OLED",
                "OPD",
                "V-OTFT",
                "Covid-19"
            ]
        },
        "title": "Covid-19 Detection using Organic LED and Photo Diode based sensor device"
    },
    {
        "authors": [
            "Akshatha P S",
            "S M Dilip Kumar"
        ],
        "published_in": "Published in: IEEE Transactions on Industrial Informatics ( Early Access )",
        "date_of_publication": "23 October 2024",
        "doi": "10.1109/TII.2024.3476538",
        "publisher": "IEEE",
        "abstract": "e-healthcare industry is one of the innovative concepts introduced in many developing countries in the recent years. Internet of Things (IoT) has been widely used to connect valuable clinical assets and provide patients with intelligent and high-quality healthcare services. Continuous monitoring and data transmission with minimum delay are necessary for e-healthcare industrial applications. In the e-healthcare industry, various message queuing telemetry transport (MQTT) brokers (both open-source and commercially available) can be used; however, the abundance of options might be challenging to choose the best broker option. This article proposes to evaluate four prominent MQTT brokers in three realistic test scenarios to analyze their performance based on four metrics, viz. delay, mean time between failures, mean time to repair, and jitter. The main contribution of this article is the detailed evaluation of four MQTT brokers including Mosquitto , EMQx , HiveMQ , and Fluux , using the three different scenarios namely Broker Pinger , Broker Availability , and Maximum Client Connection implemented using Python. Wireshark packet analyzer is used to capture packets while messages are being transmitted with selected public brokers to analyze the performance parameters. Mathematical analysis of performance measurement parameters is also presented in this article. Appropriate use of the aforementioned three scenarios in MQTT brokers allows the e-healthcare industry to monitor patient data regularly without omitting any data. The outcome of this work exhibits that Mosquitto and Fluux brokers have a higher availability percentage, whereas the EMQx broker could connect more significant numbers of parallel clients, and HiveMQ provided fewer variations in delay.",
        "issn": {
            "Print ISSN": "1551-3203",
            "Electronic ISSN": "1941-0050"
        },
        "keywords": {
            "IEEE Keywords": [
                "Delays",
                "Internet of Things",
                "Protocols",
                "Medical services",
                "Quality of service",
                "Monitoring",
                "Jitter",
                "Industries",
                "Payloads",
                "Mathematical analysis"
            ],
            "Author Keywords": [
                "Delay",
                "e-healthcare",
                "Internet of Things",
                "jitter",
                "MQTT brokers",
                "message queuing telemetry transport",
                "publish/subscribe"
            ]
        },
        "title": "Analysis and Evaluation of MQTT Brokers for e-Healthcare Applications"
    },
    {
        "authors": [
            "Jiajie Yin",
            "Zhiqing Tang",
            "Jiong Lou",
            "Jianxiong Guo",
            "Hui Cai",
            "Xiaoming Wu",
            "Tian Wang",
            "Weijia Jia"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "30 August 2024",
        "doi": "10.1109/JIOT.2024.3452111",
        "publisher": "IEEE",
        "abstract": "Multiple Unmanned Aerial Vehicle (UAV)-assisted Mobile Edge Computing (MEC) leverages UAVs equipped with computational resources as mobile edge servers, providing flexibility and low-latency connections, especially beneficial in smart cities and the Internet of Things (IoT). Maximizing Quality of Services (QoS) while minimizing energy consumption necessitates developing a suitable offloading ratio and trajectory control algorithm for UAVs. However, existing research on UAV control algorithms overlooks significant challenges like the heterogeneity of User Equipments (UEs) and offloading failures. Furthermore, there is a dearth of experimental validation in large-scale UAV-assisted MEC scenarios. To bridge these gaps, we introduce a QoS-aware Energy-efficient Multi-UAV Offloading ratio and Trajectory control algorithm (QEMUOT). Specifically, 1) A composite UE mobility model is proposed to enhance system heterogeneous modeling, encompassing models for high-speed, low-speed, and fixed UEs. 2) QEMUOT is devised using multi-agent reinforcement learning algorithms to determine offloading ratio and trajectory control decisions. To tackle sparse reward space and offloading failures, we employ expert demonstrations for pretraining and enhance reward mechanisms. 3) Experimental simulations illustrate that our algorithm outperforms baseline algorithms in user QoS with reduced energy consumption and demonstrates superior scalability in scenarios with numerous UAVs and UEs.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Quality of service",
                "Autonomous aerial vehicles",
                "Internet of Things",
                "Heuristic algorithms",
                "Mobility models",
                "Energy consumption"
            ],
            "Author Keywords": [
                "Mobile Edge Computing",
                "Multi-agent Deep Reinforcement Learning",
                "Unmanned Aerial Vehicle",
                "Heterogeneous Mobility Pattern"
            ]
        },
        "title": "QoS-Aware Energy-Efficient Multi-UAV Offloading Ratio and Trajectory Control Algorithm in Mobile Edge Computing"
    },
    {
        "authors": [
            "Min Gao",
            "Lifang Yi",
            "Jinyeong Moon"
        ],
        "published_in": "Published in: IEEE Transactions on Industrial Electronics ( Early Access )",
        "date_of_publication": "14 August 2024",
        "doi": "10.1109/TIE.2024.3433376",
        "publisher": "IEEE",
        "abstract": "Current-transformer-based (CT-based) magnetic energy harvesting (EH) extracts energy from ac current-induced magnetic fields, outperforming other ambient EH methods in harvested energy level and predictability. As a result, it emerges as a suitable power supply for self-powering low-power devices like embedded systems, sensor nodes, Internet-of-Things (IoT), and cyber-physical systems (CPS). Prior studies indicated that optimal EH occurs when the magnetic core operates in a suitably saturated state as the magnetically stored energy in a physical core volume increases as the core approaches saturation. However, the actual power transfer electrically ceases once the core reaches full saturation and no current circulates through loads. This article introduces an innovative desaturation strategy, using four bidirectional switches in a crisscross pattern to alter the core voltage direction and prevent magnetic saturation, to enhance EH level. This study also examines the impact of various factors such as load voltage, the count of EH windows, and primary current on the harvested energy, aiming to find the optimal load voltage for maximum energy. Through circuit simulations and experiments, the proposed method shows the harvested energy is significantly increased, exceeding six times the output of basic systems relying on passive rectifiers. Moreover, our new method demonstrates distinctive EH traits, wherein energy extraction quadratically increases with the primary current, unlike conventional methods where energy extraction scales linearly with the primary current. This advantage allows a significantly denser energy harvester to be built under higher magnetic field configurations.",
        "issn": {
            "Print ISSN": "0278-0046",
            "Electronic ISSN": "1557-9948"
        },
        "keywords": {
            "IEEE Keywords": [
                "Magnetic cores",
                "Saturation magnetization",
                "Toroidal magnetic fields",
                "Circuits",
                "Transformer cores",
                "Magnetic circuits",
                "Coils"
            ],
            "Author Keywords": [
                "Bidirectional switches",
                "cascaded cores",
                "desaturation",
                "energy harvesting (EH)",
                "magnetic"
            ]
        },
        "title": "Intracycle Gapless Core Desaturation via Crisscross Switches for Maximal Magnetic Energy Harvesting"
    },
    {
        "authors": [
            "Demin Gao",
            "Haoyu Wang",
            "Shuai Wang",
            "Weizheng Wang",
            "Zhimeng Yin",
            "Shahid Mumtaz",
            "Xingwang Li",
            "Valerio Frascolla",
            "Arumugam Nallanathan"
        ],
        "published_in": "Published in: IEEE Transactions on Communications ( Early Access )",
        "date_of_publication": "16 September 2024",
        "doi": "10.1109/TCOMM.2024.3461574",
        "publisher": "IEEE",
        "abstract": "Wi-Fi is a very common means for providing wireless access to the Internet, e.g., using the 2.4GHz Industrial, Scientific, and Medical (ISM) band and more recently also the 6 GHz band via Wi-Fi 6E. Thanks to a chip recently launched by Semtech, in the same 2.4GHz band now can also operate Long Range (LoRa), which is widely used in Internet of Things (IoT) applications due to its low power consumption and wide coverage range. To allow for data interchange among these technologies, multi-radio gateways are needed, which introduce additional costs, complexities, and potential points of failure. To address this challenge, we propose the concept of Wireless to LoRa (WiLo) to make directional communication from Wi-Fi to LoRa. WiLo uses physical-layer (PHY) communication and dedicated input chips in the 2.4 GHz band to transmit information. To overcome the modulation technique differences between Wi-Fi and LoRa, WiLo leverages narrow-band communication, a technique that generates ultra-narrowband signals using single-tone sinusoidal signals by manipulating the payload of Wi-Fi devices. These signals can be detected by LoRa Wide Area Network base stations due to their high receiver sensitivity for long-range communication. Our experiments, which make use of both Universal Software Radio Peripheral (USRP) and commodity devices, demonstrate that WiLo can achieve concurrent wireless communication over a distance of 500 m, from commercial Wi-Fi chips to a LoRaWAN, with more than 96% frame reception rate. These findings show the effectiveness of WiLo in enabling reliable and efficient wireless communication over long distances, making it particularly relevant for applications such as remote monitoring systems, sensor networks, and smart cities.",
        "issn": {
            "Print ISSN": "0090-6778",
            "Electronic ISSN": "1558-0857"
        },
        "keywords": {
            "IEEE Keywords": [
                "LoRa",
                "Wireless fidelity",
                "Wireless communication",
                "Wireless sensor networks",
                "Modulation",
                "Internet of Things",
                "Emulation"
            ],
            "Author Keywords": [
                "Cross-Technology Communication",
                "LoRa",
                "Wi-Fi",
                "Low-Power Wide-Area Networks"
            ]
        },
        "title": "WiLo: Long-Range Cross-Technology Communication from Wi-Fi to LoRa"
    },
    {
        "authors": [
            "Fei Deng",
            "Liqing Zhang"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "03 October 2024",
        "doi": "10.1109/JIOT.2024.3472052",
        "publisher": "IEEE",
        "abstract": "With the exponential growth of Internet of things (IoT) devices in the Era of Internet of Everything (IoE), two major issues arise: data processing speed and interpretability in neural networks. Specifically, training neural networks to handle IoE data often results in poor convergence speed, overfitting of the weights, and fluctuations in the error function. To address these challenges, this paper introduces a novel neural network, the recurrent Sigma-Pi-Sigma neural network (RSPSNN), trained using a batch gradient algorithm enhanced with smoothing L1 Lasso regularization and an adaptive momentum term. This approach not only improves convergence speed but also enhances generalization capabilities and reduces oscillations. Furthermore, the interpretability of RSPSNN is theoretically demonstrated through characteristics of monotonicity, strong/weak convergence, and stability. Finally, the theoretical findings are supported by experiment results in classification, recognition, and prediction tasks.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Neural networks",
                "Convergence",
                "Internet of Things",
                "Smoothing methods",
                "Training",
                "Heuristic algorithms",
                "Classification algorithms",
                "Adaptation models",
                "Accuracy",
                "Vectors"
            ],
            "Author Keywords": [
                "IoE",
                "RSPSNN",
                "Smoothing Lasso regularization",
                "Adaptive momentum",
                "Interpretability"
            ]
        },
        "title": "Performance Optimization and Interpretability of Recurrent Sigma-Pi-Sigma Neural Networks on Application of IoE data"
    },
    {
        "authors": [
            "Wei Jiang",
            "Xiao Yuan",
            "Caishi Huang",
            "Liping Qian"
        ],
        "published_in": "Published in: IEEE Transactions on Cognitive Communications and Networking ( Early Access )",
        "date_of_publication": "08 July 2024",
        "doi": "10.1109/TCCN.2024.3424845",
        "publisher": "IEEE",
        "abstract": "In this paper, we investigate a marine Internet of Things (M-IoT) network with high altitude platform (HAP) secure computation offloading at risk of eavesdropping. To ensure the security of HAP’s information transmission, we utilize a group of unmanned surface vehicles (USVs) with an HAP to form a non-orthogonal multiple access (NOMA) transmission group to provide co-channel interference. Our goal is to minimize the total energy consumption by jointly optimizing HAP’s computation offloading workload, HAP’s transmission power, data transmission time, and USV’s transmission power while meeting the security and delay requirements. Although this problem is strictly non-convex optimization, we use problem transformation and vertical decomposition methods to decompose the problem into an underlying problem and a top-level problem. The underlying problem is to determine the HAP’s transmission power and HAP’s computation offloading workload, and the top-level problem is to determine the data transmission time, which is solved by using proximal policy optimization (PPO). The two subproblems are solved iteratively over each other to obtain the minimum total energy consumption. Simulation results show that the proposed algorithm converges faster and reduces the energy consumption of 30.34% compared to asynchronous advantage actor-critic (A3C).",
        "issn": {
            "Electronic ISSN": "2332-7731"
        },
        "keywords": {
            "IEEE Keywords": [
                "NOMA",
                "Interference",
                "Security",
                "Energy consumption",
                "Eavesdropping",
                "Optimization",
                "Throughput"
            ],
            "Author Keywords": [
                "Mobile edge computing",
                "non-orthogonal multiple access",
                "physical layer security",
                "marine Internet of Things"
            ]
        },
        "title": "NOMA-Assisted Secure Computation Offloading and Resource Allocation in Marine Internet of Things"
    },
    {
        "authors": [
            "Baochuang Wang",
            "Yiling Xie",
            "Lin Cheng",
            "Jianping Guo"
        ],
        "published_in": "Published in: IEEE Journal of Solid-State Circuits ( Early Access )",
        "date_of_publication": "16 September 2024",
        "doi": "10.1109/JSSC.2024.3454078",
        "publisher": "IEEE",
        "abstract": "An ultralow quiescent current dual-mode dc – dc buck converter is presented in this article to achieve high efficiency over a wide load range for Internet of Thing (IoT) applications. In medium and heavy load conditions, the valley-current mode (VCM) with adaptive on-time (AOT) is employed to guarantee loop stability and seamless transition between pulsewidth modulation (PWM) and pulse-frequency modulation (PFM). A hiccup mode (HM) is proposed to minimize the power consumption of control circuits in light load conditions. Based on the compensator in the VCM, a built-in mode tracking technology is proposed to achieve the predictable and seamless mode transition without load current sensing circuits. Implemented in a 0.18- $\\mu$ m BCD technology, the proposed converter has an efficiency higher than 90% over 10- $\\mu$ A to 500-mA loading range within the supply range of a single lithium-ion battery. Under a 2.4–5.5-V input voltage and 0–1-A loading current range, the output ripple is less than 20 mV. When the load current steps from 2.4 $\\mu$ A to 200 mA within 10 ns, the output undershoot is 152 mV.",
        "issn": {
            "Print ISSN": "0018-9200",
            "Electronic ISSN": "1558-173X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Circuits",
                "Buck converters",
                "Pulse width modulation",
                "Loading",
                "Internet of Things",
                "Inductors",
                "Power demand"
            ],
            "Author Keywords": [
                "Current mode",
                "dc–dc buck converter",
                "hiccup mode (HM)",
                "high efficiency",
                "low power",
                "mode transition"
            ]
        },
        "title": "A Single Li-Ion Battery Powered Buck Converter With\n>\n90% Efficiency Over 10-\nμ\nA to 500-mA Loading Range by Utilizing Compensator-Based Built-In Mode Tracking Technology"
    },
    {
        "authors": [
            "Andrea Ortiz",
            "Rostyslav Olshevskyi",
            "Daniel Barragan-Yani"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "25 October 2024",
        "doi": "10.1109/JIOT.2024.3486446",
        "publisher": "IEEE",
        "abstract": "The importance of Massive Machine-Type Communications (mMTC) in Beyond 5G and 6G networks is supported by the ever-increasing number of connected devices in what are known as massive Internet of Things (IoT) networks. These networks bring unprecedented challenges for the distribution of the available communication resources because the allocation problems often lead to combinatorial optimization formulations which are known to be NP-hard. A fact that limits the performance of state-of-the-art techniques when the network size increases. To address this challenge, we take a new direction and propose a method based on statistical physics to address resource allocation problems in large networks. To this aim, we first show that resource allocation problems have the same structure as the problem of finding specific configurations in spin glasses, a type of disordered physical systems. Based on this parallel, we propose Momentum Survey Propagation, a resource allocation method to minimize the interference in mMTC networks. Our proposed approach extends the Survey Propagation method of statistical physics. Specifically, it exploits the so-called momentum technique, widely used in the context of neural networks, to improve the convergence properties of Survey Propagation. Our implementation is the first application of Survey Propagation to a wireless communication network. Through numerical simulations we show that Momentum Survey Propagation is a promising tool for the efficient allocation of communication resources in mMTC.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Resource management",
                "Surveys",
                "Glass",
                "Interference",
                "Physics",
                "Optimization",
                "Internet of Things",
                "Data aggregation",
                "Base stations",
                "Wireless communication"
            ],
            "Author Keywords": [
                "Massive Machine-Type Communications",
                "Survey Propagation",
                "Resource Allocation",
                "Interference Minimization"
            ]
        },
        "title": "Momentum Survey Propagation: A Statistical Physics Approach to Resource Allocation in mMTC"
    },
    {
        "authors": [
            "Sin-Ye Jhong",
            "Guan-Ting Li",
            "Chih-Hsien Hsia"
        ],
        "published_in": "Published in: IEEE Transactions on Consumer Electronics ( Early Access )",
        "date_of_publication": "07 October 2024",
        "doi": "10.1109/TCE.2024.3474911",
        "publisher": "IEEE",
        "abstract": "In personal healthcare, scalp health is often overlooked despite its critical role in systemic health. Traditional manual diagnostic methods for scalp conditions are invasive and subjective, resulting in variability and discomfort. This paper introduces the first Internet of Things (IoT)-based intelligent scalp inspection system leveraging edge-cloud collaborative computing to provide autonomous and real-time scalp health analysis using videocapillaroscope images. The system incorporates a new data augmentation strategy, Curriculum Enhanced Localizable Swap and Rotation (CE-LoSwRot), which mitigates data imbalance issues, and robust representation learning through Modified Balanced Contrastive Learning (M-BCL) and Test-Time Adaptation (TTA) to enhance feature representation and model adaptability. By integrating edge and cloud computing, the proposed system reduces network latency and congestion while ensuring real-time data processing and analysis. The comprehensive scalp problem classification dataset collected for this study demonstrates the system’s superior performance compared to existing state-of-the-art methods, establishing a new benchmark in scalp health diagnostics",
        "issn": {
            "Print ISSN": "0098-3063",
            "Electronic ISSN": "1558-4127"
        },
        "keywords": {
            "IEEE Keywords": [
                "Scalp",
                "Inspection",
                "Collaboration",
                "Cloud computing",
                "Artificial intelligence",
                "Training",
                "Real-time systems",
                "Image edge detection",
                "Data models",
                "Computational modeling"
            ],
            "Author Keywords": [
                "Internet of Things",
                "Edge-Cloud Computing",
                "Scalp Health",
                "Videocapillaroscope Sensor",
                "Representation Learning"
            ]
        },
        "title": "An Edge-Cloud Collaborative Scalp Inspection System Based on Robust Representation Learning"
    },
    {
        "authors": [
            "Lingwei Xu",
            "Xinpeng Zhou",
            "Shubo Cao",
            "Muhammad Asif",
            "Xingwang Li",
            "Khaled M. Rabie",
            "Yong Fu",
            "T. Aaron Gulliver"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "20 May 2024",
        "doi": "10.1109/JIOT.2024.3402827",
        "publisher": "IEEE",
        "abstract": "Emerging technologies such as artificial intelligence and big data have made numerous Internet of things (IoT) applications possible. In particular, the Artificial Intelligence of Things (AIoT) has the potential to promote the digitization and intelligent connection of all things. However, the openness and diversity of AIoT makes data information vulnerable to security attacks which can lead to a disruption of mobile communication networks. The complexity of real-time data security events requires accurate prediction of AIoT security performance. In this paper, a secure communication system model based on decode-and-forward (DF) relaying is proposed and its security performance is analyzed. Expressions for the secrecy outage probability (SOP) are derived, and these are used to evaluate the security performance. For this purpose, an intelligent SOP prediction algorithm based on MS-Net is proposed. MobileNet and SqueezeNet networks are used to design an improved lightweight MS-Net model, which is composed of a depth separable convolution block and a fire module in parallel. The fire module is used to reduce the number of parameters in the first branch, and the depth-separable convolution block is employed in the second branch instead of the standard convolution. This can adapt to nonlinear characteristic in the AIoT safety data and reduce energy consumption. Afterwards, the convolutional block attention module(CBAM) attention mechanism is used to improve the model’s ability to capture features. The proposed algorithm provides better AIoT security performance than other algorithms. In particular, the mean squared error (MSE) is 68.1% better than that of RegNet.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet of Things",
                "Artificial intelligence",
                "Mobile communication",
                "Prediction algorithms",
                "Data security",
                "Medical diagnostic imaging",
                "Medical services"
            ],
            "Author Keywords": [
                "AIoT",
                "physical layer security",
                "performance prediction",
                "MS-Net"
            ]
        },
        "title": "Security Performance Prediction Method of Artificial Intelligence of Things Based on Lightweight MS-Net Network"
    },
    {
        "authors": [
            "Joseph Khoury",
            "Christelle Nader",
            "Morteza Safaei Pour",
            "Elias Bou-Harb"
        ],
        "published_in": "Published in: IEEE Internet of Things Magazine ( Early Access )",
        "date_of_publication": "03 July 2024",
        "doi": "10.1109/IOTM.001.2400044",
        "publisher": "IEEE",
        "abstract": "The cyber domain demonstrates a profound interconnection with diverse global events, exerting its influence across social, political, and military realms. As a result, it is both rational and imperative to maintain a keen awareness of the threats that arise within the cyber domain. This can be achieved through robust cyber analytics and data-driven techniques to identify, analyze, and mitigate relevant cyber risks. As such, in this article, we elaborate on a unique, broadly-applicable, empirically-driven capability to enable the consistent measurement, identification and characterization of cyber threat dynamics. Specifically, we investigate and explore Internet-wide empirical data from diverse sources, namely, dark IP address spaces on the Internet to detect backscatter and scanning probes, globally distributed user datagram protocol (UDP) sensors to quantify reflective amplification attempts, and route collectors to ingest Border Gateway Protocol (BGP) routing data. As a case study, throughout an extensive 7-month measurement period, we employ the proposed approach to shed light on the 2022 Russo-Ukrainian cyber threat activities by drawing upon more than 150GB of real network and security data. We infer DDoS and UDP reflective attacks targeting federal agencies in Russia, and media entities in Ukraine. We further perceive an upsurge of Russian and Ukrainian Remotely Triggered Black Hole (RTBH) techniques employed to block attacks targeting multiple Russian \".ru\" country code top-level domain (ccTLD) and media companies. Additionally, we uncover an escalation of reconnaissance events, some of which are generated by the IoT-centric Mirai malware and others which target critical infrastructure. We report our findings while postulating thoughts on intriguing observations.",
        "issn": {
            "Print ISSN": "2576-3180",
            "Electronic ISSN": "2576-3199"
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer crime",
                "IP networks",
                "Protocols",
                "Probes",
                "Denial-of-service attack",
                "Backscatter",
                "Sensors"
            ],
            "Author Keywords": []
        },
        "title": "An Internet-Scale Data-Driven Approach for Exploring Cyber Threats Amid Global Conflicts"
    },
    {
        "authors": [
            "Ke Chen",
            "Honggang Wang",
            "Andrew Catlin",
            "Ashwin Satyanarayana",
            "Ramana Vinjamuri",
            "Sai Praveen Kadiyala"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "11 November 2024",
        "doi": "10.1109/JIOT.2024.3491674",
        "publisher": "IEEE",
        "abstract": "The demand for surface electromyography (sEMG) based exoskeletons is rapidly increasing due to their non-invasive nature and ease of use. With increase in use of Internet-of-Things (IoT) based devices in daily life, there is a greater acceptance of exoskeleton based rehab. As a result, there is a need for highly accurate and generalizable gesture classification mechanisms based on sEMG data. In this work, we present a framework which pre-processes raw sEMG signals with Empirical Fourier Decomposition (EFD) based approach followed by dimension reduction. This resulted in improved performance of the hand gesture classification. EFD decomposition’s efficacy of handling mode mixing problem on non-stationary signals, resulted in less number of decomposed components. In the next step, a thorough analysis of decomposed components as well as inter-channel analysis is performed to identify the key components and channels that contribute towards the improved gesture classification accuracy. As a third step, we conducted ablation studies on time-domain features to observe the variations in accuracy on different models. Finally, we present a case study of comparison of automated feature extraction based gesture classification vs. manual feature extraction based methods. Experimental results show that manual feature based gesture classification method thoroughly outperformed automated feature extraction based methods, thus emphasizing a need for rigorous fine tuning of automated models.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Electrodes",
                "Stroke (medical condition)",
                "Signal resolution",
                "Principal component analysis",
                "Electromyography",
                "Accuracy",
                "Manuals",
                "Time-domain analysis",
                "Muscles"
            ],
            "Author Keywords": [
                "surface electromyography",
                "Empirical Fourier Decomposition",
                "gesture classification",
                "stroke rehabilitation"
            ]
        },
        "title": "A Framework for Empirical Fourier Decomposition based Gesture Classification for Stroke Rehabilitation"
    },
    {
        "authors": [
            "Sukriti Gautam",
            "Suman Kumar"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "29 August 2024",
        "doi": "10.1109/JIOT.2024.3451698",
        "publisher": "IEEE",
        "abstract": "IoT applications like livestock and personalised health care monitoring systems have large amounts of data obtained per second from sensors interfaced to battery operated sensor nodes. In such networks, all the data generated at the nodes must be successfully sent to the central receiving device ensuring minimal power consumption of the data transmission operation, to enable longer lifetimes of battery-operated sensor nodes. In this paper, in order to minimize energy consumption of sensor nodes, we propose a Bluetooth Low Energy (BLE) based data transfer approach relying on BLE periodic advertisements, which allow up to 255 bytes of payload to be sent in one packet and minimize the involvement of interference prone primary advertising channels in the data transfer process. By using primary channel transmissions only for one-time device discovery and then disabling them, the proposed technique ensures that number of transmissions made by each sensor node for successful data delivery in large sensor networks is reduced in comparison with conventional extended and periodic advertisements. This offers reduction in energy consumption of sensor nodes due to reduction in radio on time. Through extensive real-time and simulation-based experimentation conducted, we have shown that, in a network of 200 sensor nodes, the proposed periodic advertising based technique requires much lesser radio on time for reliable data delivery and reduces energy consumption of the sensor nodes by almost 51% and 20% as compared to the consumption when extended and standardized periodic advertisements are used to communicate the data, respectively.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Advertising",
                "Data transfer",
                "Monitoring",
                "Energy efficiency",
                "Energy consumption",
                "Reliability",
                "Real-time systems"
            ],
            "Author Keywords": [
                "Bluetooth Low Energy",
                "BLE advertisements",
                "BLE periodic advertisements",
                "energy efficient sensor networks",
                "energy efficient data transfer",
                "monitoring applications",
                "sensor networks"
            ]
        },
        "title": "BLE Periodic Advertising Based Energy Efficient Sensor Node Operation for Transfer of Large Data in Monitoring Applications"
    },
    {
        "authors": [
            "Zeliang An",
            "Yuqing Xu",
            "Abdullah Tahir",
            "Jun Wang",
            "Baoze Ma",
            "Gert Frølund Pedersen",
            "Ming Shen"
        ],
        "published_in": "Published in: IEEE Transactions on Industrial Informatics ( Early Access )",
        "date_of_publication": "18 September 2024",
        "doi": "10.1109/TII.2024.3452274",
        "publisher": "IEEE",
        "abstract": "Modulation recognition (MR) plays a pivotal role due to its application in the spectrum sensing of 5G industrial cognitive communications and radio interference detection at the physical layer of the Internet of Things (IoT). Previous works have mainly focused on simulated fourth-generation (4G) multicarrier systems and ideal radio frequency (RF) scenarios. To bridge the gap between practice and theory, we propose a viable MR algorithm on all-physical testbeds, with nonlinear impairments of 28 GHz active phased arrays (APA). Specifically, our testbed is built on the Rohde&Schwarz (R&S) vector signal generation R&S-SMBV100B and spectrum analyzer R&S-FSW 67 GHz. To extract salient modulation patterns, we develop a physical-informed scattering transform (SCT) MR network (SCTMR-Net). With SCT modules, SCTMR-Net produces the translation-invariant and deformation-stable representations of 5-G signals by wavelet convolution, nonlinear modulus and low-pass filters. Extensive experiments on real-world measurement verify the viability of SCTMR-Net for high robustness to APA impairments.",
        "issn": {
            "Print ISSN": "1551-3203",
            "Electronic ISSN": "1941-0050"
        },
        "keywords": {
            "IEEE Keywords": [
                "OFDM",
                "Convolution",
                "Radio frequency",
                "Internet of Things",
                "Feature extraction",
                "Wavelet transforms",
                "Signal processing algorithms"
            ],
            "Author Keywords": [
                "5-G new radio (NR) industrial cognitive communications",
                "deep learning (DL)",
                "modulation recognition",
                "nonlinear active phased arrays (APA) impairments",
                "scattering transform modulation recognition-network (SCTMR-Net)"
            ]
        },
        "title": "Physics-Informed Scattering Transform Network for Modulation Recognition in 5G Industrial Cognitive Communications Considering Nonlinear Impairments in Active Phased Arrays"
    },
    {
        "authors": [
            "Zhenglin Li",
            "Ziyang Wang",
            "Qi Li",
            "Yiming Wang",
            "Biao Xiao",
            "Yang Gao",
            "Mingliang Zhu",
            "Fuzhen Xuan"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "03 May 2024",
        "doi": "10.1109/JSEN.2024.3394572",
        "publisher": "IEEE",
        "abstract": "With the development of the Internet of Things (IoT), stretchable sensors with high sensitivity and good conformality have attracted tremendous interest. In recent years, the laser micro-fabrication (LMF) method has shown the potential application in fabricating various stretchable sensors. Nevertheless, how to modulate the properties of the sensing element (SE) and conductive interconnect structure (CIS) in the LMF process to meet the application requirements for stretchable sensors is still a challenge for LMF. This study proposed an all-LMF technology to fabricate stretchable sensors. Laser induced reduction and laser induced ablation (LIA) are used to prepare serpentine Cu-CIS with a high conductivity of 123 μΩ·cm, minimized response to deformation (sensitivity<1x10 -3 ), and a high stability at strain up to 150%. Laser induced pyrolysis and LIA is used to synthesize graphene-SE with bio-inspired microcracks, demonstrating a ~210 times increment in sensitivity than the one without microcracks. The conjunction of the CIS and SE is formed by Cu nanoparticle and graphene composites, exhibiting insensitivity (sensitivity<0.42) to mechanical deformations. Representative applications fully demonstrate that LMF devices can be customized to the size and structure required, with stability and economy.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Strain",
                "Sensitivity",
                "Deformation",
                "Laser stability",
                "Substrates",
                "Graphene"
            ],
            "Author Keywords": [
                "lasermicro-fabrication",
                "stretchable sensor",
                "graphene",
                "serpentine structure"
            ]
        },
        "title": "All Laser Micro-fabrication of Stretchable Sensors"
    },
    {
        "authors": [
            "Junna Zhang",
            "Guoxian Zhang",
            "Xinxin Wang",
            "Xiaoyan Zhao",
            "Peiyan Yuan",
            "Hu Jin"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "30 October 2024",
        "doi": "10.1109/JIOT.2024.3488210",
        "publisher": "IEEE",
        "abstract": "Task offloading can meet users’ demands for the latency and energy consumption by offloading tasks from resource-constrained IoT devices to relatively resource-rich edge servers. Traditional task offloading usually makes use of fixed base stations or servers as edge servers. This would lead to limited range of services and increased costs due to large-scale deployment of edge servers. Therefore, deploying unmanned aerial vehicles (UAVs) as mobile edge servers for task offloading in complex terrains (e.g., forest, desert, etc.) is a worthwhile research problem. To this end, this paper proposes a UAV-assisted task offloading mechanism. The mechanism aims to minimize the weighted sum of latency and energy consumption through jointly optimizing resource allocation, offloading decision, and UAV trajectory. We first transform the non-convex optimization problem into convex optimization subproblems to obtain the optimal resource allocation. Second, we use an improved particle swarm optimization algorithm to find the optimal offloading decision. Finally, we present the deep determination policy gradient algorithm to optimize the UAV trajectory which is a kind of deep reinforcement learning algorithm. Through simulation experiments, we show that the proposed mechanism can efficiently reduce the weighted sum of latency and energy consumption.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Autonomous aerial vehicles",
                "Internet of Things",
                "Resource management",
                "Servers",
                "Energy consumption",
                "Costs",
                "Monitoring",
                "Trajectory optimization",
                "Surveys",
                "Prediction algorithms"
            ],
            "Author Keywords": [
                "Edge computing",
                "task offloading",
                "UAV trajectory",
                "deep determination policy gradient algorithm",
                "resource allocation"
            ]
        },
        "title": "UAV-Assisted Task Offloading in Edge Computing"
    },
    {
        "authors": [
            "Vincenzo Rega",
            "Domenico Capriglione",
            "Fabrizio Marignetti",
            "Mario Molinara",
            "Andrea Amodei"
        ],
        "published_in": "Published in: IEEE Access ( Early Access )",
        "date_of_publication": "05 November 2024",
        "doi": "10.1109/ACCESS.2024.3491916",
        "publisher": "IEEE",
        "abstract": "In the field of cybersecurity, the ability to gather detailed information about target systems is a critical component of the reconnaissance phase of cyber attacks. This phase, known as cybersecurity reconnaissance, involves techniques that adversaries use to collect information vital for the success of subsequent attack stages. Traditionally, reconnaissance activities include network scanning, sniffing, and social engineering, which allow attackers to map the network, identify vulnerabilities, and plan their exploits. In this paper, we explore a novel application of side-channel analysis within the domain of system-based reconnaissance. Side-channel attacks, typically used to extract cryptographic keys or sensitive data through indirect observations such as power consumption or electromagnetic emissions, are here repurposed for a different kind of system intrusion. Specifically, we demonstrate how side-channel analysis and machine learning techniques can classify running processes on a target system very popular in common IoT applications. This method could enable to identify which applications are active without needing direct access to the system’s internal data. By categorizing this approach as a form of local system-based reconnaissance, we highlight its potential to silently gather critical information about a system’s state. Such capabilities represent a significant breach of privacy and provide attackers with the intelligence needed to carry out more targeted and effective attacks. This research also underscores the evolving nature of reconnaissance techniques and the growing risks of advanced side-channel cybersecurity methods.",
        "issn": {
            "Electronic ISSN": "2169-3536"
        },
        "keywords": {
            "IEEE Keywords": [
                "Power demand",
                "Side-channel attacks",
                "Computer security",
                "Reconnaissance",
                "Electromagnetics",
                "Privacy",
                "Feature extraction",
                "Timing",
                "Performance evaluation",
                "Information leakage"
            ],
            "Author Keywords": [
                "cybersecurity",
                "side-channel",
                "machine learning",
                "measurements",
                "vulnerability",
                "application profiling"
            ]
        },
        "title": "Profiling Running Applications in Connected Devices Through Side-Channel and Machine Learning Techniques"
    },
    {
        "authors": [
            "Martin Lefebvre",
            "David Bol"
        ],
        "published_in": "Published in: IEEE Journal of Solid-State Circuits ( Early Access )",
        "date_of_publication": "11 November 2024",
        "doi": "10.1109/JSSC.2024.3484766",
        "publisher": "IEEE",
        "abstract": "Recent advances in artificial intelligence (AI) have prompted the search for enhanced algorithms and hardware to support the deployment of machine learning (ML) at the edge. More specifically, in the context of the Internet of Things (IoT), vision chips must be able to fulfill the tasks of low to medium complexity, such as feature extraction (FE) or region-of-interest (RoI) detection, with a sub-mW power budget imposed by the use of small batteries or energy harvesting. Mixed-signal vision chips relying on in-or near-sensor processing have emerged as an interesting candidate because of their favorable tradeoff between energy efficiency (EE) and computational accuracy compared with digital systems for these specific tasks. In this article, we introduce a mixed-signal convolutional imager system-on-chip (SoC) codenamed MANTIS, featuring a unique combination of large 16 \\times\n16 4b-weighted filters, operation at multiple scales, and double sampling, well suited to the requirements of medium-complexity tasks. The main contributions are (i) circuits called DS3 units combining delta-reset sampling (DRS), image downsampling (DS), and voltage downshifting and (ii) charge-domain multiply-and-accumulate (MAC) operations based on switched-capacitor (SC) amplifiers and charge sharing in the capacitive DAC of the successive-approximation (SAR) ADCs, MANTIS achieves peak EEs normalized to 1b operations of 4.6 and 84.1 TOPS/W at the accelerator and SoC levels, while computing feature maps (fmaps) with a root-mean-square error (RMSE) ranging from 3 to 11.3%. It also demonstrates a face RoI detection with a false negative rate (FNR) of 11.5%, while discarding 81.3% of image patches and reducing the data transmitted off chip by 13 \\times\ncompared with the raw image.",
        "issn": {
            "Print ISSN": "0018-9200",
            "Electronic ISSN": "1558-173X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Convolution",
                "Pipelines",
                "Filters",
                "Voltage",
                "Iron",
                "System-on-chip",
                "Registers",
                "Internet of Things",
                "Feature extraction",
                "Convolutional codes"
            ],
            "Author Keywords": [
                "Charge domain",
                "CMOS image sensor (CIS)",
                "convolutional neural network (CNN)",
                "feature extraction (FE)",
                "mixed signal",
                "multiply-and-accumulate (MAC) operations",
                "near sensor",
                "region-of-interest (RoI) detection",
                "system-on-chip (SoC)"
            ]
        },
        "title": "MANTIS: A Mixed-Signal Near-Sensor Convolutional Imager SoC Using Charge-Domain 4b-Weighted 5-to-84-TOPS/W MAC Operations for Feature Extraction and Region-of-Interest Detection"
    },
    {
        "authors": [
            "Subhrajit Das",
            "Janaka Senarathna",
            "Yunke Ren",
            "Vu Dinh",
            "Mingyao Ying",
            "Ralph Etienne-Cummings",
            "Arvind P. Pathak"
        ],
        "published_in": "Published in: IEEE Transactions on Biomedical Engineering ( Early Access )",
        "date_of_publication": "24 September 2024",
        "doi": "10.1109/TBME.2024.3467221",
        "publisher": "IEEE",
        "abstract": "Recent advances in low-power wireless-capable system-on-chips (SoCs) have accelerated diverse Internet of Things (IoT) applications, encompassing wearables, asset monitoring, and more. Concurrently, the field of neuroimaging has experienced escalating demand for lightweight, untethered, low-power systems capable of imaging in small animals. This article explores the feasibility of using a low-power asset monitoring system as the basis of a new architecture for fluorescence and hemodynamic contrast-based wireless functional imaging. The core system architecture hinges on the fusion of a Bluetooth Low Energy (BLE) 5.2 SoC and a low-power 560×560, 8-bit monochrome CMOS image sensor module. Successful integration of a multicontrast optical front-end consisting of a fluorescence channel (FL) and an intrinsic optical signal (IOS) channel resulted in the creation of a wireless microscope called ‘BLEscope’. Next, we developed a wireless (i.e. BLE) protocol to remotely operate the BLEscope via a laptop and acquire in vivo images at 1 frame per second (fps). We then conducted a comprehensive characterization of the BLEscope to assess its optical capabilities and power consumption. We report a new benchmark for continuous wireless imaging of ∼1.5 hours with a 100 mAh battery. Via the FL channel of the BLEscope, we successfully tracked the kinetics of an intravenously injected fluorescent tracer and acquired images of fluorescent brain tumor cells in vivo. Via the IOS channel, we characterized the differential response of normal and tumor-associated blood vessels to a carbogen gas inhalation challenge. When miniaturized, the BLEscope will result in a new class of low-power, implantable or wireless microscopes that could transform preclinical and clinical neuroimaging applications.",
        "issn": {
            "Print ISSN": "0018-9294",
            "Electronic ISSN": "1558-2531"
        },
        "keywords": {
            "IEEE Keywords": [
                "Imaging",
                "Wireless communication",
                "Wireless sensor networks",
                "Microscopy",
                "Optical sensors",
                "Optical imaging",
                "Image sensors"
            ],
            "Author Keywords": [
                "Bluetooth Low Energy (BLE)",
                "microscopy",
                "multicontrast",
                "neuroimaging",
                "smartphone",
                "low-power",
                "wireless"
            ]
        },
        "title": "BLEscope: A Bluetooth Low Energy (BLE) Microscope for Wireless Multicontrast Functional Imaging"
    },
    {
        "authors": [
            "Song Yang",
            "Keming Qiu",
            "Fei Zhang",
            "Lu Cao",
            "Fan Li",
            "Min Tang",
            "Liehuang Zhu"
        ],
        "published_in": "Published in: IEEE Open Journal of the Communications Society ( Early Access )",
        "date_of_publication": "06 November 2024",
        "doi": "10.1109/OJCOMS.2024.3492914",
        "publisher": "IEEE",
        "abstract": "Edge devices (e.g., smartphones, tablelet PC, IoT devices) are becoming more prevalent in people’s daily lives. With advanced sensors and processors, these devices can create massive data. These data can be used for predictive maintenance, enhancing user experience, increasing productivity, etc. These valuable data allow data producers to sell to consumers directly to generate income. Blockchain and smart-contract technology can be used to ensure transactions to be unmodifiable and undeniable. This paper first proposes a blockchain-based data relay and transaction model for the data producer, relay and consumer in edge computing. We then present a new consensus mechanism Proof-of-Data-Trading (PoDT) by combining Proof-of-Work (PoW) mechanism with Proof-of-Stake (PoS) consensus mechanism, which enables the proposed blockchain system to reach consensus with low energy consumption for edge devices. Moreover, we develop an approximation algorithm to store encrypted copies of data items on relays with smaller costs. Extensive simulations show that our proposed blockchain system works efficiently in edge computing. It achieves up to 8.19% higher profit for the data producer with the help of relays and consumers using 84.6% less time to get the data item. In addition, the new consensus mechanism consumes 87% less time when compared with the traditional PoW consensus mechanism.",
        "issn": {
            "Electronic ISSN": "2644-125X"
        },
        "keywords": {
            "IEEE Keywords": [
                "Blockchains",
                "Relays",
                "Edge computing",
                "Consensus protocol",
                "Smart contracts",
                "Data models",
                "Costs",
                "Computational modeling",
                "Peer-to-peer computing",
                "Internet of Things"
            ],
            "Author Keywords": [
                "Blockchain",
                "edge computing",
                "data trading",
                "data placement"
            ]
        },
        "title": "Efficient Data Trading and Placement in Blockchain-based Edge Computing Systems"
    },
    {
        "authors": [
            "Jernej Hribar",
            "Ryoichi Shinkuma",
            "Kuon Akiyama",
            "George Iosifidis",
            "Ivana Dusparic"
        ],
        "published_in": "Published in: IEEE Sensors Journal ( Early Access )",
        "date_of_publication": "03 October 2024",
        "doi": "10.1109/JSEN.2024.3469539",
        "publisher": "IEEE",
        "abstract": "The development of environmentally friendly, green communications is at the forefront of designing future Internet of Things (IoT) networks, although many opportunities to improve energy conservation from Energy Harvesting (EH) sensors remain unexplored. Ubiquitous computing power, available in the form of cloudlets, enables processing of the collected observations at the network edge. Often, the information that the Artificial Intelligence of Things (AIoT) application obtains by processing observations from one sensor can also be obtained by processing observations from another sensor. Consequently, a sensor can take advantage of the correlation between processed observations to avoid unnecessary transmissions and save energy. For example, when two cameras monitoring the same intersection detect the same vehicles, the system can recognize this overlap and reduce redundant data transmissions. This approach allows the network to conserve energy while still ensuring accurate vehicle detection, thereby maintaining the overall performance of the AIoT task. In this paper, we consider such a system and develop a novel solution named Balancing Energy Efficiency in Sensor Networks with Multi-Agent Reinforcement Learning (BEES-MARL). Our proposed solution is capable of taking advantage of correlations in a system with multiple EH-powered sensors observing the same scene and transmitting their observations to a cloudlet. We evaluate the proposed solution in two data-driven use cases to verify its benefits and in a general setting to demonstrate scalability. Our solution improves task performance, measured by recall, by up to 16% over a heuristic approach, while minimizing latency and preventing outages.",
        "issn": {
            "Print ISSN": "1530-437X",
            "Electronic ISSN": "1558-1748"
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensors",
                "Correlation",
                "Performance evaluation",
                "Internet of Things",
                "Sensor systems",
                "Intelligent sensors",
                "Cameras",
                "Cloud computing",
                "Artificial intelligence",
                "Servers"
            ],
            "Author Keywords": [
                "Artificial Intelligence of Things (AIoT)",
                "Energy-harvesting",
                "Edge Computing",
                "Deep Learning",
                "Green Communications",
                "Multi-agent Reinforcement Learning"
            ]
        },
        "title": "Balancing Energy Preservation and Performance in Energy-harvesting Sensor Networks"
    },
    {
        "authors": [
            "Gang Sun",
            "Yuhui Wang",
            "Hongfang Yu",
            "Mohsen Guizani"
        ],
        "published_in": "Published in: IEEE Transactions on Services Computing ( Early Access )",
        "date_of_publication": "11 October 2024",
        "doi": "10.1109/TSC.2024.3478730",
        "publisher": "IEEE",
        "abstract": "Space-Air-Ground Integrated Networks (SAGIN) is considered as the key structure of the next generation network. The space satellites and air nodes are potential candidates to assist and offload the computing tasks. An Unmanned Aerial Vehicle (UAV) collects computing tasks from IoT devices and then makes online offloading decisions. However, UAVs belonging to different service providers compete for computing resources from ground base stations during task scheduling, resulting in extremely long queue delays and load imbalance. In this paper, we designed a task scheduling algorithm based on Proportional Fairness-Aware Auction with Proximal Policy Optimization (PFAPPO), which decouples the task scheduling process in competitive scenarios into two parts: resource allocation and task offloading decision-making. We first propose an auction algorithm to allocate computing resources reasonably to each UAV, after resource allocation is completed, the UAV learns its available computing resources at each offloading destination. Based on the heterogeneous characteristics of the tasks, the UAV makes intelligent offloading decisions using the distributed deep reinforcement learning PPO algorithm. The simulation results show that our proposed PFAPPO has obvious performance improvement compared with existing methods in terms of system profit, load balancing, and system fairness",
        "issn": {
            "Electronic ISSN": "1939-1374"
        },
        "keywords": {
            "IEEE Keywords": [
                "Resource management",
                "Autonomous aerial vehicles",
                "Optimization",
                "Heuristic algorithms",
                "Space-air-ground integrated networks",
                "Internet of Things",
                "Mathematical models",
                "Dynamic scheduling",
                "Delays",
                "Base stations"
            ],
            "Author Keywords": [
                "Space-Air-Ground Integrated Networks",
                "Task Schedule",
                "Reinforcement Learning",
                "Auction Mechanism",
                "Proportional Fairness"
            ]
        },
        "title": "Proportional Fairness-Aware Task Scheduling in Space-Air-Ground Integrated Networks"
    },
    {
        "authors": [
            "Khalid Mahmood",
            "Zahid Ghaffar",
            "Lata Nautiyal",
            "Muhammad Wahid Akram",
            "Ashok Kumar Das",
            "Mohammed J. F. Alenazi"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "01 October 2024",
        "doi": "10.1109/JIOT.2024.3471861",
        "publisher": "IEEE",
        "abstract": "The Internet of Drones (IoD) offers supervised admittance to drones in a targeted fly zone as the byproduct of the Internet of Things (IoT). The term drone is the trendy alias for Intelligent Flying Vehicle (IFV). The contemporary sensing, processing, and connectivity services enrich the use of drones in many civilian and military applications. In these applications, consumers can acquire real-time information directly from flying drones in a smart city environment. While this feature undeniably empowers consumers, it poses significant security risks due to the direct access privilege. We propose an anonymous protocol for consumer flying vehicles within smart city applications to mitigate these threats. The proposed protocol utilizes a physically unclonable function to sustain the physical security of flying vehicles. We ratify our protocol’s security fortitude and persistence through inclusive security analysis. We demonstrate the performance evaluation under diverse performance metrics, which shows that the proposed protocol achieves 40.69% and 17.91% efficiency as compared to related protocols in terms of computation and communication cost comparison, respectively.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Drones",
                "Protocols",
                "Security",
                "Internet of Things",
                "Servers",
                "Authentication",
                "Smart cities",
                "Costs",
                "Real-time systems",
                "Databases"
            ],
            "Author Keywords": [
                "Privacy-Preserving",
                "Authentication protocol",
                "Key Agreement",
                "Internet of Things",
                "Drones"
            ]
        },
        "title": "A Privacy-Preserving Access Control Protocol for Consumer Flying Vehicles in Smart City Applications"
    },
    {
        "authors": [
            "Caihong Yan",
            "Xiaofeng Lu",
            "Pietro Lio",
            "Pan Hui",
            "Daojing He"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "25 September 2024",
        "doi": "10.1109/JIOT.2024.3462724",
        "publisher": "IEEE",
        "abstract": "A heterogeneous information network (heterogeneous graph) federated learning plays a crucial role in enabling multi-party collaboration in the IoT system. However, due to differences in business and data, the local models of each participant are heterogeneous and unable to achieve federated aggregation. Furthermore, the non-independent and identically distributed (non-IID) coupling topology structure among participants severely impacts the performance of federated learning. Given the lack of appropriate solutions to these issues, this study proposes a novel heterogeneous graph federated learning framework (HGFL+) based on self-simulation and meta-model aggregation, which includes the following two innovative techniques: (1) The missing coupling supplement module simulates new neighbor nodes on its original heterogeneous graph, and constructs associated edges using multiple encoder-decoder structures, thereby achieving the supplement of missing neighbors with better results than external generative methods. (2) The heterogeneous model aggregation algorithm realizes the fusion of multi-party heterogeneous graph information through mapping, splitting, aggregating, and recombining multiple stages based on the meta-model (the largest basic model unit among participants). We theoretically analyzed the applicability and effectiveness of HGFL+, demonstrating the generalization boundary of HGFL+. Meanwhile, multi-dimensional empirical verification of classification performance, convergence effect, time overhead, model size, and application extension (model, task, domain) validates the effectiveness of the proposed method.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Federated learning",
                "Couplings",
                "Data models",
                "Graph neural networks",
                "Analytical models",
                "Semantics",
                "Internet of Things"
            ],
            "Author Keywords": [
                "Federated Learning",
                "Coupled Heterogeneous Graphs",
                "Missing Graph Completion",
                "Heterogeneous Model Aggregation"
            ]
        },
        "title": "Self-simulation and Meta-Model Aggregation Based Heterogeneous Graph Coupled Federated Learning"
    },
    {
        "authors": [
            "Sushil Kumar Singh",
            "Manish Kumar",
            "Ashish Khanna",
            "Bal Virdee"
        ],
        "published_in": "Published in: IEEE Internet of Things Journal ( Early Access )",
        "date_of_publication": "11 October 2024",
        "doi": "10.1109/JIOT.2024.3478820",
        "publisher": "IEEE",
        "abstract": "Smart farming influences advanced technologies to optimize agricultural procedures, yet it meets significant cybersecurity challenges, particularly in External Intrusion Detection (EID). This article proposes a novel architecture combining Blockchain Technology and Federated Learning (FL) to reinforce the security of Smart Farming Systems (SMS) against external threats. The integration of Blockchain ensures data authentication and transparent data storage, while FL enables collaborative model training without compromising data privacy. Our architecture employs Ensemble Learning (EL) for the Local Model at the Ensemble Layer to train each Smart Land’s data and offers privacy-prevented security. These devices utilize FL techniques to collaboratively train intrusion detection models while preserving the confidentiality of sensitive data. The Aggregated Model completes data aggregation at the Authentication Layer, and the PoAh Consensus Algorithm is leveraged for smart land’s data authentication. The IoT Sensor device’s identical information of smart lands is stored at the Macro Base Stations (MBSs). After downloading the aggregated values of the aggregated model, the local model transfers the smart lands information to the Cloud layer for decision-making and decentralized storage. The validation outcomes of the proposed architecture demonstrate excellent performance, with an average processing time of 3.663 secs and 0.9956 accuracy for Smart Land compared to existing frameworks.",
        "issn": {
            "Electronic ISSN": "2327-4662"
        },
        "keywords": {
            "IEEE Keywords": [
                "Smart agriculture",
                "Blockchains",
                "Intrusion detection",
                "Security",
                "Authentication",
                "Farming",
                "Internet of Things",
                "Agriculture",
                "Privacy",
                "Data privacy"
            ],
            "Author Keywords": [
                "Blockchain",
                "Enhance External Intrusion Detection",
                "Smart Farming",
                "Federated Learning",
                "Privacy",
                "and Security"
            ]
        },
        "title": "Blockchain and FL-Based Secure Architecture for Enhanced External Intrusion Detection in Smart Farming"
    },
    {
        "authors": [
            "Rongkai Liu",
            "Yuting Wu",
            "Kongyange Zhao",
            "Zhi Zhou",
            "Xiang Gao",
            "Xianchen Lin",
            "Xiaoxi Zhang",
            "Xu Chen",
            "Gang Lu"
        ],
        "published_in": "Published in: IEEE Transactions on Emerging Topics in Computing ( Early Access )",
        "date_of_publication": "30 May 2024",
        "doi": "10.1109/TETC.2024.3403874",
        "publisher": "IEEE",
        "abstract": "Driven by the accelerated convergence of artificial intelligence (AI) and the Internet of Things (IoT), the recent years have witnessed the booming of Artificial Intelligence of Things (AIoT). Edge clouds place computing and service capabilities at the network edges to reduce network transmission overhead, which has been widely recognized as the critical infrastructure for AIoT applications. Meanwhile, to accelerate computationintensive edge cloud AI operations, specialized AI accelerators such as GPU, NPU, and TPU have been increasingly integrated into edge clouds. For such emerging XPU edge clouds, utilizing costly XPUs more efficiently has become a significant challenge. In this paper, we present an online optimization framework for joint resource provisioning and batch scheduling for more costefficient AIoT inference serving in an XPU edge cloud. The essential optimization process for the online framework is to first adaptively batch inference tasks to increase the system throughput without compromising the service level agreement (SLA). Next, heterogeneous XPU resources are provisioned for the batches. Finally, the resource instance is consolidated to a minimum of physical servers. Via extensive trace-driven simulations, we verify the performance of the presented online optimization framework.",
        "issn": {
            "Electronic ISSN": "2168-6750"
        },
        "keywords": {
            "IEEE Keywords": [
                "Task analysis",
                "Cloud computing",
                "Servers",
                "Costs",
                "Optimization",
                "Internet of Things",
                "Delays"
            ],
            "Author Keywords": [
                "Inference Serving",
                "XPU",
                "Edge Cloud",
                "Batch Scheduling",
                "Resource Provisioning",
                "Online Optimization"
            ]
        },
        "title": "Online Resource Provisioning and Batch Scheduling for AIoT Inference Serving in an XPU Edge Cloud"
    }
]